{"doc_id": "53719258", "sentence": "Recently , three dimensional ( 3D ) convolutional neural networks ( CNNs ) have emerged as dominant methods to capture spatiotemporal representations in videos , by adding to pre - existing 2D CNNs a third , temporal dimension .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["2D CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["2D CNNs", "Part-Of", "convolutional neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["2D CNNs:Method", "Part-Of", "convolutional neural networks:Method"]]}
{"doc_id": "53719258", "sentence": "Such 3D CNNs , however , are anti - causal ( i.e. , they exploit information from both the past and the future frames to produce feature representations , thus preventing their use in online settings ) , constrain the temporal reasoning horizon to the size of the temporal convolution kernel , and are not temporal resolution - preserving for video sequence - to - sequence modelling , as , for instance , in action detection .", "ner": [["3D CNNs", "Method"], ["temporal convolution kernel", "Method"], ["video sequence - to - sequence modelling", "Task"], ["action detection", "Task"]], "rel": [["temporal convolution kernel", "Part-Of", "3D CNNs"]], "rel_plus": [["temporal convolution kernel:Method", "Part-Of", "3D CNNs:Method"]]}
{"doc_id": "53719258", "sentence": "Namely , we propose a novel Recurrent Convolutional Network ( RCN ) , which relies on recurrence to capture the temporal context across frames at each network level .", "ner": [["Recurrent Convolutional Network", "Method"], ["RCN", "Method"]], "rel": [["RCN", "Synonym-Of", "Recurrent Convolutional Network"]], "rel_plus": [["RCN:Method", "Synonym-Of", "Recurrent Convolutional Network:Method"]]}
{"doc_id": "53719258", "sentence": "Our experiments on the large - scale large Kinetics and MultiThumos datasets show that the proposed method performs comparably to anti - causal 3D CNNs , while being causal and using fewer parameters .", "ner": [["Kinetics", "Dataset"], ["MultiThumos", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Evaluated-With", "Kinetics"], ["3D CNNs", "Evaluated-With", "MultiThumos"]], "rel_plus": [["3D CNNs:Method", "Evaluated-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Evaluated-With", "MultiThumos:Dataset"]]}
{"doc_id": "53719258", "sentence": "Convolutional neural networks ( CNN ) are starting to exhibit gains in action recognition from videos similar to those previously observed in image recognition [ 2 1 , 3 6 ] thanks to new 3D CNNs [ 3 , 5 2 , 4 5 , 1 1 , 5 1 ] .", "ner": [["Convolutional neural networks", "Method"], ["CNN", "Method"], ["action recognition", "Task"], ["image recognition", "Task"], ["3D CNNs", "Method"]], "rel": [["CNN", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "action recognition"], ["Convolutional neural networks", "Used-For", "image recognition"]], "rel_plus": [["CNN:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "action recognition:Task"], ["Convolutional neural networks:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "53719258", "sentence": "For instance , Hare et al. [ 1 1 ] have shown that that is the case for the 3D version of 2D residual networks ( ResNets ) [ 1 2 ] .", "ner": [["2D residual networks", "Method"], ["ResNets", "Method"]], "rel": [["ResNets", "Synonym-Of", "2D residual networks"]], "rel_plus": [["ResNets:Method", "Synonym-Of", "2D residual networks:Method"]]}
{"doc_id": "53719258", "sentence": "Other recent works [ 5 2 , 4 5 ] show that 3D convolutions can be decomposed into 2D ( spatial ) and 1D ( temporal ) convolutions ( yielding the S 3 D architecture ) , and that these separate convolution operators not only have fewer parameters to train [ 5 2 ] , but also perform better than 3D ( spatiotemporal ) convolutions .", "ner": [["3D convolutions", "Method"], ["1D ( temporal ) convolutions", "Method"], ["S 3 D", "Method"], ["3D ( spatiotemporal ) convolutions", "Method"]], "rel": [["1D ( temporal ) convolutions", "Part-Of", "3D convolutions"], ["3D convolutions", "Part-Of", "S 3 D"], ["1D ( temporal ) convolutions", "Part-Of", "S 3 D"], ["3D convolutions", "Compare-With", "3D ( spatiotemporal ) convolutions"]], "rel_plus": [["1D ( temporal ) convolutions:Method", "Part-Of", "3D convolutions:Method"], ["3D convolutions:Method", "Part-Of", "S 3 D:Method"], ["1D ( temporal ) convolutions:Method", "Part-Of", "S 3 D:Method"], ["3D convolutions:Method", "Compare-With", "3D ( spatiotemporal ) convolutions:Method"]]}
{"doc_id": "53719258", "sentence": "Causal inference is essential for many problems in video understanding , e.g. , online action detection [ 3 8 , 4 0 ] , future action label prediction [ 2 0 ] , and future representation prediction [ 4 7 ] .", "ner": [["Causal inference", "Method"], ["video understanding", "Task"], ["online action detection", "Task"], ["future action label prediction", "Task"], ["future representation prediction", "Task"]], "rel": [["Causal inference", "Used-For", "video understanding"], ["online action detection", "SubTask-Of", "video understanding"], ["future action label prediction", "SubTask-Of", "video understanding"], ["future representation prediction", "SubTask-Of", "video understanding"], ["Causal inference", "Used-For", "online action detection"], ["Causal inference", "Used-For", "future action label prediction"], ["Causal inference", "Used-For", "future representation prediction"]], "rel_plus": [["Causal inference:Method", "Used-For", "video understanding:Task"], ["online action detection:Task", "SubTask-Of", "video understanding:Task"], ["future action label prediction:Task", "SubTask-Of", "video understanding:Task"], ["future representation prediction:Task", "SubTask-Of", "video understanding:Task"], ["Causal inference:Method", "Used-For", "online action detection:Task"], ["Causal inference:Method", "Used-For", "future action label prediction:Task"], ["Causal inference:Method", "Used-For", "future representation prediction:Task"]]}
{"doc_id": "53719258", "sentence": "Preserving temporal resolution , in opposition , is essential in problems such where we needs predictions to be made on each frame of input clip while reasoning about temporal context , e.g. bounding box regression on each frame for action tube detection [ 9 , 3 8 ] or temporal label prediction on each frame for temporal action segmentation [ 3 4 , 3 0 ] or online video segmentation [ 5 4 ] .", "ner": [["bounding box regression", "Task"], ["action tube detection", "Task"], ["temporal label prediction", "Task"], ["temporal action segmentation", "Task"], ["online video segmentation", "Task"]], "rel": [["bounding box regression", "Used-For", "action tube detection"], ["temporal label prediction", "Used-For", "temporal action segmentation"], ["temporal label prediction", "Used-For", "online video segmentation"]], "rel_plus": [["bounding box regression:Task", "Used-For", "action tube detection:Task"], ["temporal label prediction:Task", "Used-For", "temporal action segmentation:Task"], ["temporal label prediction:Task", "Used-For", "online video segmentation:Task"]]}
{"doc_id": "53719258", "sentence": "Hidden state models , such as Markov ones [ 1 ] , recurrent neural networks ( RNN ) [ 1 5 , 8 ] , and long short - term memory ( LSTM ) networks [ 1 3 ] can all be used to model temporal dynamics in videos [ 7 , 2 8 ] , allowing flexible temporal reasoning .", "ner": [["recurrent neural networks", "Method"], ["RNN", "Method"], ["long short - term memory", "Method"], ["LSTM", "Method"]], "rel": [["RNN", "Synonym-Of", "recurrent neural networks"], ["LSTM", "Synonym-Of", "long short - term memory"]], "rel_plus": [["RNN:Method", "Synonym-Of", "recurrent neural networks:Method"], ["LSTM:Method", "Synonym-Of", "long short - term memory:Method"]]}
{"doc_id": "53719258", "sentence": "In an approach which aims to combine the representation power of explicit dynamical models with the discriminative power of 3D networks , in this work we propose a recurrent alternative to 3D convolution illustrated in Figure 1 ( c ) . [ 3 ] or C 3 D [ 4 4 ] . ( b ) 3D convolution decomposed into a 2D spatial convolution followed by a 1D temporal one , as in S 3 D [ 5 2 ] .", "ner": [["3D convolution", "Method"], ["C 3 D", "Method"], ["3D convolution", "Method"], ["2D spatial convolution", "Method"], ["1D temporal", "Method"], ["S 3 D", "Method"]], "rel": [["C 3 D", "Synonym-Of", "3D convolution"], ["2D spatial convolution", "Part-Of", "3D convolution"], ["1D temporal", "Part-Of", "3D convolution"], ["1D temporal", "Part-Of", "S 3 D"], ["3D convolution", "Part-Of", "S 3 D"], ["2D spatial convolution", "Part-Of", "S 3 D"]], "rel_plus": [["C 3 D:Method", "Synonym-Of", "3D convolution:Method"], ["2D spatial convolution:Method", "Part-Of", "3D convolution:Method"], ["1D temporal:Method", "Part-Of", "3D convolution:Method"], ["1D temporal:Method", "Part-Of", "S 3 D:Method"], ["3D convolution:Method", "Part-Of", "S 3 D:Method"], ["2D spatial convolution:Method", "Part-Of", "S 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Our proposed method , in opposition , solves both problems via a recurrent convolutional network ( RCN ) which explictly performs temporal reasoning at each level of the network thanks to recurrence , while maintaining temporal resolution and being causal , without decline in performance .", "ner": [["recurrent convolutional network", "Method"], ["RCN", "Method"]], "rel": [["RCN", "Synonym-Of", "recurrent convolutional network"]], "rel_plus": [["RCN:Method", "Synonym-Of", "recurrent convolutional network:Method"]]}
{"doc_id": "53719258", "sentence": "Famously , when Tran et al. [ 4 4 ] first proposed 3D CNNs for video action recognition their observed performance turned out to be merely comparable to that of 2D CNNs [ 3 5 ] , e.g. , on the Sports 1 M dataset [ 1 7 ] .", "ner": [["3D CNNs", "Method"], ["video action recognition", "Task"], ["2D CNNs", "Method"], ["Sports 1 M", "Dataset"]], "rel": [["3D CNNs", "Used-For", "video action recognition"], ["2D CNNs", "Used-For", "video action recognition"], ["Sports 1 M", "Benchmark-For", "video action recognition"], ["3D CNNs", "Compare-With", "2D CNNs"], ["3D CNNs", "Evaluated-With", "Sports 1 M"], ["2D CNNs", "Evaluated-With", "Sports 1 M"]], "rel_plus": [["3D CNNs:Method", "Used-For", "video action recognition:Task"], ["2D CNNs:Method", "Used-For", "video action recognition:Task"], ["Sports 1 M:Dataset", "Benchmark-For", "video action recognition:Task"], ["3D CNNs:Method", "Compare-With", "2D CNNs:Method"], ["3D CNNs:Method", "Evaluated-With", "Sports 1 M:Dataset"], ["2D CNNs:Method", "Evaluated-With", "Sports 1 M:Dataset"]]}
{"doc_id": "53719258", "sentence": "For these reasons , Carreira et al. [ 3 ] later proposed to use transfer learning to boost 3D CNN performance .", "ner": [["transfer learning", "Task"], ["3D CNN", "Method"]], "rel": [["transfer learning", "Used-For", "3D CNN"]], "rel_plus": [["transfer learning:Task", "Used-For", "3D CNN:Method"]]}
{"doc_id": "53719258", "sentence": "There , 2D CNNs are inflated into 3D ones by replacing 2D convolutions with 3D convolution : as a result , 2D network weights as pretrained on ImageNet [ 6 ] can be used to initialise their 3D CNNs .", "ner": [["2D CNNs", "Method"], ["2D convolutions", "Method"], ["3D convolution", "Method"], ["ImageNet", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D convolution", "Part-Of", "2D CNNs"], ["3D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["3D convolution:Method", "Part-Of", "2D CNNs:Method"], ["3D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "This makes the use of 3D CNNs more widely accessible , for training a full 3D CNN is a computationally expensive task : 6 4 GPUs were used to train the latest state - of - the - art 3D CNNs [ 3 , 4 5 , 2 ] , which is a big ask for smaller research groups .", "ner": [["3D CNNs", "Method"], ["3D CNN", "Method"], ["3D CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Unlike Tran et al. [ 4 5 ] , where the number of filters changes , our recurrent convolutional network exhibits similar performance improvement gains when it comes to ImageNet initialisation as those of inflated 3D CNNs ( I 3 D ) [ 3 ] .", "ner": [["recurrent convolutional network", "Method"], ["ImageNet", "Dataset"], ["inflated 3D CNNs", "Method"], ["I 3 D", "Method"]], "rel": [["inflated 3D CNNs", "Trained-With", "ImageNet"], ["recurrent convolutional network", "Trained-With", "ImageNet"], ["I 3 D", "Synonym-Of", "inflated 3D CNNs"]], "rel_plus": [["inflated 3D CNNs:Method", "Trained-With", "ImageNet:Dataset"], ["recurrent convolutional network:Method", "Trained-With", "ImageNet:Dataset"], ["I 3 D:Method", "Synonym-Of", "inflated 3D CNNs:Method"]]}
{"doc_id": "53719258", "sentence": "Interestingly , Le et al. [ 2 2 ] show that simple RNNs can exhibit long - term memory properties if appropriately initialised , even better than LSTMs .", "ner": [["RNNs", "Method"], ["LSTMs", "Method"]], "rel": [["RNNs", "Compare-With", "LSTMs"]], "rel_plus": [["RNNs:Method", "Compare-With", "LSTMs:Method"]]}
{"doc_id": "53719258", "sentence": "They use ReLU [ 1 0 ] activation functions because of their fast convergence and sparsity properties [ 1 0 ] , as opposed to what happens with traditional RNNs , and in line with standard practice in CNNs .", "ner": [["ReLU", "Method"], ["RNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Contributions : In summary , we present a new approach to video feature extraction based on an original convolutional network with recurrent hidden states at each depth level , which : \u2022 allows flexible temporal reasoning , exploiting information coming from all the input sequence observed up to time t ; \u2022 generates output representations in a causal way , allowing online video processing and enabling the use of 3D networks in scenarios in which causality is key ; \u2022 preserves temporal resolution to produce predictions for each frame , e.g. segmentation [ 3 0 , 3 4 , 5 4 ] . \u2022 is designed to directly benefit from model initialisation via ImageNet pre - trained weights , as opposed to state of the art approaches , and in line with clear emerging trends in the field .", "ner": [["video feature extraction", "Task"], ["segmentation", "Task"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "In our experiments we show that our proposed RCN outperforms baseline I 3 D models , while displying all the above desirable properties .", "ner": [["RCN", "Method"], ["I 3 D", "Method"]], "rel": [["RCN", "Compare-With", "I 3 D"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Since the two - stream 2D CNNs proposed by Simonyan et al. [ 3 5 ] produced performances comparable to that of traditional features such as IDT [ 4 8 , 4 9 ] , HOG [ 4 ] , HOG 3 D [ 1 9 ] , HOF [ 5 ] , 2D features has been extensively used in action recognition and detection .", "ner": [["2D CNNs", "Method"], ["IDT", "Method"], ["HOG", "Method"], ["HOG 3 D", "Method"], ["HOF", "Method"], ["2D features", "Method"], ["action recognition", "Task"], ["detection", "Task"]], "rel": [["2D CNNs", "Compare-With", "IDT"], ["2D CNNs", "Compare-With", "HOG"], ["2D CNNs", "Compare-With", "HOG 3 D"], ["2D CNNs", "Compare-With", "HOF"], ["2D CNNs", "Compare-With", "2D features"], ["2D CNNs", "Used-For", "action recognition"], ["IDT", "Used-For", "action recognition"], ["HOG", "Used-For", "action recognition"], ["HOG 3 D", "Used-For", "action recognition"], ["HOF", "Used-For", "action recognition"], ["2D features", "Used-For", "action recognition"], ["2D CNNs", "Used-For", "detection"], ["IDT", "Used-For", "detection"], ["HOG", "Used-For", "detection"], ["HOG 3 D", "Used-For", "detection"], ["HOF", "Used-For", "detection"], ["2D features", "Used-For", "detection"]], "rel_plus": [["2D CNNs:Method", "Compare-With", "IDT:Method"], ["2D CNNs:Method", "Compare-With", "HOG:Method"], ["2D CNNs:Method", "Compare-With", "HOG 3 D:Method"], ["2D CNNs:Method", "Compare-With", "HOF:Method"], ["2D CNNs:Method", "Compare-With", "2D features:Method"], ["2D CNNs:Method", "Used-For", "action recognition:Task"], ["IDT:Method", "Used-For", "action recognition:Task"], ["HOG:Method", "Used-For", "action recognition:Task"], ["HOG 3 D:Method", "Used-For", "action recognition:Task"], ["HOF:Method", "Used-For", "action recognition:Task"], ["2D features:Method", "Used-For", "action recognition:Task"], ["2D CNNs:Method", "Used-For", "detection:Task"], ["IDT:Method", "Used-For", "detection:Task"], ["HOG:Method", "Used-For", "detection:Task"], ["HOG 3 D:Method", "Used-For", "detection:Task"], ["HOF:Method", "Used-For", "detection:Task"], ["2D features:Method", "Used-For", "detection:Task"]]}
{"doc_id": "53719258", "sentence": "For instance , Donahue used LSTMs [ 7 , 5 5 ] on top of 2D CNN features .", "ner": [["LSTMs", "Method"], ["2D CNN", "Method"]], "rel": [["2D CNN", "Part-Of", "LSTMs"]], "rel_plus": [["2D CNN:Method", "Part-Of", "LSTMs:Method"]]}
{"doc_id": "53719258", "sentence": "Other approaches include , among others , CNN features in combination with LSTMs [ 2 4 ] for temporal action detection , 2D features used in an encoderdecoder setup along with temporal convolutions [ 3 0 ] , and conditional random field on series of 2D features [ 3 3 ] for temporal action detection and recognition .", "ner": [["CNN features", "Method"], ["LSTMs", "Method"], ["temporal action detection", "Task"], ["2D features", "Method"], ["encoderdecoder", "Method"], ["temporal convolutions", "Method"], ["conditional random field", "Method"], ["2D features", "Method"], ["temporal action detection", "Task"], ["recognition", "Task"]], "rel": [["CNN features", "Part-Of", "LSTMs"], ["LSTMs", "Used-For", "temporal action detection"], ["2D features", "Part-Of", "encoderdecoder"], ["temporal convolutions", "Part-Of", "encoderdecoder"], ["2D features", "Part-Of", "conditional random field"], ["conditional random field", "Used-For", "temporal action detection"], ["encoderdecoder", "Used-For", "temporal action detection"], ["conditional random field", "Used-For", "temporal action detection"], ["conditional random field", "Used-For", "recognition"], ["encoderdecoder", "Used-For", "recognition"], ["conditional random field", "Used-For", "recognition"]], "rel_plus": [["CNN features:Method", "Part-Of", "LSTMs:Method"], ["LSTMs:Method", "Used-For", "temporal action detection:Task"], ["2D features:Method", "Part-Of", "encoderdecoder:Method"], ["temporal convolutions:Method", "Part-Of", "encoderdecoder:Method"], ["2D features:Method", "Part-Of", "conditional random field:Method"], ["conditional random field:Method", "Used-For", "temporal action detection:Task"], ["encoderdecoder:Method", "Used-For", "temporal action detection:Task"], ["conditional random field:Method", "Used-For", "temporal action detection:Task"], ["conditional random field:Method", "Used-For", "recognition:Task"], ["encoderdecoder:Method", "Used-For", "recognition:Task"], ["conditional random field:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "53719258", "sentence": "Later , the 3D CNNs proposed by Ji et al. [ 1 4 ] and Tran et al. [ 1 4 , 4 4 ] ( C 3 D architecture ) promised to be able to perform spatial and temporal reasoning at the same time .", "ner": [["3D CNNs", "Method"], ["C 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Carreira et al. [ 3 ] thus proposed to address both problems by inflating 2D CNNs into 3D CNNs .", "ner": [["2D CNNs", "Method"], ["3D CNNs", "Method"]], "rel": [["2D CNNs", "Part-Of", "3D CNNs"]], "rel_plus": [["2D CNNs:Method", "Part-Of", "3D CNNs:Method"]]}
{"doc_id": "53719258", "sentence": "They used the weights of 2D CNNs pre - trained on ImageNet [ 6 ] to initialise 3D networks , and trained the latter on the large scale Kinetics dataset [ 1 8 ] .", "ner": [["2D CNNs", "Method"], ["ImageNet", "Dataset"], ["3D networks", "Method"], ["Kinetics", "Dataset"]], "rel": [["2D CNNs", "Trained-With", "ImageNet"], ["2D CNNs", "Part-Of", "3D networks"], ["3D networks", "Trained-With", "Kinetics"]], "rel_plus": [["2D CNNs:Method", "Trained-With", "ImageNet:Dataset"], ["2D CNNs:Method", "Part-Of", "3D networks:Method"], ["3D networks:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "53719258", "sentence": "This inspired [ 5 2 , 2 9 , 4 5 ] to decompose 3D convolutions into 2D ( spatial ) and 1D ( temporal ) convolutions .", "ner": [["3D convolutions", "Method"], ["1D ( temporal ) convolutions", "Method"]], "rel": [["1D ( temporal ) convolutions", "Part-Of", "3D convolutions"]], "rel_plus": [["1D ( temporal ) convolutions:Method", "Part-Of", "3D convolutions:Method"]]}
{"doc_id": "53719258", "sentence": "Also , temporal resolution is not preserved in temporal convolutions with strides : to address this problem , Shou et al. [ 3 2 ] uses temporal deconvolution layers on top of C 3 D network [ 4 4 ] to produce oneto - one mapping from input frames to coorsponding framelevel label prediction for temporal action detection .", "ner": [["temporal convolutions", "Method"], ["temporal deconvolution", "Method"], ["C 3 D", "Method"], ["temporal action detection", "Task"]], "rel": [["temporal deconvolution", "Part-Of", "C 3 D"], ["C 3 D", "Used-For", "temporal action detection"]], "rel_plus": [["temporal deconvolution:Method", "Part-Of", "C 3 D:Method"], ["C 3 D:Method", "Used-For", "temporal action detection:Task"]]}
{"doc_id": "53719258", "sentence": "We propose to solve all the above described problems with 3D CNNs ( causality , long - term dependencies , temporal resolution ) thanks to our proposed Recurrent Convolutional Network ( RCN ) .", "ner": [["3D CNNs", "Method"], ["causality", "Task"], ["long - term dependencies", "Task"], ["temporal resolution", "Task"], ["Recurrent Convolutional Network", "Method"], ["RCN", "Method"]], "rel": [["3D CNNs", "Used-For", "causality"], ["3D CNNs", "Used-For", "long - term dependencies"], ["3D CNNs", "Used-For", "temporal resolution"], ["RCN", "Synonym-Of", "Recurrent Convolutional Network"]], "rel_plus": [["3D CNNs:Method", "Used-For", "causality:Task"], ["3D CNNs:Method", "Used-For", "long - term dependencies:Task"], ["3D CNNs:Method", "Used-For", "temporal resolution:Task"], ["RCN:Method", "Synonym-Of", "Recurrent Convolutional Network:Method"]]}
{"doc_id": "53719258", "sentence": "In addition , our RCN uses fewer parameters compared to any existing 3D CNN architecture but still perform better than I 3 D networks .", "ner": [["RCN", "Method"], ["3D CNN", "Method"], ["I 3 D", "Method"]], "rel": [["RCN", "Compare-With", "3D CNN"], ["RCN", "Compare-With", "I 3 D"]], "rel_plus": [["RCN:Method", "Compare-With", "3D CNN:Method"], ["RCN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Recurrent convolutions have indeed been tried for image generation [ 1 6 , 2 6 ] , scene labeling [ 2 7 ] , and scene text recognition [ 3 1 ] and video reperesnetations [ 5 3 ] .", "ner": [["Recurrent convolutions", "Method"], ["image generation", "Task"], ["scene labeling", "Task"], ["scene text recognition", "Task"], ["video reperesnetations", "Task"]], "rel": [["Recurrent convolutions", "Used-For", "image generation"], ["Recurrent convolutions", "Used-For", "scene labeling"], ["Recurrent convolutions", "Used-For", "scene text recognition"], ["Recurrent convolutions", "Used-For", "video reperesnetations"]], "rel_plus": [["Recurrent convolutions:Method", "Used-For", "image generation:Task"], ["Recurrent convolutions:Method", "Used-For", "scene labeling:Task"], ["Recurrent convolutions:Method", "Used-For", "scene text recognition:Task"], ["Recurrent convolutions:Method", "Used-For", "video reperesnetations:Task"]]}
{"doc_id": "53719258", "sentence": "In particular , the convolutional LSTM ( C - LSTM ) proposed in [ 5 3 ] for precipitation forecasting is closely related to our work .", "ner": [["convolutional LSTM", "Method"], ["C - LSTM", "Method"]], "rel": [["C - LSTM", "Synonym-Of", "convolutional LSTM"]], "rel_plus": [["C - LSTM:Method", "Synonym-Of", "convolutional LSTM:Method"]]}
{"doc_id": "53719258", "sentence": "The authors proposed to use a network made of convolutional LSTMs , whereas we use 2D convolutions for spatial reasoning and an additional convolution , applied in a recurrent fashion , for temporal reasoning .", "ner": [["convolutional LSTMs", "Method"], ["2D convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "C - LSTM has been applied to videos [ 2 3 ] to capture spatial attention over time on top of 2D feature maps .", "ner": [["C - LSTM", "Method"], ["spatial attention", "Method"]], "rel": [["C - LSTM", "Used-For", "spatial attention"]], "rel_plus": [["C - LSTM:Method", "Used-For", "spatial attention:Method"]]}
{"doc_id": "53719258", "sentence": "Our Recurrent Convolutional Network exploits both these benefits , along with the 3D CNN philosophy and wisdom .", "ner": [["Recurrent Convolutional Network", "Method"], ["3D CNN", "Method"]], "rel": [["3D CNN", "Part-Of", "Recurrent Convolutional Network"]], "rel_plus": [["3D CNN:Method", "Part-Of", "Recurrent Convolutional Network:Method"]]}
{"doc_id": "53719258", "sentence": "There are two main reason why 3D CNNs [ 3 , 4 5 , 5 2 , 5 1 ] evolved from 2D CNNs [ 3 6 , 5 0 ] perform better than 3D CNNs built from scratch [ 4 4 , 1 4 ] .", "ner": [["3D CNNs", "Method"], ["2D CNNs", "Method"], ["3D CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Firstly , 2D CNNs are well tried and tested on the problem of image recognition and a video is , after all , a sequence of images -hence , transferability makes sense .", "ner": [["2D CNNs", "Method"], ["image recognition", "Task"]], "rel": [["2D CNNs", "Used-For", "image recognition"]], "rel_plus": [["2D CNNs:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "53719258", "sentence": "In this section we recall the two basic types of 3D CNNs that can be built using a 2D CNN architecture .", "ner": [["3D CNNs", "Method"], ["2D CNN", "Method"]], "rel": [["2D CNN", "Part-Of", "3D CNNs"]], "rel_plus": [["2D CNN:Method", "Part-Of", "3D CNNs:Method"]]}
{"doc_id": "53719258", "sentence": "Usually , the kernel 's temporal dimension n is set to be equal to the spatial dimension d , as in the inflated 3D network ( I 3 D ) [ 3 ] or the convolutional 3D network ( C 3 D ) [ 4 4 ] .", "ner": [["inflated 3D network", "Method"], ["I 3 D", "Method"], ["convolutional 3D network", "Method"], ["C 3 D", "Method"]], "rel": [["I 3 D", "Synonym-Of", "inflated 3D network"], ["C 3 D", "Synonym-Of", "convolutional 3D network"]], "rel_plus": [["I 3 D:Method", "Synonym-Of", "inflated 3D network:Method"], ["C 3 D:Method", "Synonym-Of", "convolutional 3D network:Method"]]}
{"doc_id": "53719258", "sentence": "Here , we inflate the 1 8 layer ResNet [ 1 2 ] network into an I 3 D one as shown in Table 1 , where each 2D convolution is inflated into a 3D convolution .", "ner": [["ResNet", "Method"], ["I 3 D", "Method"], ["2D convolution", "Method"], ["3D convolution", "Method"]], "rel": [["ResNet", "Part-Of", "I 3 D"], ["2D convolution", "Part-Of", "3D convolution"]], "rel_plus": [["ResNet:Method", "Part-Of", "I 3 D:Method"], ["2D convolution:Method", "Part-Of", "3D convolution:Method"]]}
{"doc_id": "53719258", "sentence": "Similarly to the I 3 D network in [ 3 ] , a convolutional layer is used for classification , instead of the fully connected layer used in [ 4 5 , 5 1 ] .", "ner": [["I 3 D", "Method"], ["convolutional layer", "Method"], ["classification", "Task"], ["fully connected layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "I 3 D"], ["convolutional layer", "Used-For", "classification"], ["I 3 D", "Used-For", "classification"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "I 3 D:Method"], ["convolutional layer:Method", "Used-For", "classification:Task"], ["I 3 D:Method", "Used-For", "classification:Task"]]}
{"doc_id": "53719258", "sentence": "Usually , the size of the temporal kernel t is set to be equal to its spatial dimension d , as in both I 3 D [ 3 ] and C 3 D [ 4 4 ] .", "ner": [["I 3 D", "Method"], ["C 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Such a separated convolutional network ( S 3 D ) was introduced by Xi et al. [ 5 2 ] .", "ner": [["separated convolutional network", "Method"], ["S 3 D", "Method"]], "rel": [["S 3 D", "Synonym-Of", "separated convolutional network"]], "rel_plus": [["S 3 D:Method", "Synonym-Of", "separated convolutional network:Method"]]}
{"doc_id": "53719258", "sentence": "After taking a closer look at 3D convolution separation , Tran et al. [ 4 5 ] argued that if the number of kernels M i used in spatial convolution ( Figure   1(b ) ) are increased in such way that the numbers of parameters of spatial and temporal convolution combined are equal to the number of parameters in 3D convolution , then performance actually improves over 3D networks .", "ner": [["3D convolution", "Method"], ["spatial convolution", "Method"], ["temporal convolution", "Method"], ["3D convolution", "Method"], ["3D networks", "Method"]], "rel": [["spatial convolution", "Part-Of", "3D convolution"], ["temporal convolution", "Part-Of", "3D convolution"]], "rel_plus": [["spatial convolution:Method", "Part-Of", "3D convolution:Method"], ["temporal convolution:Method", "Part-Of", "3D convolution:Method"]]}
{"doc_id": "53719258", "sentence": "Although the latter can be considered a special case of Pseudo - 3 D networks ( P 3 D ) [ 2 9 ] models , because of its homogeneity and simplicity the ( 2 + 1 )D model performs better than P 3 D .", "ner": [["Pseudo - 3 D networks", "Method"], ["P 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["P 3 D", "Method"]], "rel": [["P 3 D", "Synonym-Of", "Pseudo - 3 D networks"], ["( 2 + 1 )D", "Compare-With", "P 3 D"]], "rel_plus": [["P 3 D:Method", "Synonym-Of", "Pseudo - 3 D networks:Method"], ["( 2 + 1 )D:Method", "Compare-With", "P 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "We re - implemented ( 2 + 1 )D without ImageNet initialisation as an additional baseline as it has the most promising result without any additional trick like gating in S 3 D .", "ner": [["( 2 + 1 )D", "Method"], ["ImageNet", "Dataset"], ["S 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "In this section , we describe the architecture of our Recurrent Convolutional ( 3D ) Network ( RCN ) and its properties in detail .", "ner": [["Recurrent Convolutional ( 3D ) Network", "Method"], ["RCN", "Method"]], "rel": [["RCN", "Synonym-Of", "Recurrent Convolutional ( 3D ) Network"]], "rel_plus": [["RCN:Method", "Synonym-Of", "Recurrent Convolutional ( 3D ) Network:Method"]]}
{"doc_id": "53719258", "sentence": "Firstly , we show how Recurrent Convolutional Units ( RCU ) A pictorial diagram of our proposed recurrent convolutional unit ( RCU ) in Figure 1(c ) .", "ner": [["Recurrent Convolutional Units", "Method"], ["RCU", "Method"], ["recurrent convolutional unit", "Method"], ["RCU", "Method"]], "rel": [["RCU", "Synonym-Of", "Recurrent Convolutional Units"], ["RCU", "Synonym-Of", "recurrent convolutional unit"]], "rel_plus": [["RCU:Method", "Synonym-Of", "Recurrent Convolutional Units:Method"], ["RCU:Method", "Synonym-Of", "recurrent convolutional unit:Method"]]}
{"doc_id": "53719258", "sentence": "Analytically , a recurrent convolutional unit can be described by the following relation : where w hh and w xh are parameters of the RCU , and * represents the convolution operator .", "ner": [["recurrent convolutional unit", "Method"], ["RCU", "Method"], ["convolution operator", "Method"]], "rel": [["convolution operator", "Part-Of", "RCU"]], "rel_plus": [["convolution operator:Method", "Part-Of", "RCU:Method"]]}
{"doc_id": "53719258", "sentence": "Figure 2 represents a simple recurrent convolutional network ( RCN ) composed by a single RCU unit , unrolled up to time t. At each time step t , an input x t is processed by the RCU and the other layers to produce an output y t .", "ner": [["recurrent convolutional network", "Method"], ["RCN", "Method"], ["RCU unit", "Method"], ["RCU", "Method"]], "rel": [["RCN", "Synonym-Of", "recurrent convolutional network"], ["RCU unit", "Part-Of", "recurrent convolutional network"]], "rel_plus": [["RCN:Method", "Synonym-Of", "recurrent convolutional network:Method"], ["RCU unit:Method", "Part-Of", "recurrent convolutional network:Method"]]}
{"doc_id": "53719258", "sentence": "The unrolling principle allows us to build an RCN from 2D/ 3 D networks , e.g. by replacing 3D convolutions with RCUs in any I 3 D network .", "ner": [["RCN", "Method"], ["2D/ 3 D networks", "Method"], ["3D convolutions", "Method"], ["RCUs", "Method"], ["I 3 D", "Method"]], "rel": [["2D/ 3 D networks", "Used-For", "RCN"], ["RCUs", "Part-Of", "I 3 D"]], "rel_plus": [["2D/ 3 D networks:Method", "Used-For", "RCN:Method"], ["RCUs:Method", "Part-Of", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Indeed , the network architecture of our proposed model builds on the I 3 D network architecture shown in Table 1 , where the same parameters ( d , number of kernels ) used for 3D convolutions are used for our RCU .", "ner": [["I 3 D", "Method"], ["3D convolutions", "Method"], ["RCU", "Method"]], "rel": [["3D convolutions", "Part-Of", "RCU"]], "rel_plus": [["3D convolutions:Method", "Part-Of", "RCU:Method"]]}
{"doc_id": "53719258", "sentence": "Unlike I 3 D , however , our RCN does not require a temporal convolution size n ( cfr .", "ner": [["I 3 D", "Method"], ["RCN", "Method"]], "rel": [["I 3 D", "Compare-With", "RCN"]], "rel_plus": [["I 3 D:Method", "Compare-With", "RCN:Method"]]}
{"doc_id": "53719258", "sentence": "As in 2D or I 3 D ResNet models [ 1 2 , 4 5 , 1 1 ] , our proposed RCN also has residual connections .", "ner": [["I 3 D ResNet", "Method"], ["RCN", "Method"], ["residual connections", "Method"]], "rel": [["residual connections", "Part-Of", "I 3 D ResNet"], ["residual connections", "Part-Of", "RCN"]], "rel_plus": [["residual connections:Method", "Part-Of", "I 3 D ResNet:Method"], ["residual connections:Method", "Part-Of", "RCN:Method"]]}
{"doc_id": "53719258", "sentence": "The initial hidden state h 0 , as shown in Figure 2 , is initialised by the output of the bottom 2D convolution layer at t 0 .", "ner": [["2D convolution layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "The hidden state h t at time t is considered to be the output at that time instant -as such , it acts as input to next hidden state and to the whole next depth - level layer .   The output sizes for both I 3 D and our proposed RCN are shown in Table 1 .", "ner": [["I 3 D", "Method"], ["RCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Our RCN only uses spatial pooling and a convolutional layer for classification , unlike the spatiotemporal pooling of [ 3 , 5 1 , 4 5 ] .", "ner": [["RCN", "Method"], ["spatial pooling", "Method"], ["convolutional layer", "Method"], ["classification", "Task"]], "rel": [["spatial pooling", "Part-Of", "RCN"], ["convolutional layer", "Part-Of", "RCN"], ["RCN", "Used-For", "classification"]], "rel_plus": [["spatial pooling:Method", "Part-Of", "RCN:Method"], ["convolutional layer:Method", "Part-Of", "RCN:Method"], ["RCN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "53719258", "sentence": "From Table 1 , compared to I 3 D , RCN produces 1 6 classification score vectors with an input sequence length of 1 6 .", "ner": [["I 3 D", "Method"], ["RCN", "Method"]], "rel": [["RCN", "Compare-With", "I 3 D"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "This one - to - one mapping from input to output is essential in many tasks , ranging from temporal action segmentation [ 3 2 , 3 0 ] , to temporal action detection [ 3 7 ] , to action tube detection [ 3 8 ] .", "ner": [["temporal action segmentation", "Task"], ["temporal action detection", "Task"], ["action tube detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Unlike the temporal deconvolution proposed in [ 3 2 , 3 0 ] , our RCN inherently addresses this problem ( see Table 1 ) .", "ner": [["temporal deconvolution", "Method"], ["RCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Thus our RCN as presented here is not only causal , but poses no constraints on the modelling of temporal dependencies ( as opposed to the upper bound of n in the case of temporal convolutions ) .", "ner": [["RCN", "Method"], ["temporal convolutions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "The I 3 D model proposed by Carreira et al. [ 3 ] greatly owes its success to a good initialisation from 2D models trained on ImageNet [ 6 ] .", "ner": [["I 3 D", "Method"], ["ImageNet", "Dataset"]], "rel": [["I 3 D", "Trained-With", "ImageNet"]], "rel_plus": [["I 3 D:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "It is noteworthy that the other state - of - the - art ( 2 + 1 )D model by Tran et al. [ 4 5 ] can not , instead , exploit ImageNet initialisation , because of the change in the number of kernels .", "ner": [["( 2 + 1 )D", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "In response to a similar issue , Le et al. [ 2 2 ] presented a simple way to initialise RNNs when used with ReLU [ 1 0 ] activation functions .", "ner": [["RNNs", "Method"], ["ReLU", "Method"]], "rel": [["ReLU", "Part-Of", "RNNs"]], "rel_plus": [["ReLU:Method", "Part-Of", "RNNs:Method"]]}
{"doc_id": "53719258", "sentence": "In this section , we evaluate our recurrent convolutional network on the challenging Kinetics [ 1 8 ] and UCF 1 0 1 [ 4 1 ] datasets to study its various original features and compare it with state - of - the - art 3D CNN models .", "ner": [["recurrent convolutional network", "Method"], ["Kinetics", "Dataset"], ["UCF 1 0 1", "Dataset"], ["3D CNN", "Method"]], "rel": [["recurrent convolutional network", "Evaluated-With", "Kinetics"], ["3D CNN", "Evaluated-With", "Kinetics"], ["recurrent convolutional network", "Evaluated-With", "UCF 1 0 1"], ["3D CNN", "Evaluated-With", "UCF 1 0 1"], ["recurrent convolutional network", "Compare-With", "3D CNN"]], "rel_plus": [["recurrent convolutional network:Method", "Evaluated-With", "Kinetics:Dataset"], ["3D CNN:Method", "Evaluated-With", "Kinetics:Dataset"], ["recurrent convolutional network:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["3D CNN:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["recurrent convolutional network:Method", "Compare-With", "3D CNN:Method"]]}
{"doc_id": "53719258", "sentence": "Kinetics has become a defacto benchmark for recent action recognition works [ 2 , 5 2 , 4 5 , 3 ] .", "ner": [["Kinetics", "Dataset"], ["action recognition", "Task"]], "rel": [["Kinetics", "Benchmark-For", "action recognition"]], "rel_plus": [["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "53719258", "sentence": "UCF 1 0 1 dataset has 1 0 1 classes and 1 3 K videos ; nowadays , it is used to evaluate the action recognition [ 4 1 ] and transfer learning [ 3 ] capabilities of 3D CNNs .", "ner": [["UCF 1 0 1", "Dataset"], ["action recognition", "Task"], ["transfer learning", "Task"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Evaluated-With", "UCF 1 0 1"], ["UCF 1 0 1", "Benchmark-For", "action recognition"], ["3D CNNs", "Used-For", "action recognition"], ["UCF 1 0 1", "Benchmark-For", "transfer learning"], ["3D CNNs", "Used-For", "transfer learning"]], "rel_plus": [["3D CNNs:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["UCF 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"], ["3D CNNs:Method", "Used-For", "action recognition:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "transfer learning:Task"], ["3D CNNs:Method", "Used-For", "transfer learning:Task"]]}
{"doc_id": "53719258", "sentence": "E.g. , the ResNet - 1 8 - based I 3 D model trained by Tran et al. [ 4 5 ] is 1 0 % better in terms of final video accuracy as compared to a similar model trained by Hara et al. [ 1 1 ] .", "ner": [["ResNet - 1 8", "Method"], ["I 3 D", "Method"]], "rel": [["ResNet - 1 8", "Part-Of", "I 3 D"]], "rel_plus": [["ResNet - 1 8:Method", "Part-Of", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Another important aspect of the training process is the presence of various input data argumentation operations , e.g. , random crop , horizontal flip , image intensities jittering and temporal jittering .", "ner": [["data argumentation operations", "Method"], ["random crop", "Method"], ["horizontal flip", "Method"], ["image intensities jittering", "Method"], ["temporal jittering", "Method"]], "rel": [["random crop", "SubClass-Of", "data argumentation operations"], ["horizontal flip", "SubClass-Of", "data argumentation operations"], ["image intensities jittering", "SubClass-Of", "data argumentation operations"], ["temporal jittering", "SubClass-Of", "data argumentation operations"]], "rel_plus": [["random crop:Method", "SubClass-Of", "data argumentation operations:Method"], ["horizontal flip:Method", "SubClass-Of", "data argumentation operations:Method"], ["image intensities jittering:Method", "SubClass-Of", "data argumentation operations:Method"], ["temporal jittering:Method", "SubClass-Of", "data argumentation operations:Method"]]}
{"doc_id": "53719258", "sentence": "Therefore we worked to reproduce the results in [ 4 4 ] for I 3 D and their proposed ( 2 + 1 )D model using our training setup for fair comparison , as shown in Tables 2 and 3 .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "We used 8 or 1 6 frames long RGB clips to train our RCN and baseline I 3 D and ( 2 + 1 )D , models .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "As for data augmentation , we used random crop and horizontal flip with 5 0 % probability , along with mean subtraction and normalisation with standard deviation .", "ner": [["data augmentation", "Method"], ["random crop", "Method"], ["horizontal flip", "Method"]], "rel": [["random crop", "SubClass-Of", "data augmentation"], ["horizontal flip", "SubClass-Of", "data augmentation"]], "rel_plus": [["random crop:Method", "SubClass-Of", "data augmentation:Method"], ["horizontal flip:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "53719258", "sentence": "As mentioned , we re - implemented the I 3 D and ( 2 + 1 )D models using ResNet - 1 8 and ResNet - 3 4 as a backbone .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["ResNet - 1 8", "Method"], ["ResNet - 3 4", "Method"]], "rel": [["ResNet - 3 4", "Part-Of", "I 3 D"], ["ResNet - 1 8", "Part-Of", "I 3 D"], ["ResNet - 3 4", "Part-Of", "( 2 + 1 )D"], ["ResNet - 1 8", "Part-Of", "( 2 + 1 )D"]], "rel_plus": [["ResNet - 3 4:Method", "Part-Of", "I 3 D:Method"], ["ResNet - 1 8:Method", "Part-Of", "I 3 D:Method"], ["ResNet - 3 4:Method", "Part-Of", "( 2 + 1 )D:Method"], ["ResNet - 1 8:Method", "Part-Of", "( 2 + 1 )D:Method"]]}
{"doc_id": "53719258", "sentence": "The Resnet - 1 8 - I 3 D architecture is presented in Table 1 .", "ner": [["Resnet - 1 8 - I 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Clip - level and video - level action recognition accuracy on the validation set of the Kinetics dataset for different ResNet - 1 8 - based models , trained using 8 - frame - long clips as input .", "ner": [["action recognition", "Task"], ["Kinetics", "Dataset"], ["ResNet - 1 8 - based models", "Method"]], "rel": [["ResNet - 1 8 - based models", "Used-For", "action recognition"], ["Kinetics", "Benchmark-For", "action recognition"], ["ResNet - 1 8 - based models", "Evaluated-With", "Kinetics"]], "rel_plus": [["ResNet - 1 8 - based models:Method", "Used-For", "action recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"], ["ResNet - 1 8 - based models:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "53719258", "sentence": "Comparison of our RCN with state - of - the - art I 3 D and ( 2 + 1 )D models on the validation set of Kinetics using a ResNet - 3 4 architecture trained using 1 6 - frame - long clips . we matched the number of parameters of separated convolutions to that of standard 3D convolutions , as explained in [ 4 5 ] .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["Kinetics", "Dataset"], ["ResNet - 3 4", "Method"], ["convolutions", "Method"], ["3D convolutions", "Method"]], "rel": [["ResNet - 3 4", "Part-Of", "RCN"], ["RCN", "Compare-With", "I 3 D"], ["ResNet - 3 4", "Part-Of", "I 3 D"], ["RCN", "Compare-With", "( 2 + 1 )D"], ["ResNet - 3 4", "Part-Of", "( 2 + 1 )D"], ["RCN", "Evaluated-With", "Kinetics"], ["I 3 D", "Evaluated-With", "Kinetics"], ["( 2 + 1 )D", "Evaluated-With", "Kinetics"]], "rel_plus": [["ResNet - 3 4:Method", "Part-Of", "RCN:Method"], ["RCN:Method", "Compare-With", "I 3 D:Method"], ["ResNet - 3 4:Method", "Part-Of", "I 3 D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"], ["ResNet - 3 4:Method", "Part-Of", "( 2 + 1 )D:Method"], ["RCN:Method", "Evaluated-With", "Kinetics:Dataset"], ["I 3 D:Method", "Evaluated-With", "Kinetics:Dataset"], ["( 2 + 1 )D:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "53719258", "sentence": "The results of the I 3 D and ( 2 + 1 )D implementations reported in Tran et al. [ 4 4 ] are shown in the top half of Table 2 .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "The number of parameters in our proposed RCN model is 1 2 . 8 million ( M ) , as opposed to 3 3 . 4 M in both the I 3 D and ( 2 + 1 )D models , see Table 2 .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [["RCN", "Compare-With", "I 3 D"], ["RCN", "Compare-With", "( 2 + 1 )D"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"]]}
{"doc_id": "53719258", "sentence": "It is remarkable to see that , despite a 2. 6 times reduction in the number of parameters , RCN still outperforms both I 3 D and ( 2 + 1 )D when trained using ImageNet initialisation .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["ImageNet", "Dataset"]], "rel": [["RCN", "Compare-With", "I 3 D"], ["RCN", "Compare-With", "( 2 + 1 )D"], ["RCN", "Trained-With", "ImageNet"], ["I 3 D", "Trained-With", "ImageNet"], ["( 2 + 1 )D", "Trained-With", "ImageNet"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"], ["RCN:Method", "Trained-With", "ImageNet:Dataset"], ["I 3 D:Method", "Trained-With", "ImageNet:Dataset"], ["( 2 + 1 )D:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "Further , RCN surpasses I 3 D also under random initialisation , while using 2. 6 times fewer model parameters .", "ner": [["RCN", "Method"], ["I 3 D", "Method"]], "rel": [["RCN", "Compare-With", "I 3 D"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "Similarly , Table 3 shows that RCN outperforms both I 3 D and ( 2 + 1 )D models , when the base model is ResNet - 3 4 and the input clip length is 1 6 , again , while using far fewer parameters .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["ResNet - 3 4", "Method"]], "rel": [["ResNet - 3 4", "Part-Of", "RCN"], ["RCN", "Compare-With", "I 3 D"], ["ResNet - 3 4", "Part-Of", "I 3 D"], ["RCN", "Compare-With", "( 2 + 1 )D"], ["ResNet - 3 4", "Part-Of", "( 2 + 1 )D"]], "rel_plus": [["ResNet - 3 4:Method", "Part-Of", "RCN:Method"], ["RCN:Method", "Compare-With", "I 3 D:Method"], ["ResNet - 3 4:Method", "Part-Of", "I 3 D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"], ["ResNet - 3 4:Method", "Part-Of", "( 2 + 1 )D:Method"]]}
{"doc_id": "53719258", "sentence": "ImageNet initialisation proves to be useful for both the I 3 D and our RCN models .", "ner": [["ImageNet", "Dataset"], ["I 3 D", "Method"], ["RCN", "Method"]], "rel": [["I 3 D", "Trained-With", "ImageNet"], ["RCN", "Trained-With", "ImageNet"]], "rel_plus": [["I 3 D:Method", "Trained-With", "ImageNet:Dataset"], ["RCN:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "While ( 2 + 1 )D performs ( Table 2 , row 7 ) better than RCN ( row 6 ) with random initialisation , our RCN recovers to improve over ( 2 + 1 )D ( row 7 ) with ImageNet initialisation , whereas ( 2 + 1 )D can not make use of free ImageNet initialisation .", "ner": [["( 2 + 1 )D", "Method"], ["RCN", "Method"], ["RCN", "Method"], ["( 2 + 1 )D", "Method"], ["ImageNet", "Dataset"], ["( 2 + 1 )D", "Method"], ["ImageNet", "Dataset"]], "rel": [["( 2 + 1 )D", "Compare-With", "RCN"], ["RCN", "Compare-With", "( 2 + 1 )D"], ["RCN", "Trained-With", "ImageNet"], ["( 2 + 1 )D", "Trained-With", "ImageNet"]], "rel_plus": [["( 2 + 1 )D:Method", "Compare-With", "RCN:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"], ["RCN:Method", "Trained-With", "ImageNet:Dataset"], ["( 2 + 1 )D:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "This seems to be a severe drawback for the ( 2 + 1 )D model , and a big advantage for I 3 D and RCN .", "ner": [["( 2 + 1 )D", "Method"], ["I 3 D", "Method"], ["RCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "One may argue that , if the purpose is to build on existing 2D models , then RCN and I 3 D are a better choice , whereas if new 3D models are preferred then ( 2 + 1 )D might prove useful .", "ner": [["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [["I 3 D", "Compare-With", "( 2 + 1 )D"], ["RCN", "Compare-With", "( 2 + 1 )D"]], "rel_plus": [["I 3 D:Method", "Compare-With", "( 2 + 1 )D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D:Method"]]}
{"doc_id": "53719258", "sentence": "A comparison with other 3D causal networks is a must , as we claim the causal nature of the network to be one of the main contributions of our work , making RCN best suited to online applications such as action detection and prediction .", "ner": [["RCN", "Method"], ["action detection", "Task"], ["prediction", "Task"]], "rel": [["RCN", "Used-For", "action detection"], ["RCN", "Used-For", "prediction"]], "rel_plus": [["RCN:Method", "Used-For", "action detection:Task"], ["RCN:Method", "Used-For", "prediction:Task"]]}
{"doc_id": "53719258", "sentence": "Their sequential version of I 3 D , however , shows a drop in performance as compared to I 3 D as seen in lines 1 and 2 of Table 4 .", "ner": [["I 3 D", "Method"], ["I 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Clip - length Acc% As to our new model , we can show that CRN consistently outperforms I 3 D in all the considered settings , namely : when using ResNet - 1 8 as base models , input clip size equal to 8 , with or without ImageNet initialisation ( cf .", "ner": [["CRN", "Method"], ["I 3 D", "Method"], ["ResNet - 1 8", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet - 1 8", "Part-Of", "CRN"], ["CRN", "Compare-With", "I 3 D"], ["ResNet - 1 8", "Part-Of", "I 3 D"]], "rel_plus": [["ResNet - 1 8:Method", "Part-Of", "CRN:Method"], ["CRN:Method", "Compare-With", "I 3 D:Method"], ["ResNet - 1 8:Method", "Part-Of", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "It is fair to say that our CRN network is the best performing causal network out there when compared with the corresponding I 3 D version .", "ner": [["CRN", "Method"], ["I 3 D", "Method"]], "rel": [["CRN", "Compare-With", "I 3 D"]], "rel_plus": [["CRN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "The S 3 Dg model has a gating operation applied to the outputs of Separated 3D ( S 3 D ) [ 5 2 ] .", "ner": [["S 3 Dg", "Method"], ["gating operation", "Method"], ["Separated 3D", "Method"], ["S 3 D", "Method"]], "rel": [["gating operation", "Part-Of", "S 3 Dg"], ["Separated 3D", "Part-Of", "S 3 Dg"], ["S 3 D", "Synonym-Of", "Separated 3D"]], "rel_plus": [["gating operation:Method", "Part-Of", "S 3 Dg:Method"], ["Separated 3D:Method", "Part-Of", "S 3 Dg:Method"], ["S 3 D:Method", "Synonym-Of", "Separated 3D:Method"]]}
{"doc_id": "53719258", "sentence": "Video - level action classification accuracy of different models on the validation set of the Kinetics dataset .", "ner": [["Video - level action classification", "Task"], ["Kinetics", "Dataset"]], "rel": [["Kinetics", "Benchmark-For", "Video - level action classification"]], "rel_plus": [["Kinetics:Dataset", "Benchmark-For", "Video - level action classification:Task"]]}
{"doc_id": "53719258", "sentence": "Non - Local ( NL ) operations as proposed by Wang et al. [ 5 1 ] also can be proved to improve performance over the base I 3 D model , as shown in the second part of Table 5 .", "ner": [["Non - Local", "Method"], ["NL", "Method"], ["I 3 D", "Method"]], "rel": [["NL", "Synonym-Of", "Non - Local"], ["Non - Local", "Compare-With", "I 3 D"]], "rel_plus": [["NL:Method", "Synonym-Of", "Non - Local:Method"], ["Non - Local:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "What needs to be remarked is that gating and NL operations are not at all constrained to be applied on top of I 3 D or S 3 D models : indeed , they can also be used in conjunction with ( 2 + 1 )D and our own RCN model .", "ner": [["NL", "Method"], ["I 3 D", "Method"], ["S 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["RCN", "Method"]], "rel": [["NL", "Part-Of", "( 2 + 1 )D"], ["NL", "Part-Of", "RCN"]], "rel_plus": [["NL:Method", "Part-Of", "( 2 + 1 )D:Method"], ["NL:Method", "Part-Of", "RCN:Method"]]}
{"doc_id": "53719258", "sentence": "As in this work we focus on comparing our network with other 3D models , we chose I 3 D and ( 2 + 1 )D as baselines ( Sec. 5. 2 ) .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "From the last three rows of Table 5 , we can sensibly speculate that , as our RCN performs better than those 3D models , the application of further gating and NL layers is likely to lead to performances superior to those of [ 5 2 ] and [ 5 1 ] .", "ner": [["RCN", "Method"], ["NL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "Among the latter , ResNet 1 0 1 - I 3 D - NL [ 5 1 ] is shown to work better with an even longer input clip length .", "ner": [["ResNet 1 0 1 - I 3 D - NL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "In our experiments , as mentioned , we stuck to 1 6 - frame clips as input and compared our proposed RCN with baseline I 3 D models ( bottom rows of Table 5 ) .", "ner": [["RCN", "Method"], ["I 3 D", "Method"]], "rel": [["RCN", "Compare-With", "I 3 D"]], "rel_plus": [["RCN:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "53719258", "sentence": "The last three rows of Table 6 shows the performance of baseline I 3 D , ( 2 + 1 )D and RCN models on UCF 1 0 1 .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["RCN", "Method"], ["UCF 1 0 1", "Dataset"]], "rel": [["I 3 D", "Evaluated-With", "UCF 1 0 1"], ["( 2 + 1 )D", "Evaluated-With", "UCF 1 0 1"], ["RCN", "Evaluated-With", "UCF 1 0 1"]], "rel_plus": [["I 3 D:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["( 2 + 1 )D:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["RCN:Method", "Evaluated-With", "UCF 1 0 1:Dataset"]]}
{"doc_id": "53719258", "sentence": "As on Kinetics , our RCN ( last row ) outperforms our baseline implementation of I 3 D and ( 2 + 1 )D. The second and third part of Table 6 shows the state - ofthe - art results achieved by the S 3 Dg [ 5 2 ] and ( 2 + 1 )D models as implemented in [ 4 5 ] Table 7 .", "ner": [["Kinetics", "Dataset"], ["RCN", "Method"], ["I 3 D", "Method"], ["( 2 + 1 )D.", "Method"], ["S 3 Dg", "Method"], ["( 2 + 1 )D", "Method"]], "rel": [["RCN", "Evaluated-With", "Kinetics"], ["RCN", "Compare-With", "I 3 D"], ["RCN", "Compare-With", "( 2 + 1 )D."]], "rel_plus": [["RCN:Method", "Evaluated-With", "Kinetics:Dataset"], ["RCN:Method", "Compare-With", "I 3 D:Method"], ["RCN:Method", "Compare-With", "( 2 + 1 )D.:Method"]]}
{"doc_id": "53719258", "sentence": "Video - level action recognition accuracy on the Kinetics validation set for different ResNet - 1 8 - based RCN models . 6 .", "ner": [["Video - level action recognition", "Task"], ["Kinetics", "Dataset"], ["ResNet - 1 8", "Method"], ["RCN", "Method"]], "rel": [["Kinetics", "Benchmark-For", "Video - level action recognition"], ["RCN", "Used-For", "Video - level action recognition"], ["RCN", "Evaluated-With", "Kinetics"], ["ResNet - 1 8", "Part-Of", "RCN"]], "rel_plus": [["Kinetics:Dataset", "Benchmark-For", "Video - level action recognition:Task"], ["RCN:Method", "Used-For", "Video - level action recognition:Task"], ["RCN:Method", "Evaluated-With", "Kinetics:Dataset"], ["ResNet - 1 8:Method", "Part-Of", "RCN:Method"]]}
{"doc_id": "53719258", "sentence": "Discussion Two basic things are clear from our experience with training heavy 3D models ( I 3 D , ( 2 + 1 )D , RCN ) on largescale datasets such as Kinetics .", "ner": [["I 3 D", "Method"], ["( 2 + 1 )D", "Method"], ["RCN", "Method"], ["Kinetics", "Dataset"]], "rel": [["RCN", "Evaluated-With", "Kinetics"], ["( 2 + 1 )D", "Evaluated-With", "Kinetics"], ["I 3 D", "Evaluated-With", "Kinetics"]], "rel_plus": [["RCN:Method", "Evaluated-With", "Kinetics:Dataset"], ["( 2 + 1 )D:Method", "Evaluated-With", "Kinetics:Dataset"], ["I 3 D:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "53719258", "sentence": "In the case of both I 3 D and RCN , ImageNet initialisation improves the video classification accuracy on Kinetics by almost 3% compared to random initialisation when using the same number of training iterations , as shown in the first and last row of Table 7 .", "ner": [["I 3 D", "Method"], ["RCN", "Method"], ["ImageNet", "Dataset"], ["video classification", "Task"], ["Kinetics", "Dataset"]], "rel": [["RCN", "Trained-With", "ImageNet"], ["I 3 D", "Trained-With", "ImageNet"], ["I 3 D", "Used-For", "video classification"], ["RCN", "Used-For", "video classification"], ["I 3 D", "Evaluated-With", "Kinetics"], ["RCN", "Evaluated-With", "Kinetics"]], "rel_plus": [["RCN:Method", "Trained-With", "ImageNet:Dataset"], ["I 3 D:Method", "Trained-With", "ImageNet:Dataset"], ["I 3 D:Method", "Used-For", "video classification:Task"], ["RCN:Method", "Used-For", "video classification:Task"], ["I 3 D:Method", "Evaluated-With", "Kinetics:Dataset"], ["RCN:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "53719258", "sentence": "The bottom line is that we should strive for more efficient implementations of 3D models for the sake of their adoption .   To conclude , it is interesting to take a closer look at the statistics for the weight matrices ( w hh ) associated with the hidden state at every RCU layer in the RCN network .", "ner": [["RCU", "Method"], ["RCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719258", "sentence": "We showed that ImageNet - based initialisation is at the heart of the success of 3D CNNs .", "ner": [["ImageNet", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "53719258", "sentence": "The causal nature of our recurrent 3D convolutional network opens up manifold research directions , with direct and promising potential application in areas such as online action detection and future event/action prediction .", "ner": [["3D convolutional network", "Method"], ["online action detection", "Task"], ["event/action prediction", "Task"]], "rel": [["3D convolutional network", "Used-For", "online action detection"], ["3D convolutional network", "Used-For", "event/action prediction"]], "rel_plus": [["3D convolutional network:Method", "Used-For", "online action detection:Task"], ["3D convolutional network:Method", "Used-For", "event/action prediction:Task"]]}
{"doc_id": "2032742", "sentence": "Phenomenally successful in practical inference problems , convolutional neural networks ( CNN ) are widely deployed in mobile devices , data centers , and even supercomputers .", "ner": [["convolutional neural networks", "Method"], ["CNN", "Method"]], "rel": [["CNN", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "2032742", "sentence": "While pruning the fully connected layers reduces a CNN 's size considerably , it does not improve inference speed noticeably as the compute heavy parts lie in convolutions .", "ner": [["fully connected layers", "Method"], ["CNN", "Method"], ["convolutions", "Method"]], "rel": [["fully connected layers", "Part-Of", "CNN"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "2032742", "sentence": "Pruning CNNs in a way that increase inference speed often imposes specific sparsity structures , thus limiting the achievable sparsity levels .    We present a method to realize simultaneously size economy and speed improvement while pruning CNNs .", "ner": [["Pruning CNNs", "Method"], ["pruning CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Deep neural networks , especially CNNs , have been pervasive in computer vision recently and served as a foundation for many critical applications ranging from image recognition [ 1 7 ] and video analytics [ 2 1 ] to autonomous driving [ 1 ] .", "ner": [["Deep neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["image recognition", "Task"], ["video analytics", "Task"], ["autonomous driving", "Task"]], "rel": [["CNNs", "SubClass-Of", "Deep neural networks"], ["Deep neural networks", "Used-For", "computer vision"], ["CNNs", "Used-For", "computer vision"], ["image recognition", "SubTask-Of", "computer vision"], ["video analytics", "SubTask-Of", "computer vision"], ["autonomous driving", "SubTask-Of", "computer vision"]], "rel_plus": [["CNNs:Method", "SubClass-Of", "Deep neural networks:Method"], ["Deep neural networks:Method", "Used-For", "computer vision:Task"], ["CNNs:Method", "Used-For", "computer vision:Task"], ["image recognition:Task", "SubTask-Of", "computer vision:Task"], ["video analytics:Task", "SubTask-Of", "computer vision:Task"], ["autonomous driving:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "2032742", "sentence": "The trained CNN models are deployed broadly for classification on a large variety of platforms , covering the spectrum of data center servers to mobile clients such as smart phones and autonomous - driving cars .", "ner": [["CNN", "Method"], ["classification", "Task"], ["autonomous - driving", "Task"]], "rel": [["CNN", "Used-For", "classification"]], "rel_plus": [["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "When the trained CNN models perform classification on these platforms , especially on resource constraint mobile platforms , accuracy , ( classification ) speed , and model size are the three key requirements that together form the threepronged \" golden trident \" .", "ner": [["CNN", "Method"], ["classification", "Task"], ["classification", "Task"]], "rel": [["CNN", "Used-For", "classification"]], "rel_plus": [["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "Exploiting or imposing sparsity in CNNs [ 1 2 , 1 1 , 7 , 1 9 , 8 , 1 4 ] have been recently studied to reduce model size and/or accelerate classification speed with minimal or zero accuracy loss .", "ner": [["CNNs", "Method"], ["classification", "Task"]], "rel": [["CNNs", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "One research thrust [ 1 2 , 1 1 , 7 ] of sparse CNN has been focusing on reducing model size by sparsifying/compressing fully connected layers that have most of the parameters of the entire CNN model .", "ner": [["sparse CNN", "Method"], ["fully connected layers", "Method"], ["CNN", "Method"]], "rel": [["fully connected layers", "Part-Of", "sparse CNN"], ["fully connected layers", "Part-Of", "CNN"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "sparse CNN:Method"], ["fully connected layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "2032742", "sentence": "However , such designs provide limited benefits to the classification speed of CNN , because the majority of the computation , i.e. , FLOPS , are within the convolution layers instead .", "ner": [["classification", "Task"], ["CNN", "Method"], ["convolution layers", "Method"]], "rel": [["CNN", "Used-For", "classification"], ["convolution layers", "Part-Of", "CNN"]], "rel_plus": [["CNN:Method", "Used-For", "classification:Task"], ["convolution layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "2032742", "sentence": "For example , in AlexNet , while fully connected layers comprise more than 9 0 % of the total model size , it is the convolution layers that comprise more than 9 0 % of the total computation .", "ner": [["AlexNet", "Method"], ["fully connected layers", "Method"], ["convolution layers", "Method"]], "rel": [["fully connected layers", "Part-Of", "AlexNet"], ["convolution layers", "Part-Of", "AlexNet"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "AlexNet:Method"], ["convolution layers:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "2032742", "sentence": "Therefore , another research thrust [ 1 9 , 8 , 1 4 ] of sparse CNN focuses on convolution layers to reduce compute requirements and thus to improve classification speed .", "ner": [["sparse CNN", "Method"], ["convolution layers", "Method"], ["classification", "Task"]], "rel": [["convolution layers", "Part-Of", "sparse CNN"], ["sparse CNN", "Used-For", "classification"]], "rel_plus": [["convolution layers:Method", "Part-Of", "sparse CNN:Method"], ["sparse CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "However , there are challenges for sparse CNN to achieve its full potential , including 1 ) sparse convolution has lower arithmetic intensity , thus additional data transfers involved in lowering tensors to matrices used in [ 1 9 , 8 , 1 4 ] add higher overhead , 2 ) some prior sparse convolution methods [ 8 , 1 4 ] are only applicable to large filters ( e.g. , 9 \u00d7 9 ) , which limits their usage for modern CNNs where small filters ( e.g. , 3 \u00d7 3 ) are popular .", "ner": [["sparse CNN", "Method"], ["sparse convolution", "Method"], ["sparse convolution", "Method"], ["CNNs", "Method"]], "rel": [["sparse convolution", "Part-Of", "sparse CNN"]], "rel_plus": [["sparse convolution:Method", "Part-Of", "sparse CNN:Method"]]}
{"doc_id": "2032742", "sentence": "Moreover , while current sparse CNN methods focus mostly on either fully connected layers or convolution layers , it is desirable to apply sparse methods to both layers simultaneously .", "ner": [["sparse CNN", "Method"], ["fully connected layers", "Method"], ["convolution layers", "Method"]], "rel": [["fully connected layers", "Part-Of", "sparse CNN"], ["convolution layers", "Part-Of", "sparse CNN"]], "rel_plus": [["fully connected layers:Method", "Part-Of", "sparse CNN:Method"], ["convolution layers:Method", "Part-Of", "sparse CNN:Method"]]}
{"doc_id": "2032742", "sentence": "We also provide guidelines for the range of sparsity that should be targeted during training . \u2022 A highly - optimized sparse CNN implementation that provides 3. 4 \u00d7 and 7. 3 \u00d7 speedups of convolution layers in AlexNet over the best dense direct convolution performance on Xeon and Atom processors , respectively , with no accuracy drop .", "ner": [["sparse CNN", "Method"], ["convolution layers", "Method"], ["AlexNet", "Method"]], "rel": [["convolution layers", "Part-Of", "AlexNet"]], "rel_plus": [["convolution layers:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "2032742", "sentence": "We also show 2, 3 7 1 and 1 2 0 images per second AlexNet classification throughput , which are significantly higher than the best published performance on respective platforms .", "ner": [["AlexNet", "Method"], ["classification", "Task"]], "rel": [["AlexNet", "Used-For", "classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "Sparse methods have been popular to reduce model size and accelerate classification speed of CNNs with minimal or zero accuracy loss .", "ner": [["classification", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "However , the speedup of end - to - end performance is limited , because the fully connected layers only account for less than 1 0 % total computation in most CNNs such as AlexNet [ 1 7 ] .", "ner": [["fully connected layers", "Method"], ["CNNs", "Method"], ["AlexNet", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNNs"], ["fully connected layers", "Part-Of", "CNNs"], ["fully connected layers", "Part-Of", "AlexNet"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNNs:Method"], ["fully connected layers:Method", "Part-Of", "CNNs:Method"], ["fully connected layers:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "2032742", "sentence": "Another research thrust [ 1 9 , 8 , 1 4 ] of sparse CNN focuses on convolution layers , mostly targeting the classification speed .", "ner": [["sparse CNN", "Method"], ["convolution layers", "Method"], ["classification", "Task"]], "rel": [["convolution layers", "Part-Of", "sparse CNN"], ["sparse CNN", "Used-For", "classification"]], "rel_plus": [["convolution layers:Method", "Part-Of", "sparse CNN:Method"], ["sparse CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "However , prior work on sparse convolution involves the overhead of transforming tensors to matrices , and some are inapplicable to small kernel size and thus can not exploit the full potential of the sparse convolution .", "ner": [["sparse convolution", "Method"], ["sparse convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "While Perforated - CNN [ 9 ] also improves the classification speed of convolution layers by eliminating redundant computations , it relies on interpolating redundant convolution computation instead of using sparse methods .", "ner": [["Perforated - CNN", "Method"], ["classification", "Task"], ["convolution layers", "Method"]], "rel": [["convolution layers", "Part-Of", "Perforated - CNN"], ["Perforated - CNN", "Used-For", "classification"]], "rel_plus": [["convolution layers:Method", "Part-Of", "Perforated - CNN:Method"], ["Perforated - CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "Therefore , the acceleration due to sparse methods on classification speed of CNNs is not as good as the sparse - method - enabled reduction of model size .", "ner": [["classification", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "The second challenge is the lack of holistical optimization of speed and size of sparse CNNs ( convolution layers and fully connected layers mostly affect speed and size , respectively ) .", "ner": [["CNNs", "Method"], ["convolution layers", "Method"], ["fully connected layers", "Method"]], "rel": [["convolution layers", "Part-Of", "CNNs"], ["fully connected layers", "Part-Of", "CNNs"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNNs:Method"], ["fully connected layers:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "2032742", "sentence": "This section first presents our direct sparse convolution algorithm to advance the stateof - the - art in sparse convolution for speeding up CNNs .", "ner": [["sparse convolution", "Method"], ["sparse convolution", "Method"], ["CNNs", "Method"]], "rel": [["sparse convolution", "Part-Of", "CNNs"]], "rel_plus": [["sparse convolution:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "2032742", "sentence": "Finally , we discuss the synergies between sparse convolution and sparse methods on fully connected layers .", "ner": [["sparse convolution", "Method"], ["fully connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Figure 3 : Ideal speedups of sparse convolution over dense convolution . conv 2 / 4 _direct : direct sparse convolution , conv 2 / 4 _lowered : sparse convolution on tensors lowered to matrices ( see Section 3. 1 . 2 for details ) .", "ner": [["sparse convolution", "Method"], ["dense convolution", "Method"], ["sparse convolution", "Method"], ["sparse convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "In convolution layers of CNN , an input channel is reused against multiple output channels and vice versa , thus the arithmetic intensity of our direct sparse convolution algorithm is higher than SpMV that is memory bandwidth bound but lower than dense convolution that is compute - bound .", "ner": [["convolution layers", "Method"], ["CNN", "Method"], ["sparse convolution", "Method"], ["SpMV", "Method"], ["dense convolution", "Method"]], "rel": [["convolution layers", "Part-Of", "CNN"], ["sparse convolution", "Compare-With", "SpMV"], ["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNN:Method"], ["sparse convolution:Method", "Compare-With", "SpMV:Method"], ["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "The potential acceleration of sparse convolution over dense convolution can be expressed by where x is the proportion of non - zeros in filters ( lower the x , higher the sparsity of weight tensor W ) , and this proportion directly affects the final speedup .", "ner": [["sparse convolution", "Method"], ["dense convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "Therefore , sparse convolution 's actual FLOP/s is lower than that of dense convolution .", "ner": [["sparse convolution", "Method"], ["dense convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "Hence , there is a lower bound of useful sparsity such that , with a sparsity lower than that , sparse convolution provides no speedup over dense convolution .", "ner": [["sparse convolution", "Method"], ["dense convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "Therefore , in Atom processors , sparse convolution outperform dense convolution in a wider range of sparsity and the speedups are higher as will be shown in Section 5 .", "ner": [["sparse convolution", "Method"], ["dense convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "The lowering and lifting process has demonstrated overhead for dense convolution [ 1 0 ] , and it is particularly problematic for sparse convolution with intensity already much lower than its dense counter part .", "ner": [["dense convolution", "Method"], ["sparse convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Even though a bulk of computation belongs to convolution layers , fully connected layers can become a bottleneck after sparse convolution optimizations .", "ner": [["convolution layers", "Method"], ["fully connected layers", "Method"], ["sparse convolution", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Exploiting sparsity in fully connected layers is actually simpler than convolution layers , because fully connected layers are implemented as GEMM and we can leverage work done before on sparse matrix and dense matrix multiplication ( SpMDM ) .", "ner": [["fully connected layers", "Method"], ["convolution layers", "Method"], ["fully connected layers", "Method"], ["GEMM", "Method"], ["sparse matrix and dense matrix multiplication", "Method"], ["SpMDM", "Method"]], "rel": [["fully connected layers", "Compare-With", "convolution layers"], ["fully connected layers", "Part-Of", "GEMM"], ["SpMDM", "Synonym-Of", "sparse matrix and dense matrix multiplication"]], "rel_plus": [["fully connected layers:Method", "Compare-With", "convolution layers:Method"], ["fully connected layers:Method", "Part-Of", "GEMM:Method"], ["SpMDM:Method", "Synonym-Of", "sparse matrix and dense matrix multiplication:Method"]]}
{"doc_id": "2032742", "sentence": "Simultaneously learning the sparsity of convolution and fully connected layers is beneficial in amortizing the training cost and in performing holistic trade - off among accuracy , classification speed , and model size .", "ner": [["convolution", "Method"], ["fully connected layers", "Method"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "However , when targeting no accuracy loss from sparse models , we find that using the same regularization parameters across all layers tends to provide a high sparsity on fully connected ( fc ) layers but not enough sparsity on convolution layers to obtain speedups from sparse convolutions .", "ner": [["fully connected", "Method"], ["fc", "Method"], ["convolution layers", "Method"], ["sparse convolutions", "Method"]], "rel": [["fc", "Synonym-Of", "fully connected"], ["fully connected", "Compare-With", "convolution layers"]], "rel_plus": [["fc:Method", "Synonym-Of", "fully connected:Method"], ["fully connected:Method", "Compare-With", "convolution layers:Method"]]}
{"doc_id": "2032742", "sentence": "Therefore , trading off a small increase in model size of fc layers for a large speedup of convolution layers can be often beneficial for holistically optimizing the accuracy - speed - size of CNN .", "ner": [["convolution layers", "Method"], ["CNN", "Method"]], "rel": [["convolution layers", "Part-Of", "CNN"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "2032742", "sentence": "The lower convolution layers such as conv 1 in AlexNet are much harder to sparsify than higher layers such as conv 2 and above unless we can tolerate a large accuracy drop .", "ner": [["convolution layers", "Method"], ["conv 1", "Method"], ["AlexNet", "Method"], ["conv 2", "Method"]], "rel": [["conv 1", "SubClass-Of", "convolution layers"], ["conv 1", "Part-Of", "AlexNet"], ["convolution layers", "Part-Of", "AlexNet"], ["conv 2", "Part-Of", "AlexNet"], ["conv 1", "Compare-With", "conv 2"]], "rel_plus": [["conv 1:Method", "SubClass-Of", "convolution layers:Method"], ["conv 1:Method", "Part-Of", "AlexNet:Method"], ["convolution layers:Method", "Part-Of", "AlexNet:Method"], ["conv 2:Method", "Part-Of", "AlexNet:Method"], ["conv 1:Method", "Compare-With", "conv 2:Method"]]}
{"doc_id": "2032742", "sentence": "Specifically , we use weight decay 1e - 5 , 5e - 5 , and 6e - 5 , for FC decay multiplier 1 , 0. 1 , and 0.0 1 , respectively . among accuracy , speed , and model size .", "ner": [["weight decay", "Method"], ["FC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Our Holistic SparseCNN achieves a classification speed of 2, 3 7 1 images per second on BDW , without accuracy drop compared to the reference Caffe model .", "ner": [["Holistic SparseCNN", "Method"], ["classification", "Task"]], "rel": [["Holistic SparseCNN", "Used-For", "classification"]], "rel_plus": [["Holistic SparseCNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "2032742", "sentence": "With no accuracy loss , our Holistic SparseCNN achieves a classification speed of 1 2 0 images per second , about 2. 8 \u00d7 higher than a best case dense CNN implementation where all conv and fc layers would perform at the speed of SGEMM .", "ner": [["Holistic SparseCNN", "Method"], ["classification", "Task"], ["dense CNN", "Method"]], "rel": [["Holistic SparseCNN", "Used-For", "classification"], ["Holistic SparseCNN", "Compare-With", "dense CNN"]], "rel_plus": [["Holistic SparseCNN:Method", "Used-For", "classification:Task"], ["Holistic SparseCNN:Method", "Compare-With", "dense CNN:Method"]]}
{"doc_id": "2032742", "sentence": "This provides high sparsity on fc layers but sparsity on conv layers is so low that sparse convolution is slower than dense convolution on BDW .", "ner": [["conv layers", "Method"], ["sparse convolution", "Method"], ["dense convolution", "Method"]], "rel": [["sparse convolution", "Compare-With", "dense convolution"]], "rel_plus": [["sparse convolution:Method", "Compare-With", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "We use highly optimized SGEMM performance as a proxy of dense convolution performance to quantify layer - wise speedups of our Holistic SparseCNN .", "ner": [["SGEMM", "Method"], ["dense convolution", "Method"], ["Holistic SparseCNN", "Method"]], "rel": [["SGEMM", "Part-Of", "dense convolution"]], "rel_plus": [["SGEMM:Method", "Part-Of", "dense convolution:Method"]]}
{"doc_id": "2032742", "sentence": "As a centerpiece of deep learning , CNNs are under relentless pressure to be smaller , faster , and with higher evaluation/classification accuracy .", "ner": [["deep learning", "Method"], ["CNNs", "Method"], ["evaluation/classification", "Task"]], "rel": [["CNNs", "SubClass-Of", "deep learning"], ["CNNs", "Used-For", "evaluation/classification"]], "rel_plus": [["CNNs:Method", "SubClass-Of", "deep learning:Method"], ["CNNs:Method", "Used-For", "evaluation/classification:Task"]]}
{"doc_id": "2032742", "sentence": "In this paper , we present Holistic SparseCNN , a collection of techniques that pave the road to unleash the full potential of sparse CNNs .", "ner": [["Holistic SparseCNN", "Method"], ["sparse CNNs", "Method"]], "rel": [["Holistic SparseCNN", "SubClass-Of", "sparse CNNs"]], "rel_plus": [["Holistic SparseCNN:Method", "SubClass-Of", "sparse CNNs:Method"]]}
{"doc_id": "2032742", "sentence": "Secondly , unlike prior research focusing on using sparse methods on convolution layers or fully connected layers in isolation , our new cross - layer holistic methodology not only exploits the synergies of sparse methods on convolution layers and fully connected layers but also enables optimizing accuracy , speed , and size together .", "ner": [["convolution layers", "Method"], ["fully connected layers", "Method"], ["convolution layers", "Method"], ["fully connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "2032742", "sentence": "Our results on AlexNet demonstrated 3. 4 \u00d7 and 7. 3 \u00d7 speedups of convolution layers over the best dense convolution on representative server and mobile platforms as well as the best classification throughputs on these platforms .", "ner": [["AlexNet", "Method"], ["convolution layers", "Method"], ["dense convolution", "Method"], ["classification", "Task"]], "rel": [["convolution layers", "Part-Of", "AlexNet"], ["AlexNet", "Used-For", "classification"]], "rel_plus": [["convolution layers:Method", "Part-Of", "AlexNet:Method"], ["AlexNet:Method", "Used-For", "classification:Task"]]}
{"doc_id": "198967567", "sentence": "Over the past few years , we have witnessed the success of deep learning in image recognition thanks to the availability of large - scale human - annotated datasets such as PASCAL VOC , ImageNet , and COCO .", "ner": [["deep learning", "Method"], ["image recognition", "Task"], ["PASCAL VOC", "Dataset"], ["ImageNet", "Dataset"], ["COCO", "Dataset"]], "rel": [["deep learning", "Used-For", "image recognition"], ["PASCAL VOC", "Benchmark-For", "image recognition"], ["ImageNet", "Benchmark-For", "image recognition"], ["COCO", "Benchmark-For", "image recognition"]], "rel_plus": [["deep learning:Method", "Used-For", "image recognition:Task"], ["PASCAL VOC:Dataset", "Benchmark-For", "image recognition:Task"], ["ImageNet:Dataset", "Benchmark-For", "image recognition:Task"], ["COCO:Dataset", "Benchmark-For", "image recognition:Task"]]}
{"doc_id": "198967567", "sentence": "To evaluate and validate the performance of our approach , we have built a few - shot segmentation dataset , FSS - 1 0 0 0 , which consists of 1 0 0 0 object classes with pixelwise annotation of ground - truth segmentation .", "ner": [["few - shot segmentation", "Task"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "We build our baseline model using standard backbone networks such as VGG - 1 6 , ResNet - 1 0 1 , and Inception .", "ner": [["VGG - 1 6", "Method"], ["ResNet - 1 0 1", "Method"], ["Inception", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "To our surprise , we found that training our model from scratch using FSS - 1 0 0 0 achieves comparable and even better results than training with weights pre - trained by ImageNet which is more than 1 0 0 times larger than FSS - 1 0 0 0 .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["ImageNet", "Dataset"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Although unprecedented in the number of object categories when first released , contemporary image datasets for training deep neural networks such as PASCAL VOC [ 5 ] ( 1 9 , 7 4 0 images , 2 0 classes ) , ILSVRC [ 2 8 ] ( 1, 2 8 1 , 1 6 7 images , 1, 0 0 0 classes ) , and COCO [ 2 1 ] ( 2 0 4 , 7 2 1 images , 8 0 classes ) are actually quite limited for visual recognition tasks in the real world : a rough estimate of the number of different objects on the Earth falls in the range of 5 0 0 , 0 0 0 to 7 0 0 , 0 0 0 , following the total number of nouns in the En - * Equal contribution . glish language .", "ner": [["deep neural networks", "Method"], ["PASCAL VOC", "Dataset"], ["ILSVRC", "Dataset"], ["COCO", "Dataset"], ["visual recognition", "Task"]], "rel": [["deep neural networks", "Trained-With", "PASCAL VOC"], ["deep neural networks", "Trained-With", "ILSVRC"], ["deep neural networks", "Trained-With", "COCO"]], "rel_plus": [["deep neural networks:Method", "Trained-With", "PASCAL VOC:Dataset"], ["deep neural networks:Method", "Trained-With", "ILSVRC:Dataset"], ["deep neural networks:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "198967567", "sentence": "Thus , Few - Shot Learning has emerged as an attractive alternative for important computer vision tasks , especially when the given new dataset is very small and dissimilar so relying on the aforementioned pre - trained weights may not work well .", "ner": [["Few - Shot Learning", "Task"], ["computer vision", "Task"]], "rel": [["Few - Shot Learning", "Used-For", "computer vision"]], "rel_plus": [["Few - Shot Learning:Task", "Used-For", "computer vision:Task"]]}
{"doc_id": "198967567", "sentence": "Previous research on few - shot segmentation relies on a manual split of the PASCAL VOC dataset to train and evaluate a new model [ 3 0 , 2 4 ] , but only 2 0 and 8 0 classes in the PASCAL VOC and COCO datasets respectively contain pixelwise segmentation information .", "ner": [["few - shot segmentation", "Task"], ["PASCAL VOC", "Dataset"], ["PASCAL VOC", "Dataset"], ["COCO", "Dataset"], ["pixelwise segmentation", "Task"]], "rel": [["PASCAL VOC", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["PASCAL VOC:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "FSS - 1 0 0 0 is the first large - scale dataset for few - shot segmentation with built - in object category hierarchy which emphasizes the number of object classes rather than the number of images .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "All existing datasets are biased toward a number of object categories except FSS - 1 0 0 0 ( red ) . pending a decoder module to the relation network [ 3 3 ] , which is a simple and elegant deep model effective and originally designed for few - shot image classification only .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot image classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Reshaping the relation network into a fully - convolutional U - Net architecture [ 2 6 ] , our extensive experimental results show that this baseline model trained from scratch on FSS - 1 0 0 0 , which is less than 1% of the size of contemporary large - scale datasets , outperforms the model fine - tuned from weights pre - trained on ImageNet/COCO dataset .", "ner": [["fully - convolutional U - Net", "Method"], ["FSS - 1 0 0 0", "Dataset"], ["ImageNet/COCO", "Dataset"]], "rel": [["fully - convolutional U - Net", "Trained-With", "FSS - 1 0 0 0"]], "rel_plus": [["fully - convolutional U - Net:Method", "Trained-With", "FSS - 1 0 0 0:Dataset"]]}
{"doc_id": "198967567", "sentence": "With its excellent segmentation performance as well as extensibility , FSS - 1 0 0 0 is expected to make a lasting contribution to few - shot image segmentation .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot image segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot image segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot image segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "We first review the relationship and difference between FSS - 1 0 0 0 and modern datasets aiming to solve image segmentation and few - shot classification .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["image segmentation", "Task"], ["few - shot classification", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "image segmentation"], ["FSS - 1 0 0 0", "Benchmark-For", "few - shot classification"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "image segmentation:Task"], ["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot classification:Task"]]}
{"doc_id": "198967567", "sentence": "Then we review contemporary research on few - shot learning and semantic segmentation and discuss how we relate the few - shot segmentation to previous research .", "ner": [["few - shot learning", "Task"], ["semantic segmentation", "Task"], ["few - shot segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "The PASCAL VOC [ 5 ] was the first to provide a challenging image dataset for object class recognition and semantic segmentation .", "ner": [["PASCAL VOC", "Dataset"], ["object class recognition", "Task"], ["semantic segmentation", "Task"]], "rel": [["PASCAL VOC", "Benchmark-For", "object class recognition"], ["PASCAL VOC", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["PASCAL VOC:Dataset", "Benchmark-For", "object class recognition:Task"], ["PASCAL VOC:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "The latest version VOC 2 0 1 2 contains 2 0 object classes and 9, 9 9 3 images with segmentation annotations .", "ner": [["VOC 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Despite the absence of segmentation labels , the Imagenet [ 4 ] is built upon the backbone of WordNet and provides image - level labels for 5, 2 4 7 classes for training , out of which a subset of 1, 0 0 0 categories are split out to form the ILSVRC [ 2 8 ] dataset .", "ner": [["Imagenet", "Dataset"], ["WordNet", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "This challenge has made a significant impact on the rapid progress in visual recognition task and computer vision in recent years .", "ner": [["visual recognition", "Task"], ["computer vision", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "The latest Open Image dataset [ 1 7 ] contains 7, 1 8 6 trainable distinct object classes for classification and 6 0 0 classes for detection , making it the largest existing dataset with object classes and location annotations .", "ner": [["Open Image", "Dataset"], ["classification", "Task"], ["detection", "Task"]], "rel": [["Open Image", "Benchmark-For", "classification"], ["Open Image", "Benchmark-For", "detection"]], "rel_plus": [["Open Image:Dataset", "Benchmark-For", "classification:Task"], ["Open Image:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "198967567", "sentence": "Following the PASCAL VOC and ImageNet , the COCO segmentation dataset [ 2 1 ] includes more than 2 0 0 , 0 0 0 images with instance - wise semantic segmentation labels .", "ner": [["PASCAL VOC", "Dataset"], ["ImageNet", "Dataset"], ["COCO segmentation", "Dataset"], ["instance - wise semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Our FSS - 1 0 0 0 consists of 1, 0 0 0 object classes , wherein each class we label 1 0 images with binary segmentation annotation .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["binary segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "We are particularly interested in segmentation due to its obvious benefits : segmentation captures the essential feature of an object without background ; instance level segmentation can be ready from segmentation .", "ner": [["segmentation", "Task"], ["segmentation", "Task"], ["instance level segmentation", "Task"], ["segmentation", "Task"]], "rel": [["instance level segmentation", "SubTask-Of", "segmentation"]], "rel_plus": [["instance level segmentation:Task", "SubTask-Of", "segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "But none of these few - shot learning datasets incorporate dense pixelwise segmentation labels , which is essential in training a deep network model for semantic segmentation .", "ner": [["few - shot learning", "Task"], ["pixelwise segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [["pixelwise segmentation", "SubTask-Of", "semantic segmentation"]], "rel_plus": [["pixelwise segmentation:Task", "SubTask-Of", "semantic segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Few - Shot Learning Recent research in few - shot classification can be classified into 1 ) learn a good initial condition for the network to be fine - tuned on extremely small training set , as proposed in [ 8 , 2 5 ] ; 2 ) rely on memory properties of RNN , introduced in [ 2 2 , 2 9 ] ; 3 ) learn a metric between few - shot samples and queries , as in [ 2 , 1 0 , 1 8 , 1 6 , 3 3 ] .", "ner": [["Few - Shot Learning", "Task"], ["few - shot classification", "Task"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "In this paper , we simply modify the loss to calculate pixelwise differences between the segmentation ground truth and heatmap .", "ner": [], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "In OSLSM [ 3 0 ] , the authors proposed a two - branch network to solve few - shot segmentation .", "ner": [["OSLSM", "Method"], ["few - shot segmentation", "Task"]], "rel": [["OSLSM", "Used-For", "few - shot segmentation"]], "rel_plus": [["OSLSM:Method", "Used-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Semantic Image Segmentation Previous research exploiting CNN to make dense prediction often relied on patchwise training [ 3 , 6 , 2 3 ] and pre - and post - processing of superpixels [ 6 , 1 1 ] .", "ner": [["Semantic Image Segmentation", "Task"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "In [ 3 1 ] the authors first proposed a simple and elegant fully convolutional network ( FCN ) to solve semantic segmentation .", "ner": [["fully convolutional network", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCN", "Synonym-Of", "fully convolutional network"], ["fully convolutional network", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional network:Method"], ["fully convolutional network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Notably , this is the first work which was trained end - to - end on a fully convolutional network for dense pixel prediction , which showed that the last layer feature maps from a good backbone network such as VGG - 1 6 contain sufficient foreground features which can be decoded by the upsampling network to produce segmentation results .", "ner": [["fully convolutional network", "Method"], ["dense pixel prediction", "Task"], ["VGG - 1 6", "Method"], ["segmentation", "Task"]], "rel": [["fully convolutional network", "Used-For", "dense pixel prediction"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "dense pixel prediction:Task"]]}
{"doc_id": "198967567", "sentence": "Recent few - shot datasets [ 1 8 , 3 5 ] support few - shot classification but there is no large - scale few - shot segmentation dataset .", "ner": [["few - shot classification", "Task"], ["few - shot segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "FSS - 1 0 0 0 targets at solving general objects few - shot segmentation problem .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["objects few - shot segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "objects few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "objects few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Object Classes We first referred to the classes in ILSVRC [ 2 8 ] in our choice of object categories for FSS - 1 0 0 0 .", "ner": [["ILSVRC", "Dataset"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Consequently , FSS - 1 0 0 0 has 5 8 4 classes out of its 1, 0 0 0 classes overlap with the classes in the ILSVRC dataset .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Pixelwise Segmentation Annotation We used Photoshop 's \" quick selection \" tool which allows users to loosely select an object automatically , and refined or corrected the selected area to produce the desired segmentation .", "ner": [["Pixelwise Segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "This section summarizes the three desirable properties of FSS - 1 0 0 0 : Scalability To extend FSS - 1 0 0 0 to include a new class , all it takes are 1 0 images with pixelwise binary segmen - Figure 3 .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "This is significantly easier than other datasets such as PASCAL VOC and COCO .", "ner": [["PASCAL VOC", "Dataset"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Instance FSS - 1 0 0 0 dataset supports instance - level segmentation with instance segmentation labels in 7 5 8 out of the 1, 0 0 0 classes in the dataset , which are significantly more classes than PASCAL VOC and MS COCO .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["instance - level segmentation", "Task"], ["instance segmentation", "Task"], ["PASCAL VOC", "Dataset"], ["MS COCO", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Compare-With", "PASCAL VOC"], ["FSS - 1 0 0 0", "Compare-With", "MS COCO"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Compare-With", "PASCAL VOC:Dataset"], ["FSS - 1 0 0 0:Dataset", "Compare-With", "MS COCO:Dataset"]]}
{"doc_id": "198967567", "sentence": "One major difference between our dataset and PASCAL VOC / MS COCO instance level segmentation is that our dataset only annotates one type of objects in one image , despite there may be other object categories appearing in the background .", "ner": [["PASCAL VOC", "Dataset"], ["MS COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "We annotate at most 1 0 instances in a single image , which follows the same instance annotation principle adopted by COCO .   In few - shot learning , the train - test split is on object categories .", "ner": [["COCO", "Dataset"], ["few - shot learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "In few - shot segmentation , we adopt this notation but extend the query output to be per - pixel classification of the query image , rather than a single class label .", "ner": [["few - shot segmentation", "Task"], ["per - pixel classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "However , a general C - way - K - shot segmentation could be solved by a union of C binary segmentation tasks .", "ner": [["C - way - K - shot segmentation", "Task"], ["C binary segmentation", "Task"]], "rel": [["C binary segmentation", "SubTask-Of", "C - way - K - shot segmentation"]], "rel_plus": [["C binary segmentation:Task", "SubTask-Of", "C - way - K - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "One can design his/her own or choose any popular feature extraction backbone such as VGG - 1 6 [ 3 2 ] , ResNet [ 1 3 ] and Inception [ 3 4 ] as the encoder module inside the network .", "ner": [["feature extraction backbone", "Method"], ["VGG - 1 6", "Method"], ["ResNet", "Method"], ["Inception", "Method"]], "rel": [["VGG - 1 6", "SubClass-Of", "feature extraction backbone"], ["ResNet", "SubClass-Of", "feature extraction backbone"], ["Inception", "SubClass-Of", "feature extraction backbone"]], "rel_plus": [["VGG - 1 6:Method", "SubClass-Of", "feature extraction backbone:Method"], ["ResNet:Method", "SubClass-Of", "feature extraction backbone:Method"], ["Inception:Method", "SubClass-Of", "feature extraction backbone:Method"]]}
{"doc_id": "198967567", "sentence": "ReLU activation is applied throughout the deep network except for the last layer 's activation where Sigmoid is used in order to scale the output to a suitable range to calculate cross - entropy loss .", "ner": [["ReLU", "Method"], ["Sigmoid", "Method"], ["cross - entropy loss", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Method MeanIoU OSLSM - 1 shot [ 3 0 ] 7 0 . 2 9 % OSLSM - 5 shot 7 3 . 0 2 % Guided Network - 1 shot [ 2 4 ] 7 1 . 9 4 % Guided Network - 5 shot 7 4 . 2 7 % Ours - 1 shot 7 3 . 4 7 % Ours - 5 shot 8 0 . 1 2 % Table 4 .", "ner": [["OSLSM - 1 shot", "Method"], ["OSLSM - 5 shot", "Method"], ["Guided Network - 1 shot", "Method"], ["Guided Network - 5 shot", "Method"], ["Ours - 5 shot", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "GN is Guided Network and Ours * is our model trained on FSS - 1 0 0 0 .", "ner": [["GN", "Method"], ["Guided Network", "Method"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["GN", "Synonym-Of", "Guided Network"], ["GN", "Trained-With", "FSS - 1 0 0 0"]], "rel_plus": [["GN:Method", "Synonym-Of", "Guided Network:Method"], ["GN:Method", "Trained-With", "FSS - 1 0 0 0:Dataset"]]}
{"doc_id": "198967567", "sentence": "We evaluate models with the same network architecture but trained on different datasets to show that FSS - 1 0 0 0 is the best choice for few - shot segmentation task .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Finally we illustrate that models trained on FSS - 1 0 0 0 are capable to generalize the few - shot segmentation knowledge to new unseen classes .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Table 2 tabulates the respective performance on VGG - 1 6 , ResNet - 1 0 1 and InceptionNet as backbone , and BCE and MSE as loss function .", "ner": [["VGG - 1 6", "Method"], ["ResNet - 1 0 1", "Method"], ["InceptionNet", "Method"], ["BCE", "Method"], ["MSE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Based on the result , we choose VGG - 1 6 as feature extractor and use BCE loss in our model throughout the experimental section .   We train OSLSM and Guided Network on FSS - 1 0 0 0 to provide benchmarks and justify our dataset .", "ner": [["VGG - 1 6", "Method"], ["feature extractor", "Method"], ["BCE loss", "Method"], ["OSLSM", "Method"], ["Guided Network", "Method"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["VGG - 1 6", "SubClass-Of", "feature extractor"], ["OSLSM", "Trained-With", "FSS - 1 0 0 0"], ["Guided Network", "Trained-With", "FSS - 1 0 0 0"]], "rel_plus": [["VGG - 1 6:Method", "SubClass-Of", "feature extractor:Method"], ["OSLSM:Method", "Trained-With", "FSS - 1 0 0 0:Dataset"], ["Guided Network:Method", "Trained-With", "FSS - 1 0 0 0:Dataset"]]}
{"doc_id": "198967567", "sentence": "Each model ( row ) shows the training stages , e.g. , model I uses the pretrained weights from ImageNet then fine - tuned on fsPASCAL .", "ner": [["ImageNet", "Dataset"], ["fsPASCAL", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Most importantly , we train our network merely on FSS - 1 0 0 0 without fine - tuning on PASCAL - 5 i to avoid any potential overfitting , and this model achieves much better results compared to models trained on PASCAL - 5 i , which justify the effectiveness of FSS - 1 0 0 0 .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["PASCAL - 5 i", "Dataset"], ["PASCAL - 5 i", "Dataset"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "We compare our network model trained on different datasets to demonstrate the effectiveness of FSS - 1 0 0 0 in few - shot segmentation .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["few - shot segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Since there are no publicly available few - shot image segmentation datasets , we convert PASCAL VOC 2 0 1 2 and COCO datasets by setting the desired foreground class label as positive and all others as negative , followed by the identical clean - up stage described in section 3. 1 to the binarized labels .", "ner": [["few - shot image segmentation", "Task"], ["PASCAL VOC 2 0 1 2", "Dataset"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Image results of our baseline model respectively trained on fsPASCAL , fsCOCO and FSS - 1 0 0 0 .", "ner": [["fsPASCAL", "Dataset"], ["fsCOCO", "Dataset"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "The classes in the first two rows are present in fs - PASCAL and fsCOCO whereas the rest are unique in FSS - 1 0 0 0 . thus produced : fsPASCAL and fsCOCO .", "ner": [["fs - PASCAL", "Dataset"], ["fsCOCO", "Dataset"], ["FSS - 1 0 0 0", "Dataset"], ["fsPASCAL", "Dataset"], ["fsCOCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "There are respectively 4, 3 1 8 image and label pairs in 2 0 object classes in fsPASCAL , and 4 8 , 0 1 5 image and label pairs in 8 0 object classes in fsCOCO .", "ner": [["fsPASCAL", "Dataset"], ["fsCOCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "All available data in fsPASCAL and fs - COCO are used in training .", "ner": [["fsPASCAL", "Dataset"], ["fs - COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Using the pre - trained weights from ImageNet , Model III trained on FSS - 1 0 0 0 outperforms respectively the fsPAS - CAL model I and fsCOCO model II by over a large margin of 2 0 % and 1 0 % .", "ner": [["ImageNet", "Dataset"], ["FSS - 1 0 0 0", "Dataset"], ["fsCOCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Interestingly , Model VI pre - trained on COCO and finetuned on FSS - 1 0 0 0 achieves the best result , outperforming the model III pre - trained on ILSRVC .", "ner": [["COCO", "Dataset"], ["FSS - 1 0 0 0", "Dataset"], ["ILSRVC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "We believe this is due to the difference in requirement of feature maps ideal for classification and segmentation task .", "ner": [["classification", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Intuitively , semantic segmentation requires more accurate low - level features to produce fine details in segmentation map , while classification focuses on high - level features for image understanding .", "ner": [["semantic segmentation", "Task"], ["segmentation map", "Task"], ["classification", "Task"], ["image understanding", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "segmentation map"], ["classification", "SubTask-Of", "image understanding"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "segmentation map:Task"], ["classification:Task", "SubTask-Of", "image understanding:Task"]]}
{"doc_id": "198967567", "sentence": "Overall , models respectively trained on fsPASCAL and fsCOCO produce quite good results in object classes that are included in PASCAL and COCO , or similar to PAS - CAL and COCO classes .", "ner": [["fsPASCAL", "Dataset"], ["fsCOCO", "Dataset"], ["PASCAL", "Dataset"], ["COCO", "Dataset"], ["PAS - CAL", "Dataset"], ["COCO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "For these classes , sometimes their segmentation results are better in local details compared to the results produced by models trained on FSS - 1 0 0 0 due to more variations in the support training set .", "ner": [["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "However , they respectively fail in classes significantly different from the 2 0 PASCAL classes and 8 0 COCO classes .", "ner": [["PASCAL", "Dataset"], ["COCO", "Dataset"]], "rel": [["PASCAL", "Compare-With", "COCO"]], "rel_plus": [["PASCAL:Dataset", "Compare-With", "COCO:Dataset"]]}
{"doc_id": "198967567", "sentence": "Figure 8 demonstrates the effect of support set , which shows that scale and pose of the object to be segmented are the most important characteristics to guide few - shot semantic segmentation on FSS - 1 0 0 0 .", "ner": [["few - shot semantic segmentation", "Task"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot semantic segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot semantic segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Since FSS - 1 0 0 0 does not explicitly consider scale variations ( future work ) , a tiny or oversized object in the support set is not a good reference for segmentation .", "ner": [["FSS - 1 0 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Besides , significantly different poses in support and query sets can result in bad segmentation results , due to the intrinsic fragility to rotation in CNN features .", "ner": [["CNN features", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "Table 6 tabulates the tradeoff in time and accuracy for annotating 5 0 0 test images in FSS - 1 0 0 0 by humans ( using Photoshop and GrabCut [ 2 7 ] algorithm ) and our few - shot segmentation .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["GrabCut", "Method"], ["few - shot segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "From top to bottom : android robot ; the river from UC Merced Land Use Dataset [ 3 7 ] ; a large cell image cropped into patches ; herds of sheep ; penguin from Oxford penguin counting dataset [ 1 ] ; flock of wild goose ; different images of fields of sunflower depict various scales in the presence of occlusion and perspective distortion . ple where saliency detection does not work in general .", "ner": [["UC Merced Land Use", "Dataset"], ["Oxford penguin counting", "Dataset"], ["saliency detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "The cell example shows the good potential of FSS - 1 0 0 0 in instance segmentation which significantly contributes to cell counting in medical image analysis where , for instance , a patient 's health directly correlates to his or her red blood cell count .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["instance segmentation", "Task"], ["medical image analysis", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "instance segmentation"], ["instance segmentation", "SubTask-Of", "medical image analysis"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "instance segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "medical image analysis:Task"]]}
{"doc_id": "198967567", "sentence": "With the advance of whole slide images ( WSI ) in which the width and height often exceed 1 0 0 , 0 0 0 pixels ( and thus many cells to count ) , using our few - shot segmentation trained on FSS - 1 0 0 0 , pathologists only need to label 5 image relevant regions and then the rest of the WSI will be automatically labeled .", "ner": [["few - shot segmentation", "Task"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Similarly , the related animal examples of sheep , penguin and wild goose show FSS - 1 0 0 0 's potential for large - scale instance segmentation .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["instance segmentation", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "instance segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "Few - shot learning/segmentation is an emerging attractive alternative , where prediction is made given only a few training examples .", "ner": [["Few - shot learning/segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "198967567", "sentence": "In this paper , we address the limitation of existing large - scale datasets in their biases and lack of scalability , and build the first few - shot segmentation dataset FSS - 1 0 0 0 emphasizing class diversity rather than dataset size .", "ner": [["few - shot segmentation", "Task"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "This baseline few - shot segmentation model , even trained exclusively on FSS - 1 0 0 0 without using pre - trained weights , achieves higher accuracy than previous methods .", "ner": [["few - shot segmentation", "Task"], ["FSS - 1 0 0 0", "Dataset"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "few - shot segmentation"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot segmentation:Task"]]}
{"doc_id": "198967567", "sentence": "We further demonstrated the efficacy and potential of FSS - 1 0 0 0 in large - scale segmentation on totally unseen classes without re - training or fine - tuning , and showed its promise on few - shot instance segmentation and iterative few - shot recognition tasks .", "ner": [["FSS - 1 0 0 0", "Dataset"], ["large - scale segmentation", "Task"], ["few - shot instance segmentation", "Task"], ["iterative few - shot recognition", "Task"]], "rel": [["FSS - 1 0 0 0", "Benchmark-For", "large - scale segmentation"], ["FSS - 1 0 0 0", "Benchmark-For", "few - shot instance segmentation"], ["FSS - 1 0 0 0", "Benchmark-For", "iterative few - shot recognition"]], "rel_plus": [["FSS - 1 0 0 0:Dataset", "Benchmark-For", "large - scale segmentation:Task"], ["FSS - 1 0 0 0:Dataset", "Benchmark-For", "few - shot instance segmentation:Task"], ["FSS - 1 0 0 0:Dataset", "Benchmark-For", "iterative few - shot recognition:Task"]]}
{"doc_id": "201646244", "sentence": "We introduce FinBERT , a language model based on BERT , to tackle NLP tasks in the financial domain .", "ner": [["FinBERT", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["FinBERT", "SubClass-Of", "BERT"], ["FinBERT", "Used-For", "NLP"]], "rel_plus": [["FinBERT:Method", "SubClass-Of", "BERT:Method"], ["FinBERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "201646244", "sentence": "Hence , automated sentiment or polarity analysis of texts produced by financial actors using natural language processing ( NLP ) methods has gained popularity during the last decade [ 4 ] .", "ner": [["natural language processing", "Task"], ["NLP", "Task"]], "rel": [["NLP", "Synonym-Of", "natural language processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "natural language processing:Task"]]}
{"doc_id": "201646244", "sentence": "It requires to address two challenges : 1 ) The most sophisticated classification methods that make use of neural nets require vast amounts of labeled data and labeling financial text snippets requires costly expertise . 2 ) The sentiment analysis models trained on general corpora are not suited to the task , because financial texts have a specialized language with unique vocabulary and have a tendency to use vague expressions instead of easilyidentified negative/positive words .", "ner": [["neural nets", "Method"], ["sentiment analysis", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "NLP transfer learning methods look like a promising solution to both of the challenges mentioned above , and are the focus of this thesis .", "ner": [["NLP transfer learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "For that , sentiment of a sentence from a financial news article towards the financial actor depicted in the sentence will be tried to be predicted , using the Financial PhraseBank created by Malo et al. ( 2 0 1 4 ) [ 1 7 ] and FiQA Task 1 sentiment scoring dataset [ 1 5 ] .", "ner": [["Financial PhraseBank", "Dataset"], ["FiQA Task 1 sentiment scoring", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "The main contributions of this thesis are the following : \u2022 We introduce FinBERT , which is a language model based on BERT for financial NLP tasks .", "ner": [["FinBERT", "Method"], ["BERT", "Method"], ["financial NLP", "Task"]], "rel": [["FinBERT", "SubClass-Of", "BERT"], ["FinBERT", "Used-For", "financial NLP"]], "rel_plus": [["FinBERT:Method", "SubClass-Of", "BERT:Method"], ["FinBERT:Method", "Used-For", "financial NLP:Task"]]}
{"doc_id": "201646244", "sentence": "We evaluate FinBERT on two financial sentiment analysis datasets . \u2022 We achieve the state - of - the - art on FiQA sentiment scoring and Financial PhraseBank . \u2022 We implement two other pre - trained language models , ULMFit and ELMo for financial sentiment analysis and compare these with FinBERT . \u2022 We conduct experiments to investigate several aspects of the model , including : effects of further pre - training on financial corpus , training strategies to prevent catastrophic forgetting and fine - tuning only a small subset of model layers for decreasing training time without a significant drop in performance .", "ner": [["FinBERT", "Method"], ["financial sentiment analysis", "Task"], ["FiQA sentiment scoring", "Dataset"], ["Financial PhraseBank", "Dataset"], ["ULMFit", "Method"], ["ELMo", "Method"], ["financial sentiment analysis", "Task"], ["FinBERT", "Method"]], "rel": [["FinBERT", "Used-For", "financial sentiment analysis"], ["ULMFit", "Used-For", "financial sentiment analysis"], ["ELMo", "Used-For", "financial sentiment analysis"], ["FinBERT", "Used-For", "financial sentiment analysis"], ["ULMFit", "Compare-With", "FinBERT"], ["ELMo", "Compare-With", "FinBERT"]], "rel_plus": [["FinBERT:Method", "Used-For", "financial sentiment analysis:Task"], ["ULMFit:Method", "Used-For", "financial sentiment analysis:Task"], ["ELMo:Method", "Used-For", "financial sentiment analysis:Task"], ["FinBERT:Method", "Used-For", "financial sentiment analysis:Task"], ["ULMFit:Method", "Compare-With", "FinBERT:Method"], ["ELMo:Method", "Compare-With", "FinBERT:Method"]]}
{"doc_id": "201646244", "sentence": "This section describes previous research conducted on sentiment analysis in finance ( 2. 1 ) and text classification using pre - trained language models ( 2. 2 ) .", "ner": [["sentiment analysis", "Task"], ["text classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "Financial sentiment analysis differs from general sentiment analysis not only in domain , but also the purpose .", "ner": [["Financial sentiment analysis", "Task"], ["sentiment analysis", "Task"]], "rel": [["Financial sentiment analysis", "SubTask-Of", "sentiment analysis"]], "rel_plus": [["Financial sentiment analysis:Task", "SubTask-Of", "sentiment analysis:Task"]]}
{"doc_id": "201646244", "sentence": "Sohangir et al. ( 2 0 1 8) [ 2 6 ] apply several generic neural network architectures to a StockTwits dataset , finding CNN as the best performing neural network architecture .", "ner": [["neural network", "Method"], ["StockTwits", "Dataset"], ["CNN", "Method"], ["neural network", "Method"]], "rel": [["neural network", "Used-For", "StockTwits"], ["CNN", "Used-For", "StockTwits"], ["CNN", "SubClass-Of", "neural network"]], "rel_plus": [["neural network:Method", "Used-For", "StockTwits:Dataset"], ["CNN:Method", "Used-For", "StockTwits:Dataset"], ["CNN:Method", "SubClass-Of", "neural network:Method"]]}
{"doc_id": "201646244", "sentence": "Lutz et al. 2 0 1 8 [ 1 3 ] take the approach of using doc 2 vec to generate sentence embeddings in a particular company ad - hoc announcement and utilize multi - instance learning to predict stock market outcomes . [ 1 4 ] use a combination of text simplification and LSTM network to classify a set of sentences from financial news according to their sentiment and achieve stateof - the - art results for the Financial PhraseBank , which is used in thesis as well .", "ner": [["doc 2 vec", "Method"], ["sentence embeddings", "Task"], ["text simplification", "Task"], ["LSTM", "Method"], ["Financial PhraseBank", "Dataset"]], "rel": [["doc 2 vec", "Used-For", "sentence embeddings"], ["LSTM", "Evaluated-With", "Financial PhraseBank"]], "rel_plus": [["doc 2 vec:Method", "Used-For", "sentence embeddings:Task"], ["LSTM:Method", "Evaluated-With", "Financial PhraseBank:Dataset"]]}
{"doc_id": "201646244", "sentence": "One of the most important recent developments in natural language processing is the realization that a model trained for language modeling can be successfully fine - tuned for most down - stream NLP tasks with small modifications .", "ner": [["natural language processing", "Task"], ["NLP", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "Using the pre - trained weights of ELMo , contextualized word embeddings can be calculated for any piece of text .", "ner": [["ELMo", "Method"], ["contextualized word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "Initializing embeddings for down - stream tasks with those were shown to improve performance on most tasks compared to static word embeddings such as word 2 vec or GloVe .", "ner": [["word embeddings", "Method"], ["word 2 vec", "Method"], ["GloVe", "Method"]], "rel": [["word 2 vec", "SubClass-Of", "word embeddings"], ["GloVe", "SubClass-Of", "word embeddings"]], "rel_plus": [["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"], ["GloVe:Method", "SubClass-Of", "word embeddings:Method"]]}
{"doc_id": "201646244", "sentence": "For text classification tasks like SST - 5 , it achieved state - of - the - art performance when used together with a bi - attentive classification network [ 2 0 ] .", "ner": [["text classification", "Task"], ["SST - 5", "Dataset"]], "rel": [["SST - 5", "Benchmark-For", "text classification"]], "rel_plus": [["SST - 5:Dataset", "Benchmark-For", "text classification:Task"]]}
{"doc_id": "201646244", "sentence": "ULMFit ( Universal Language Model Fine - tuning ) [ 5 ] was the first paper to achieve true transfer learning for NLP , as using novel techniques such as discriminative fine - tuning , slanted triangular learning rates and gradual unfreezing .", "ner": [["ULMFit", "Method"], ["Universal Language Model Fine - tuning", "Method"], ["transfer learning", "Task"], ["NLP", "Task"], ["discriminative fine - tuning", "Method"], ["slanted triangular learning rates", "Method"], ["gradual unfreezing", "Method"]], "rel": [["discriminative fine - tuning", "Used-For", "ULMFit"], ["slanted triangular learning rates", "Used-For", "ULMFit"], ["gradual unfreezing", "Used-For", "ULMFit"], ["ULMFit", "Synonym-Of", "Universal Language Model Fine - tuning"], ["transfer learning", "SubTask-Of", "NLP"], ["ULMFit", "Used-For", "NLP"]], "rel_plus": [["discriminative fine - tuning:Method", "Used-For", "ULMFit:Method"], ["slanted triangular learning rates:Method", "Used-For", "ULMFit:Method"], ["gradual unfreezing:Method", "Used-For", "ULMFit:Method"], ["ULMFit:Method", "Synonym-Of", "Universal Language Model Fine - tuning:Method"], ["transfer learning:Task", "SubTask-Of", "NLP:Task"], ["ULMFit:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "201646244", "sentence": "ULMFit 's main idea of efficiently fine - tuning a pre - trained a language model for down - stream tasks was brought to another level with Bidirectional Encoder Representations from Transformers ( BERT ) [ 3 ] , which is also the main focus of this paper .", "ner": [["ULMFit", "Method"], ["Bidirectional Encoder Representations from Transformers", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Synonym-Of", "Bidirectional Encoder Representations from Transformers"]], "rel_plus": [["BERT:Method", "Synonym-Of", "Bidirectional Encoder Representations from Transformers:Method"]]}
{"doc_id": "201646244", "sentence": "These two factors enabled in to achieve state - of - the - art results in multiple NLP tasks such as , natural language inference or question answering .", "ner": [["NLP", "Task"], ["natural language inference", "Task"], ["question answering", "Task"]], "rel": [["natural language inference", "SubTask-Of", "NLP"], ["question answering", "SubTask-Of", "NLP"]], "rel_plus": [["natural language inference:Task", "SubTask-Of", "NLP:Task"], ["question answering:Task", "SubTask-Of", "NLP:Task"]]}
{"doc_id": "201646244", "sentence": "The specifics of fine - tuning BERT for text classification has not been researched thoroughly .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "201646244", "sentence": "They conduct a series of experiments regarding different configurations of BERT for text classification .", "ner": [["BERT", "Method"], ["text classification", "Task"]], "rel": [["BERT", "Used-For", "text classification"]], "rel_plus": [["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "201646244", "sentence": "In this section , we will present our BERT implementation for financial domain named as FinBERT , after giving a brief background on relevant neural architectures . 3. 1 . 1 LSTM .", "ner": [["BERT", "Method"], ["FinBERT", "Method"], ["LSTM", "Method"]], "rel": [["FinBERT", "SubClass-Of", "BERT"]], "rel_plus": [["FinBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "201646244", "sentence": "Long short - term memory ( LSTM ) is a type of recurrent neural network that allows long - term dependencies in a sequence to persist in the network by using \" forget \" and \" update \" gates .", "ner": [["Long short - term memory", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long short - term memory"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long short - term memory:Method"]]}
{"doc_id": "201646244", "sentence": "Since a text is a sequence of tokens , the first choice for any LSTM natural language processing model is determining how to initially represent a single token .", "ner": [["LSTM", "Method"], ["natural language processing", "Task"]], "rel": [["LSTM", "Used-For", "natural language processing"]], "rel_plus": [["LSTM:Method", "Used-For", "natural language processing:Task"]]}
{"doc_id": "201646244", "sentence": "One such pre - training algorithm is GLoVe ( Global Vectors for Word Representation ) [ 2 2 ] .", "ner": [["GLoVe", "Method"], ["Global Vectors for Word Representation", "Method"]], "rel": [["GLoVe", "Synonym-Of", "Global Vectors for Word Representation"]], "rel_plus": [["GLoVe:Method", "Synonym-Of", "Global Vectors for Word Representation:Method"]]}
{"doc_id": "201646244", "sentence": "In the center of ELMo , there is a bidirectional language model with multiple LSTM layers .", "ner": [["ELMo", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "ELMo"]], "rel_plus": [["LSTM:Method", "Part-Of", "ELMo:Method"]]}
{"doc_id": "201646244", "sentence": "Once the contextualized representations are extracted , these can be used to initialize any down - stream NLP task 2 . 3. 1 . 3 ULMFit .", "ner": [["NLP", "Task"], ["ULMFit", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "ULMFit is a transfer learning model for down - stream NLP tasks , that make use of language model pre - training [ 5 ] .", "ner": [["ULMFit", "Method"], ["transfer learning", "Task"], ["NLP", "Task"]], "rel": [["ULMFit", "Used-For", "transfer learning"], ["ULMFit", "Used-For", "NLP"]], "rel_plus": [["ULMFit:Method", "Used-For", "transfer learning:Task"], ["ULMFit:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "201646244", "sentence": "Unlike ELMo , with ULMFit , the whole language model is fine - tuned together with the task - specific layers .", "ner": [["ELMo", "Method"], ["ULMFit", "Method"]], "rel": [["ELMo", "Compare-With", "ULMFit"]], "rel_plus": [["ELMo:Method", "Compare-With", "ULMFit:Method"]]}
{"doc_id": "201646244", "sentence": "The underlying language model used in ULMFit is AWD - LSTM , which uses sophisticated dropout tuning strategies to better regularize its LSTM model [ 2 1 ] .", "ner": [["ULMFit", "Method"], ["AWD - LSTM", "Method"], ["dropout tuning strategies", "Method"], ["LSTM", "Method"]], "rel": [["AWD - LSTM", "Part-Of", "ULMFit"], ["dropout tuning strategies", "Used-For", "AWD - LSTM"], ["LSTM", "Part-Of", "AWD - LSTM"]], "rel_plus": [["AWD - LSTM:Method", "Part-Of", "ULMFit:Method"], ["dropout tuning strategies:Method", "Used-For", "AWD - LSTM:Method"], ["LSTM:Method", "Part-Of", "AWD - LSTM:Method"]]}
{"doc_id": "201646244", "sentence": "For classification using ULMFit two linear layers are added to the pre - trained AWD - LSTM , first of which takes the pooled last hidden states as input .", "ner": [["classification", "Task"], ["ULMFit", "Method"], ["linear layers", "Method"], ["AWD - LSTM", "Method"]], "rel": [["ULMFit", "Used-For", "classification"], ["linear layers", "Part-Of", "AWD - LSTM"]], "rel_plus": [["ULMFit:Method", "Used-For", "classification:Task"], ["linear layers:Method", "Part-Of", "AWD - LSTM:Method"]]}
{"doc_id": "201646244", "sentence": "We implement these strategies with FinBERT as explained in section 3. 2 . 1 The pre - trained weights for GLoVE can be found here : https://nlp.stanford.edu/projects/glove/ 2 The pre - trained ELMo models can be found here : https://allennlp.org/elmo 3. 1 . 4 Transformer .", "ner": [["FinBERT", "Method"], ["GLoVE", "Method"], ["ELMo", "Method"], ["Transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "The Transformer is an attention - based architecture for modeling sequential information , that is an alternative to recurrent neural networks [ 2 9 ] .", "ner": [["Transformer", "Method"], ["recurrent neural networks", "Method"]], "rel": [["Transformer", "SubClass-Of", "recurrent neural networks"]], "rel_plus": [["Transformer:Method", "SubClass-Of", "recurrent neural networks:Method"]]}
{"doc_id": "201646244", "sentence": "As it was argued by Vaswani 2 0 1 7 [ 2 9 ] , Transformer architecture has several advantages over the RNN - based approaches .", "ner": [["Transformer", "Method"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "Because of RNNs ' sequential nature , they are much harder to parallelize on GPUs and too many steps between far away elements in a sequence make it hard for information to persist . 3. 1 . 5 BERT .", "ner": [["RNNs", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "BERT [ 3 ] is in essence a language model that consists of a set of Transformer encoders stacked on top of each other .", "ner": [["BERT", "Method"], ["Transformer encoders", "Method"]], "rel": [["Transformer encoders", "Part-Of", "BERT"]], "rel_plus": [["Transformer encoders:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "201646244", "sentence": "However it defines the language modeling task differently from ELMo and AWD - LSTM .", "ner": [["ELMo", "Method"], ["AWD - LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "A second task BERT is trained on is \" next sentence prediction \" .", "ner": [["BERT", "Method"], ["next sentence prediction", "Task"]], "rel": [["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "201646244", "sentence": "For all classification tasks , including the next sentence prediction , [ CLS ] token is used .", "ner": [["classification", "Task"], ["next sentence prediction", "Task"]], "rel": [["next sentence prediction", "SubTask-Of", "classification"]], "rel_plus": [["next sentence prediction:Task", "SubTask-Of", "classification:Task"]]}
{"doc_id": "201646244", "sentence": "BERT has two versions : BERT - base , with 1 2 encoder layers , hidden size of 7 6 8 , 1 2 multi - head attention heads and 1 1 0 M parameters in total and BERT - large , with 2 4 encoder layers , hidden size of 1 0 2 4 , 1 6 multi - head attention heads and 3 4 0 M parameters .", "ner": [["BERT", "Method"], ["BERT - base", "Method"], ["multi - head attention heads", "Method"], ["BERT - large", "Method"], ["multi - head attention", "Method"]], "rel": [["BERT - base", "SubClass-Of", "BERT"], ["BERT - large", "SubClass-Of", "BERT"], ["multi - head attention heads", "Part-Of", "BERT - base"], ["multi - head attention", "Part-Of", "BERT - large"]], "rel_plus": [["BERT - base:Method", "SubClass-Of", "BERT:Method"], ["BERT - large:Method", "SubClass-Of", "BERT:Method"], ["multi - head attention heads:Method", "Part-Of", "BERT - base:Method"], ["multi - head attention:Method", "Part-Of", "BERT - large:Method"]]}
{"doc_id": "201646244", "sentence": "Both of these models have been trained on BookCorpus [ 3 3 ] and English Wikipedia , which have in total more than 3, 5 0 0 M words 3 .", "ner": [["BookCorpus", "Dataset"], ["English Wikipedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "In this subsection we will describe our implementation of BERT : 1 ) how further pre - training on domain corpus is done , 2 - 3 ) how we implemented BERT for classification and regression tasks , 4 ) training strategies we used during fine - tuning to prevent catastrophic forgetting .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["classification", "Task"], ["regression", "Task"]], "rel": [["BERT", "Used-For", "classification"], ["BERT", "Used-For", "regression"]], "rel_plus": [["BERT:Method", "Used-For", "classification:Task"], ["BERT:Method", "Used-For", "regression:Task"]]}
{"doc_id": "201646244", "sentence": "FinBERT for text classification .", "ner": [["FinBERT", "Method"], ["text classification", "Task"]], "rel": [["FinBERT", "Used-For", "text classification"]], "rel_plus": [["FinBERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "201646244", "sentence": "This is the recommended practice for using BERT for any classification task [ 3 ] .", "ner": [["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "In order to deal with this phenomenon , we apply three techniques as it was proposed by Howard and Ruder ( 2 0 1 8) : slanted triangular learning rates , discriminative fine - tuning and gradual unfreezing .", "ner": [["slanted triangular learning rates", "Method"], ["discriminative fine - tuning", "Method"], ["gradual unfreezing", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "We aim to answer the following research questions : ( RQ 1 ) What is the performance of FinBERT in short sentence classification compared with the other transfer learning methods like ELMo and ULMFit ? [ 1 5 ] is a dataset that was created for WWW ' 1 8 conference financial opinion mining and question answering challenge 6 .", "ner": [["FinBERT", "Method"], ["transfer learning", "Task"], ["ELMo", "Method"], ["ULMFit", "Method"]], "rel": [["FinBERT", "Compare-With", "ELMo"], ["FinBERT", "Compare-With", "ULMFit"]], "rel_plus": [["FinBERT:Method", "Compare-With", "ELMo:Method"], ["FinBERT:Method", "Compare-With", "ULMFit:Method"]]}
{"doc_id": "201646244", "sentence": "For contrastive experiments , we consider baselines with three different methods : LSTM classifier with GLoVe embeddings , LSTM classifier with ELMo embeddings and ULMFit classifier .", "ner": [["LSTM", "Method"], ["GLoVe embeddings", "Method"], ["LSTM", "Method"], ["ELMo embeddings", "Method"], ["ULMFit", "Method"]], "rel": [["GLoVe embeddings", "Part-Of", "LSTM"], ["ELMo embeddings", "Part-Of", "LSTM"]], "rel_plus": [["GLoVe embeddings:Method", "Part-Of", "LSTM:Method"], ["ELMo embeddings:Method", "Part-Of", "LSTM:Method"]]}
{"doc_id": "201646244", "sentence": "The difference between two models is that one uses GLoVe embeddings , while the other uses ELMo embeddings .", "ner": [["GLoVe embeddings", "Method"], ["ELMo embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "As it was explained in section 3. 1 . 3 , classification with ULMFit consists of three steps .", "ner": [["classification", "Task"], ["ULMFit", "Method"]], "rel": [["ULMFit", "Used-For", "classification"]], "rel_plus": [["ULMFit:Method", "Used-For", "classification:Task"]]}
{"doc_id": "201646244", "sentence": "We first further pre - train AWD - LSTM language model on TRC 2 - financial corpus for 3 epochs .", "ner": [["AWD - LSTM", "Method"], ["TRC 2 - financial corpus", "Dataset"]], "rel": [["AWD - LSTM", "Trained-With", "TRC 2 - financial corpus"]], "rel_plus": [["AWD - LSTM:Method", "Trained-With", "TRC 2 - financial corpus:Dataset"]]}
{"doc_id": "201646244", "sentence": "After that , we fine - tune the model for classification on Financial 6 Data can be found here : https://sites.google.com/view/fiqa/home PhraseBank dataset , by adding a fully - connected layer to the output of pre - trained language model .", "ner": [["PhraseBank", "Dataset"], ["fully - connected layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "Since our data , Financial PhraseBank suffers from label imbalance ( almost 6 0 % of all sentences are neutral ) , this gives another good measure of the classification performance .", "ner": [["Financial PhraseBank", "Dataset"], ["classification", "Task"]], "rel": [["Financial PhraseBank", "Benchmark-For", "classification"]], "rel_plus": [["Financial PhraseBank:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "201646244", "sentence": "For our implementation BERT , we use a dropout probability of p = 0. 1 , warm - up proportion of 0. 2 , maximum sequence length of 6 4 tokens , a learning rate of 2e \u2212 5 and a mini - batch size of 6 4 .", "ner": [["BERT", "Method"], ["dropout", "Method"], ["warm - up", "Method"]], "rel": [["dropout", "Used-For", "BERT"], ["warm - up", "Used-For", "BERT"]], "rel_plus": [["dropout:Method", "Used-For", "BERT:Method"], ["warm - up:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "201646244", "sentence": "The results of FinBERT , the baseline methods and state - of - the - art on Financial PhraseBank dataset classification task can be seen on table 2 .", "ner": [["FinBERT", "Method"], ["Financial PhraseBank", "Dataset"], ["classification", "Task"]], "rel": [["FinBERT", "Evaluated-With", "Financial PhraseBank"], ["FinBERT", "Used-For", "classification"], ["Financial PhraseBank", "Benchmark-For", "classification"]], "rel_plus": [["FinBERT:Method", "Evaluated-With", "Financial PhraseBank:Dataset"], ["FinBERT:Method", "Used-For", "classification:Task"], ["Financial PhraseBank:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "201646244", "sentence": "For all of the measured metrics , FinBERT performs clearly the best among both the methods we implemented ourselves ( LSTM and ULMFit ) and the models reported by other papers ( LPS [ 1 7 ] , HSC [ 8 ] , FinSSLX [ 1 4 ] ) .", "ner": [["FinBERT", "Method"], ["LSTM", "Method"], ["ULMFit", "Method"], ["LPS", "Method"], ["HSC", "Method"], ["FinSSLX", "Method"]], "rel": [["FinBERT", "Compare-With", "LSTM"], ["FinBERT", "Compare-With", "ULMFit"], ["FinBERT", "Compare-With", "LPS"]], "rel_plus": [["FinBERT:Method", "Compare-With", "LSTM:Method"], ["FinBERT:Method", "Compare-With", "ULMFit:Method"], ["FinBERT:Method", "Compare-With", "LPS:Method"]]}
{"doc_id": "201646244", "sentence": "In terms of accuracy , it is close to LPS and HSC , ( even better than LPS for examples with full agreement ) , however it produces a low F 1 - score .", "ner": [["LPS", "Method"], ["HSC", "Method"], ["LPS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "LSTM classifier with ELMo embeddings improves upon LSTM with static embeddings in all of the measured metrics .", "ner": [["LSTM classifier with ELMo embeddings", "Method"], ["LSTM", "Method"]], "rel": [["LSTM classifier with ELMo embeddings", "Compare-With", "LSTM"]], "rel_plus": [["LSTM classifier with ELMo embeddings:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "201646244", "sentence": "But it 's performance is comparable with LPS and HSC , besting them in accuracy .", "ner": [["LPS", "Method"], ["HSC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "It also handily beats the machine learning based models LPS and HSC .", "ner": [["machine learning based models", "Method"], ["LPS", "Method"], ["HSC", "Method"]], "rel": [["LPS", "SubClass-Of", "machine learning based models"], ["HSC", "SubClass-Of", "machine learning based models"]], "rel_plus": [["LPS:Method", "SubClass-Of", "machine learning based models:Method"], ["HSC:Method", "SubClass-Of", "machine learning based models:Method"]]}
{"doc_id": "201646244", "sentence": "ULMFit also outperforms FinSSLX , which has a text simplification step as well as pre - training of word embeddings on a large financial corpus with sentiment labels .", "ner": [["ULMFit", "Method"], ["FinSSLX", "Method"], ["text simplification", "Task"], ["word embeddings", "Method"]], "rel": [["ULMFit", "Compare-With", "FinSSLX"], ["text simplification", "Used-For", "FinSSLX"], ["word embeddings", "Used-For", "FinSSLX"]], "rel_plus": [["ULMFit:Method", "Compare-With", "FinSSLX:Method"], ["text simplification:Task", "Used-For", "FinSSLX:Method"], ["word embeddings:Method", "Used-For", "FinSSLX:Method"]]}
{"doc_id": "201646244", "sentence": "FinBERT outperforms ULMFit , and consequently all of the other methods in all metrics .", "ner": [["FinBERT", "Method"], ["ULMFit", "Method"]], "rel": [["FinBERT", "Compare-With", "ULMFit"]], "rel_plus": [["FinBERT:Method", "Compare-With", "ULMFit:Method"]]}
{"doc_id": "201646244", "sentence": "In order to measure the performance of the models on different sizes of labeled training datasets , we ran LSTM classifiers , ULMFit and FinBERT on 5 different configurations .", "ner": [["LSTM", "Method"], ["ULMFit", "Method"], ["FinBERT", "Method"]], "rel": [["LSTM", "Compare-With", "ULMFit"], ["LSTM", "Compare-With", "FinBERT"], ["ULMFit", "Compare-With", "FinBERT"]], "rel_plus": [["LSTM:Method", "Compare-With", "ULMFit:Method"], ["LSTM:Method", "Compare-With", "FinBERT:Method"], ["ULMFit:Method", "Compare-With", "FinBERT:Method"]]}
{"doc_id": "201646244", "sentence": "However , once the training size becomes 2 5 0 , ULMFit and FinBERT starts to successfully differentiate between labels , with an accuracy as high as 8 0 % for FinBERT .", "ner": [["ULMFit", "Method"], ["FinBERT", "Method"], ["FinBERT", "Method"]], "rel": [["ULMFit", "Compare-With", "FinBERT"], ["FinBERT", "Compare-With", "FinBERT"]], "rel_plus": [["ULMFit:Method", "Compare-With", "FinBERT:Method"], ["FinBERT:Method", "Compare-With", "FinBERT:Method"]]}
{"doc_id": "201646244", "sentence": "All of the methods consistently get better with more data , but ULMFit and FinBERT does better with 2 5 0 examples than LSTM classifiers do with the whole dataset .", "ner": [["ULMFit", "Method"], ["FinBERT", "Method"], ["LSTM", "Method"]], "rel": [["ULMFit", "Compare-With", "LSTM"], ["FinBERT", "Compare-With", "LSTM"]], "rel_plus": [["ULMFit:Method", "Compare-With", "LSTM:Method"], ["FinBERT:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "201646244", "sentence": "We compare three models : 1 ) No further pre - training ( denoted by Vanilla BERT ) , 2 ) Further pre - training on classification training set ( denoted by FinBERT - task ) , 3 ) Further pre - training on domain corpus , TRC 2 - financial ( denoted by FinBERT - domain ) .", "ner": [["Vanilla BERT", "Method"], ["FinBERT - task", "Method"], ["TRC 2 - financial", "Dataset"], ["FinBERT - domain", "Method"]], "rel": [["FinBERT - domain", "Trained-With", "TRC 2 - financial"]], "rel_plus": [["FinBERT - domain:Method", "Trained-With", "TRC 2 - financial:Dataset"]]}
{"doc_id": "201646244", "sentence": "We think that the last explanation is the likeliest , because for the subset of Financial Phrasebank that all of the annotators agree on the result , accuracy of Vanilla BERT is already 0. 9 6 .", "ner": [["Financial Phrasebank", "Dataset"], ["Vanilla BERT", "Method"]], "rel": [["Vanilla BERT", "Evaluated-With", "Financial Phrasebank"]], "rel_plus": [["Vanilla BERT:Method", "Evaluated-With", "Financial Phrasebank:Dataset"]]}
{"doc_id": "201646244", "sentence": "For measuring the performance of the techniques against catastrophic forgetting , we try four different settings : No adjustment ( NA ) , only with slanted triangular learning rate ( STL ) , slanted triangular learning rate and gradual unfreezing ( STL+GU ) and the techniques in the previous one , together with discriminative finetuning .", "ner": [["slanted triangular learning rate", "Method"], ["STL", "Method"], ["slanted triangular learning rate and gradual unfreezing", "Method"], ["STL+GU", "Method"], ["discriminative finetuning", "Method"]], "rel": [["STL", "Synonym-Of", "slanted triangular learning rate"], ["STL+GU", "Synonym-Of", "slanted triangular learning rate and gradual unfreezing"]], "rel_plus": [["STL:Method", "Synonym-Of", "slanted triangular learning rate:Method"], ["STL+GU:Method", "Synonym-Of", "slanted triangular learning rate and gradual unfreezing:Method"]]}
{"doc_id": "201646244", "sentence": "We see from table 5 that using only discriminative fine - tuning with slanted triangular learning rates performs worse than using the slanted triangular learning rates alone .", "ner": [["discriminative fine - tuning with slanted triangular learning rates", "Method"], ["slanted triangular learning rates", "Method"]], "rel": [["discriminative fine - tuning with slanted triangular learning rates", "Compare-With", "slanted triangular learning rates"]], "rel_plus": [["discriminative fine - tuning with slanted triangular learning rates:Method", "Compare-With", "slanted triangular learning rates:Method"]]}
{"doc_id": "201646244", "sentence": "BERT has 1 2 Transformer encoder layers .", "ner": [["BERT", "Method"], ["Transformer encoder", "Method"]], "rel": [["Transformer encoder", "Part-Of", "BERT"]], "rel_plus": [["Transformer encoder:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "201646244", "sentence": "In this paper , we implemented BERT for the financial domain by further pre - training it on a financial corpus and fine - tuning it for sentiment analysis ( FinBERT ) .", "ner": [["BERT", "Method"], ["sentiment analysis", "Task"], ["FinBERT", "Method"]], "rel": [["FinBERT", "SubClass-Of", "BERT"], ["BERT", "Used-For", "sentiment analysis"], ["FinBERT", "Used-For", "sentiment analysis"]], "rel_plus": [["FinBERT:Method", "SubClass-Of", "BERT:Method"], ["BERT:Method", "Used-For", "sentiment analysis:Task"], ["FinBERT:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "201646244", "sentence": "In addition to BERT , we also implemented other pre - training language models like ELMo and ULMFit for comparison purposes .", "ner": [["BERT", "Method"], ["ELMo", "Method"], ["ULMFit", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201646244", "sentence": "ULMFit , further pre - trained on a financial corpus , beat the previous state - of - the art for the classification task , only to a smaller degree than BERT .", "ner": [["ULMFit", "Method"], ["classification", "Task"], ["BERT", "Method"]], "rel": [["ULMFit", "Used-For", "classification"], ["ULMFit", "Compare-With", "BERT"]], "rel_plus": [["ULMFit:Method", "Used-For", "classification:Task"], ["ULMFit:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "201646244", "sentence": "Another possible extension can be using FinBERT for other natural language processing tasks such as named entity recognition or question answering in financial domain .", "ner": [["FinBERT", "Method"], ["natural language processing", "Task"], ["named entity recognition", "Task"], ["question answering", "Task"]], "rel": [["FinBERT", "Used-For", "natural language processing"], ["FinBERT", "Used-For", "named entity recognition"], ["FinBERT", "Used-For", "question answering"]], "rel_plus": [["FinBERT:Method", "Used-For", "natural language processing:Task"], ["FinBERT:Method", "Used-For", "named entity recognition:Task"], ["FinBERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "201646244", "sentence": "I am grateful to NIST , for sharing Reuters TRC - 2 corpus with me and to Malo et al. for making the excellent Financial PhraseBank publicly available .", "ner": [["Reuters TRC - 2", "Dataset"], ["Financial PhraseBank", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Beyond single - image activity recognition , a new temporal feature extractor was proposed to achieve video - based activity recognition [ 1 6 ] .", "ner": [["single - image activity recognition", "Task"], ["temporal feature extractor", "Method"], ["video - based activity recognition", "Task"]], "rel": [["temporal feature extractor", "Used-For", "video - based activity recognition"]], "rel_plus": [["temporal feature extractor:Method", "Used-For", "video - based activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "Existing research simply modifies single - activity recognition models with a sigmoid output layer or uses multiple single - activity recognizers for concurrent activity recognition ; these have failed to achieve satisfactory performance [ 1 8 ] .", "ner": [["single - activity recognition models", "Method"], ["sigmoid", "Method"], ["multiple single - activity recognizers", "Method"], ["concurrent activity recognition", "Task"]], "rel": [["sigmoid", "Part-Of", "single - activity recognition models"], ["multiple single - activity recognizers", "Used-For", "concurrent activity recognition"], ["single - activity recognition models", "Used-For", "concurrent activity recognition"]], "rel_plus": [["sigmoid:Method", "Part-Of", "single - activity recognition models:Method"], ["multiple single - activity recognizers:Method", "Used-For", "concurrent activity recognition:Task"], ["single - activity recognition models:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "Firstly , single - activity recognition focuses on selecting the most representative features associated with a certain activity ; many attentionbased methods were proposed to improve this feature extraction .", "ner": [["single - activity recognition", "Task"], ["feature extraction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Lastly , both types of activity recognition use probabilistic inference ( softmax ) for decision making .", "ner": [["activity recognition", "Task"], ["probabilistic inference", "Method"], ["softmax", "Method"], ["decision making", "Task"]], "rel": [["probabilistic inference", "Used-For", "activity recognition"], ["softmax", "SubClass-Of", "probabilistic inference"], ["activity recognition", "Used-For", "decision making"]], "rel_plus": [["probabilistic inference:Method", "Used-For", "activity recognition:Task"], ["softmax:Method", "SubClass-Of", "probabilistic inference:Method"], ["activity recognition:Task", "Used-For", "decision making:Task"]]}
{"doc_id": "54457603", "sentence": "However , simply applying softmax with a threshold or using a parallel softmax layer for concurrent activity recognition would ignore the associations between activities .", "ner": [["softmax with a threshold", "Method"], ["parallel softmax", "Method"], ["concurrent activity recognition", "Task"]], "rel": [["parallel softmax", "Used-For", "concurrent activity recognition"], ["softmax with a threshold", "Used-For", "concurrent activity recognition"]], "rel_plus": [["parallel softmax:Method", "Used-For", "concurrent activity recognition:Task"], ["softmax with a threshold:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "To address these challenges , we introduce a concurrent activity recognition model with a feature - to - activity attention for feature extraction and a tri - axial self - attention encoder - decoder for multi - label prediction .", "ner": [["concurrent activity recognition model", "Method"], ["feature - to - activity attention", "Method"], ["feature extraction", "Task"], ["tri - axial self - attention encoder - decoder", "Method"], ["multi - label prediction", "Task"]], "rel": [["feature - to - activity attention", "Part-Of", "concurrent activity recognition model"], ["concurrent activity recognition model", "Used-For", "feature extraction"], ["tri - axial self - attention encoder - decoder", "Used-For", "multi - label prediction"]], "rel_plus": [["feature - to - activity attention:Method", "Part-Of", "concurrent activity recognition model:Method"], ["concurrent activity recognition model:Method", "Used-For", "feature extraction:Task"], ["tri - axial self - attention encoder - decoder:Method", "Used-For", "multi - label prediction:Task"]]}
{"doc_id": "54457603", "sentence": "The proposed feature - to - activity attention maintains unaggregated temporal information to preserve temporal ordering ; different activities may occur at different times and require different temporal attentions , which would be Tri - axial Self - Attention for Concurrent Activity Recognition An overview of proposed concurrent activity recognition system that generates separated spatial - temporal features for independent activities . ignored by simple aggregation .", "ner": [["feature - to - activity attention", "Method"], ["temporal attentions", "Method"], ["Tri - axial Self - Attention", "Method"], ["Concurrent Activity Recognition", "Task"], ["proposed concurrent activity recognition system", "Method"], ["generates separated spatial - temporal features", "Task"]], "rel": [["Tri - axial Self - Attention", "SubClass-Of", "temporal attentions"], ["Tri - axial Self - Attention", "Used-For", "Concurrent Activity Recognition"], ["proposed concurrent activity recognition system", "Used-For", "generates separated spatial - temporal features"]], "rel_plus": [["Tri - axial Self - Attention:Method", "SubClass-Of", "temporal attentions:Method"], ["Tri - axial Self - Attention:Method", "Used-For", "Concurrent Activity Recognition:Task"], ["proposed concurrent activity recognition system:Method", "Used-For", "generates separated spatial - temporal features:Task"]]}
{"doc_id": "54457603", "sentence": "We tested our system with published datasets : the hockey dataset [ 5 ] ( 1 2 activities ) , the volleyball dataset [ 1 5 ] ( 9 activities ) , and the charades dataset [ 2 5 ] ( 1 5 7 activities ) .", "ner": [["hockey", "Dataset"], ["volleyball", "Dataset"], ["charades", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Our contributions are : \u2022 A three level feature extraction strategy with a novel feature - to - activity attention for concurrent activity recognition . \u2022 A triaxial self - attention that learns independent temporal associations for different activities and makes concurrent activity predictions while being aware of possible activity combinations . \u2022 An activity association mask that helps the self - attentionbased decoder better capture the associations between activities .", "ner": [["feature extraction strategy", "Method"], ["feature - to - activity attention", "Method"], ["concurrent activity recognition", "Task"], ["triaxial self - attention", "Method"], ["self - attentionbased decoder", "Method"]], "rel": [["feature - to - activity attention", "Part-Of", "feature extraction strategy"], ["feature extraction strategy", "Used-For", "concurrent activity recognition"]], "rel_plus": [["feature - to - activity attention:Method", "Part-Of", "feature extraction strategy:Method"], ["feature extraction strategy:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "Traditional research was mainly based on hand - crafted features such as Histogram of Oriented Gradients ( HOG ) and Histogram of Optical Flow ( HOF ) [ 8 ] .", "ner": [["Histogram of Oriented Gradients", "Method"], ["HOG", "Method"], ["Histogram of Optical Flow", "Method"], ["HOF", "Method"]], "rel": [["HOG", "Synonym-Of", "Histogram of Oriented Gradients"], ["HOF", "Synonym-Of", "Histogram of Optical Flow"]], "rel_plus": [["HOG:Method", "Synonym-Of", "Histogram of Oriented Gradients:Method"], ["HOF:Method", "Synonym-Of", "Histogram of Optical Flow:Method"]]}
{"doc_id": "54457603", "sentence": "Recently , deep learning has been commonly applied to activity recognition , initially using image recognition approaches with temporal feature fusion [ 1 6 ] .", "ner": [["deep learning", "Method"], ["activity recognition", "Task"], ["image recognition approaches", "Method"], ["temporal feature fusion", "Method"]], "rel": [["deep learning", "Used-For", "activity recognition"], ["image recognition approaches", "Used-For", "activity recognition"], ["temporal feature fusion", "Used-For", "image recognition approaches"]], "rel_plus": [["deep learning:Method", "Used-For", "activity recognition:Task"], ["image recognition approaches:Method", "Used-For", "activity recognition:Task"], ["temporal feature fusion:Method", "Used-For", "image recognition approaches:Method"]]}
{"doc_id": "54457603", "sentence": "However , 2D CNNs originally used for image classification do not properly model spatio - temporal associations , which are critical for video - based activity recognition .", "ner": [["2D CNNs", "Method"], ["image classification", "Task"], ["video - based activity recognition", "Task"]], "rel": [["2D CNNs", "Used-For", "image classification"]], "rel_plus": [["2D CNNs:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "54457603", "sentence": "CNN - LSTM strategies benefited from LSTM temporal association modeling .", "ner": [["CNN - LSTM", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "CNN - LSTM"]], "rel_plus": [["LSTM:Method", "Part-Of", "CNN - LSTM:Method"]]}
{"doc_id": "54457603", "sentence": "Spatial attention ( or region - based methods ) as well as temporal attention were proposed to help networks better focus on activity - associated features [ 2 4 ] [ 1 9 ] .", "ner": [["Spatial attention", "Method"], ["region - based methods", "Method"], ["temporal attention", "Method"], ["activity - associated features", "Task"]], "rel": [["Spatial attention", "SubClass-Of", "region - based methods"], ["Spatial attention", "Used-For", "activity - associated features"], ["temporal attention", "Used-For", "activity - associated features"]], "rel_plus": [["Spatial attention:Method", "SubClass-Of", "region - based methods:Method"], ["Spatial attention:Method", "Used-For", "activity - associated features:Task"], ["temporal attention:Method", "Used-For", "activity - associated features:Task"]]}
{"doc_id": "54457603", "sentence": "The iDT [ 3 1 ] and TDD [ 3 4 ] works showed that manually - crafted ( as opposed to learned ) spatio - temporal descriptors can achieve good activity recognition performance .", "ner": [["iDT", "Method"], ["TDD", "Method"], ["activity recognition", "Task"]], "rel": [["TDD", "Used-For", "activity recognition"], ["iDT", "Used-For", "activity recognition"]], "rel_plus": [["TDD:Method", "Used-For", "activity recognition:Task"], ["iDT:Method", "Used-For", "activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "Most existing approaches simply modify single - activity recognition strategies for concurrent activity recognition .", "ner": [["single - activity recognition strategies", "Method"], ["concurrent activity recognition", "Task"]], "rel": [["single - activity recognition strategies", "Used-For", "concurrent activity recognition"]], "rel_plus": [["single - activity recognition strategies:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "A CNN - LSTM structure was tested for concurrent activity recognition with limited success [ 1 8 ] .", "ner": [["CNN - LSTM", "Method"], ["concurrent activity recognition", "Task"]], "rel": [["CNN - LSTM", "Used-For", "concurrent activity recognition"]], "rel_plus": [["CNN - LSTM:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "More effective feature extraction frameworks were proposed in recent years , including a multi - stream network [ 1 4 ] , a 3D convolution network [ 7 ] [ 3 7 ] , and a non - local neural network [ 3 2 ] .", "ner": [["feature extraction frameworks", "Method"], ["multi - stream network", "Method"], ["3D convolution network", "Method"], ["non - local neural network", "Method"]], "rel": [["multi - stream network", "SubClass-Of", "feature extraction frameworks"], ["3D convolution network", "SubClass-Of", "feature extraction frameworks"], ["non - local neural network", "SubClass-Of", "feature extraction frameworks"]], "rel_plus": [["multi - stream network:Method", "SubClass-Of", "feature extraction frameworks:Method"], ["3D convolution network:Method", "SubClass-Of", "feature extraction frameworks:Method"], ["non - local neural network:Method", "SubClass-Of", "feature extraction frameworks:Method"]]}
{"doc_id": "54457603", "sentence": "The features of visual objects are first extracted by the pre - trained VGG net on the ImageNet dataset .", "ner": [["VGG", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG", "Trained-With", "ImageNet"]], "rel_plus": [["VGG:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "54457603", "sentence": "This is because unlike single - activity recognition , multiple activities might be distributed through the entire video clip ; the unspecific temporal attention will not be able to highlight the activityspecific features in time .", "ner": [["single - activity recognition", "Task"], ["unspecific temporal attention", "Method"], ["activityspecific features", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Previous research modeled temporal features with a 3D ConvNet , LSTMs with attention , or 3D descriptors [ 1 8 ] [ 7 ] .", "ner": [["3D ConvNet", "Method"], ["LSTMs", "Method"], ["attention", "Method"], ["3D descriptors", "Method"]], "rel": [["3D descriptors", "Part-Of", "3D ConvNet"], ["attention", "Part-Of", "3D ConvNet"], ["3D descriptors", "Part-Of", "LSTMs"], ["attention", "Part-Of", "LSTMs"]], "rel_plus": [["3D descriptors:Method", "Part-Of", "3D ConvNet:Method"], ["attention:Method", "Part-Of", "3D ConvNet:Method"], ["3D descriptors:Method", "Part-Of", "LSTMs:Method"], ["attention:Method", "Part-Of", "LSTMs:Method"]]}
{"doc_id": "54457603", "sentence": "Additionally , many previous works simply used the top N results or a sigmoid layer for concurrent activity recognition , which ignores the possible associations between activities .", "ner": [["sigmoid", "Method"], ["concurrent activity recognition", "Task"]], "rel": [["sigmoid", "Used-For", "concurrent activity recognition"]], "rel_plus": [["sigmoid:Method", "Used-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "Previous decoder based on fully - connected layers with sigmoid [ 1 0 ] [ 7 ] ignored the feature associations between different activities .", "ner": [["decoder", "Method"], ["fully - connected layers", "Method"], ["sigmoid", "Method"]], "rel": [["fully - connected layers", "Part-Of", "decoder"], ["sigmoid", "Part-Of", "fully - connected layers"]], "rel_plus": [["fully - connected layers:Method", "Part-Of", "decoder:Method"], ["sigmoid:Method", "Part-Of", "fully - connected layers:Method"]]}
{"doc_id": "54457603", "sentence": "Batch normalization was used after each convolutional layer , and dropout ( rate= 0 . 5 ) was used after each dense layer to avoid overfitting .", "ner": [["Batch normalization", "Method"], ["convolutional layer", "Method"], ["dropout", "Method"], ["dense layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "We used the Keras built - in VGG parameters for feature extraction , and set the last block as tunable .", "ner": [["VGG", "Method"], ["feature extraction", "Task"]], "rel": [["VGG", "Used-For", "feature extraction"]], "rel_plus": [["VGG:Method", "Used-For", "feature extraction:Task"]]}
{"doc_id": "54457603", "sentence": "For the tunable VLAD descriptors , we made changes based on the action VLAD source code [ 3 6 ] .   We tested our system with three commonly used concurrent activity recognition datasets : Hockey Dataset [ 5 ] : This was collected from real university - level hockey matches using two fixed cameras positioned at both ends of the rink on the spectator 's side .", "ner": [["VLAD", "Method"], ["VLAD", "Method"], ["concurrent activity recognition", "Task"], ["Hockey", "Dataset"]], "rel": [["Hockey", "Benchmark-For", "concurrent activity recognition"]], "rel_plus": [["Hockey:Dataset", "Benchmark-For", "concurrent activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "We compared our proposed method on the hockey and volleyball dataset with several baselines including : 1 .", "ner": [["hockey", "Dataset"], ["volleyball", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "AlexNet for framewise activity recognition . 2 .", "ner": [["AlexNet", "Method"], ["framewise activity recognition", "Task"]], "rel": [["AlexNet", "Used-For", "framewise activity recognition"]], "rel_plus": [["AlexNet:Method", "Used-For", "framewise activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "CNN - LSTM with sigmoid layer [ 1 8 ] .", "ner": [["CNN - LSTM", "Method"], ["sigmoid", "Method"]], "rel": [["sigmoid", "Part-Of", "CNN - LSTM"]], "rel_plus": [["sigmoid:Method", "Part-Of", "CNN - LSTM:Method"]]}
{"doc_id": "54457603", "sentence": "CNNs over time [ 2 8 ] , and various SVMs .", "ner": [["CNNs", "Method"], ["SVMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "We also compared our method on the charades dataset ( Table 2 ) , the result shows our method outperformed most of the researches except the i 3 D and non - local neural network [ 3 2 ] .", "ner": [["charades", "Dataset"], ["i 3 D", "Method"], ["non - local neural network", "Method"]], "rel": [["non - local neural network", "Evaluated-With", "charades"], ["i 3 D", "Evaluated-With", "charades"]], "rel_plus": [["non - local neural network:Method", "Evaluated-With", "charades:Dataset"], ["i 3 D:Method", "Evaluated-With", "charades:Dataset"]]}
{"doc_id": "54457603", "sentence": "These two methods have much better performance on charades dataset , but they pre - trained their model on kinetics dataset ( 5 0 times larger ) and the time complexity of their methods are 5 times larger than our proposed method .", "ner": [["charades", "Dataset"], ["kinetics", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Our model achieved comparable performance with only 2 0 % time complexity compared with i 3 D in charades .", "ner": [["i 3 D", "Method"], ["charades", "Dataset"]], "rel": [["i 3 D", "Evaluated-With", "charades"]], "rel_plus": [["i 3 D:Method", "Evaluated-With", "charades:Dataset"]]}
{"doc_id": "54457603", "sentence": "F 1 VGG Net [ 2 7 ] 9 1 System MAP complexity ( MACC ) AlexNet [ 1 7 ] 1 1 . 2 1. 2 B C 3 D [ 2 5 ] 1 0 . 7 8 0 B * Two - Stream + IDT [ 2 5 ] 1 8 . 6 / CoViAR [ 3 5 ] 2 1 . 9 / Asynchronous Temporal Fields [ 2 6 ] 2 2 . 4 / i 3 D [ 7 ] 3 4 . 4 1 6 5 B Action VLAD [ 1 0 ] 1 7 . 6 3 4 B Non - local Neural Network [ 3 2 ] 3 7 . 5 1 6 5 B+ Our Model 2 2 . 6 3 4 B Table 2 : Experiment results on charades .", "ner": [["VGG Net", "Method"], ["AlexNet", "Method"], ["Two - Stream + IDT", "Method"], ["CoViAR", "Method"], ["Asynchronous Temporal Fields", "Method"], ["i 3 D", "Method"], ["Action VLAD", "Method"], ["Non - local Neural Network", "Method"], ["B+ Our Model", "Method"], ["charades", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "We tried to replace the triaxial self - attention with a traditional LSTM encoder - decoder .", "ner": [["triaxial self - attention", "Method"], ["LSTM encoder - decoder", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54457603", "sentence": "Although we only demonstrated our proposed system on concurrent activity recognition , the core idea of feature - toactivity attention and tri - axial self - attention can be extended to general multi - label classification .", "ner": [["concurrent activity recognition", "Task"], ["feature - toactivity attention", "Method"], ["tri - axial self - attention", "Method"], ["multi - label classification", "Task"]], "rel": [["tri - axial self - attention", "Used-For", "multi - label classification"], ["feature - toactivity attention", "Used-For", "multi - label classification"]], "rel_plus": [["tri - axial self - attention:Method", "Used-For", "multi - label classification:Task"], ["feature - toactivity attention:Method", "Used-For", "multi - label classification:Task"]]}
{"doc_id": "54457603", "sentence": "Many recent works have demonstrated that non - local neural networks and i 3 D have good performance on activity recognition .", "ner": [["non - local neural networks", "Method"], ["i 3 D", "Method"], ["activity recognition", "Task"]], "rel": [["non - local neural networks", "Used-For", "activity recognition"], ["i 3 D", "Used-For", "activity recognition"]], "rel_plus": [["non - local neural networks:Method", "Used-For", "activity recognition:Task"], ["i 3 D:Method", "Used-For", "activity recognition:Task"]]}
{"doc_id": "54457603", "sentence": "We designed a novel deep learning architecture for recognizing concurrent activities that outperformed stateof - the - art mechanisms in three published datasets ( charades , volleyball , and hockey ) .", "ner": [["deep learning", "Method"], ["recognizing concurrent activities", "Task"], ["charades", "Dataset"], ["volleyball", "Dataset"], ["hockey", "Dataset"]], "rel": [["charades", "Benchmark-For", "recognizing concurrent activities"], ["volleyball", "Benchmark-For", "recognizing concurrent activities"], ["hockey", "Benchmark-For", "recognizing concurrent activities"], ["deep learning", "Used-For", "recognizing concurrent activities"]], "rel_plus": [["charades:Dataset", "Benchmark-For", "recognizing concurrent activities:Task"], ["volleyball:Dataset", "Benchmark-For", "recognizing concurrent activities:Task"], ["hockey:Dataset", "Benchmark-For", "recognizing concurrent activities:Task"], ["deep learning:Method", "Used-For", "recognizing concurrent activities:Task"]]}
{"doc_id": "54457603", "sentence": "We hope this paper delivers the following contribution to the society : \u2022 A modified VLAD feature extractor and novel activity mapping layer that extracts independent features for each activity while preserving the temporal information . \u2022 A novel triaxial self - attention that learns independent temporal associations for different activities and makes concurrent activity predictions aware of possible activity combinations . \u2022 A novel use of the self - attention decoder that helps extract hidden associations between activities and avoid redundant weights . \u2022 An implemented network architecture that can serve as a reference for any multi - label classification problem given sequential input .", "ner": [["VLAD feature extractor", "Method"], ["activity mapping layer", "Method"], ["triaxial self - attention", "Method"], ["concurrent activity predictions", "Task"], ["self - attention decoder", "Method"], ["multi - label classification", "Task"]], "rel": [["triaxial self - attention", "Used-For", "concurrent activity predictions"]], "rel_plus": [["triaxial self - attention:Method", "Used-For", "concurrent activity predictions:Task"]]}
{"doc_id": "121101928", "sentence": "In this work , we tackle the problem of efficient keypoint - based object detection and introduce CornerNet - Lite .", "ner": [["keypoint - based object detection", "Method"], ["CornerNet - Lite", "Method"]], "rel": [["CornerNet - Lite", "Used-For", "keypoint - based object detection"]], "rel_plus": [["CornerNet - Lite:Method", "Used-For", "keypoint - based object detection:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Lite is a combination of two efficient variants of CornerNet : CornerNet - Saccade , which uses an attention mechanism to eliminate the need for exhaustively processing all pixels of the image , and CornerNet - Squeeze , which introduces a new compact backbone architecture .", "ner": [["CornerNet - Lite", "Method"], ["CornerNet", "Method"], ["CornerNet - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Saccade", "Part-Of", "CornerNet - Lite"], ["CornerNet - Squeeze", "Part-Of", "CornerNet - Lite"], ["CornerNet - Lite", "SubClass-Of", "CornerNet"], ["CornerNet - Saccade", "SubClass-Of", "CornerNet"], ["CornerNet - Squeeze", "SubClass-Of", "CornerNet"]], "rel_plus": [["CornerNet - Saccade:Method", "Part-Of", "CornerNet - Lite:Method"], ["CornerNet - Squeeze:Method", "Part-Of", "CornerNet - Lite:Method"], ["CornerNet - Lite:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Squeeze:Method", "SubClass-Of", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Saccade is suitable for offline processing , improving the efficiency of CornerNet by 6. 0 x and the AP by 1. 0 % on COCO .", "ner": [["CornerNet - Saccade", "Method"], ["offline processing", "Task"], ["CornerNet", "Method"], ["COCO", "Dataset"]], "rel": [["CornerNet - Saccade", "Used-For", "offline processing"], ["CornerNet - Saccade", "SubClass-Of", "CornerNet"], ["CornerNet - Saccade", "Compare-With", "CornerNet"], ["CornerNet - Saccade", "Evaluated-With", "COCO"], ["CornerNet", "Evaluated-With", "COCO"]], "rel_plus": [["CornerNet - Saccade:Method", "Used-For", "offline processing:Task"], ["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Saccade:Method", "Compare-With", "CornerNet:Method"], ["CornerNet - Saccade:Method", "Evaluated-With", "COCO:Dataset"], ["CornerNet:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze is suitable for real - time detection , improving both the efficiency and accuracy of the popular real - time detector YOLOv 3 ( 3 4 . 4 % AP at 3 0 ms for CornerNet - Squeeze compared to 3 3 . 0 % AP at 3 9 ms for YOLOv 3 on COCO ) .", "ner": [["CornerNet - Squeeze", "Method"], ["real - time detection", "Task"], ["YOLOv 3", "Method"], ["CornerNet - Squeeze", "Method"], ["YOLOv 3", "Method"], ["COCO", "Dataset"]], "rel": [["CornerNet - Squeeze", "Used-For", "real - time detection"], ["CornerNet - Squeeze", "Compare-With", "YOLOv 3"], ["CornerNet - Squeeze", "Compare-With", "YOLOv 3"], ["YOLOv 3", "Evaluated-With", "COCO"], ["CornerNet - Squeeze", "Evaluated-With", "COCO"]], "rel_plus": [["CornerNet - Squeeze:Method", "Used-For", "real - time detection:Task"], ["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3:Method"], ["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3:Method"], ["YOLOv 3:Method", "Evaluated-With", "COCO:Dataset"], ["CornerNet - Squeeze:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "121101928", "sentence": "CornerNet [ 2 6 ] , the stateof - the - art among them , detects and groups the top - left and bottom - right corners of bounding boxes ; it uses a stacked hourglass network [ 3 9 ] to predict the heatmaps of the corners and then uses associate embeddings [ 3 8 ] to group them .", "ner": [["CornerNet", "Method"], ["stacked hourglass network", "Method"]], "rel": [["stacked hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["stacked hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet allows a simplified design that eliminates the need for anchor boxes [ 4 6 ] , and has achieved state - ofthe - art accuracy on COCO [ 3 2 ] among single - stage detectors .", "ner": [["CornerNet", "Method"], ["COCO", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "COCO"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "121101928", "sentence": "However , a major drawback of CornerNet is its inference    CornerNet [ Law & Deng ' 1 8 ] YOLOv 3 [ Redmon & Farhadi ' 1 8 ] Figure 1 : We introduce CornerNet - Saccade and CornerNetSqueeze ( collectively as CornerNet - Lite ) , two efficient object detectors based on CornerNet [ 2 6 ] , a state - of - the - art keypoint based object detector .", "ner": [["CornerNet", "Method"], ["CornerNet", "Method"], ["YOLOv 3", "Method"], ["CornerNet - Saccade", "Method"], ["CornerNetSqueeze", "Method"], ["CornerNet - Lite", "Method"], ["CornerNet", "Method"], ["keypoint based object detector", "Method"]], "rel": [["CornerNet - Saccade", "SubClass-Of", "CornerNet - Lite"], ["CornerNetSqueeze", "SubClass-Of", "CornerNet - Lite"], ["CornerNet - Saccade", "SubClass-Of", "CornerNet"], ["CornerNetSqueeze", "SubClass-Of", "CornerNet"], ["CornerNet - Lite", "SubClass-Of", "CornerNet"], ["CornerNet", "SubClass-Of", "keypoint based object detector"]], "rel_plus": [["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet - Lite:Method"], ["CornerNetSqueeze:Method", "SubClass-Of", "CornerNet - Lite:Method"], ["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNetSqueeze:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Lite:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet:Method", "SubClass-Of", "keypoint based object detector:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Saccade speeds up the original CornerNet by 6. 0 x with a 1% increase in AP .", "ner": [["CornerNet - Saccade", "Method"], ["CornerNet", "Method"]], "rel": [["CornerNet - Saccade", "Compare-With", "CornerNet"]], "rel_plus": [["CornerNet - Saccade:Method", "Compare-With", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze is faster and more accurate than YOLOv 3 [ 4 5 ] , the state - of - the - art real time detector .", "ner": [["CornerNet - Squeeze", "Method"], ["YOLOv 3", "Method"]], "rel": [["CornerNet - Squeeze", "Compare-With", "YOLOv 3"]], "rel_plus": [["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3:Method"]]}
{"doc_id": "121101928", "sentence": "For example , single - scale processing combined with reducing the input resolution can speed up the inference of CornerNet to 4 2 ms per image , comparable to the 3 9 ms of the popular fast detector YOLOv 3 [ 4 5 ] , but would decrease the AP to 2 5 . 6 % which is much lower than YOLOv 3 's 3 3 . 0 % .", "ner": [["CornerNet", "Method"], ["YOLOv 3", "Method"], ["YOLOv 3", "Method"]], "rel": [["CornerNet", "Compare-With", "YOLOv 3"]], "rel_plus": [["CornerNet:Method", "Compare-With", "YOLOv 3:Method"]]}
{"doc_id": "121101928", "sentence": "We explore both directions and introduce two efficient variants of CornerNet : CornerNet - Saccade and CornerNet - Squeeze , which we refer to collectively as CornerNet - Lite .", "ner": [["CornerNet", "Method"], ["CornerNet - Saccade", "Method"], ["CornerNet - Squeeze", "Method"], ["CornerNet - Lite", "Method"]], "rel": [["CornerNet - Saccade", "SubClass-Of", "CornerNet"], ["CornerNet - Squeeze", "SubClass-Of", "CornerNet"], ["CornerNet - Squeeze", "SubClass-Of", "CornerNet - Lite"], ["CornerNet - Saccade", "SubClass-Of", "CornerNet - Lite"]], "rel_plus": [["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Squeeze:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Squeeze:Method", "SubClass-Of", "CornerNet - Lite:Method"], ["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet - Lite:Method"]]}
{"doc_id": "121101928", "sentence": "Experiments on COCO show that CornerNet - Saccade achieves an AP of 4 3 . 2 % at 1 9 0 ms per image , a 1% increase in AP and a 6. 0 x speed - up over the original CornerNet .", "ner": [["COCO", "Dataset"], ["CornerNet - Saccade", "Method"], ["CornerNet", "Method"]], "rel": [["CornerNet - Saccade", "Evaluated-With", "COCO"], ["CornerNet - Saccade", "Compare-With", "CornerNet"]], "rel_plus": [["CornerNet - Saccade:Method", "Evaluated-With", "COCO:Dataset"], ["CornerNet - Saccade:Method", "Compare-With", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "It incorporates ideas from SqueezeNet [ 1 9 ] and MobileNets [ 1 5 ] , and introduces a new , compact hourglass backbone that makes extensive use of 1 \u00d7 1 convolution , bottleneck layer , and depth - wise separable convolution .", "ner": [["SqueezeNet", "Method"], ["MobileNets", "Method"], ["hourglass backbone", "Method"], ["1 \u00d7 1 convolution", "Method"], ["bottleneck layer", "Method"], ["depth - wise separable convolution", "Method"]], "rel": [["1 \u00d7 1 convolution", "Part-Of", "hourglass backbone"], ["bottleneck layer", "Part-Of", "hourglass backbone"], ["depth - wise separable convolution", "Part-Of", "hourglass backbone"]], "rel_plus": [["1 \u00d7 1 convolution:Method", "Part-Of", "hourglass backbone:Method"], ["bottleneck layer:Method", "Part-Of", "hourglass backbone:Method"], ["depth - wise separable convolution:Method", "Part-Of", "hourglass backbone:Method"]]}
{"doc_id": "121101928", "sentence": "With the new hourglass backbone , CornerNet - Squeeze achieves an AP of 3 4 . 4 % on COCO at 3 0 ms , simultaneously more accurate and faster than YOLOv 3 ( 3 3 . 0 % at 3 9 ms ) .", "ner": [["hourglass backbone", "Method"], ["CornerNet - Squeeze", "Method"], ["COCO", "Dataset"], ["YOLOv 3", "Method"]], "rel": [["hourglass backbone", "Part-Of", "CornerNet - Squeeze"], ["CornerNet - Squeeze", "Evaluated-With", "COCO"], ["CornerNet - Squeeze", "Compare-With", "YOLOv 3"]], "rel_plus": [["hourglass backbone:Method", "Part-Of", "CornerNet - Squeeze:Method"], ["CornerNet - Squeeze:Method", "Evaluated-With", "COCO:Dataset"], ["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3:Method"]]}
{"doc_id": "121101928", "sentence": "Somewhat surprisingly , our experiments give a negative answer : CornerNet - Squeeze - Saccade turns out slower and less accurate than CornerNet - Squeeze .", "ner": [["CornerNet - Squeeze - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze - Saccade", "Compare-With", "CornerNet - Squeeze"]], "rel_plus": [["CornerNet - Squeeze - Saccade:Method", "Compare-With", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "Significance and novelty : Collectively , these two variants of CornerNet - Lite make the keypoint - based approach competitive , covering two popular use cases : CornerNetSaccade for offline processing , improving efficiency without sacrificing accuracy , and CornerNet - Squeeze for realtime processing , improving accuracy without sacrificing efficiency .", "ner": [["CornerNet - Lite", "Method"], ["CornerNetSaccade", "Method"], ["offline processing", "Task"], ["CornerNet - Squeeze", "Method"], ["realtime processing", "Task"]], "rel": [["CornerNetSaccade", "Used-For", "offline processing"], ["CornerNet - Squeeze", "Used-For", "realtime processing"]], "rel_plus": [["CornerNetSaccade:Method", "Used-For", "offline processing:Task"], ["CornerNet - Squeeze:Method", "Used-For", "realtime processing:Task"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Saccade is the first to integrate saccades with keypoint - based object detection .", "ner": [["CornerNet - Saccade", "Method"], ["keypoint - based object detection", "Task"]], "rel": [["CornerNet - Saccade", "Used-For", "keypoint - based object detection"]], "rel_plus": [["CornerNet - Saccade:Method", "Used-For", "keypoint - based object detection:Task"]]}
{"doc_id": "121101928", "sentence": "Prior work that employs saccade - like mechanisms either detects a single object per crop ( e.g. Faster R - CNN [ 4 6 ] ) or produces multiple detections per crop with a two - stage network involving additional sub - crops ( e.g. AutoFocus [ 3 7 ] ) .", "ner": [["Faster R - CNN", "Method"], ["AutoFocus", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze is the first to integrate SqueezeNet with the stacked hourglass architecture and to apply such a combination on object detection .", "ner": [["CornerNet - Squeeze", "Method"], ["SqueezeNet", "Method"], ["hourglass", "Method"], ["object detection", "Task"]], "rel": [["SqueezeNet", "Part-Of", "CornerNet - Squeeze"], ["hourglass", "Part-Of", "SqueezeNet"], ["CornerNet - Squeeze", "Used-For", "object detection"]], "rel_plus": [["SqueezeNet:Method", "Part-Of", "CornerNet - Squeeze:Method"], ["hourglass:Method", "Part-Of", "SqueezeNet:Method"], ["CornerNet - Squeeze:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "121101928", "sentence": "Contributions Our contributions are three - fold : ( 1 ) We propose CornerNet - Saccade and CornerNet - Squeeze , two novel approaches to improving the efficiency of keypointbased object detection ; ( 2 ) On COCO , we improve the efficiency of state - of - the - art keypoint based detection by 6 fold and the AP from 4 2 . 2 % to 4 3 . 2 % , ( 3 ) On COCO , we improve both the accuracy and efficiency of state - of - the art real - time object detection ( to 3 4 . 4 % at 3 0 ms from 3 3 . 0 % at 3 9 ms of YOLOv 3 ) .", "ner": [["CornerNet - Saccade", "Method"], ["CornerNet - Squeeze", "Method"], ["keypointbased object detection", "Task"], ["COCO", "Dataset"], ["keypoint based detection", "Task"], ["COCO", "Dataset"], ["real - time object detection", "Task"], ["YOLOv 3", "Method"]], "rel": [["CornerNet - Saccade", "Used-For", "keypointbased object detection"], ["CornerNet - Squeeze", "Used-For", "keypointbased object detection"], ["COCO", "Benchmark-For", "keypoint based detection"], ["YOLOv 3", "Evaluated-With", "COCO"], ["COCO", "Benchmark-For", "real - time object detection"], ["YOLOv 3", "Used-For", "real - time object detection"]], "rel_plus": [["CornerNet - Saccade:Method", "Used-For", "keypointbased object detection:Task"], ["CornerNet - Squeeze:Method", "Used-For", "keypointbased object detection:Task"], ["COCO:Dataset", "Benchmark-For", "keypoint based detection:Task"], ["YOLOv 3:Method", "Evaluated-With", "COCO:Dataset"], ["COCO:Dataset", "Benchmark-For", "real - time object detection:Task"], ["YOLOv 3:Method", "Used-For", "real - time object detection:Task"]]}
{"doc_id": "121101928", "sentence": "Saccades in R - CNN [ 1 1 ] , Fast R - CNN [ 1 0 ] , and Faster R - CNN [ 4 6 ] take the form of crops representing potential objects .", "ner": [["Saccades", "Method"], ["R - CNN", "Method"], ["Fast R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Saccades", "Part-Of", "R - CNN"], ["Saccades", "Part-Of", "Fast R - CNN"], ["Saccades", "Part-Of", "Faster R - CNN"]], "rel_plus": [["Saccades:Method", "Part-Of", "R - CNN:Method"], ["Saccades:Method", "Part-Of", "Fast R - CNN:Method"], ["Saccades:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "121101928", "sentence": "After processing , each crop is either rejected or converted to a single labeled box through classification and regression .", "ner": [["classification", "Task"], ["regression", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "Cascade R - CNN [ 4 ] extends Faster R - CNN by using a cascade of classifiers and regressors to iteratively reject or refine each proposal .", "ner": [["Cascade R - CNN", "Method"], ["Faster R - CNN", "Method"], ["cascade of classifiers and regressors", "Method"]], "rel": [["cascade of classifiers and regressors", "Part-Of", "Cascade R - CNN"], ["Cascade R - CNN", "SubClass-Of", "Faster R - CNN"]], "rel_plus": [["cascade of classifiers and regressors:Method", "Part-Of", "Cascade R - CNN:Method"], ["Cascade R - CNN:Method", "SubClass-Of", "Faster R - CNN:Method"]]}
{"doc_id": "121101928", "sentence": "AutoFocus [ 3 7 ] , which builds upon SNIPER [ 5 2 ] that improved R - CNN training , adds a branch to Faster R - CNN to predict the regions that are likely to contain small objects .", "ner": [["AutoFocus", "Method"], ["SNIPER", "Method"], ["R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Faster R - CNN", "Part-Of", "AutoFocus"], ["AutoFocus", "SubClass-Of", "SNIPER"], ["SNIPER", "Used-For", "R - CNN"]], "rel_plus": [["Faster R - CNN:Method", "Part-Of", "AutoFocus:Method"], ["AutoFocus:Method", "SubClass-Of", "SNIPER:Method"], ["SNIPER:Method", "Used-For", "R - CNN:Method"]]}
{"doc_id": "121101928", "sentence": "Then it applies Faster R - CNN again to each of those regions Figure 2 : Overview of CornerNet - Saccade .", "ner": [["Faster R - CNN", "Method"], ["CornerNet - Saccade", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "In AutoFocus , there are two kinds of cropping , one that can produce multiple objects ( by calling Faster R - CNN as a subroutine ) , and the other that can produce at most a single object ( cropping done within Faster R - CNN ) .", "ner": [["AutoFocus", "Method"], ["Faster R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [["Faster R - CNN", "Part-Of", "AutoFocus"], ["Faster R - CNN", "Part-Of", "AutoFocus"]], "rel_plus": [["Faster R - CNN:Method", "Part-Of", "AutoFocus:Method"], ["Faster R - CNN:Method", "Part-Of", "AutoFocus:Method"]]}
{"doc_id": "121101928", "sentence": "This means that the number of crops processed by CornerNet - Saccade can be much smaller than the number of objects , whereas for R - CNN variants and AutoFocus the number of crops must be no smaller than the number of objects .", "ner": [["CornerNet - Saccade", "Method"], ["R - CNN variants", "Method"], ["AutoFocus", "Method"]], "rel": [["CornerNet - Saccade", "Compare-With", "R - CNN variants"], ["CornerNet - Saccade", "Compare-With", "AutoFocus"]], "rel_plus": [["CornerNet - Saccade:Method", "Compare-With", "R - CNN variants:Method"], ["CornerNet - Saccade:Method", "Compare-With", "AutoFocus:Method"]]}
{"doc_id": "121101928", "sentence": "Other than accuracy [ 3 , 4 9 , 1 8 , 3 0 , 8 , 6 4 , 1 2 , 4 1 , 5 1 , 5 9 , 5 7 , 5 , 2 1 ] , many recent works have improved upon the efficiency of detectors since the introduction of R - CNN [ 1 1 ] , which applies a ConvNet [ 2 4 ] to 2 0 0 0 RoIs .", "ner": [["R - CNN", "Method"], ["ConvNet", "Method"]], "rel": [["ConvNet", "Part-Of", "R - CNN"]], "rel_plus": [["ConvNet:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "121101928", "sentence": "SPP [ 1 3 ] and Fast R - CNN [ 1 0 ] address this by applying a ConvNet fully convolutionally on the image and extracting features directly from the feature maps for each RoI. Faster R - CNN [ 4 6 ] further improves efficiency by replacing the low - level vision algorithm with a region proposal network .", "ner": [["SPP", "Method"], ["Fast R - CNN", "Method"], ["ConvNet", "Method"], ["Faster R - CNN", "Method"], ["region proposal network", "Method"]], "rel": [["ConvNet", "Part-Of", "SPP"], ["ConvNet", "Part-Of", "Fast R - CNN"], ["region proposal network", "Part-Of", "Faster R - CNN"]], "rel_plus": [["ConvNet:Method", "Part-Of", "SPP:Method"], ["ConvNet:Method", "Part-Of", "Fast R - CNN:Method"], ["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "121101928", "sentence": "R - FCN [ 7 ] replaces the expensive fully connected sub - detection network with a fully convolutional network , and Light - Head R - CNN [ 2 8 ] reduces the cost in R - FCN by applying separable convolution to reduce the number of channels in the feature maps before RoI pooling .", "ner": [["R - FCN", "Method"], ["fully connected sub - detection network", "Method"], ["fully convolutional network", "Method"], ["Light - Head R - CNN", "Method"], ["R - FCN", "Method"], ["separable convolution", "Method"]], "rel": [["fully convolutional network", "Part-Of", "R - FCN"], ["separable convolution", "Part-Of", "Light - Head R - CNN"], ["Light - Head R - CNN", "Compare-With", "R - FCN"]], "rel_plus": [["fully convolutional network:Method", "Part-Of", "R - FCN:Method"], ["separable convolution:Method", "Part-Of", "Light - Head R - CNN:Method"], ["Light - Head R - CNN:Method", "Compare-With", "R - FCN:Method"]]}
{"doc_id": "121101928", "sentence": "SqueezeNet [ 1 9 ] proposes a fire module to reduce the number of parameters in AlexNet [ 2 4 ] by 5 0 x , while achieving similar performance .", "ner": [["SqueezeNet", "Method"], ["fire module", "Method"], ["AlexNet", "Method"]], "rel": [["fire module", "Part-Of", "SqueezeNet"], ["SqueezeNet", "Compare-With", "AlexNet"]], "rel_plus": [["fire module:Method", "Part-Of", "SqueezeNet:Method"], ["SqueezeNet:Method", "Compare-With", "AlexNet:Method"]]}
{"doc_id": "121101928", "sentence": "MobileNets [ 1 5 ] are a class of lightweight networks that use depth - wise separable convolutions [ 6 ] , and proposes strategies to achieve a good tradeoff between accuracy and latency .", "ner": [["MobileNets", "Method"], ["depth - wise separable convolutions", "Method"]], "rel": [["depth - wise separable convolutions", "Part-Of", "MobileNets"]], "rel_plus": [["depth - wise separable convolutions:Method", "Part-Of", "MobileNets:Method"]]}
{"doc_id": "121101928", "sentence": "PeleeNet [ 5 5 ] , in contrast , demonstrates the effectiveness of standard convolutions by introducing an efficient variant of DenseNet [ 1 7 ] consisting of two - way dense layers and a stem block .", "ner": [["PeleeNet", "Method"], ["convolutions", "Method"], ["DenseNet", "Method"], ["two - way dense layers", "Method"], ["stem block", "Method"]], "rel": [["DenseNet", "Part-Of", "PeleeNet"], ["two - way dense layers", "Part-Of", "DenseNet"], ["stem block", "Part-Of", "DenseNet"]], "rel_plus": [["DenseNet:Method", "Part-Of", "PeleeNet:Method"], ["two - way dense layers:Method", "Part-Of", "DenseNet:Method"], ["stem block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "121101928", "sentence": "YOLOv 2 [ 4 4 ] incorporates ideas from NIN [ 2 9 ] to design a new variant of VGG [ 5 0 ] .", "ner": [["YOLOv 2", "Method"], ["NIN", "Method"], ["VGG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "YOLOv 3 [ 4 5 ] further improves DarkNet - 1 9 by making the network deeper and introducing skip connections .", "ner": [["YOLOv 3", "Method"], ["DarkNet - 1 9", "Method"], ["skip connections", "Method"]], "rel": [["skip connections", "Part-Of", "YOLOv 3"], ["YOLOv 3", "Compare-With", "DarkNet - 1 9"]], "rel_plus": [["skip connections:Method", "Part-Of", "YOLOv 3:Method"], ["YOLOv 3:Method", "Compare-With", "DarkNet - 1 9:Method"]]}
{"doc_id": "121101928", "sentence": "Each hourglass module in the network applies several convolution and downsampling layers to downsize the input feature maps .", "ner": [["hourglass module", "Method"], ["convolution", "Method"], ["downsampling layers", "Method"]], "rel": [["convolution", "Part-Of", "hourglass module"], ["downsampling layers", "Part-Of", "hourglass module"]], "rel_plus": [["convolution:Method", "Part-Of", "hourglass module:Method"], ["downsampling layers:Method", "Part-Of", "hourglass module:Method"]]}
{"doc_id": "121101928", "sentence": "We predict the attention maps by applying a 3 \u00d7 3 ConvReLU module followed by a 1 \u00d7 1 Conv - Sigmoid module to each feature map .", "ner": [["3 \u00d7 3 ConvReLU", "Method"], ["1 \u00d7 1 Conv - Sigmoid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "The new hourglass network consists of 3 hourglass modules and has a depth of 5 4 layers , while Hourglass - 1 0 4 in CornerNet consists of 2 hourglass modules and has a depth of 1 0 4 .", "ner": [["hourglass network", "Method"], ["hourglass", "Method"], ["Hourglass - 1 0 4", "Method"], ["CornerNet", "Method"], ["hourglass", "Method"]], "rel": [["hourglass", "Part-Of", "hourglass network"], ["hourglass", "Part-Of", "Hourglass - 1 0 4"], ["Hourglass - 1 0 4", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass:Method", "Part-Of", "hourglass network:Method"], ["hourglass:Method", "Part-Of", "Hourglass - 1 0 4:Method"], ["Hourglass - 1 0 4:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "Each hourglass module in Hourglass - 5 4 has fewer parameters and is shallower than the one in Hourglass - 1 0 4 .", "ner": [["hourglass module", "Method"], ["Hourglass - 5 4", "Method"], ["Hourglass - 1 0 4", "Method"]], "rel": [["hourglass module", "Part-Of", "Hourglass - 5 4"], ["Hourglass - 5 4", "Compare-With", "Hourglass - 1 0 4"]], "rel_plus": [["hourglass module:Method", "Part-Of", "Hourglass - 5 4:Method"], ["Hourglass - 5 4:Method", "Compare-With", "Hourglass - 1 0 4:Method"]]}
{"doc_id": "121101928", "sentence": "We use Adam [ 2 2 ] to optimize both the losses for the attention maps and object detection , and use the same training hyperparameters found in CornerNet .", "ner": [["Adam", "Method"], ["object detection", "Task"], ["CornerNet", "Method"]], "rel": [["Adam", "Used-For", "object detection"], ["CornerNet", "Used-For", "object detection"]], "rel_plus": [["Adam:Method", "Used-For", "object detection:Task"], ["CornerNet:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "121101928", "sentence": "In order to avoid over - fitting , we adopt the data augmentation techniques used in CornerNet .", "ner": [["data augmentation", "Method"], ["CornerNet", "Method"]], "rel": [["data augmentation", "Used-For", "CornerNet"]], "rel_plus": [["data augmentation:Method", "Used-For", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "This ensures that training and testing are consistent as the network detects objects within the crops centered at object locations .   In contrast to CornerNet - Saccade , which focuses on a subset of the pixels to reduce the amount of processing , CornerNet - Squeeze explores an alternative approach of reducing the amount of processing per pixel .", "ner": [["CornerNet - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze", "Compare-With", "CornerNet - Saccade"]], "rel_plus": [["CornerNet - Squeeze:Method", "Compare-With", "CornerNet - Saccade:Method"]]}
{"doc_id": "121101928", "sentence": "In CornerNet , most of the computational resources are spent on Hourglass - 1 0 4 .", "ner": [["CornerNet", "Method"], ["Hourglass - 1 0 4", "Method"]], "rel": [["Hourglass - 1 0 4", "Part-Of", "CornerNet"]], "rel_plus": [["Hourglass - 1 0 4:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "Hourglass - 1 0 4 is built from residual blocks which consists of two 3 \u00d7 3 convolution layers and a skip connection .", "ner": [["Hourglass - 1 0 4", "Method"], ["residual blocks", "Method"], ["3 \u00d7 3 convolution", "Method"], ["skip connection", "Method"]], "rel": [["residual blocks", "Part-Of", "Hourglass - 1 0 4"], ["3 \u00d7 3 convolution", "Part-Of", "residual blocks"], ["skip connection", "Part-Of", "residual blocks"]], "rel_plus": [["residual blocks:Method", "Part-Of", "Hourglass - 1 0 4:Method"], ["3 \u00d7 3 convolution:Method", "Part-Of", "residual blocks:Method"], ["skip connection:Method", "Part-Of", "residual blocks:Method"]]}
{"doc_id": "121101928", "sentence": "To reduce the complexity of Hourglass - 1 0 4 , we incorporate ideas from SqueezeNet [ 1 9 ] and MobileNets [ 1 5 ] to design a lightweight hourglass architecture .", "ner": [["Hourglass - 1 0 4", "Method"], ["SqueezeNet", "Method"], ["MobileNets", "Method"], ["lightweight hourglass", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "The building block of SqueezeNet , the fire module , encapsulates the first two ideas .", "ner": [["building block", "Method"], ["SqueezeNet", "Method"], ["fire module", "Method"]], "rel": [["fire module", "SubClass-Of", "building block"], ["building block", "Part-Of", "SqueezeNet"], ["fire module", "Part-Of", "SqueezeNet"]], "rel_plus": [["fire module:Method", "SubClass-Of", "building block:Method"], ["building block:Method", "Part-Of", "SqueezeNet:Method"], ["fire module:Method", "Part-Of", "SqueezeNet:Method"]]}
{"doc_id": "121101928", "sentence": "Based on the insights provided by SqueezeNet , we use the fire module in CornerNet - Squeeze instead of the residual block .", "ner": [["SqueezeNet", "Method"], ["fire module", "Method"], ["CornerNet - Squeeze", "Method"], ["residual block", "Method"]], "rel": [["fire module", "Part-Of", "CornerNet - Squeeze"]], "rel_plus": [["fire module:Method", "Part-Of", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "Furthermore , inspired by the success of MobileNets , we replace the 3 \u00d7 3 standard convolution in the second layer with a 3 \u00d7 3 depth - wise separable convolution , which further improves inference time .", "ner": [["MobileNets", "Method"], ["3 \u00d7 3 standard convolution", "Method"], ["3 \u00d7 3 depth - wise separable convolution", "Method"]], "rel": [["3 \u00d7 3 depth - wise separable convolution", "Part-Of", "MobileNets"]], "rel_plus": [["3 \u00d7 3 depth - wise separable convolution:Method", "Part-Of", "MobileNets:Method"]]}
{"doc_id": "121101928", "sentence": "Tab . 1 shows a detail comparison between the residual block in CornerNet and the new fire module in CornerNet - Squeeze .", "ner": [["residual block", "Method"], ["CornerNet", "Method"], ["fire module", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["residual block", "Part-Of", "CornerNet"], ["residual block", "Compare-With", "fire module"], ["fire module", "Part-Of", "CornerNet - Squeeze"]], "rel_plus": [["residual block:Method", "Part-Of", "CornerNet:Method"], ["residual block:Method", "Compare-With", "fire module:Method"], ["fire module:Method", "Part-Of", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze correspondingly downsizes the image three times before the hourglass module , whereas CornerNet downsizes the image twice .", "ner": [["CornerNet - Squeeze", "Method"], ["hourglass module", "Method"], ["CornerNet", "Method"]], "rel": [["hourglass module", "Part-Of", "CornerNet - Squeeze"]], "rel_plus": [["hourglass module:Method", "Part-Of", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "We use the same training losses and hyperparameters of CornerNet to train CornerNet - Squeeze .", "ner": [["CornerNet", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "We use COCO [ 3 2 ] to evaluate CornerNet - Lite and compare it with other detectors .", "ner": [["COCO", "Dataset"], ["CornerNet - Lite", "Method"]], "rel": [["CornerNet - Lite", "Evaluated-With", "COCO"]], "rel_plus": [["CornerNet - Lite:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "121101928", "sentence": "We compare the accuracy - efficiency trade - offs of CornerNet - Lite with three state - of - the - art object detectors , including YOLOv 3 [ 4 5 ] , RetinaNet 2 [ 3 1 ] and CornerNet [ 2 6 ] , on the validation set .", "ner": [["CornerNet - Lite", "Method"], ["YOLOv 3", "Method"], ["RetinaNet", "Method"], ["CornerNet", "Method"]], "rel": [["CornerNet - Lite", "Compare-With", "YOLOv 3"], ["CornerNet - Lite", "Compare-With", "RetinaNet"], ["CornerNet - Lite", "Compare-With", "CornerNet"]], "rel_plus": [["CornerNet - Lite:Method", "Compare-With", "YOLOv 3:Method"], ["CornerNet - Lite:Method", "Compare-With", "RetinaNet:Method"], ["CornerNet - Lite:Method", "Compare-With", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "For RetinaNet , we evaluate at different single scale settings , including 3 0 0 , 4 0 0 , 5 0 0 , 6 0 0 , Following the default settings of YOLOv 3 , we evaluate YOLOv 3 at 3 single image scales ( 3 2 0 , 4 1 6 and 6 0 8) .", "ner": [["RetinaNet", "Method"], ["YOLOv 3", "Method"], ["YOLOv 3", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze achieves a better accuracy and efficiency ( 3 4 . 4 % at 3 0 ms ) trade - off than YOLOv 3 ( 3 2 . 4 % at 3 9 ms ) .", "ner": [["CornerNet - Squeeze", "Method"], ["YOLOv 3", "Method"]], "rel": [["CornerNet - Squeeze", "Compare-With", "YOLOv 3"]], "rel_plus": [["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3:Method"]]}
{"doc_id": "121101928", "sentence": "We are able to train CornerNet - Saccade on only four 1 0 8 0 Ti GPUs with a total of 4 4 GB GPU memory , while CornerNet requires ten Titan X ( PASCAL ) GPUs with a total of 1 2 0 GB GPU memory .", "ner": [["CornerNet - Saccade", "Method"], ["CornerNet", "Method"]], "rel": [["CornerNet - Saccade", "Compare-With", "CornerNet"]], "rel_plus": [["CornerNet - Saccade:Method", "Compare-With", "CornerNet:Method"]]}
{"doc_id": "121101928", "sentence": "Neither CornerNet nor CornerNet - Saccade uses mixed precision training [ 3 6 ] .", "ner": [["CornerNet", "Method"], ["CornerNet - Saccade", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "Table 6 : Ablation study on CornerNet - Squeeze . * Note that CornerNet is trained with a much smaller batch size .", "ner": [["CornerNet - Squeeze", "Method"], ["CornerNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "121101928", "sentence": "However , we find that CornerNet - Squeeze - Saccade does not outperform CornerNet - Squeeze .", "ner": [["CornerNet - Squeeze - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze - Saccade", "Compare-With", "CornerNet - Squeeze"]], "rel_plus": [["CornerNet - Squeeze - Saccade:Method", "Compare-With", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "On the validation set , CornerNetSqueeze achieves an AP of 3 4 . 4 % , while CornerNetSqueeze - Saccade with k max = 3 0 achieves 3 2 . 7 % .", "ner": [["CornerNetSqueeze", "Method"], ["CornerNetSqueeze - Saccade", "Method"]], "rel": [["CornerNetSqueeze", "Compare-With", "CornerNetSqueeze - Saccade"]], "rel_plus": [["CornerNetSqueeze:Method", "Compare-With", "CornerNetSqueeze - Saccade:Method"]]}
{"doc_id": "121101928", "sentence": "If we replace the predicted attention map with the ground - truth attention map ( i.e. the object locations are known ) , we improve the AP of CornerNet - Squeeze - Saccade to 3 8 . 0 % , outperforming CornerNet - Squeeze .", "ner": [["CornerNet - Squeeze - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze - Saccade", "Compare-With", "CornerNet - Squeeze"]], "rel_plus": [["CornerNet - Squeeze - Saccade:Method", "Compare-With", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "Furthermore , CornerNetSqueeze only operates on single scale images , which provides much less room for CornerNet - Squeeze - Saccade to save .", "ner": [["CornerNetSqueeze", "Method"], ["CornerNet - Squeeze - Saccade", "Method"]], "rel": [["CornerNetSqueeze", "Compare-With", "CornerNet - Squeeze - Saccade"]], "rel_plus": [["CornerNetSqueeze:Method", "Compare-With", "CornerNet - Squeeze - Saccade:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze - Saccade may process more number of pixels than CornerNet - Squeeze , slowing down the inference time .", "ner": [["CornerNet - Squeeze - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze - Saccade", "Compare-With", "CornerNet - Squeeze"]], "rel_plus": [["CornerNet - Squeeze - Saccade:Method", "Compare-With", "CornerNet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "Table 7 : CornerNet - Squeeze - Saccade runs slower and is less accurate than Cornernet - Squeeze .", "ner": [["CornerNet - Squeeze - Saccade", "Method"], ["Cornernet - Squeeze", "Method"]], "rel": [["CornerNet - Squeeze - Saccade", "Compare-With", "Cornernet - Squeeze"]], "rel_plus": [["CornerNet - Squeeze - Saccade:Method", "Compare-With", "Cornernet - Squeeze:Method"]]}
{"doc_id": "121101928", "sentence": "We also compare CornerNet - Lite with CornerNet and YOLOv 3 on COCO test set in Tab . 8 .", "ner": [["CornerNet - Lite", "Method"], ["CornerNet", "Method"], ["YOLOv 3", "Method"], ["COCO", "Dataset"]], "rel": [["CornerNet - Lite", "Compare-With", "CornerNet"], ["CornerNet - Lite", "Compare-With", "YOLOv 3"], ["CornerNet - Lite", "Evaluated-With", "COCO"], ["CornerNet", "Evaluated-With", "COCO"], ["YOLOv 3", "Evaluated-With", "COCO"]], "rel_plus": [["CornerNet - Lite:Method", "Compare-With", "CornerNet:Method"], ["CornerNet - Lite:Method", "Compare-With", "YOLOv 3:Method"], ["CornerNet - Lite:Method", "Evaluated-With", "COCO:Dataset"], ["CornerNet:Method", "Evaluated-With", "COCO:Dataset"], ["YOLOv 3:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "121101928", "sentence": "CornerNet - Squeeze is faster and more accurate than YOLOv 3 .", "ner": [["CornerNet - Squeeze", "Method"], ["YOLOv 3 .", "Method"]], "rel": [["CornerNet - Squeeze", "Compare-With", "YOLOv 3 ."]], "rel_plus": [["CornerNet - Squeeze:Method", "Compare-With", "YOLOv 3 .:Method"]]}
{"doc_id": "121101928", "sentence": "CornerNetSaccade is more accurate than CornerNet at multi - scales and 6 times faster .   We propose CornerNet - Lite which is a combination of two efficient variant of CornerNet : CornerNet - Saccade and CornerNet - Squeeze .", "ner": [["CornerNetSaccade", "Method"], ["CornerNet", "Method"], ["CornerNet - Lite", "Method"], ["CornerNet", "Method"], ["CornerNet - Saccade", "Method"], ["CornerNet - Squeeze", "Method"]], "rel": [["CornerNetSaccade", "Compare-With", "CornerNet"], ["CornerNet", "Part-Of", "CornerNet - Lite"], ["CornerNet - Squeeze", "Part-Of", "CornerNet - Lite"], ["CornerNet - Saccade", "Part-Of", "CornerNet - Lite"], ["CornerNet - Squeeze", "SubClass-Of", "CornerNet"], ["CornerNet - Saccade", "SubClass-Of", "CornerNet"]], "rel_plus": [["CornerNetSaccade:Method", "Compare-With", "CornerNet:Method"], ["CornerNet:Method", "Part-Of", "CornerNet - Lite:Method"], ["CornerNet - Squeeze:Method", "Part-Of", "CornerNet - Lite:Method"], ["CornerNet - Saccade:Method", "Part-Of", "CornerNet - Lite:Method"], ["CornerNet - Squeeze:Method", "SubClass-Of", "CornerNet:Method"], ["CornerNet - Saccade:Method", "SubClass-Of", "CornerNet:Method"]]}
{"doc_id": "202719327", "sentence": "Language model pre - training , such as BERT , has significantly improved the performances of many natural language processing tasks .", "ner": [["BERT", "Method"], ["natural language processing", "Task"]], "rel": [["BERT", "Used-For", "natural language processing"]], "rel_plus": [["BERT:Method", "Used-For", "natural language processing:Task"]]}
{"doc_id": "202719327", "sentence": "To accelerate inference and reduce model size while maintaining accuracy , we first propose a novel Transformer distillation method that is specially designed for knowledge distillation ( KD ) of the Transformer - based models .", "ner": [["Transformer distillation", "Method"], ["knowledge distillation", "Method"], ["KD", "Method"], ["Transformer", "Method"]], "rel": [["KD", "Synonym-Of", "knowledge distillation"], ["Transformer distillation", "SubClass-Of", "knowledge distillation"], ["Transformer distillation", "Used-For", "Transformer"]], "rel_plus": [["KD:Method", "Synonym-Of", "knowledge distillation:Method"], ["Transformer distillation:Method", "SubClass-Of", "knowledge distillation:Method"], ["Transformer distillation:Method", "Used-For", "Transformer:Method"]]}
{"doc_id": "202719327", "sentence": "By leveraging this new KD method , the plenty of knowledge encoded in a large teacher BERT can be effectively transferred to a small student Tiny - BERT .", "ner": [["KD", "Method"], ["BERT", "Method"], ["Tiny - BERT", "Method"]], "rel": [["KD", "Used-For", "BERT"], ["Tiny - BERT", "SubClass-Of", "BERT"]], "rel_plus": [["KD:Method", "Used-For", "BERT:Method"], ["Tiny - BERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "Then , we introduce a new two - stage learning framework for TinyBERT , which performs Transformer distillation at both the pretraining and task - specific learning stages .", "ner": [["TinyBERT", "Method"], ["Transformer distillation", "Method"]], "rel": [["Transformer distillation", "Used-For", "TinyBERT"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "This framework ensures that TinyBERT can capture he general - domain as well as the task - specific knowledge in BERT .", "ner": [["TinyBERT", "Method"], ["BERT", "Method"]], "rel": [["TinyBERT", "SubClass-Of", "BERT"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "TinyBERT with 4 layers is empirically effective and achieves more than 9 6 . 8 % the performance of its teacher BERTBASE on GLUE benchmark , while being 7. 5 x smaller and 9. 4 x faster on inference .", "ner": [["TinyBERT", "Method"], ["BERTBASE", "Method"], ["GLUE", "Dataset"]], "rel": [["TinyBERT", "SubClass-Of", "BERTBASE"], ["TinyBERT", "Evaluated-With", "GLUE"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERTBASE:Method"], ["TinyBERT:Method", "Evaluated-With", "GLUE:Dataset"]]}
{"doc_id": "202719327", "sentence": "TinyBERT with 4 layers is also significantly better than 4 - layer state - of - the - art baselines on BERT distillation , with only about 2 8 % parameters and about 3 1 % inference time of them .", "ner": [["TinyBERT", "Method"], ["BERT distillation", "Method"]], "rel": [["TinyBERT", "Compare-With", "BERT distillation"]], "rel_plus": [["TinyBERT:Method", "Compare-With", "BERT distillation:Method"]]}
{"doc_id": "202719327", "sentence": "Moreover , TinyBERT with 6 layers performs on - par with its teacher BERTBASE .", "ner": [["TinyBERT", "Method"], ["BERTBASE", "Method"]], "rel": [["TinyBERT", "Compare-With", "BERTBASE"]], "rel_plus": [["TinyBERT:Method", "Compare-With", "BERTBASE:Method"]]}
{"doc_id": "202719327", "sentence": "Pre - training language models then fine - tuning on downstream tasks has become a new paradigm for natural language processing ( NLP ) .", "ner": [["natural language processing", "Task"], ["NLP", "Task"]], "rel": [["NLP", "Synonym-Of", "natural language processing"]], "rel_plus": [["NLP:Task", "Synonym-Of", "natural language processing:Task"]]}
{"doc_id": "202719327", "sentence": "Pre - trained language models ( PLMs ) , such as BERT ( Devlin et al. , 2 0 1 8) , XLNet ( Yang et al. , 2 0 1 9 ) , RoBERTa and SpanBERT , have achieved great success in many NLP tasks ( e.g. , the GLUE benchmark ( Wang et al. , 2 0 1 8) and the challenging multi - hop reasoning task ( Ding et al. , 2 0 1 9 ) ) .", "ner": [["Pre - trained language models", "Method"], ["PLMs", "Method"], ["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"], ["SpanBERT", "Method"], ["NLP", "Task"], ["GLUE", "Dataset"], ["multi - hop reasoning", "Task"]], "rel": [["PLMs", "Synonym-Of", "Pre - trained language models"], ["BERT", "SubClass-Of", "Pre - trained language models"], ["XLNet", "SubClass-Of", "Pre - trained language models"], ["RoBERTa", "SubClass-Of", "Pre - trained language models"], ["SpanBERT", "SubClass-Of", "Pre - trained language models"], ["BERT", "Used-For", "NLP"], ["XLNet", "Used-For", "NLP"], ["RoBERTa", "Used-For", "NLP"], ["SpanBERT", "Used-For", "NLP"], ["Pre - trained language models", "Used-For", "NLP"], ["GLUE", "Benchmark-For", "NLP"], ["Pre - trained language models", "Used-For", "multi - hop reasoning"], ["BERT", "Used-For", "multi - hop reasoning"], ["XLNet", "Used-For", "multi - hop reasoning"], ["RoBERTa", "Used-For", "multi - hop reasoning"], ["SpanBERT", "Used-For", "multi - hop reasoning"]], "rel_plus": [["PLMs:Method", "Synonym-Of", "Pre - trained language models:Method"], ["BERT:Method", "SubClass-Of", "Pre - trained language models:Method"], ["XLNet:Method", "SubClass-Of", "Pre - trained language models:Method"], ["RoBERTa:Method", "SubClass-Of", "Pre - trained language models:Method"], ["SpanBERT:Method", "SubClass-Of", "Pre - trained language models:Method"], ["BERT:Method", "Used-For", "NLP:Task"], ["XLNet:Method", "Used-For", "NLP:Task"], ["RoBERTa:Method", "Used-For", "NLP:Task"], ["SpanBERT:Method", "Used-For", "NLP:Task"], ["Pre - trained language models:Method", "Used-For", "NLP:Task"], ["GLUE:Dataset", "Benchmark-For", "NLP:Task"], ["Pre - trained language models:Method", "Used-For", "multi - hop reasoning:Task"], ["BERT:Method", "Used-For", "multi - hop reasoning:Task"], ["XLNet:Method", "Used-For", "multi - hop reasoning:Task"], ["RoBERTa:Method", "Used-For", "multi - hop reasoning:Task"], ["SpanBERT:Method", "Used-For", "multi - hop reasoning:Task"]]}
{"doc_id": "202719327", "sentence": "The most commonly used techniques include quantization ( Gong et al. , 2 0 1 4 ) , weights pruning ( Han et al. , 2 0 1 5 b ) , and knowledge distillation ( KD ) ( Romero et al. , 2 0 1 4 ) .", "ner": [["quantization", "Method"], ["weights pruning", "Method"], ["knowledge distillation", "Method"], ["KD", "Method"]], "rel": [["KD", "Synonym-Of", "knowledge distillation"]], "rel_plus": [["KD:Method", "Synonym-Of", "knowledge distillation:Method"]]}
{"doc_id": "202719327", "sentence": "Based on the framework , we propose a novel distillation method specifically for Transformer - based models ( Vaswani et al. , 2 0 1 7 ) , and use BERT as an example to investigate the KD methods for large scale PLMs .", "ner": [["Transformer", "Method"], ["BERT", "Method"], ["KD", "Method"], ["PLMs", "Method"]], "rel": [["BERT", "SubClass-Of", "PLMs"], ["KD", "Used-For", "PLMs"]], "rel_plus": [["BERT:Method", "SubClass-Of", "PLMs:Method"], ["KD:Method", "Used-For", "PLMs:Method"]]}
{"doc_id": "202719327", "sentence": "Under review KD has been extensively studied in NLP ( Kim & Rush , 2 0 1 6 ; Hu et al. , 2 0 1 8) , while designing KD methods for BERT has been less explored .", "ner": [["KD", "Method"], ["NLP", "Task"], ["KD", "Method"], ["BERT", "Method"]], "rel": [["KD", "Used-For", "NLP"], ["KD", "Used-For", "BERT"]], "rel_plus": [["KD:Method", "Used-For", "NLP:Task"], ["KD:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "The pre - training - then - fine - tuning paradigm firstly pre - trains BERT on a large scale unsupervised text corpus , then fine - tunes it on task - specific dataset , which greatly increases the difficulty of BERT distillation .", "ner": [["BERT", "Method"], ["BERT distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "To build a competitive TinyBERT , we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT .", "ner": [["TinyBERT", "Method"], ["Transformer distillation", "Method"], ["BERT", "Method"]], "rel": [["Transformer distillation", "Used-For", "TinyBERT"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "Specifically , we design several loss functions to fit different representations from BERT layers : 1 ) the output of the embedding layer ; 2 ) the hidden states and attention matrices derived from the Transformer layer ; 3 ) the logits output by the prediction layer .", "ner": [["BERT", "Method"], ["Transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "The attention based fitting is inspired by the recent findings ( Clark et al. , 2 0 1 9 ) that the attention weights learned by BERT can capture substantial linguistic knowledge , which encourages that the linguistic knowledge can be well transferred from teacher BERT to student TinyBERT .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"]], "rel": [["TinyBERT", "SubClass-Of", "BERT"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "However , it is ignored in existing KD methods of BERT , such as Distilled BiLSTM SOFT , BERT - PKD ( Sun et al. , 2 0 1 9 ) and DistilBERT 2 .", "ner": [["KD", "Method"], ["BERT", "Method"], ["Distilled BiLSTM SOFT", "Method"], ["BERT - PKD", "Method"], ["DistilBERT 2", "Method"]], "rel": [["Distilled BiLSTM SOFT", "SubClass-Of", "KD"], ["BERT - PKD", "SubClass-Of", "KD"], ["DistilBERT 2", "SubClass-Of", "KD"]], "rel_plus": [["Distilled BiLSTM SOFT:Method", "SubClass-Of", "KD:Method"], ["BERT - PKD:Method", "SubClass-Of", "KD:Method"], ["DistilBERT 2:Method", "SubClass-Of", "KD:Method"]]}
{"doc_id": "202719327", "sentence": "The student TinyBERT learns to mimic the teacher 's behavior by executing the proposed Transformer distillation on the large scale corpus from general domain .", "ner": [["TinyBERT", "Method"], ["Transformer distillation", "Method"]], "rel": [["Transformer distillation", "Used-For", "TinyBERT"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "At the task - specific distillation stage , we perform the data augmentation to provide more task - related materials for teacherstudent learning , and then re - execute the Transformer distillation on the augmented data .", "ner": [["task - specific distillation", "Method"], ["data augmentation", "Method"], ["Transformer distillation", "Method"]], "rel": [["data augmentation", "Used-For", "task - specific distillation"], ["Transformer distillation", "Used-For", "task - specific distillation"]], "rel_plus": [["data augmentation:Method", "Used-For", "task - specific distillation:Method"], ["Transformer distillation:Method", "Used-For", "task - specific distillation:Method"]]}
{"doc_id": "202719327", "sentence": "TinyBERT ( our method ) The main contributions of this work are as follows : 1 ) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be well transferred to TinyBERT . 2 ) We propose a novel two - stage learning framework with performing the proposed Transformer distillation at both the pre - training and fine - tuning stages , which ensures that Tiny - BERT can capture both the general - domain and task - specific knowledge of the teacher BERT . 3 ) We show experimentally that our TinyBERT can achieve comparable results with teacher BERT on GLUE tasks , while having much fewer parameters ( \u223c 1 3 . 3 % ) and less inference time ( \u223c 1 0 . 6 % ) , and significantly outperforms other state - of - the - art baselines on BERT distillation .", "ner": [["TinyBERT", "Method"], ["Transformer distillation", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"], ["Transformer distillation", "Method"], ["Tiny - BERT", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"], ["BERT", "Method"], ["GLUE", "Dataset"], ["BERT distillation", "Method"]], "rel": [["Transformer distillation", "Used-For", "BERT"], ["TinyBERT", "SubClass-Of", "BERT"], ["Tiny - BERT", "SubClass-Of", "BERT"], ["TinyBERT", "Compare-With", "BERT"], ["TinyBERT", "Evaluated-With", "GLUE"], ["BERT", "Evaluated-With", "GLUE"], ["BERT distillation", "Evaluated-With", "GLUE"], ["TinyBERT", "Compare-With", "BERT distillation"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "BERT:Method"], ["TinyBERT:Method", "SubClass-Of", "BERT:Method"], ["Tiny - BERT:Method", "SubClass-Of", "BERT:Method"], ["TinyBERT:Method", "Compare-With", "BERT:Method"], ["TinyBERT:Method", "Evaluated-With", "GLUE:Dataset"], ["BERT:Method", "Evaluated-With", "GLUE:Dataset"], ["BERT distillation:Method", "Evaluated-With", "GLUE:Dataset"], ["TinyBERT:Method", "Compare-With", "BERT distillation:Method"]]}
{"doc_id": "202719327", "sentence": "We firstly describe the formulation of Transformer ( Vaswani et al. , 2 0 1 7 ) and Knowledge Distillation ( Hinton et al. , 2 0 1 5 ) .", "ner": [["Transformer", "Method"], ["Knowledge Distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Our proposed Transformer distillation is a specially designed KD method for Transformer - based models .", "ner": [["Transformer distillation", "Method"], ["KD", "Method"], ["Transformer - based models", "Method"]], "rel": [["Transformer distillation", "SubClass-Of", "KD"], ["Transformer distillation", "Used-For", "Transformer - based models"]], "rel_plus": [["Transformer distillation:Method", "SubClass-Of", "KD:Method"], ["Transformer distillation:Method", "Used-For", "Transformer - based models:Method"]]}
{"doc_id": "202719327", "sentence": "Most of recent pre - trained language models ( e.g. , BERT , XLNet and RoBERTa ) are built with Transformer layers , which can capture long - term dependencies between input tokens by self - attention mechanism .", "ner": [["pre - trained language models", "Method"], ["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"], ["Transformer", "Method"], ["self - attention mechanism", "Method"]], "rel": [["BERT", "SubClass-Of", "pre - trained language models"], ["XLNet", "SubClass-Of", "pre - trained language models"], ["RoBERTa", "SubClass-Of", "pre - trained language models"], ["Transformer", "Part-Of", "pre - trained language models"], ["self - attention mechanism", "Part-Of", "Transformer"]], "rel_plus": [["BERT:Method", "SubClass-Of", "pre - trained language models:Method"], ["XLNet:Method", "SubClass-Of", "pre - trained language models:Method"], ["RoBERTa:Method", "SubClass-Of", "pre - trained language models:Method"], ["Transformer:Method", "Part-Of", "pre - trained language models:Method"], ["self - attention mechanism:Method", "Part-Of", "Transformer:Method"]]}
{"doc_id": "202719327", "sentence": "Specifically , a standard Transformer layer includes two main sub - layers : multi - head attention ( MHA ) and fully connected feed - forward network ( FFN ) .", "ner": [["Transformer layer", "Method"], ["multi - head attention", "Method"], ["MHA", "Method"], ["fully connected feed - forward network", "Method"], ["FFN", "Method"]], "rel": [["multi - head attention", "Part-Of", "Transformer layer"], ["fully connected feed - forward network", "Part-Of", "Transformer layer"], ["MHA", "Synonym-Of", "multi - head attention"], ["FFN", "Synonym-Of", "fully connected feed - forward network"]], "rel_plus": [["multi - head attention:Method", "Part-Of", "Transformer layer:Method"], ["fully connected feed - forward network:Method", "Part-Of", "Transformer layer:Method"], ["MHA:Method", "Synonym-Of", "multi - head attention:Method"], ["FFN:Method", "Synonym-Of", "fully connected feed - forward network:Method"]]}
{"doc_id": "202719327", "sentence": "Multi - Head Attention ( MHA ) .", "ner": [["Multi - Head Attention", "Method"], ["MHA", "Method"]], "rel": [["MHA", "Synonym-Of", "Multi - Head Attention"]], "rel_plus": [["MHA:Method", "Synonym-Of", "Multi - Head Attention:Method"]]}
{"doc_id": "202719327", "sentence": "The final function output is calculated as a weighted sum of values V , and the weight is computed by applying softmax ( ) operation on the matrix A. The attention matrices A of BERT can capture substantial linguistic knowledge and plays an essential role in our proposed distillation method .", "ner": [["softmax", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Position - wise Feed - Forward Network ( FFN ) .", "ner": [["Position - wise Feed - Forward Network", "Method"], ["FFN", "Method"]], "rel": [["FFN", "Synonym-Of", "Position - wise Feed - Forward Network"]], "rel_plus": [["FFN:Method", "Synonym-Of", "Position - wise Feed - Forward Network:Method"]]}
{"doc_id": "202719327", "sentence": "Transformer layer also contains a fully connected feed - forward network , which is formulated as follows : We can see that the FFN contains two linear transformations and one ReLU activation .", "ner": [["Transformer layer", "Method"], ["fully connected feed - forward network", "Method"], ["FFN", "Method"], ["linear transformations", "Method"], ["ReLU activation", "Method"]], "rel": [["fully connected feed - forward network", "Part-Of", "Transformer layer"], ["linear transformations", "Part-Of", "FFN"], ["ReLU activation", "Part-Of", "FFN"]], "rel_plus": [["fully connected feed - forward network:Method", "Part-Of", "Transformer layer:Method"], ["linear transformations:Method", "Part-Of", "FFN:Method"], ["ReLU activation:Method", "Part-Of", "FFN:Method"]]}
{"doc_id": "202719327", "sentence": "In the context of Transformer distillation , the output of MHA layer or FFN layer , or some intermediate representations ( e.g. the attention matrix A ) can be used as behavior function .", "ner": [["Transformer distillation", "Method"], ["MHA", "Method"], ["FFN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Different from previous KD methods , we also need to consider how to perform KD at the pre - training stage of BERT , which further increases the difficulty of KD for BERT .", "ner": [["KD", "Method"], ["KD", "Method"], ["BERT", "Method"], ["KD", "Method"], ["BERT", "Method"]], "rel": [["KD", "Used-For", "BERT"], ["KD", "Used-For", "BERT"]], "rel_plus": [["KD:Method", "Used-For", "BERT:Method"], ["KD:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "In this section , we propose a novel distillation method for Transformer - based models , then present a two - stage learning framework of TinyBERT .", "ner": [["Transformer", "Method"], ["TinyBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "The proposed Transformer distillation is a specially designed KD method for Transformer networks .", "ner": [["Transformer distillation", "Method"], ["KD", "Method"], ["Transformer", "Method"]], "rel": [["Transformer distillation", "SubClass-Of", "KD"], ["Transformer distillation", "Used-For", "Transformer"]], "rel_plus": [["Transformer distillation:Method", "SubClass-Of", "KD:Method"], ["Transformer distillation:Method", "Used-For", "Transformer:Method"]]}
{"doc_id": "202719327", "sentence": "Assuming that the student model has M Transformer layers and teacher model has N Transformer layers , we choose M layers from the teacher model for the Transformerlayer distillation .", "ner": [["Transformer layers", "Method"], ["Transformer layers", "Method"], ["Transformerlayer distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "The embedding - layer distillation and the prediction - layer distillation are also considered .", "ner": [["embedding - layer distillation", "Method"], ["prediction - layer distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Thus we propose the attention based distillation to ensure that the linguistic knowledge is transferred from teacher BERT to student TinyBERT .", "ner": [["attention based distillation", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"]], "rel": [["attention based distillation", "Used-For", "BERT"], ["TinyBERT", "SubClass-Of", "BERT"], ["attention based distillation", "Used-For", "TinyBERT"]], "rel_plus": [["attention based distillation:Method", "Used-For", "BERT:Method"], ["TinyBERT:Method", "SubClass-Of", "BERT:Method"], ["attention based distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "In this work , the attention matrix A i is used as the fitting target instead of its softmax output softmax(A i ) , since our experiments show that this setting has faster convergence and better performances .", "ner": [["softmax", "Method"], ["softmax(A i )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Using the above distillation objectives ( i.e. Equations 7 , 8 , 9 and 1 0 ) , we can unify the loss of model layer : In our experiments , we firstly perform intermediate layer distillation ( M \u2265 m \u2265 0 ) , then perform the prediction - layer distillation ( m = M + 1 ) .", "ner": [["intermediate layer distillation", "Method"], ["prediction - layer distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "General distillation helps student TinyBERT learn the rich knowledge embedded in teacher BERT , which plays an important role in improving the generalization capability of TinyBERT .", "ner": [["TinyBERT", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"]], "rel": [["TinyBERT", "SubClass-Of", "BERT"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "By performing the Transformer distillation 5 on the text from general domain , we obtain a general TinyBERT that can be fine - tuned for downstream tasks .", "ner": [["Transformer distillation", "Method"], ["TinyBERT", "Method"]], "rel": [["Transformer distillation", "Used-For", "TinyBERT"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "However , due to the big reductions in the hidden/embedding size and the layer number , general TinyBERT performs relatively worse than the original BERT .", "ner": [["TinyBERT", "Method"], ["BERT", "Method"]], "rel": [["TinyBERT", "Compare-With", "BERT"]], "rel_plus": [["TinyBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "To this end , we propose to derive competitive fine - tuned TinyBERTs through the task - specific distillation .", "ner": [["TinyBERTs", "Method"], ["task - specific distillation", "Method"]], "rel": [["task - specific distillation", "Used-For", "TinyBERTs"]], "rel_plus": [["task - specific distillation:Method", "Used-For", "TinyBERTs:Method"]]}
{"doc_id": "202719327", "sentence": "In the task - specific distillation , we re - perform the proposed Transformer distillation on augmented task - specific dataset ( as shown in Figure 2 ) .", "ner": [["task - specific distillation", "Method"], ["Transformer distillation", "Method"]], "rel": [["Transformer distillation", "Used-For", "task - specific distillation"]], "rel_plus": [["Transformer distillation:Method", "Used-For", "task - specific distillation:Method"]]}
{"doc_id": "202719327", "sentence": "Specifically , the fine - tuned big BERT is used as the teacher and a new data augmentation method is proposed to extend the task - specific training set .", "ner": [["BERT", "Method"], ["data augmentation", "Method"]], "rel": [["data augmentation", "Used-For", "BERT"]], "rel_plus": [["data augmentation:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "In this work , we combine a pre - trained language model BERT and GloVe ( Pennington et al. , 2 0 1 4 ) word embeddings to do word - level replacement for data augmentation .", "ner": [["pre - trained language model", "Method"], ["BERT", "Method"], ["GloVe", "Method"], ["word embeddings", "Method"], ["data augmentation", "Method"]], "rel": [["BERT", "SubClass-Of", "pre - trained language model"], ["GloVe", "Used-For", "data augmentation"], ["BERT", "Used-For", "data augmentation"]], "rel_plus": [["BERT:Method", "SubClass-Of", "pre - trained language model:Method"], ["GloVe:Method", "Used-For", "data augmentation:Method"], ["BERT:Method", "Used-For", "data augmentation:Method"]]}
{"doc_id": "202719327", "sentence": "More details of data augmentation procedure are discussed in Appendix A. The above two learning stages are complementary to each other : the general distillation provides a good initialization for the task - specific distillation , while the task - specific distillation further improves TinyBERT by focusing on learning the task - specific knowledge .", "ner": [["data augmentation", "Method"], ["task - specific distillation", "Method"], ["task - specific distillation", "Method"], ["TinyBERT", "Method"]], "rel": [["task - specific distillation", "Used-For", "TinyBERT"]], "rel_plus": [["task - specific distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "Although there is a big gap between BERT and TinyBERT in model size , by performing the proposed two - stage distillation , the TinyBERT can achieve comparable performances as large BERT in various NLP tasks .", "ner": [["BERT", "Method"], ["TinyBERT", "Method"], ["TinyBERT", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["BERT", "Compare-With", "TinyBERT"], ["TinyBERT", "Compare-With", "BERT"], ["BERT", "Used-For", "NLP"], ["TinyBERT", "Used-For", "NLP"]], "rel_plus": [["BERT:Method", "Compare-With", "TinyBERT:Method"], ["TinyBERT:Method", "Compare-With", "BERT:Method"], ["BERT:Method", "Used-For", "NLP:Task"], ["TinyBERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "202719327", "sentence": "We use g(m ) = 3 \u00d7 m as the layer mapping function , so TinyBERT learns from every 3 layers of BERT BASE .", "ner": [["TinyBERT", "Method"], ["BERT BASE", "Method"]], "rel": [["TinyBERT", "SubClass-Of", "BERT BASE"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT BASE:Method"]]}
{"doc_id": "202719327", "sentence": "We evaluate TinyBERT on the General Language Understanding Evaluation ( GLUE ) ( Wang et al. , 2 0 1 8) benchmark , which is a collection of diverse natural language understanding tasks .", "ner": [["TinyBERT", "Method"], ["General Language Understanding Evaluation", "Dataset"], ["GLUE", "Dataset"], ["natural language understanding", "Task"]], "rel": [["GLUE", "Synonym-Of", "General Language Understanding Evaluation"], ["TinyBERT", "Evaluated-With", "General Language Understanding Evaluation"], ["General Language Understanding Evaluation", "Benchmark-For", "natural language understanding"], ["TinyBERT", "Used-For", "natural language understanding"]], "rel_plus": [["GLUE:Dataset", "Synonym-Of", "General Language Understanding Evaluation:Dataset"], ["TinyBERT:Method", "Evaluated-With", "General Language Understanding Evaluation:Dataset"], ["General Language Understanding Evaluation:Dataset", "Benchmark-For", "natural language understanding:Task"], ["TinyBERT:Method", "Used-For", "natural language understanding:Task"]]}
{"doc_id": "202719327", "sentence": "The experiment results demonstrate that : 1 ) There is a large performance gap between BERT SMALL and BERT BASE due to the big reduction in model size . 2 ) TinyBERT is consistently better than BERT SMALL in all the GLUE tasks and achieves a large improvement of 6. 3 % on average .", "ner": [["BERT SMALL", "Method"], ["BERT BASE", "Method"], ["TinyBERT", "Method"], ["BERT SMALL", "Method"], ["GLUE", "Dataset"]], "rel": [["BERT SMALL", "Compare-With", "BERT BASE"], ["GLUE", "Evaluated-With", "TinyBERT"], ["TinyBERT", "Compare-With", "BERT SMALL"], ["GLUE", "Evaluated-With", "BERT SMALL"]], "rel_plus": [["BERT SMALL:Method", "Compare-With", "BERT BASE:Method"], ["GLUE:Dataset", "Evaluated-With", "TinyBERT:Method"], ["TinyBERT:Method", "Compare-With", "BERT SMALL:Method"], ["GLUE:Dataset", "Evaluated-With", "BERT SMALL:Method"]]}
{"doc_id": "202719327", "sentence": "This indicates that the proposed KD learning framework can effectively improve the performances of small models regardless of downstream tasks . 3 ) TinyBERT significantly outperforms the state - ofthe - art KD baselines ( i.e. , BERT - PKD and DistillBERT ) by a margin of at least 3. 9 % , even with only \u223c 2 8 % parameters and \u223c 3 1 % inference time of baselines ( see Table 3 ) . 4 ) Compared with the teacher BERT BASE , TinyBERT is 7. 5 x smaller and 9. 4 x faster in the model efficiency , while maintaining comparable performances . 5 ) TinyBERT has a comparable model efficiency ( slightly larger in size but faster in inference ) with Distilled BiLSTM SOFT and obtains substantially better performances in all tasks reported by the BiLSTM baseline . 6 ) For the challenging CoLA dataset ( the task of predicting linguistic acceptability judgments ) , all the distilled small models have a relatively bigger performance gap with teacher model .", "ner": [["KD", "Method"], ["TinyBERT", "Method"], ["KD", "Method"], ["BERT - PKD", "Method"], ["DistillBERT", "Method"], ["BERT BASE", "Method"], ["TinyBERT", "Method"], ["TinyBERT", "Method"], ["Distilled BiLSTM SOFT", "Method"], ["BiLSTM", "Method"], ["CoLA", "Dataset"]], "rel": [["TinyBERT", "Compare-With", "BERT - PKD"], ["KD", "Used-For", "BERT - PKD"], ["TinyBERT", "Compare-With", "DistillBERT"], ["KD", "Used-For", "DistillBERT"], ["TinyBERT", "Compare-With", "BERT BASE"], ["TinyBERT", "Compare-With", "Distilled BiLSTM SOFT"], ["TinyBERT", "Compare-With", "BiLSTM"]], "rel_plus": [["TinyBERT:Method", "Compare-With", "BERT - PKD:Method"], ["KD:Method", "Used-For", "BERT - PKD:Method"], ["TinyBERT:Method", "Compare-With", "DistillBERT:Method"], ["KD:Method", "Used-For", "DistillBERT:Method"], ["TinyBERT:Method", "Compare-With", "BERT BASE:Method"], ["TinyBERT:Method", "Compare-With", "Distilled BiLSTM SOFT:Method"], ["TinyBERT:Method", "Compare-With", "BiLSTM:Method"]]}
{"doc_id": "202719327", "sentence": "Moreover , BERT - PKD and DistillBERT initialize their student models with some layers of well pre - trained teacher BERT ( see Table 1 ) , which makes the student models have to keep the same size settings of Transformer layer ( or embedding layer ) as their teacher BERT .", "ner": [["BERT - PKD", "Method"], ["DistillBERT", "Method"], ["BERT", "Method"], ["Transformer layer", "Method"], ["BERT", "Method"]], "rel": [["Transformer layer", "Part-Of", "BERT - PKD"], ["Transformer layer", "Part-Of", "DistillBERT"], ["DistillBERT", "SubClass-Of", "BERT"], ["BERT - PKD", "SubClass-Of", "BERT"], ["Transformer layer", "Part-Of", "BERT"]], "rel_plus": [["Transformer layer:Method", "Part-Of", "BERT - PKD:Method"], ["Transformer layer:Method", "Part-Of", "DistillBERT:Method"], ["DistillBERT:Method", "SubClass-Of", "BERT:Method"], ["BERT - PKD:Method", "SubClass-Of", "BERT:Method"], ["Transformer layer:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "We evaluate how much improvement can be achieved when increasing the model size of TinyBERT on several typical GLUE tasks , where MNLI and MRPC are used in the ablation studies of Devlin et al. ( 2 0 1 8) , and CoLA is the most difficult task in GLUE .", "ner": [["TinyBERT", "Method"], ["GLUE", "Dataset"], ["MNLI", "Dataset"], ["MRPC", "Dataset"], ["CoLA", "Dataset"], ["GLUE", "Dataset"]], "rel": [["TinyBERT", "Evaluated-With", "GLUE"], ["MNLI", "SubClass-Of", "GLUE"], ["MRPC", "SubClass-Of", "GLUE"], ["TinyBERT", "Evaluated-With", "MNLI"], ["TinyBERT", "Evaluated-With", "MRPC"], ["TinyBERT", "Evaluated-With", "CoLA"], ["CoLA", "SubClass-Of", "GLUE"]], "rel_plus": [["TinyBERT:Method", "Evaluated-With", "GLUE:Dataset"], ["MNLI:Dataset", "SubClass-Of", "GLUE:Dataset"], ["MRPC:Dataset", "SubClass-Of", "GLUE:Dataset"], ["TinyBERT:Method", "Evaluated-With", "MNLI:Dataset"], ["TinyBERT:Method", "Evaluated-With", "MRPC:Dataset"], ["TinyBERT:Method", "Evaluated-With", "CoLA:Dataset"], ["CoLA:Dataset", "SubClass-Of", "GLUE:Dataset"]]}
{"doc_id": "202719327", "sentence": "We can observe that : 1 ) All the three TinyBERT variants can consistently outperform the original smallest TinyBERT , which indicates that the proposed KD method works for the student models of various model sizes . 2 ) For the CoLA task , the improvement is slight when only increasing the number of layers ( from 4 9 . 7 to 5 0 . 6 ) or hidden size ( from 4 9 . 7 to 5 0 . 5 ) .", "ner": [["TinyBERT variants", "Method"], ["TinyBERT", "Method"], ["KD", "Method"], ["CoLA", "Dataset"]], "rel": [["KD", "Used-For", "TinyBERT variants"], ["TinyBERT variants", "Compare-With", "TinyBERT"], ["KD", "Used-For", "TinyBERT"]], "rel_plus": [["KD:Method", "Used-For", "TinyBERT variants:Method"], ["TinyBERT variants:Method", "Compare-With", "TinyBERT:Method"], ["KD:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "To achieve more dramatic improve - ments , the student model should become deeper and wider ( from 4 9 . 7 to 5 4 . 0 ) . 3 ) Another interesting observation is that the smallest 4 - layer TinyBERT can even outperform the 6 - layers baselines , which further confirms the effectiveness of the proposed KD method .", "ner": [["4 - layer TinyBERT", "Method"], ["KD", "Method"]], "rel": [["KD", "Used-For", "4 - layer TinyBERT"]], "rel_plus": [["KD:Method", "Used-For", "4 - layer TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "The proposed two - stage TinyBERT learning framework ( see Figure 2 ) consists of three key procedures : TD ( Task - specific Distillation ) , GD ( General Distillation ) and DA ( Data Augmentation ) .", "ner": [["two - stage TinyBERT learning framework", "Method"], ["TD", "Method"], ["Task - specific Distillation", "Method"], ["GD", "Method"], ["General Distillation", "Method"], ["DA", "Method"], ["Data Augmentation", "Method"]], "rel": [["TD", "Part-Of", "two - stage TinyBERT learning framework"], ["GD", "Part-Of", "two - stage TinyBERT learning framework"], ["DA", "Part-Of", "two - stage TinyBERT learning framework"], ["TD", "Synonym-Of", "Task - specific Distillation"], ["GD", "Synonym-Of", "General Distillation"], ["DA", "Synonym-Of", "Data Augmentation"]], "rel_plus": [["TD:Method", "Part-Of", "two - stage TinyBERT learning framework:Method"], ["GD:Method", "Part-Of", "two - stage TinyBERT learning framework:Method"], ["DA:Method", "Part-Of", "two - stage TinyBERT learning framework:Method"], ["TD:Method", "Synonym-Of", "Task - specific Distillation:Method"], ["GD:Method", "Synonym-Of", "General Distillation:Method"], ["DA:Method", "Synonym-Of", "Data Augmentation:Method"]]}
{"doc_id": "202719327", "sentence": "The TD and DA has comparable effects in all the four tasks .", "ner": [["TD", "Method"], ["DA", "Method"]], "rel": [["TD", "Compare-With", "DA"]], "rel_plus": [["TD:Method", "Compare-With", "DA:Method"]]}
{"doc_id": "202719327", "sentence": "We can also find the task - specific procedures ( TD and DA ) are more helpful than the pre - training procedure ( GD ) in all the four tasks .", "ner": [["task - specific procedures", "Method"], ["TD", "Method"], ["DA", "Method"], ["pre - training procedure", "Method"], ["GD", "Method"]], "rel": [["TD", "SubClass-Of", "task - specific procedures"], ["DA", "SubClass-Of", "task - specific procedures"], ["GD", "SubClass-Of", "pre - training procedure"], ["task - specific procedures", "Compare-With", "pre - training procedure"]], "rel_plus": [["TD:Method", "SubClass-Of", "task - specific procedures:Method"], ["DA:Method", "SubClass-Of", "task - specific procedures:Method"], ["GD:Method", "SubClass-Of", "pre - training procedure:Method"], ["task - specific procedures:Method", "Compare-With", "pre - training procedure:Method"]]}
{"doc_id": "202719327", "sentence": "Another interesting observation is that GD has more effect on CoLA than on MNLI and MRPC .", "ner": [["GD", "Method"], ["CoLA", "Dataset"], ["MNLI", "Dataset"], ["MRPC", "Dataset"]], "rel": [["GD", "Evaluated-With", "CoLA"], ["GD", "Evaluated-With", "MNLI"], ["GD", "Evaluated-With", "MRPC"]], "rel_plus": [["GD:Method", "Evaluated-With", "CoLA:Dataset"], ["GD:Method", "Evaluated-With", "MNLI:Dataset"], ["GD:Method", "Evaluated-With", "MRPC:Dataset"]]}
{"doc_id": "202719327", "sentence": "We conjecture that the ability of linguistic generalization ( Warstadt et al. , 2 0 1 8) learned by GD plays a more important role in the downstream CoLA task ( linguistic acceptability judgments ) .", "ner": [["GD", "Method"], ["CoLA", "Dataset"], ["linguistic acceptability judgments", "Task"]], "rel": [["GD", "Evaluated-With", "CoLA"], ["CoLA", "Benchmark-For", "linguistic acceptability judgments"], ["GD", "Used-For", "linguistic acceptability judgments"]], "rel_plus": [["GD:Method", "Evaluated-With", "CoLA:Dataset"], ["CoLA:Dataset", "Benchmark-For", "linguistic acceptability judgments:Task"], ["GD:Method", "Used-For", "linguistic acceptability judgments:Task"]]}
{"doc_id": "202719327", "sentence": "Several baselines are proposed including the TinyBERT learning without the Transformer - layer distillation ( No Trm ) , embedding - layer distillation ( No Emb ) and predictionlayer distillation ( No Pred ) 6 respectively .", "ner": [["TinyBERT", "Method"], ["Transformer - layer distillation", "Method"], ["embedding - layer distillation", "Method"], ["predictionlayer distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "The performance drops significantly from 7 5 . 3 to 5 6 . 3 under the setting ( No Trm ) , which indicates Transformer - layer distillation is the key for TinyBERT learning .", "ner": [["Transformer - layer distillation", "Method"], ["TinyBERT", "Method"]], "rel": [["Transformer - layer distillation", "Used-For", "TinyBERT"]], "rel_plus": [["Transformer - layer distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "Meanwhile , these two kinds of knowledge distillation are complementary to each other , which makes TinyBERT obtain the competitive results .", "ner": [["knowledge distillation", "Method"], ["TinyBERT", "Method"]], "rel": [["knowledge distillation", "Used-For", "TinyBERT"]], "rel_plus": [["knowledge distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "We find that the top - strategy performs better than the bottom - strategy in MNLI , while being worse in MRPC and CoLA tasks , which confirms the observations that different tasks depend on the different kinds of knowledge from BERT layers .", "ner": [["MNLI", "Dataset"], ["MRPC", "Dataset"], ["CoLA", "Dataset"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "We also evaluate TinyBERT on the question answering tasks , and study whether we can use BERT SMALL as the initialization of the general TinyBERT .", "ner": [["TinyBERT", "Method"], ["question answering", "Task"], ["BERT SMALL", "Method"], ["TinyBERT", "Method"]], "rel": [["TinyBERT", "Used-For", "question answering"], ["BERT SMALL", "Used-For", "TinyBERT"]], "rel_plus": [["TinyBERT:Method", "Used-For", "question answering:Task"], ["BERT SMALL:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "The experiments are detailed in Appendix C and D. In this paper , we firstly introduce a new KD method for Transformer - based distillation , then we further propose a two - stage framework for TinyBERT learning .", "ner": [["KD", "Method"], ["Transformer - based distillation", "Method"], ["TinyBERT", "Method"]], "rel": [["KD", "Used-For", "Transformer - based distillation"]], "rel_plus": [["KD:Method", "Used-For", "Transformer - based distillation:Method"]]}
{"doc_id": "202719327", "sentence": "Extensive experiments show that the TinyBERT achieves competitive performances meanwhile significantly reducing the model size and shortening the inference time of original BERT BASE , which provides an effective way to deploy BERT - based NLP applications on the edge devices .", "ner": [["TinyBERT", "Method"], ["BERT BASE", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["TinyBERT", "Compare-With", "BERT BASE"], ["BERT", "Used-For", "NLP"]], "rel_plus": [["TinyBERT:Method", "Compare-With", "BERT BASE:Method"], ["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "202719327", "sentence": "In future work , we would study how to effectively transfer the knowledge from wider and deeper teachers ( e.g. , BERT LARGE and XLNet LARGE ) to student TinyBERT .", "ner": [["BERT LARGE", "Method"], ["XLNet LARGE", "Method"], ["TinyBERT", "Method"]], "rel": [["TinyBERT", "SubClass-Of", "BERT LARGE"], ["TinyBERT", "SubClass-Of", "XLNet LARGE"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT LARGE:Method"], ["TinyBERT:Method", "SubClass-Of", "XLNet LARGE:Method"]]}
{"doc_id": "202719327", "sentence": "The joint learning of distillation and quantization/pruning would be another promising direction to further compress the pre - trained language models .   In this section , we explain the proposed data augmentation method .", "ner": [["data augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "TinyBERT learning includes the general distillation and the task - specific distillation .", "ner": [["TinyBERT", "Method"], ["general distillation", "Method"], ["task - specific distillation", "Method"]], "rel": [["task - specific distillation", "Used-For", "TinyBERT"], ["general distillation", "Used-For", "TinyBERT"]], "rel_plus": [["task - specific distillation:Method", "Used-For", "TinyBERT:Method"], ["general distillation:Method", "Used-For", "TinyBERT:Method"]]}
{"doc_id": "202719327", "sentence": "For the general distillation , we use English Wikipedia ( 2, 5 0 0 M words ) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre - trained BERT BASE teacher and keep other hyper - parameters same as BERT pre - training ( Devlin et al. , 2 0 1 8) .", "ner": [["general distillation", "Method"], ["English Wikipedia", "Dataset"], ["intermediate layer distillation", "Method"], ["BERT BASE", "Method"], ["BERT", "Method"]], "rel": [["intermediate layer distillation", "Trained-With", "English Wikipedia"], ["intermediate layer distillation", "Used-For", "BERT BASE"]], "rel_plus": [["intermediate layer distillation:Method", "Trained-With", "English Wikipedia:Dataset"], ["intermediate layer distillation:Method", "Used-For", "BERT BASE:Method"]]}
{"doc_id": "202719327", "sentence": "For task - specific distillation , we firstly perform intermediate layer distillation on the augmented dataset for 1 0 epochs with batch size 3 2 and learning rate 5e - 5 under the supervision of a fine - tuned BERT teacher , and then perform prediction layer distillation for 3 epochs with batch size 3 2 and learning rate 3e - 5 .", "ner": [["task - specific distillation", "Method"], ["intermediate layer distillation", "Method"], ["BERT", "Method"], ["prediction layer distillation", "Method"]], "rel": [["intermediate layer distillation", "Used-For", "task - specific distillation"], ["prediction layer distillation", "Used-For", "task - specific distillation"], ["intermediate layer distillation", "Used-For", "BERT"]], "rel_plus": [["intermediate layer distillation:Method", "Used-For", "task - specific distillation:Method"], ["prediction layer distillation:Method", "Used-For", "task - specific distillation:Method"], ["intermediate layer distillation:Method", "Used-For", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "For tasks like MNLI , QQP and QNLI which have \u2265 1 0 0 K training examples , we distill intermediate layer knowledge for 5 epochs with batch size 2 5 6 on the augmented dataset .", "ner": [["MNLI", "Dataset"], ["QQP", "Dataset"], ["QNLI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Besides , for CoLA task , we perform 5 0 epochs of intermediate layer distillation .", "ner": [["CoLA", "Dataset"], ["intermediate layer distillation", "Method"]], "rel": [["intermediate layer distillation", "Trained-With", "CoLA"]], "rel_plus": [["intermediate layer distillation:Method", "Trained-With", "CoLA:Dataset"]]}
{"doc_id": "202719327", "sentence": "We use BERT - PKD and DistilBERT as our baselines .", "ner": [["BERT - PKD", "Method"], ["DistilBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "For a fair comparison , we firstly re - implemented the results of BERT - PKD and DistilBERT reported in their papers to ensure our implementation procedure is correct .", "ner": [["BERT - PKD", "Method"], ["DistilBERT", "Method"]], "rel": [["BERT - PKD", "Compare-With", "DistilBERT"]], "rel_plus": [["BERT - PKD:Method", "Compare-With", "DistilBERT:Method"]]}
{"doc_id": "202719327", "sentence": "Then following the verified implementation procedure , we trained a 4 - layer BERT - PKD and a 4 - layer DistilBERT as the baselines .", "ner": [["BERT - PKD", "Method"], ["DistilBERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "The BERT SMALL learning strictly follows the same learning strategy as described in the original BERT work ( Devlin et al. , 2 0 1 8) .", "ner": [["BERT SMALL", "Method"], ["BERT", "Method"]], "rel": [["BERT SMALL", "SubClass-Of", "BERT"]], "rel_plus": [["BERT SMALL:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "The GLUE datasets are described as follows : MNLI .", "ner": [["GLUE", "Dataset"], ["MNLI", "Dataset"]], "rel": [["MNLI", "SubClass-Of", "GLUE"]], "rel_plus": [["MNLI:Dataset", "SubClass-Of", "GLUE:Dataset"]]}
{"doc_id": "202719327", "sentence": "The Stanford Sentiment Treebank is a binary single - sentence classification task , where the goal is to predict the sentiment of movie reviews ( Socher et al. , 2 0 1 3 ) .", "ner": [["Stanford Sentiment Treebank", "Dataset"], ["binary single - sentence classification", "Task"]], "rel": [["Stanford Sentiment Treebank", "Benchmark-For", "binary single - sentence classification"]], "rel_plus": [["Stanford Sentiment Treebank:Dataset", "Benchmark-For", "binary single - sentence classification:Task"]]}
{"doc_id": "202719327", "sentence": "C SQUAD 1. 1 AND 2. 0 We also demonstrate the effectiveness of TinyBERT on the question answering ( QA ) tasks : SQuAD v 1 . 1 ( Rajpurkar et al. , 2 0 1 6 ) and v 2 . 0 ( Rajpurkar et al. , 2 0 1 8) .", "ner": [["SQUAD 1. 1 AND 2. 0", "Dataset"], ["TinyBERT", "Method"], ["question answering", "Task"], ["QA", "Task"], ["SQuAD v 1 . 1", "Dataset"]], "rel": [["TinyBERT", "Trained-With", "SQUAD 1. 1 AND 2. 0"], ["QA", "Synonym-Of", "question answering"], ["SQUAD 1. 1 AND 2. 0", "Benchmark-For", "question answering"], ["TinyBERT", "Used-For", "question answering"]], "rel_plus": [["TinyBERT:Method", "Trained-With", "SQUAD 1. 1 AND 2. 0:Dataset"], ["QA:Task", "Synonym-Of", "question answering:Task"], ["SQUAD 1. 1 AND 2. 0:Dataset", "Benchmark-For", "question answering:Task"], ["TinyBERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "202719327", "sentence": "We follow the settings of task - specific distillation in GLUE tasks , except with 3 running epochs and a learning rate of 5e - 5 for the prediction - layer distillation on the original training dataset .", "ner": [["task - specific distillation", "Method"], ["GLUE", "Dataset"], ["prediction - layer distillation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Compared with sequence - level GLUE tasks , the question answering tasks depends on more subtle knowledge to infer the correct answer , which increases the difficulty of knowledge distillation .", "ner": [["GLUE", "Dataset"], ["question answering", "Task"], ["knowledge distillation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "Initializing general TinyBERT with BERT SMALL is a straightforward idea .", "ner": [["TinyBERT", "Method"], ["BERT SMALL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719327", "sentence": "However , BERT SMALL would derive mismatched distributions in intermediate representations ( e.g. , attention matrices and hidden states ) with the teacher BERT BASE model , if without imitating the teacher 's behaviors at the pre - training stage .", "ner": [["BERT SMALL", "Method"], ["BERT BASE", "Method"]], "rel": [["BERT SMALL", "SubClass-Of", "BERT BASE"]], "rel_plus": [["BERT SMALL:Method", "SubClass-Of", "BERT BASE:Method"]]}
{"doc_id": "202719327", "sentence": "Further task - specific distillation under the supervision of BERT BASE will disturb the learned distribution/knowledge of BERT SMALL , finally leading to poor performances in some less - data tasks .", "ner": [["task - specific distillation", "Method"], ["BERT BASE", "Method"], ["BERT SMALL", "Method"]], "rel": [["task - specific distillation", "Used-For", "BERT BASE"], ["BERT SMALL", "SubClass-Of", "BERT BASE"]], "rel_plus": [["task - specific distillation:Method", "Used-For", "BERT BASE:Method"], ["BERT SMALL:Method", "SubClass-Of", "BERT BASE:Method"]]}
{"doc_id": "202719327", "sentence": "The results in Table 9 , show that the BERT SMALL ( MLM&NSP+TD ) performs worse than the BERT SMALL in MRPC and CoLA tasks , which validates our hypothesis .", "ner": [["BERT SMALL", "Method"], ["MLM&NSP+TD", "Task"], ["BERT SMALL", "Method"], ["MRPC", "Dataset"], ["CoLA", "Dataset"]], "rel": [["BERT SMALL", "Used-For", "MLM&NSP+TD"], ["BERT SMALL", "Compare-With", "BERT SMALL"], ["BERT SMALL", "Evaluated-With", "MRPC"], ["BERT SMALL", "Evaluated-With", "MRPC"], ["BERT SMALL", "Evaluated-With", "CoLA"], ["BERT SMALL", "Evaluated-With", "CoLA"]], "rel_plus": [["BERT SMALL:Method", "Used-For", "MLM&NSP+TD:Task"], ["BERT SMALL:Method", "Compare-With", "BERT SMALL:Method"], ["BERT SMALL:Method", "Evaluated-With", "MRPC:Dataset"], ["BERT SMALL:Method", "Evaluated-With", "MRPC:Dataset"], ["BERT SMALL:Method", "Evaluated-With", "CoLA:Dataset"], ["BERT SMALL:Method", "Evaluated-With", "CoLA:Dataset"]]}
{"doc_id": "202719327", "sentence": "For the intensive - data task ( e.g. MNLI ) , TD learning has enough learning materials to make BERT SMALL acquire the task - specific knowledge very well , although the pre - trained distributions have already been disturbed .", "ner": [["MNLI", "Dataset"], ["TD", "Method"], ["BERT SMALL", "Method"]], "rel": [["TD", "Used-For", "BERT SMALL"]], "rel_plus": [["TD:Method", "Used-For", "BERT SMALL:Method"]]}
{"doc_id": "202719327", "sentence": "To make TinyBERT effectively work for all tasks , we propose General Distillation ( GD ) for initialization , where the TinyBERT learns the knowledge from intermediate layers of teacher BERT at the pre - training stage .", "ner": [["TinyBERT", "Method"], ["General Distillation", "Method"], ["GD", "Method"], ["TinyBERT", "Method"], ["BERT", "Method"]], "rel": [["General Distillation", "Used-For", "TinyBERT"], ["GD", "Synonym-Of", "General Distillation"], ["TinyBERT", "SubClass-Of", "BERT"]], "rel_plus": [["General Distillation:Method", "Used-For", "TinyBERT:Method"], ["GD:Method", "Synonym-Of", "General Distillation:Method"], ["TinyBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "202719327", "sentence": "From the results of Table 9 , we find that GD can effectively transfer the knowledge from the teacher BERT to the student TinyBERT and achieve comparable results with BERT SMALL ( 6 1 . 1 vs 6 3 . 9 ) , even without performing the MLM and NSP tasks .", "ner": [["GD", "Method"], ["BERT", "Method"], ["TinyBERT", "Method"], ["BERT SMALL", "Method"], ["MLM", "Task"], ["NSP", "Task"]], "rel": [["TinyBERT", "SubClass-Of", "BERT"], ["GD", "Used-For", "BERT"], ["GD", "Used-For", "TinyBERT"], ["TinyBERT", "Compare-With", "BERT SMALL"]], "rel_plus": [["TinyBERT:Method", "SubClass-Of", "BERT:Method"], ["GD:Method", "Used-For", "BERT:Method"], ["GD:Method", "Used-For", "TinyBERT:Method"], ["TinyBERT:Method", "Compare-With", "BERT SMALL:Method"]]}
{"doc_id": "202719327", "sentence": "Furthermore , the task - specific distillation boosts the performances of TinyBERT by continuing on learning the taskspecific knowledge of fine - tuned teacher BERT BASE .", "ner": [["task - specific distillation", "Method"], ["TinyBERT", "Method"], ["BERT BASE", "Method"]], "rel": [["task - specific distillation", "Used-For", "TinyBERT"], ["TinyBERT", "SubClass-Of", "BERT BASE"]], "rel_plus": [["task - specific distillation:Method", "Used-For", "TinyBERT:Method"], ["TinyBERT:Method", "SubClass-Of", "BERT BASE:Method"]]}
{"doc_id": "199668729", "sentence": "An end - to - end multi - task network ( MTN ) is designed to perform human detection , pose estimation , and person re - identification ( Re - ID ) tasks simultaneously .", "ner": [["multi - task network", "Method"], ["MTN", "Method"], ["human detection", "Task"], ["pose estimation", "Task"], ["person re - identification", "Task"], ["Re - ID", "Task"]], "rel": [["MTN", "Synonym-Of", "multi - task network"], ["multi - task network", "Used-For", "human detection"], ["multi - task network", "Used-For", "pose estimation"], ["multi - task network", "Used-For", "person re - identification"], ["Re - ID", "Synonym-Of", "person re - identification"]], "rel_plus": [["MTN:Method", "Synonym-Of", "multi - task network:Method"], ["multi - task network:Method", "Used-For", "human detection:Task"], ["multi - task network:Method", "Used-For", "pose estimation:Task"], ["multi - task network:Method", "Used-For", "person re - identification:Task"], ["Re - ID:Task", "Synonym-Of", "person re - identification:Task"]]}
{"doc_id": "199668729", "sentence": "To alleviate the performance bottleneck caused by scale variation problem , a paradigm which exploits scale - normalized image and feature pyramids ( SIFP ) is proposed to boost both performance and speed .", "ner": [["scale - normalized image and feature pyramids", "Method"], ["SIFP", "Method"]], "rel": [["SIFP", "Synonym-Of", "scale - normalized image and feature pyramids"]], "rel_plus": [["SIFP:Method", "Synonym-Of", "scale - normalized image and feature pyramids:Method"]]}
{"doc_id": "199668729", "sentence": "Given the results of MTN , we adopt an occlusion - aware Re - ID feature strategy in the pose tracking module , where pose information is utilized to infer the occlusion state to make better use of Re - ID feature .", "ner": [["MTN", "Method"], ["Re - ID", "Task"], ["pose tracking module", "Method"], ["Re - ID", "Task"]], "rel": [["MTN", "Used-For", "Re - ID"], ["pose tracking module", "Used-For", "Re - ID"]], "rel_plus": [["MTN:Method", "Used-For", "Re - ID:Task"], ["pose tracking module:Method", "Used-For", "Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "In experiments , we demonstrate that the pose estimation and tracking performance improves steadily utilizing SIFP through different backbones .", "ner": [["pose estimation", "Task"], ["tracking", "Task"], ["SIFP", "Method"]], "rel": [["SIFP", "Used-For", "pose estimation"], ["SIFP", "Used-For", "tracking"]], "rel_plus": [["SIFP:Method", "Used-For", "pose estimation:Task"], ["SIFP:Method", "Used-For", "tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Using ResNet - 1 8 and ResNet - 5 0 as backbones , the overall pose tracking framework achieves competitive performance with 2 9 . 4 FPS and 1 2 . 2 FPS , respectively .", "ner": [["ResNet - 1 8", "Method"], ["ResNet - 5 0", "Method"], ["pose tracking", "Task"]], "rel": [["ResNet - 5 0", "Used-For", "pose tracking"], ["ResNet - 1 8", "Used-For", "pose tracking"]], "rel_plus": [["ResNet - 5 0:Method", "Used-For", "pose tracking:Task"], ["ResNet - 1 8:Method", "Used-For", "pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Methods involved are PoseTrack [ 2 8 ] , JointFlow [ 1 6 ] , PoseFlow [ 5 3 ] , Detect - and - Track [ 2 1 ] , FlowTrack [ 5 0 ] , and our FastPose framework with various backbones .", "ner": [["PoseTrack", "Method"], ["JointFlow", "Method"], ["PoseFlow", "Method"], ["Detect - and - Track", "Method"], ["FlowTrack", "Method"], ["FastPose", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Previous pose estimation systems address single prelocated person , which exploit pictorial structures model [ 4 , 1 8 ] and following deep convolutional neural networks ( DC - NNs ) approaches [ 4 7 , 4 6 , 4 8 , 3 6 , 5 6 ] .", "ner": [["pose estimation", "Task"], ["deep convolutional neural networks", "Method"], ["DC - NNs", "Method"]], "rel": [["deep convolutional neural networks", "Used-For", "pose estimation"], ["DC - NNs", "Synonym-Of", "deep convolutional neural networks"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "pose estimation:Task"], ["DC - NNs:Method", "Synonym-Of", "deep convolutional neural networks:Method"]]}
{"doc_id": "199668729", "sentence": "Motivated by practical applications in video surveillance , human - computer interaction and action recognition , researchers now focus on the multi - person pose estimation in unconstrained environment .", "ner": [["video surveillance", "Task"], ["human - computer interaction", "Task"], ["action recognition", "Task"], ["multi - person pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Generally , two - stages methods achieve the state - of - theart results both on pose estimation and pose tracking tasks , beyond the performance of unified approach .", "ner": [["pose estimation", "Task"], ["pose tracking", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Based on the detection result of the first stage , the second stage only focuses on the task of keypoint detection on a fixed scale .", "ner": [["detection", "Task"], ["keypoint detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Despite the leading performance , these methods ca n't perform real - time inference as their complex procedures , including human detection , cropping and scaling images , and pose estimation .", "ner": [["human detection", "Task"], ["cropping and scaling images", "Task"], ["pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Although many methods [ 3 3 , 2 3 ] have been proposed to alleviate scale variation problem in face detection or object detection area , there are few researches focusing on dealing with the scale variation in unified multi - person pose estimation .", "ner": [["face detection", "Task"], ["object detection", "Task"], ["multi - person pose estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Different from multiple object tracking that focuses on instance identification assignment , pose tracking aims to address a more complex problem of articulated multi - person pose tracking in videos .", "ner": [["multiple object tracking", "Task"], ["instance identification", "Task"], ["pose tracking", "Task"], ["multi - person pose tracking", "Task"]], "rel": [["pose tracking", "Compare-With", "multiple object tracking"]], "rel_plus": [["pose tracking:Task", "Compare-With", "multiple object tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Based on the top - down pose estimation methods , [ 5 0 ] exploits flow - based pose similarity as metric and solves the matching problem in a greedy fashion . [ 2 1 ] proposes a 3D extension of Mask R - CNN , which predicts the location of person tubes and corresponding poses simultaneously .", "ner": [["pose estimation methods", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Based on the above analyses , this paper develops FastPose , a pose tracking framework which can perform pose estimation and tracking towards real - time speed .", "ner": [["FastPose", "Method"], ["pose tracking framework", "Method"], ["pose estimation", "Task"], ["tracking towards real - time speed", "Task"]], "rel": [["FastPose", "SubClass-Of", "pose tracking framework"], ["FastPose", "Used-For", "pose estimation"], ["FastPose", "Used-For", "tracking towards real - time speed"]], "rel_plus": [["FastPose:Method", "SubClass-Of", "pose tracking framework:Method"], ["FastPose:Method", "Used-For", "pose estimation:Task"], ["FastPose:Method", "Used-For", "tracking towards real - time speed:Task"]]}
{"doc_id": "199668729", "sentence": "Specifically , we first build a multi - task network ( MTN ) which jointly optimizes three tasks simultaneously , including human detection , pose estimation , and person Re - ID .", "ner": [["multi - task network", "Method"], ["MTN", "Method"], ["human detection", "Task"], ["pose estimation", "Task"], ["person Re - ID", "Task"]], "rel": [["MTN", "Synonym-Of", "multi - task network"], ["multi - task network", "Used-For", "human detection"], ["multi - task network", "Used-For", "pose estimation"], ["multi - task network", "Used-For", "person Re - ID"]], "rel_plus": [["MTN:Method", "Synonym-Of", "multi - task network:Method"], ["multi - task network:Method", "Used-For", "human detection:Task"], ["multi - task network:Method", "Used-For", "pose estimation:Task"], ["multi - task network:Method", "Used-For", "person Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "The main contributions of this paper can be described as follows : ( 1 ) Taking the person Re - ID features into account , we design an end - to - end multi - task network which performs human detection , pose estimation , and person Re - ID simultaneously .", "ner": [["person Re - ID", "Task"], ["multi - task network", "Method"], ["human detection", "Task"], ["pose estimation", "Task"], ["person Re - ID", "Task"]], "rel": [["multi - task network", "Used-For", "human detection"], ["multi - task network", "Used-For", "pose estimation"], ["multi - task network", "Used-For", "person Re - ID"]], "rel_plus": [["multi - task network:Method", "Used-For", "human detection:Task"], ["multi - task network:Method", "Used-For", "pose estimation:Task"], ["multi - task network:Method", "Used-For", "person Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "The network 's outputs provide the necessary informations for the following pose tracking strategy . ( 2 ) We propose a paradigm named scale - normalized image and feature pyramid ( SIFP ) for alleviating scale variation problem which is the performance bottleneck of unified top - down pose estimation methods .", "ner": [["pose tracking", "Task"], ["scale - normalized image and feature pyramid", "Method"], ["SIFP", "Method"], ["pose estimation", "Task"]], "rel": [["SIFP", "Synonym-Of", "scale - normalized image and feature pyramid"], ["scale - normalized image and feature pyramid", "Used-For", "pose estimation"]], "rel_plus": [["SIFP:Method", "Synonym-Of", "scale - normalized image and feature pyramid:Method"], ["scale - normalized image and feature pyramid:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "199668729", "sentence": "Combining feature pyramid networks ( FPN ) with the scale distribution can help the network to avoid multi - scale testing . ( 3 ) Utilizing the outputs of our multi - task network , an occlusion - aware strategy is exploited to perform articulated multi - person pose tracking in videos .", "ner": [["feature pyramid networks", "Method"], ["FPN", "Method"], ["multi - task network", "Method"], ["occlusion - aware strategy", "Method"], ["multi - person pose tracking", "Task"]], "rel": [["FPN", "Synonym-Of", "feature pyramid networks"], ["multi - task network", "Used-For", "multi - person pose tracking"], ["occlusion - aware strategy", "Used-For", "multi - person pose tracking"]], "rel_plus": [["FPN:Method", "Synonym-Of", "feature pyramid networks:Method"], ["multi - task network:Method", "Used-For", "multi - person pose tracking:Task"], ["occlusion - aware strategy:Method", "Used-For", "multi - person pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Specifically , the pose information is utilized to infer occlusion state and achieve the occlusion - aware Re - ID strategy which dramatically reduce the identification ( ID ) switches during tracking .    Pose estimation has underwent a long way as a basic research topic of computer vision .", "ner": [["Re - ID", "Task"], ["Pose estimation", "Task"], ["computer vision", "Task"]], "rel": [["Pose estimation", "SubTask-Of", "computer vision"]], "rel_plus": [["Pose estimation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "199668729", "sentence": "CPN [ 1 2 ] is the leading method on COCO 2 0 1 7 keypoint challenge .", "ner": [["CPN", "Method"], ["COCO 2 0 1 7", "Dataset"]], "rel": [["CPN", "Evaluated-With", "COCO 2 0 1 7"]], "rel_plus": [["CPN:Method", "Evaluated-With", "COCO 2 0 1 7:Dataset"]]}
{"doc_id": "199668729", "sentence": "It involves skip layer feature concatenation and an online hard keypoint mining step . [ 5 0 ] adopts FPN - DCN as the human detector and adds a few deconvolutional layers on single - person pose estimation network to improve the performance .", "ner": [["FPN - DCN", "Method"], ["human detector", "Method"], ["deconvolutional layers", "Method"], ["single - person pose estimation network", "Method"]], "rel": [["FPN - DCN", "SubClass-Of", "human detector"], ["deconvolutional layers", "Part-Of", "single - person pose estimation network"]], "rel_plus": [["FPN - DCN:Method", "SubClass-Of", "human detector:Method"], ["deconvolutional layers:Method", "Part-Of", "single - person pose estimation network:Method"]]}
{"doc_id": "199668729", "sentence": "Based on the multi - person pose estimation approaches described above , it is natural to extend them to multiperson pose tracking in video .", "ner": [["multi - person pose estimation approaches", "Method"], ["multiperson pose tracking", "Task"]], "rel": [["multi - person pose estimation approaches", "Used-For", "multiperson pose tracking"]], "rel_plus": [["multi - person pose estimation approaches:Method", "Used-For", "multiperson pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "In [ 2 8 , 2 6 ] , authors firstly estimate human pose with a bottom - up method , and then transform the problem into solving an energy minimizing function over a spatiotemporal graph constructed on the detected joints . [ 1 6 ] proposes a model to predict Temporal Flow Fields ( TTF ) to formulate a similarity measure of detected joints .", "ner": [["Temporal Flow Fields", "Method"], ["TTF", "Method"]], "rel": [["TTF", "Synonym-Of", "Temporal Flow Fields"]], "rel_plus": [["TTF:Method", "Synonym-Of", "Temporal Flow Fields:Method"]]}
{"doc_id": "199668729", "sentence": "Based on the top - down pose estimation methods , [ 2 1 ] proposes an extended Mask R - CNN and solves the bipartite graph matching problem based on IoU. [ 5 0 ] exploits flow - based pose similarity as metric and solves the matching problem in a greedy fashion .", "ner": [["pose estimation", "Task"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "pose estimation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "199668729", "sentence": "Based on the obtained pose of single person , [ 5 3 ] proposes to construct pose flow and perform pose flow non maximum suppression ( NMS ) to eliminate issues like ID switches .", "ner": [["non maximum suppression", "Method"], ["NMS", "Method"]], "rel": [["NMS", "Synonym-Of", "non maximum suppression"]], "rel_plus": [["NMS:Method", "Synonym-Of", "non maximum suppression:Method"]]}
{"doc_id": "199668729", "sentence": "Multi - task learning [ 9 , 6 0 , 2 0 , 3 2 ] has been used successfully in applications of natural language processing [ 1 3 , 4 2 ] , speech recognition [ 1 5 ] , computer vision [ 2 2 , 6 1 , 5 2 ] .", "ner": [["Multi - task learning", "Method"], ["natural language processing", "Task"], ["speech recognition", "Task"], ["computer vision", "Task"]], "rel": [["Multi - task learning", "Used-For", "natural language processing"], ["Multi - task learning", "Used-For", "speech recognition"], ["Multi - task learning", "Used-For", "computer vision"]], "rel_plus": [["Multi - task learning:Method", "Used-For", "natural language processing:Task"], ["Multi - task learning:Method", "Used-For", "speech recognition:Task"], ["Multi - task learning:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "199668729", "sentence": "Especially in many computer vision tasks , the effectiveness of multi - task learning has been proved .", "ner": [["computer vision", "Task"], ["multi - task learning", "Method"]], "rel": [["multi - task learning", "Used-For", "computer vision"]], "rel_plus": [["multi - task learning:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "199668729", "sentence": "Fast R - CNN [ 2 2 ] and Faster R - CNN [ 4 0 ] jointly predict the class and the coordinates of objects in an image .", "ner": [["Fast R - CNN", "Method"], ["Faster R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Mask R - CNN [ 2 4 ] can efficiently detect objects in an image while simultaneously generating a high - quality segmentation mask for each instance .", "ner": [["Mask R - CNN", "Method"], ["detect objects", "Task"]], "rel": [["Mask R - CNN", "Used-For", "detect objects"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "detect objects:Task"]]}
{"doc_id": "199668729", "sentence": "Large scale variation is one of major factors to influence the performance of many computer vision tasks like face detection , object detection and pose estimation .", "ner": [["computer vision", "Task"], ["face detection", "Task"], ["object detection", "Task"], ["pose estimation", "Task"]], "rel": [["face detection", "SubTask-Of", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["pose estimation", "SubTask-Of", "computer vision"]], "rel_plus": [["face detection:Task", "SubTask-Of", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["pose estimation:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "199668729", "sentence": "To address the problem that large strides of deep CNNs make small object detection very difficult , object detector [ 1 0 , 1 4 ] use dilated/atrous convolutions to increase the resolution of the feature map .", "ner": [["CNNs", "Method"], ["small object detection", "Task"], ["object detector", "Method"], ["dilated/atrous convolutions", "Method"]], "rel": [["CNNs", "Used-For", "small object detection"], ["dilated/atrous convolutions", "Part-Of", "object detector"]], "rel_plus": [["CNNs:Method", "Used-For", "small object detection:Task"], ["dilated/atrous convolutions:Method", "Part-Of", "object detector:Method"]]}
{"doc_id": "199668729", "sentence": "SDP [ 5 4 ] , SSH [ 3 5 ] and MS - CNN [ 7 ] make predictions of small objects on the lower layer and big objects on the higher layers respectively .", "ner": [["SDP", "Method"], ["SSH", "Method"], ["MS - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Furthermore , methods like FPN [ 3 3 ] and Mask - RCNN [ 2 4 ] propose a pyramidal representation and fuse adjacent scale feature maps to combine features which have semantic and detail informations .", "ner": [["FPN", "Method"], ["Mask - RCNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Besides , some methods , like SNIP [ 4 4 ] and SNIPER [ 4 5 ] , propose advanced and efficient data argumentation methods to illustrate the scale variation problem .", "ner": [["SNIP", "Method"], ["SNIPER", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Given an original image as input , the multi - task network ( MTN ) can predict the bounding boxes , keypoint coordinates and Re - ID features in the scene .", "ner": [["multi - task network", "Method"], ["MTN", "Method"], ["predict the bounding boxes", "Task"], ["keypoint coordinates", "Task"], ["Re - ID", "Task"]], "rel": [["MTN", "Synonym-Of", "multi - task network"], ["multi - task network", "Used-For", "predict the bounding boxes"], ["multi - task network", "Used-For", "keypoint coordinates"], ["multi - task network", "Used-For", "Re - ID"]], "rel_plus": [["MTN:Method", "Synonym-Of", "multi - task network:Method"], ["multi - task network:Method", "Used-For", "predict the bounding boxes:Task"], ["multi - task network:Method", "Used-For", "keypoint coordinates:Task"], ["multi - task network:Method", "Used-For", "Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "A scale - normalized image pyramid and feature pyramid ( SIFP ) paradigm is exploited to alleviate the scale variation problem of MTN .", "ner": [["scale - normalized image pyramid and feature pyramid", "Method"], ["SIFP", "Method"], ["MTN", "Method"]], "rel": [["SIFP", "Synonym-Of", "scale - normalized image pyramid and feature pyramid"]], "rel_plus": [["SIFP:Method", "Synonym-Of", "scale - normalized image pyramid and feature pyramid:Method"]]}
{"doc_id": "199668729", "sentence": "The MTN adopts the similar unified procedure as Mask R - CNN .", "ner": [["MTN", "Method"], ["Mask R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "A fully convolutional network , called a Region Proposal Network ( RPN ) , is built upon these feature maps to propose candidate human bounding boxes .", "ner": [["fully convolutional network", "Method"], ["Region Proposal Network", "Method"], ["RPN", "Method"]], "rel": [["Region Proposal Network", "SubClass-Of", "fully convolutional network"], ["RPN", "Synonym-Of", "Region Proposal Network"]], "rel_plus": [["Region Proposal Network:Method", "SubClass-Of", "fully convolutional network:Method"], ["RPN:Method", "Synonym-Of", "Region Proposal Network:Method"]]}
{"doc_id": "199668729", "sentence": "Based on the candidate boxes and their corresponding features extracted from the sharing feature maps , Mask R - CNN has two branches , one branch performs classification and bounding - box regression .", "ner": [["Mask R - CNN", "Method"], ["classification", "Task"], ["bounding - box regression", "Task"]], "rel": [["Mask R - CNN", "Used-For", "classification"], ["Mask R - CNN", "Used-For", "bounding - box regression"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "classification:Task"], ["Mask R - CNN:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "199668729", "sentence": "Network Architecture : Similar with Mask R - CNN , our proposed network can be instantiated with multiple architectures : ( i ) the backbone network used for feature extraction over an entire image , and ( ii ) the head networks for human detection ( bounding - box classification and regression ) , pose estimation and person Re - ID that are applied separately to each RoI. For the backbone network , deeper architecture gains the effectiveness of extracted features , but brings longer training and inference time .", "ner": [["Mask R - CNN", "Method"], ["head networks", "Method"], ["human detection", "Task"], ["bounding - box classification and regression", "Method"], ["pose estimation", "Task"], ["person Re - ID", "Task"]], "rel": [["bounding - box classification and regression", "Used-For", "human detection"], ["head networks", "Used-For", "human detection"], ["head networks", "Used-For", "pose estimation"], ["head networks", "Used-For", "person Re - ID"]], "rel_plus": [["bounding - box classification and regression:Method", "Used-For", "human detection:Task"], ["head networks:Method", "Used-For", "human detection:Task"], ["head networks:Method", "Used-For", "pose estimation:Task"], ["head networks:Method", "Used-For", "person Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "To provide a trade - off between accuracy and speed when MTN is adopted in practical applications , we evaluate MobileNet - v 2 [ 4 3 ] and ResNet [ 2 5 ] with FPN [ 3 3 ] of depth 1 8 , 5 0 and 1 0 1 layers .", "ner": [["MTN", "Method"], ["MobileNet - v 2", "Method"], ["ResNet", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "MobileNet - v 2"], ["FPN", "Part-Of", "ResNet"]], "rel_plus": [["FPN:Method", "Part-Of", "MobileNet - v 2:Method"], ["FPN:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "199668729", "sentence": "For the pose estimation head network , Mask R - CNN adopts a straightforward structure , which limits the precision of keypoints localization .", "ner": [["pose estimation head network", "Method"], ["Mask R - CNN", "Method"]], "rel": [["Mask R - CNN", "Part-Of", "pose estimation head network"]], "rel_plus": [["Mask R - CNN:Method", "Part-Of", "pose estimation head network:Method"]]}
{"doc_id": "199668729", "sentence": "In Mask R - CNN , 1 4 \u00d7 1 4 feature maps of 5 1 2 channels are extracted by RoIAlign for each proposal .", "ner": [["Mask R - CNN", "Method"], ["RoIAlign", "Method"]], "rel": [["RoIAlign", "Part-Of", "Mask R - CNN"]], "rel_plus": [["RoIAlign:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "199668729", "sentence": "In MTN , we utilize a padding operation to maintaining the ratio of the person in the 2 2 \u00d7 1 6 feature maps extracted by RoIAlign .", "ner": [["MTN", "Method"], ["padding operation", "Method"], ["RoIAlign", "Method"]], "rel": [["RoIAlign", "Part-Of", "MTN"], ["padding operation", "Part-Of", "MTN"]], "rel_plus": [["RoIAlign:Method", "Part-Of", "MTN:Method"], ["padding operation:Method", "Part-Of", "MTN:Method"]]}
{"doc_id": "199668729", "sentence": "As this head network is based upon the backbone and RPN , so it need the training data composed by images within multi - person and corresponding ID annotation , like some person search datasets [ 5 1 , 6 2 ] .", "ner": [["head network", "Method"], ["RPN", "Method"], ["person search", "Task"]], "rel": [["RPN", "Part-Of", "head network"], ["head network", "Used-For", "person search"]], "rel_plus": [["RPN:Method", "Part-Of", "head network:Method"], ["head network:Method", "Used-For", "person search:Task"]]}
{"doc_id": "199668729", "sentence": "The MTN can provide necessary informations to the occlusion - aware strategy introduced in Sec. 3. 3 to perform pose tracking .", "ner": [["MTN", "Method"], ["occlusion - aware strategy", "Method"], ["pose tracking", "Task"]], "rel": [["MTN", "Used-For", "occlusion - aware strategy"], ["MTN", "Used-For", "pose tracking"], ["occlusion - aware strategy", "Used-For", "pose tracking"]], "rel_plus": [["MTN:Method", "Used-For", "occlusion - aware strategy:Method"], ["MTN:Method", "Used-For", "pose tracking:Task"], ["occlusion - aware strategy:Method", "Used-For", "pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "As described above , MTN performs human detection , pose estimation and person Re - ID simultaneously .", "ner": [["MTN", "Method"], ["human detection", "Task"], ["pose estimation", "Task"], ["person Re - ID", "Task"]], "rel": [["MTN", "Used-For", "human detection"], ["MTN", "Used-For", "pose estimation"], ["MTN", "Used-For", "person Re - ID"]], "rel_plus": [["MTN:Method", "Used-For", "human detection:Task"], ["MTN:Method", "Used-For", "pose estimation:Task"], ["MTN:Method", "Used-For", "person Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "However , MTN , a unified network , builds all the head networks upon the RoIs generated by RPN .", "ner": [["MTN", "Method"], ["head networks", "Method"], ["RPN", "Method"]], "rel": [["head networks", "Part-Of", "MTN"], ["RPN", "Part-Of", "head networks"]], "rel_plus": [["head networks:Method", "Part-Of", "MTN:Method"], ["RPN:Method", "Part-Of", "head networks:Method"]]}
{"doc_id": "199668729", "sentence": "So inspired by [ 4 4 ] , we develop a scalenormalized paradigm exploiting both image pyramid and feature pyramid ( SIFP ) to achieve enhanced scale invariance capability of MTN .", "ner": [["scalenormalized paradigm exploiting both image pyramid and feature pyramid", "Method"], ["SIFP", "Method"], ["MTN", "Method"]], "rel": [["SIFP", "Synonym-Of", "scalenormalized paradigm exploiting both image pyramid and feature pyramid"]], "rel_plus": [["SIFP:Method", "Synonym-Of", "scalenormalized paradigm exploiting both image pyramid and feature pyramid:Method"]]}
{"doc_id": "199668729", "sentence": "To maxmize the inference speed without reducing performance , SIFP exploits FPN to tackle this dilemma .", "ner": [["SIFP", "Method"], ["FPN", "Method"]], "rel": [["FPN", "Part-Of", "SIFP"]], "rel_plus": [["FPN:Method", "Part-Of", "SIFP:Method"]]}
{"doc_id": "199668729", "sentence": "Due to FPN , MTN enhances scale invariance capability to alleviate the domain - shift brought by single scale testing .", "ner": [["FPN", "Method"], ["MTN", "Method"]], "rel": [["FPN", "Part-Of", "MTN"]], "rel_plus": [["FPN:Method", "Part-Of", "MTN:Method"]]}
{"doc_id": "199668729", "sentence": "In conclusion , SIFP is a modified version of SNIP .", "ner": [["SIFP", "Method"], ["SNIP", "Method"]], "rel": [["SIFP", "SubClass-Of", "SNIP"]], "rel_plus": [["SIFP:Method", "SubClass-Of", "SNIP:Method"]]}
{"doc_id": "199668729", "sentence": "Combining with FPN helps SNIP to avoid slower inference speed brought by multi - scale testing .", "ner": [["FPN", "Method"], ["SNIP", "Method"]], "rel": [["FPN", "Part-Of", "SNIP"]], "rel_plus": [["FPN:Method", "Part-Of", "SNIP:Method"]]}
{"doc_id": "199668729", "sentence": "Based on the detection box , keypoints and Re - ID feature provided by MTN , pose tracking is performed by an occlusion - aware strategy .", "ner": [["detection box", "Task"], ["keypoints", "Task"], ["Re - ID", "Task"], ["MTN", "Method"], ["pose tracking", "Task"], ["occlusion - aware strategy", "Method"]], "rel": [["MTN", "Used-For", "detection box"], ["MTN", "Used-For", "keypoints"], ["MTN", "Used-For", "Re - ID"], ["occlusion - aware strategy", "Used-For", "pose tracking"]], "rel_plus": [["MTN:Method", "Used-For", "detection box:Task"], ["MTN:Method", "Used-For", "keypoints:Task"], ["MTN:Method", "Used-For", "Re - ID:Task"], ["occlusion - aware strategy:Method", "Used-For", "pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "As described in Sec. 3. 1 , using a deeper backbone network ( ResNet - 5 0 or ResNet - 1 0 1 ) , the numbers of convolutional layers in pose estimation head and Re - ID head are 8 and 4 respectively .", "ner": [["ResNet - 5 0", "Method"], ["ResNet - 1 0 1", "Method"], ["convolutional layers", "Method"], ["pose estimation head", "Method"], ["Re - ID head", "Method"]], "rel": [["convolutional layers", "Part-Of", "pose estimation head"], ["convolutional layers", "Part-Of", "Re - ID head"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "pose estimation head:Method"], ["convolutional layers:Method", "Part-Of", "Re - ID head:Method"]]}
{"doc_id": "199668729", "sentence": "When using a smaller backbone ( ResNet - 1 8 or MobileNet - v 2 ) , they are changed to 4 and 2 .", "ner": [["ResNet - 1 8", "Method"], ["MobileNet - v 2", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "MPII [ 3 ] and PoseTrack [ 2 ] datasets are utilized for training pose estimation task .", "ner": [["MPII", "Dataset"], ["PoseTrack", "Dataset"], ["pose estimation", "Task"]], "rel": [["MPII", "Benchmark-For", "pose estimation"], ["PoseTrack", "Benchmark-For", "pose estimation"]], "rel_plus": [["MPII:Dataset", "Benchmark-For", "pose estimation:Task"], ["PoseTrack:Dataset", "Benchmark-For", "pose estimation:Task"]]}
{"doc_id": "199668729", "sentence": "Person search datasets including SSM [ 5 1 ] and PRW [ 6 2 ] datasets are for training person Re - ID task .", "ner": [["Person search", "Task"], ["SSM", "Dataset"], ["PRW", "Dataset"], ["person Re - ID", "Task"]], "rel": [["SSM", "Benchmark-For", "Person search"], ["PRW", "Benchmark-For", "Person search"], ["PRW", "Benchmark-For", "person Re - ID"], ["SSM", "Benchmark-For", "person Re - ID"]], "rel_plus": [["SSM:Dataset", "Benchmark-For", "Person search:Task"], ["PRW:Dataset", "Benchmark-For", "Person search:Task"], ["PRW:Dataset", "Benchmark-For", "person Re - ID:Task"], ["SSM:Dataset", "Benchmark-For", "person Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "Utilizing non - maximum suppression , the highest scoring 1 0 0 detection boxes are fed into pose estimation and person Re - ID branches to obtain the heat maps of K keypoints and 1 2 8 - d Re - ID feature for each human boxes .", "ner": [["non - maximum suppression", "Method"], ["detection boxes", "Task"], ["pose estimation", "Task"], ["person Re - ID", "Task"], ["Re - ID", "Task"]], "rel": [["non - maximum suppression", "Used-For", "detection boxes"], ["non - maximum suppression", "Used-For", "pose estimation"], ["non - maximum suppression", "Used-For", "person Re - ID"], ["non - maximum suppression", "Used-For", "Re - ID"]], "rel_plus": [["non - maximum suppression:Method", "Used-For", "detection boxes:Task"], ["non - maximum suppression:Method", "Used-For", "pose estimation:Task"], ["non - maximum suppression:Method", "Used-For", "person Re - ID:Task"], ["non - maximum suppression:Method", "Used-For", "Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "In this section , we perform thorough ablation experiments for both pose estimation and pose tracking tasks , and compare our FastPose framework with the state - of - theart methods on PoseTrack [ 2 ] dataset .", "ner": [["pose estimation", "Task"], ["pose tracking", "Task"], ["FastPose", "Method"], ["PoseTrack", "Dataset"]], "rel": [["FastPose", "Used-For", "pose estimation"], ["PoseTrack", "Benchmark-For", "pose estimation"], ["FastPose", "Used-For", "pose tracking"], ["PoseTrack", "Benchmark-For", "pose tracking"], ["FastPose", "Evaluated-With", "PoseTrack"]], "rel_plus": [["FastPose:Method", "Used-For", "pose estimation:Task"], ["PoseTrack:Dataset", "Benchmark-For", "pose estimation:Task"], ["FastPose:Method", "Used-For", "pose tracking:Task"], ["PoseTrack:Dataset", "Benchmark-For", "pose tracking:Task"], ["FastPose:Method", "Evaluated-With", "PoseTrack:Dataset"]]}
{"doc_id": "199668729", "sentence": "Pose estimation task is evaluated on 5k validation images ( minival ) of COCO [ 3 4 ] dataset and PoseTrack [ 2 ] dataset .", "ner": [["Pose estimation", "Task"], ["COCO", "Dataset"], ["PoseTrack", "Dataset"]], "rel": [["PoseTrack", "Benchmark-For", "Pose estimation"], ["COCO", "Benchmark-For", "Pose estimation"]], "rel_plus": [["PoseTrack:Dataset", "Benchmark-For", "Pose estimation:Task"], ["COCO:Dataset", "Benchmark-For", "Pose estimation:Task"]]}
{"doc_id": "199668729", "sentence": "Backbone Architecture and SIFP for FastPose : As shown in Table 1 ( b ) , our proposed FastPose also shows steady improvement by using deeper backbone models . mAP and MOTA are two main metrics on PoseTrack dataset .", "ner": [["SIFP", "Method"], ["FastPose", "Method"], ["FastPose", "Method"], ["PoseTrack", "Dataset"]], "rel": [["SIFP", "Part-Of", "FastPose"], ["FastPose", "Evaluated-With", "PoseTrack"]], "rel_plus": [["SIFP:Method", "Part-Of", "FastPose:Method"], ["FastPose:Method", "Evaluated-With", "PoseTrack:Dataset"]]}
{"doc_id": "199668729", "sentence": "Using MobileNet - v 2 or ResNet 1 8 as the backbone , FastPose can achieve real - time pose tracking .", "ner": [["MobileNet - v 2", "Method"], ["ResNet 1 8", "Method"], ["FastPose", "Method"], ["real - time pose tracking", "Task"]], "rel": [["ResNet 1 8", "Part-Of", "FastPose"], ["MobileNet - v 2", "Part-Of", "FastPose"], ["FastPose", "Used-For", "real - time pose tracking"]], "rel_plus": [["ResNet 1 8:Method", "Part-Of", "FastPose:Method"], ["MobileNet - v 2:Method", "Part-Of", "FastPose:Method"], ["FastPose:Method", "Used-For", "real - time pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Although FastPose - MobileNet - v 2 has lower metric ( 6 2 . 1 on mAP and 5 5 . 6 on MOTA ) than FastPose - 1 8 ( 6 3 . 1 and 5 6 . 8) , its properties make it particularly suitable for mobile applications .", "ner": [["FastPose - MobileNet - v 2", "Method"], ["FastPose - 1 8", "Method"]], "rel": [["FastPose - MobileNet - v 2", "Compare-With", "FastPose - 1 8"]], "rel_plus": [["FastPose - MobileNet - v 2:Method", "Compare-With", "FastPose - 1 8:Method"]]}
{"doc_id": "199668729", "sentence": "It proves SIFP can stably improve the performance of pose estimation and tracking on PoseTrack dataset .", "ner": [["SIFP", "Method"], ["pose estimation", "Task"], ["tracking", "Task"], ["PoseTrack", "Dataset"]], "rel": [["SIFP", "Used-For", "pose estimation"], ["PoseTrack", "Benchmark-For", "pose estimation"], ["SIFP", "Used-For", "tracking"], ["PoseTrack", "Benchmark-For", "tracking"], ["SIFP", "Evaluated-With", "PoseTrack"]], "rel_plus": [["SIFP:Method", "Used-For", "pose estimation:Task"], ["PoseTrack:Dataset", "Benchmark-For", "pose estimation:Task"], ["SIFP:Method", "Used-For", "tracking:Task"], ["PoseTrack:Dataset", "Benchmark-For", "tracking:Task"], ["SIFP:Method", "Evaluated-With", "PoseTrack:Dataset"]]}
{"doc_id": "199668729", "sentence": "Pose estimation performance of FastPose on COCO dataset is reported in the supplementary material due to the page limit .", "ner": [["Pose estimation", "Task"], ["FastPose", "Method"], ["COCO", "Dataset"]], "rel": [["FastPose", "Used-For", "Pose estimation"], ["COCO", "Benchmark-For", "Pose estimation"], ["FastPose", "Evaluated-With", "COCO"]], "rel_plus": [["FastPose:Method", "Used-For", "Pose estimation:Task"], ["COCO:Dataset", "Benchmark-For", "Pose estimation:Task"], ["FastPose:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "199668729", "sentence": "Besides , we evaluate MTN on the person Re - ID dataset SSM and the mAP on SSM test is 8 9 . 3 8 , which suggests that it is feasible to extract Re - ID features in MTN .", "ner": [["MTN", "Method"], ["person Re - ID", "Task"], ["SSM", "Dataset"], ["SSM", "Dataset"], ["Re - ID", "Task"], ["MTN", "Method"]], "rel": [["MTN", "Used-For", "person Re - ID"], ["SSM", "Benchmark-For", "person Re - ID"], ["SSM", "Benchmark-For", "person Re - ID"], ["MTN", "Evaluated-With", "SSM"], ["MTN", "Evaluated-With", "SSM"], ["MTN", "Used-For", "Re - ID"]], "rel_plus": [["MTN:Method", "Used-For", "person Re - ID:Task"], ["SSM:Dataset", "Benchmark-For", "person Re - ID:Task"], ["SSM:Dataset", "Benchmark-For", "person Re - ID:Task"], ["MTN:Method", "Evaluated-With", "SSM:Dataset"], ["MTN:Method", "Evaluated-With", "SSM:Dataset"], ["MTN:Method", "Used-For", "Re - ID:Task"]]}
{"doc_id": "199668729", "sentence": "More complex designs have the potential to improve performance but are not the focus of this work .   we compare our FastPose framework with the state - ofthe - art methods on PoseTrack Dataset [ 2 ] , including PoseTrack [ 2 ] , JointFlow [ 1 6 ] , PoseFlow [ 5 3 ] , Detect - and - Track [ 2 1 ] and FlowTrack [ 5 0 ] . top - down or bottom - up approach , are all have lower mAP and slower speed than FastPose - 5 0 or FastPose - 1 0 1 .", "ner": [["FastPose", "Method"], ["PoseTrack", "Dataset"], ["PoseTrack", "Method"], ["JointFlow", "Method"], ["PoseFlow", "Method"], ["Detect - and - Track", "Method"], ["FlowTrack", "Method"], ["FastPose - 5 0", "Method"], ["FastPose - 1 0 1", "Method"]], "rel": [["FastPose", "Evaluated-With", "PoseTrack"], ["PoseTrack", "Evaluated-With", "PoseTrack"], ["JointFlow", "Evaluated-With", "PoseTrack"], ["PoseFlow", "Evaluated-With", "PoseTrack"], ["Detect - and - Track", "Evaluated-With", "PoseTrack"], ["FlowTrack", "Evaluated-With", "PoseTrack"], ["FastPose", "Compare-With", "PoseTrack"], ["FastPose", "Compare-With", "JointFlow"], ["FastPose", "Compare-With", "PoseFlow"], ["FastPose", "Compare-With", "Detect - and - Track"], ["FastPose", "Compare-With", "FlowTrack"]], "rel_plus": [["FastPose:Method", "Evaluated-With", "PoseTrack:Dataset"], ["PoseTrack:Method", "Evaluated-With", "PoseTrack:Dataset"], ["JointFlow:Method", "Evaluated-With", "PoseTrack:Dataset"], ["PoseFlow:Method", "Evaluated-With", "PoseTrack:Dataset"], ["Detect - and - Track:Method", "Evaluated-With", "PoseTrack:Dataset"], ["FlowTrack:Method", "Evaluated-With", "PoseTrack:Dataset"], ["FastPose:Method", "Compare-With", "PoseTrack:Method"], ["FastPose:Method", "Compare-With", "JointFlow:Method"], ["FastPose:Method", "Compare-With", "PoseFlow:Method"], ["FastPose:Method", "Compare-With", "Detect - and - Track:Method"], ["FastPose:Method", "Compare-With", "FlowTrack:Method"]]}
{"doc_id": "199668729", "sentence": "On PoseTrack val , Only FlowTrack - 1 5 2 with Flow has 6 5 . 4 MOTA higher than 6 3 . 2 of our FastPose - 1 0 1 .", "ner": [["PoseTrack", "Dataset"], ["FlowTrack - 1 5 2 with Flow", "Method"], ["FastPose - 1 0 1", "Method"]], "rel": [["FlowTrack - 1 5 2 with Flow", "Evaluated-With", "PoseTrack"], ["FlowTrack - 1 5 2 with Flow", "Compare-With", "FastPose - 1 0 1"]], "rel_plus": [["FlowTrack - 1 5 2 with Flow:Method", "Evaluated-With", "PoseTrack:Dataset"], ["FlowTrack - 1 5 2 with Flow:Method", "Compare-With", "FastPose - 1 0 1:Method"]]}
{"doc_id": "199668729", "sentence": "But its slower detector FPN - DCN and the optical flow estimation take much inference time , which causes the speed of FlowTrack - 1 5 2 is only 0. 2 FPS .", "ner": [["FPN - DCN", "Method"], ["optical flow estimation", "Method"], ["FlowTrack - 1 5 2", "Method"]], "rel": [["FPN - DCN", "Compare-With", "FlowTrack - 1 5 2"], ["optical flow estimation", "Compare-With", "FlowTrack - 1 5 2"]], "rel_plus": [["FPN - DCN:Method", "Compare-With", "FlowTrack - 1 5 2:Method"], ["optical flow estimation:Method", "Compare-With", "FlowTrack - 1 5 2:Method"]]}
{"doc_id": "199668729", "sentence": "Although using Flow and adopting FPN - DCN as human detector , FlowTrack - 5 0 achieves MOTA of 6 2 . 9 which is still caught up by our FastPose - 5 0 with MOTA of 6 2 . 8 .", "ner": [["Flow", "Method"], ["FPN - DCN", "Method"], ["human detector", "Method"], ["FlowTrack - 5 0", "Method"], ["FastPose - 5 0", "Method"]], "rel": [["Flow", "Part-Of", "FPN - DCN"], ["FPN - DCN", "SubClass-Of", "human detector"], ["FlowTrack - 5 0", "Compare-With", "FastPose - 5 0"]], "rel_plus": [["Flow:Method", "Part-Of", "FPN - DCN:Method"], ["FPN - DCN:Method", "SubClass-Of", "human detector:Method"], ["FlowTrack - 5 0:Method", "Compare-With", "FastPose - 5 0:Method"]]}
{"doc_id": "199668729", "sentence": "On PoseTrack test , FastPose - 5 0 and FastPose - 1 0 1 achieve MOTA of 5 6 . 6 and 5 7 . 4 , which are close to the state - of - the - art performance .", "ner": [["PoseTrack", "Dataset"], ["FastPose - 5 0", "Method"], ["FastPose - 1 0 1", "Method"]], "rel": [["FastPose - 5 0", "Evaluated-With", "PoseTrack"], ["FastPose - 1 0 1", "Evaluated-With", "PoseTrack"]], "rel_plus": [["FastPose - 5 0:Method", "Evaluated-With", "PoseTrack:Dataset"], ["FastPose - 1 0 1:Method", "Evaluated-With", "PoseTrack:Dataset"]]}
{"doc_id": "199668729", "sentence": "The inference time of FastPose comes from two aspects : MTN and tracking strategy .", "ner": [["FastPose", "Method"], ["MTN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "In this paper , we present FastPose , a fast and unified pose estimation and tracking framework , which utilizes a multi - task network ( MTN ) to integrates three tasks together .", "ner": [["FastPose", "Method"], ["pose estimation", "Task"], ["tracking", "Task"], ["multi - task network", "Method"], ["MTN", "Method"]], "rel": [["multi - task network", "Part-Of", "FastPose"], ["FastPose", "Used-For", "pose estimation"], ["FastPose", "Used-For", "tracking"], ["MTN", "Synonym-Of", "multi - task network"]], "rel_plus": [["multi - task network:Method", "Part-Of", "FastPose:Method"], ["FastPose:Method", "Used-For", "pose estimation:Task"], ["FastPose:Method", "Used-For", "tracking:Task"], ["MTN:Method", "Synonym-Of", "multi - task network:Method"]]}
{"doc_id": "199668729", "sentence": "An occlusion - aware strategy following MTN performs pose tracking .", "ner": [["occlusion - aware strategy", "Method"], ["MTN", "Method"], ["pose tracking", "Task"]], "rel": [["MTN", "Used-For", "pose tracking"], ["occlusion - aware strategy", "Used-For", "pose tracking"]], "rel_plus": [["MTN:Method", "Used-For", "pose tracking:Task"], ["occlusion - aware strategy:Method", "Used-For", "pose tracking:Task"]]}
{"doc_id": "199668729", "sentence": "Besides , a paradigm named Scale - normalized Image and Feature Pyramid ( SIFP ) is designed to deal with severe scale variation widely existed in unified pose approaches .", "ner": [["Scale - normalized Image and Feature Pyramid", "Method"], ["SIFP", "Method"]], "rel": [["SIFP", "Synonym-Of", "Scale - normalized Image and Feature Pyramid"]], "rel_plus": [["SIFP:Method", "Synonym-Of", "Scale - normalized Image and Feature Pyramid:Method"]]}
{"doc_id": "199668729", "sentence": "In ablation studies , we prove the stable improvements brought by MTN , SIFP and occlusion - aware strategy .", "ner": [["MTN", "Method"], ["SIFP", "Method"], ["occlusion - aware strategy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668729", "sentence": "Moreover , with different configurations , FastPose can achieve real - time inference or competitive performance , which is helpful to adopt pose tracking in actual scenarios .", "ner": [["FastPose", "Method"], ["pose tracking", "Task"]], "rel": [["FastPose", "Used-For", "pose tracking"]], "rel_plus": [["FastPose:Method", "Used-For", "pose tracking:Task"]]}
{"doc_id": "8238530", "sentence": "However , in the field of semantic segmenta - tion , where we need to perform dense per - pixel prediction , we find that the large kernel ( and effective receptive field ) plays an important role when we have to perform the clas - sification and localization tasks simultaneously .", "ner": [["semantic segmenta - tion", "Task"], ["dense per - pixel prediction", "Task"], ["clas - sification", "Task"], ["localization", "Task"]], "rel": [["clas - sification", "SubTask-Of", "semantic segmenta - tion"], ["localization", "SubTask-Of", "semantic segmenta - tion"], ["semantic segmenta - tion", "SubTask-Of", "dense per - pixel prediction"]], "rel_plus": [["clas - sification:Task", "SubTask-Of", "semantic segmenta - tion:Task"], ["localization:Task", "SubTask-Of", "semantic segmenta - tion:Task"], ["semantic segmenta - tion:Task", "SubTask-Of", "dense per - pixel prediction:Task"]]}
{"doc_id": "8238530", "sentence": "Following our design principle , we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation .", "ner": [["Global Convolutional Network", "Method"], ["classification", "Task"], ["localization", "Task"], ["semantic segmentation", "Task"]], "rel": [["Global Convolutional Network", "Used-For", "classification"], ["Global Convolutional Network", "Used-For", "localization"], ["localization", "SubTask-Of", "semantic segmentation"], ["classification", "SubTask-Of", "semantic segmentation"], ["Global Convolutional Network", "Used-For", "semantic segmentation"]], "rel_plus": [["Global Convolutional Network:Method", "Used-For", "classification:Task"], ["Global Convolutional Network:Method", "Used-For", "localization:Task"], ["localization:Task", "SubTask-Of", "semantic segmentation:Task"], ["classification:Task", "SubTask-Of", "semantic segmentation:Task"], ["Global Convolutional Network:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Our approach achieves state - of - art perfor - mance on two public benchmarks and significantly outper - forms previous results , 8 2 . 2 % ( vs 8 0 . 2 % ) on PASCAL VOC 2 0 1 2 dataset and 7 6 . 9 % ( vs 7 1 . 8 % ) on Cityscapes dataset .", "ner": [["PASCAL VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Semantic segmentation can be considered as a per - pixel classification problem .", "ner": [["Semantic segmentation", "Task"], ["per - pixel classification", "Task"]], "rel": [["Semantic segmentation", "SubTask-Of", "per - pixel classification"]], "rel_plus": [["Semantic segmentation:Task", "SubTask-Of", "per - pixel classification:Task"]]}
{"doc_id": "8238530", "sentence": "There are two challenges in this task : 1 ) classification : an object associated to a specific semantic concept should be marked correctly ; 2 ) localization : the classification label for a pixel must be aligned to the appropriate coordinates in output score map .", "ner": [["classification", "Task"], ["localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "The conventional semantic segmentation algorithms mainly target for the localization issue , as shown in Figure 1 B. But this might decrease the In this paper , we propose an improved net architecture , called Global Convolutional Network ( GCN ) , to deal with the above two challenges simultaneously .", "ner": [["semantic segmentation", "Task"], ["Global Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["Global Convolutional Network", "Used-For", "semantic segmentation"], ["GCN", "Synonym-Of", "Global Convolutional Network"]], "rel_plus": [["Global Convolutional Network:Method", "Used-For", "semantic segmentation:Task"], ["GCN:Method", "Synonym-Of", "Global Convolutional Network:Method"]]}
{"doc_id": "8238530", "sentence": "We follow two design principles : 1 ) from the localization view , the model structure should be fully convolutional to retain the localization performance and no fully - connected or global pooling layers should be used as these layers will discard the localization information ; 2 ) from the classification view , large kernel size should be adopted in the network architecture to enable densely connections between feature maps and per - pixel classifiers , which enhances the capability to handle different transformations .", "ner": [["localization", "Task"], ["fully convolutional", "Method"], ["fully - connected", "Method"], ["global pooling", "Method"], ["classification", "Task"], ["densely connections", "Method"], ["per - pixel classifiers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "These two principles lead to our GCN , as in Figure 2 A. The FCN [ 2 5 ] -like structure is employed as our basic framework and our GCN is used to generate semantic score maps .", "ner": [["GCN", "Method"], ["FCN", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "To further improve the localization ability near the object boundaries , we introduce boundary refinement block to model the boundary alignment as a residual structure , shown in Figure 2 C. Unlike the CRF - like post - process [ 6 ] , our boundary refinement block is integrated into the network and trained end - to - end .", "ner": [["localization", "Task"], ["boundary refinement block", "Method"], ["CRF", "Method"], ["boundary refinement block", "Method"]], "rel": [["boundary refinement block", "Used-For", "localization"], ["CRF", "Compare-With", "boundary refinement block"]], "rel_plus": [["boundary refinement block:Method", "Used-For", "localization:Task"], ["CRF:Method", "Compare-With", "boundary refinement block:Method"]]}
{"doc_id": "8238530", "sentence": "Our contributions are summarized as follows : 1 ) we propose Global Convolutional Network for semantic segmentation which explicitly address the \" classification \" and \" localization \" problems simultaneously ; 2 ) a Boundary Refinement block is introduced which can further improve the localization performance near the object boundaries ; 3 ) we achieve state - of - art results on two standard benchmarks , with 8 2 . 2 % on PASCAL VOC 2 0 1 2 and 7 6 . 9 % on the Cityscapes .", "ner": [["Global Convolutional Network", "Method"], ["semantic segmentation", "Task"], ["classification", "Task"], ["localization", "Task"], ["localization", "Task"], ["PASCAL VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["Global Convolutional Network", "Used-For", "semantic segmentation"], ["Global Convolutional Network", "Used-For", "classification"], ["Global Convolutional Network", "Used-For", "localization"]], "rel_plus": [["Global Convolutional Network:Method", "Used-For", "semantic segmentation:Task"], ["Global Convolutional Network:Method", "Used-For", "classification:Task"], ["Global Convolutional Network:Method", "Used-For", "localization:Task"]]}
{"doc_id": "8238530", "sentence": "One of the most popular CNN based work is the Fully Convolutional Network ( FCN ) [ 2 5 ] .", "ner": [["CNN", "Method"], ["Fully Convolutional Network", "Method"], ["FCN", "Method"]], "rel": [["Fully Convolutional Network", "SubClass-Of", "CNN"], ["FCN", "Synonym-Of", "Fully Convolutional Network"]], "rel_plus": [["Fully Convolutional Network:Method", "SubClass-Of", "CNN:Method"], ["FCN:Method", "Synonym-Of", "Fully Convolutional Network:Method"]]}
{"doc_id": "8238530", "sentence": "By converting the fully - connected layers into convolutional layers and concatenating the intermediate score maps , FCN has outperformed a lot of traditional methods on semantic segmentation .", "ner": [["fully - connected layers", "Method"], ["convolutional layers", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["convolutional layers", "Part-Of", "FCN"], ["fully - connected layers", "Part-Of", "FCN"], ["FCN", "Used-For", "semantic segmentation"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "FCN:Method"], ["fully - connected layers:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Following the structure of FCN , there are several works trying to improve the semantic segmentation task based on the following three aspects .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Further , Dilated - Net [ 3 7 ] appends several layers after the score map to embed the multi - scale context , and Deeplab - V 2 [ 7 ] uses the Atrous Spatial Pyramid Pooling , which is a combination of convolutions , to embed the context directly from feature map .", "ner": [["Dilated - Net", "Method"], ["Deeplab - V 2", "Method"], ["Atrous Spatial Pyramid Pooling", "Method"], ["convolutions", "Method"]], "rel": [["Atrous Spatial Pyramid Pooling", "Part-Of", "Deeplab - V 2"], ["convolutions", "Part-Of", "Atrous Spatial Pyramid Pooling"]], "rel_plus": [["Atrous Spatial Pyramid Pooling:Method", "Part-Of", "Deeplab - V 2:Method"], ["convolutions:Method", "Part-Of", "Atrous Spatial Pyramid Pooling:Method"]]}
{"doc_id": "8238530", "sentence": "Initially , FCN [ 2 5 ] proposes the deconvolution ( i.e. inverse of convolution ) operation to increase the resolution of small score map .", "ner": [["FCN", "Method"], ["deconvolution", "Method"]], "rel": [["deconvolution", "Part-Of", "FCN"]], "rel_plus": [["deconvolution:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "8238530", "sentence": "Further , Deconv - Net [ 2 7 ] and SegNet [ 3 ] introduce the unpooling operation ( i.e. inverse of pooling ) and a glass - like network to learn the upsampling process .", "ner": [["Deconv - Net", "Method"], ["SegNet", "Method"], ["unpooling operation", "Method"], ["inverse of pooling", "Method"]], "rel": [["unpooling operation", "Part-Of", "Deconv - Net"], ["unpooling operation", "Part-Of", "SegNet"], ["inverse of pooling", "Synonym-Of", "unpooling operation"]], "rel_plus": [["unpooling operation:Method", "Part-Of", "Deconv - Net:Method"], ["unpooling operation:Method", "Part-Of", "SegNet:Method"], ["inverse of pooling:Method", "Synonym-Of", "unpooling operation:Method"]]}
{"doc_id": "8238530", "sentence": "Instead of learning the upsampling process , Deeplab [ 2 4 ] and Dilated - Net [ 3 7 ] propose a special dilated convolution to directly increase the spatial size of small feature maps , resulting in a larger score map .", "ner": [["Deeplab", "Method"], ["Dilated - Net", "Method"], ["dilated convolution", "Method"]], "rel": [["dilated convolution", "Part-Of", "Deeplab"], ["dilated convolution", "Part-Of", "Dilated - Net"]], "rel_plus": [["dilated convolution:Method", "Part-Of", "Deeplab:Method"], ["dilated convolution:Method", "Part-Of", "Dilated - Net:Method"]]}
{"doc_id": "8238530", "sentence": "Among the many methods , Conditional Random Field ( CRF ) is often employed here because of its good mathematical formation .", "ner": [["Conditional Random Field", "Method"], ["CRF", "Method"]], "rel": [["CRF", "Synonym-Of", "Conditional Random Field"]], "rel_plus": [["CRF:Method", "Synonym-Of", "Conditional Random Field:Method"]]}
{"doc_id": "8238530", "sentence": "Deeplab [ 6 ] directly employs denseCRF [ 1 8 ] , which is a CRF - variant built on fully - connected graph , as a post - processing method after CNN .", "ner": [["Deeplab", "Method"], ["denseCRF", "Method"], ["CRF", "Method"], ["CNN", "Method"]], "rel": [["denseCRF", "Part-Of", "Deeplab"], ["denseCRF", "SubClass-Of", "CRF"]], "rel_plus": [["denseCRF:Method", "Part-Of", "Deeplab:Method"], ["denseCRF:Method", "SubClass-Of", "CRF:Method"]]}
{"doc_id": "8238530", "sentence": "Then CRFAsRNN [ 3 8 ] models the denseCRF into a RNN - style operator and proposes an end - to - end pipeline , yet it involves too much CPU computation on Permutohedral Lattice [ 1 ] .", "ner": [["CRFAsRNN", "Method"], ["denseCRF", "Method"], ["RNN - style operator", "Method"]], "rel": [["denseCRF", "Part-Of", "CRFAsRNN"], ["denseCRF", "SubClass-Of", "RNN - style operator"]], "rel_plus": [["denseCRF:Method", "Part-Of", "CRFAsRNN:Method"], ["denseCRF:Method", "SubClass-Of", "RNN - style operator:Method"]]}
{"doc_id": "8238530", "sentence": "DPN [ 2 4 ] makes a different approxima - tion on denseCRF and put the whole pipeline completely on GPU .", "ner": [["DPN", "Method"], ["denseCRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Furthermore , Adelaide [ 2 1 ] deeply incorporates CRF and CNN where hand - crafted potentials is replaced by convolutions and nonlinearities .", "ner": [["Adelaide", "Method"], ["CRF", "Method"], ["CNN", "Method"], ["convolutions", "Method"], ["nonlinearities", "Method"]], "rel": [["CRF", "Part-Of", "Adelaide"], ["CNN", "Part-Of", "Adelaide"], ["convolutions", "Part-Of", "CNN"], ["nonlinearities", "Part-Of", "CNN"]], "rel_plus": [["CRF:Method", "Part-Of", "Adelaide:Method"], ["CNN:Method", "Part-Of", "Adelaide:Method"], ["convolutions:Method", "Part-Of", "CNN:Method"], ["nonlinearities:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "8238530", "sentence": "Besides , there are also some alternatives to CRF . [ 4 ] presents a similar model to CRF , called Bilateral Solver , yet achieves 1 0 x speed and comparable performance . [ 1 6 ] introduces the bilateral filter to learn the specific pairwise potentials within CNN .", "ner": [["CRF", "Method"], ["CRF", "Method"], ["Bilateral Solver", "Method"], ["CNN", "Method"]], "rel": [["Bilateral Solver", "SubClass-Of", "CRF"]], "rel_plus": [["Bilateral Solver:Method", "SubClass-Of", "CRF:Method"]]}
{"doc_id": "8238530", "sentence": "In contrary to previous works , we argues that semantic segmentation is a classification task on large feature map and our Global Convolutional Network could simultaneously fulfill the demands of classification and localization .", "ner": [["semantic segmentation", "Task"], ["classification", "Task"], ["Global Convolutional Network", "Method"], ["classification", "Task"], ["localization", "Task"]], "rel": [["Global Convolutional Network", "Used-For", "semantic segmentation"], ["semantic segmentation", "SubTask-Of", "classification"], ["Global Convolutional Network", "Used-For", "classification"], ["Global Convolutional Network", "Used-For", "classification"], ["Global Convolutional Network", "Used-For", "localization"]], "rel_plus": [["Global Convolutional Network:Method", "Used-For", "semantic segmentation:Task"], ["semantic segmentation:Task", "SubTask-Of", "classification:Task"], ["Global Convolutional Network:Method", "Used-For", "classification:Task"], ["Global Convolutional Network:Method", "Used-For", "classification:Task"], ["Global Convolutional Network:Method", "Used-For", "localization:Task"]]}
{"doc_id": "8238530", "sentence": "In this section , we first propose a novel Global Convolutional Network ( GCN ) to address the contradictory aspects -classification and localization in semantic segmentation .", "ner": [["Global Convolutional Network", "Method"], ["GCN", "Method"], ["-classification", "Task"], ["localization", "Task"], ["semantic segmentation", "Task"]], "rel": [["GCN", "Synonym-Of", "Global Convolutional Network"], ["Global Convolutional Network", "Used-For", "-classification"], ["Global Convolutional Network", "Used-For", "localization"], ["Global Convolutional Network", "Used-For", "semantic segmentation"], ["-classification", "SubTask-Of", "semantic segmentation"], ["localization", "SubTask-Of", "semantic segmentation"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Global Convolutional Network:Method"], ["Global Convolutional Network:Method", "Used-For", "-classification:Task"], ["Global Convolutional Network:Method", "Used-For", "localization:Task"], ["Global Convolutional Network:Method", "Used-For", "semantic segmentation:Task"], ["-classification:Task", "SubTask-Of", "semantic segmentation:Task"], ["localization:Task", "SubTask-Of", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Then using GCN we design a fully - convolutional framework for semantic segmentation task .", "ner": [["GCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["GCN", "Used-For", "semantic segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "The task of semantic segmentation , or pixel - wise classification , requires to output a score map assigning each pixel from the input image with semantic label .", "ner": [["semantic segmentation", "Task"], ["pixel - wise classification", "Task"]], "rel": [["semantic segmentation", "SubTask-Of", "pixel - wise classification"]], "rel_plus": [["semantic segmentation:Task", "SubTask-Of", "pixel - wise classification:Task"]]}
{"doc_id": "8238530", "sentence": "As mentioned in Introduction section , this task implies two challenges : classification and localization .", "ner": [["classification", "Task"], ["localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "However , we find that the requirements of classification and localization problems are naturally contradictory : ( 1 ) For classification task , models are required invariant to transformation on the inputs -objects may be shifted , rotated or rescaled but the classification results are expected to be unchanged . ( 2 ) While for localization task , models should be transformation - sensitive because the localization results depend on the positions of inputs .", "ner": [["classification", "Task"], ["localization", "Task"], ["classification", "Task"], ["classification", "Task"], ["localization", "Task"]], "rel": [["classification", "Compare-With", "localization"]], "rel_plus": [["classification:Task", "Compare-With", "localization:Task"]]}
{"doc_id": "8238530", "sentence": "In deep learning , the differences between classification and localization lead to different styles of models .", "ner": [["deep learning", "Method"], ["classification", "Task"], ["localization", "Task"]], "rel": [["classification", "Compare-With", "localization"]], "rel_plus": [["classification:Task", "Compare-With", "localization:Task"]]}
{"doc_id": "8238530", "sentence": "For classification , most modern frameworks such as AlexNet [ 2 0 ] , VGG Net [ 3 1 ] , GoogleNet [ 3 2 , 3 3 ] or ResNet [ 1 4 ] employ the \" Cone - shaped \" networks shown in Figure 1 A : features are extracted from a relatively small hidden layer , which is coarse on spatial dimensions , and classifiers are densely connected to entire feature map via fullyconnected layer [ 2 0 , 3 1 ] or global pooling layer [ 3 2 , 3 3 , 1 4 ] , which makes features robust to locally disturbances and allows classifiers to handle different types of input transformations .", "ner": [["classification", "Task"], ["AlexNet", "Method"], ["VGG Net", "Method"], ["GoogleNet", "Method"], ["ResNet", "Method"], ["fullyconnected layer", "Method"], ["global pooling layer", "Method"]], "rel": [["AlexNet", "Used-For", "classification"], ["VGG Net", "Used-For", "classification"], ["GoogleNet", "Used-For", "classification"], ["ResNet", "Used-For", "classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "classification:Task"], ["VGG Net:Method", "Used-For", "classification:Task"], ["GoogleNet:Method", "Used-For", "classification:Task"], ["ResNet:Method", "Used-For", "classification:Task"]]}
{"doc_id": "8238530", "sentence": "That is why most semantic segmentation frameworks , such as FCN [ 2 5 , 3 0 ] , U - Net [ 2 8 ] , DeepLab [ 6 , 7 ] , Deconv - Net [ 2 7 ] , adopt \" Barrel - shaped \" networks shown in Figure 1 B .", "ner": [["semantic segmentation", "Task"], ["FCN", "Method"], ["U - Net", "Method"], ["DeepLab", "Method"], ["Deconv - Net", "Method"]], "rel": [["FCN", "Used-For", "semantic segmentation"], ["U - Net", "Used-For", "semantic segmentation"], ["DeepLab", "Used-For", "semantic segmentation"], ["Deconv - Net", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"], ["U - Net:Method", "Used-For", "semantic segmentation:Task"], ["DeepLab:Method", "Used-For", "semantic segmentation:Task"], ["Deconv - Net:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "We notice that current state - of - the - art semantic segmentation models [ 2 5 , 6 , 2 7 ] mainly follow the design principles for localization , however , which may be suboptimal for classification .", "ner": [["semantic segmentation", "Task"], ["localization", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "At first , the valid receptive filed ( VRF ) 1 is large enough to hold the entire object .", "ner": [["valid receptive filed", "Method"], ["VRF", "Method"]], "rel": [["VRF", "Synonym-Of", "valid receptive filed"]], "rel_plus": [["VRF:Method", "Synonym-Of", "valid receptive filed:Method"]]}
{"doc_id": "8238530", "sentence": "However , if the input object is resized to a large scale , then VRF can only cover a part of the object , which may be harmful for classification .", "ner": [["VRF", "Method"], ["classification", "Task"]], "rel": [["VRF", "Used-For", "classification"]], "rel_plus": [["VRF:Method", "Used-For", "classification:Task"]]}
{"doc_id": "8238530", "sentence": "It will be even worse if larger feature maps are used , because the gap between classification and localization becomes larger .", "ner": [["classification", "Task"], ["localization", "Task"]], "rel": [["classification", "Compare-With", "localization"]], "rel_plus": [["classification:Task", "Compare-With", "localization:Task"]]}
{"doc_id": "8238530", "sentence": "First from the localization view , the structure must be fully - convolutional without any fully - connected layer or global pooling layer that used by many classification networks , since the latter will 1 Feature maps from modern networks such as GoolgeNet or ResNet usually have very large receptive field because of the deep architecture .", "ner": [["localization", "Task"], ["fully - convolutional", "Method"], ["fully - connected layer", "Method"], ["global pooling layer", "Method"], ["classification", "Task"], ["GoolgeNet", "Method"], ["ResNet", "Method"]], "rel": [["fully - convolutional", "Used-For", "localization"], ["global pooling layer", "Used-For", "classification"], ["fully - connected layer", "Used-For", "classification"]], "rel_plus": [["fully - convolutional:Method", "Used-For", "localization:Task"], ["global pooling layer:Method", "Used-For", "classification:Task"], ["fully - connected layer:Method", "Used-For", "classification:Task"]]}
{"doc_id": "8238530", "sentence": "However , studies [ 3 9 ] show that network tends to gather information mainly from a much smaller region in the receptive field , which is called valid receptive field ( VRF ) in this paper . discard localization information .", "ner": [["valid receptive field", "Method"], ["VRF", "Method"]], "rel": [["VRF", "Synonym-Of", "valid receptive field"]], "rel_plus": [["VRF:Method", "Synonym-Of", "valid receptive field:Method"]]}
{"doc_id": "8238530", "sentence": "Second from the classification view , motivated by the densely - connected structure of classification models , the kernel size of the convolutional structure should be as large as possible .", "ner": [["classification", "Task"], ["classification", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Based on these two principles , we propose a novel Global Convolutional Network ( GCN ) in Figure 2 B. Instead of directly using larger kernel or global convolution , our GCN module employs a combination of 1 \u00d7 k + k \u00d7 1 and k \u00d7 1 + 1 \u00d7 k convolutions , which enables densely connections within a large k \u00d7 k region in the feature map .", "ner": [["Global Convolutional Network", "Method"], ["GCN", "Method"], ["GCN", "Method"], ["k \u00d7 1 + 1 \u00d7 k convolutions", "Method"]], "rel": [["GCN", "Synonym-Of", "Global Convolutional Network"], ["k \u00d7 1 + 1 \u00d7 k convolutions", "Part-Of", "GCN"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Global Convolutional Network:Method"], ["k \u00d7 1 + 1 \u00d7 k convolutions:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "8238530", "sentence": "We use pretrained ResNet [ 1 4 ] as the feature network and FCN 4 [ 2 5 , 3 6 ] as the segmentation framework .", "ner": [["ResNet", "Method"], ["feature network", "Method"], ["FCN 4", "Method"], ["segmentation framework", "Method"]], "rel": [["ResNet", "SubClass-Of", "feature network"], ["FCN 4", "SubClass-Of", "segmentation framework"]], "rel_plus": [["ResNet:Method", "SubClass-Of", "feature network:Method"], ["FCN 4:Method", "SubClass-Of", "segmentation framework:Method"]]}
{"doc_id": "8238530", "sentence": "For traditional segmentation model , even though the receptive field is as large as the input image , however , the VRF just covers the bird ( A ) and fails to hold the entire object if the input resized to a larger scale ( B ) .", "ner": [["VRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "As a comparison , our Global Convolution Network significantly enlarges the VRF ( C ) . class .", "ner": [["Global Convolution Network", "Method"], ["VRF ( C )", "Method"]], "rel": [["Global Convolution Network", "Compare-With", "VRF ( C )"]], "rel_plus": [["Global Convolution Network:Method", "Compare-With", "VRF ( C ):Method"]]}
{"doc_id": "8238530", "sentence": "The details can be referred to Figure 2   We evaluate our approach on the standard benchmark PASCAL VOC 2 0 1 2 [ 1 1 , 1 0 ] and Cityscapes [ 8 ] .", "ner": [["PASCAL VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "We choose the state - of - the - art network ResNet 1 5 2 [ 1 4 ] ( pretrained on ImageNet [ 2 9 ] ) as our base model for fine tuning .", "ner": [["ResNet 1 5 2", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet 1 5 2", "Trained-With", "ImageNet"]], "rel_plus": [["ResNet 1 5 2:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "8238530", "sentence": "During the training time , we use standard SGD [ 2 0 ] with batch size 1 , momentum 0. 9 9 and weight decay 0.0 0 0 5 .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "8238530", "sentence": "Then we will report the full results on PASCAL VOC 2 0 1 2 and Cityscapes .", "ner": [["PASCAL VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "For all succeeding experiments , we pad each input image into 5 1 2 \u00d7 5 1 2 so that the top - most feature map is 1 6 \u00d7 1 6 .   In Section 3. 1 we propose Global Convolutional Network ( GCN ) to enable densely connections between classifiers and features .", "ner": [["Global Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "Global Convolutional Network"]], "rel_plus": [["GCN:Method", "Synonym-Of", "Global Convolutional Network:Method"]]}
{"doc_id": "8238530", "sentence": "Further Discussion : In the experiments in Table 1 , since there are other differences between baseline and different versions of GCN , it seems not so confirmed to attribute the improvements to large kernels or GCN .", "ner": [["GCN", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "The score is measured under standard mean IoU(% ) , and the 3rd and 4th rows show number of parameters of GCN and trivial Convolution after res - 5 . larger kernel will result in better performance if k \u2264 5 , yet for k \u2265 7 the performance drops .", "ner": [["GCN", "Method"], ["Convolution", "Method"]], "rel": [["Convolution", "Part-Of", "GCN"]], "rel_plus": [["Convolution:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "8238530", "sentence": "Thus the actual reason still needs further study . ( 2 ) GCN vs. Stack of small convolutions .", "ner": [["GCN", "Method"], ["convolutions", "Method"]], "rel": [["convolutions", "Part-Of", "GCN"]], "rel_plus": [["convolutions:Method", "Part-Of", "GCN:Method"]]}
{"doc_id": "8238530", "sentence": "Instead of GCN , another trivial approach to form a large kernel is to use stack of small kernel convolutions(for example , stack of 3 \u00d7 3 kernels in Figure 4 D ) , , which is very common in modern CNN architectures such as VGG - net [ 3 1 ] .", "ner": [["GCN", "Method"], ["convolutions(for", "Method"], ["3 \u00d7 3 kernels", "Method"], ["CNN", "Method"], ["VGG - net", "Method"]], "rel": [["convolutions(for", "Part-Of", "GCN"], ["3 \u00d7 3 kernels", "Part-Of", "convolutions(for"], ["VGG - net", "SubClass-Of", "CNN"]], "rel_plus": [["convolutions(for:Method", "Part-Of", "GCN:Method"], ["3 \u00d7 3 kernels:Method", "Part-Of", "convolutions(for:Method"], ["VGG - net:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "8238530", "sentence": "Comparison Experiments between Global Convolutional Network and the equivalent stack of small kernel convolutions .", "ner": [["Global Convolutional Network", "Method"], ["convolutions", "Method"]], "rel": [["convolutions", "Part-Of", "Global Convolutional Network"]], "rel_plus": [["convolutions:Method", "Part-Of", "Global Convolutional Network:Method"]]}
{"doc_id": "8238530", "sentence": "The score is measured under standard mean IoU. GCN outperforms the convolutional stack design with less parameters . ( 3 ) How GCN contributes to the segmentation results ?", "ner": [["GCN", "Method"], ["GCN", "Method"], ["segmentation", "Task"]], "rel": [["GCN", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "In Section 3. 1 , we claim that GCN improves the classification capability of segmentation model by introducing densely connections to the feature map , which is helpful to handle large variations of transformations .", "ner": [["GCN", "Method"], ["classification", "Task"], ["segmentation", "Task"]], "rel": [["GCN", "Used-For", "classification"], ["GCN", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "classification:Task"], ["GCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Based on this , we can infer that pixels lying in the center of large objects may benefit more from GCN because it is very close to \" pure \" classification problem .", "ner": [["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "We evaluate our segmentation model ( GCN with k = 1 5 ) in both regions .", "ner": [["segmentation", "Task"], ["GCN", "Method"]], "rel": [["GCN", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "In contrary to GCN structure , BF mainly improves the accuracy in boundary region , which also confirms its effectiveness .   In the above subsection our segmentation models are finetuned from ResNet - 1 5 2 network .", "ner": [["GCN", "Method"], ["BF", "Method"], ["segmentation", "Task"], ["ResNet - 1 5 2", "Method"]], "rel": [["GCN", "Compare-With", "BF"], ["ResNet - 1 5 2", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Compare-With", "BF:Method"], ["ResNet - 1 5 2:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Since large kernel plays a critical role in segmentation tasks , it is nature to apply the idea of GCN also on the pretrained model .", "ner": [["segmentation", "Task"], ["GCN", "Method"]], "rel": [["GCN", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "Thus we propose a new ResNet - GCN structure , as shown in Figure 5 .", "ner": [["ResNet - GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "We remove the first two layers in the original bottleneck structure used by ResNet , and replace them with a GCN module .", "ner": [["ResNet", "Method"], ["GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "In order to keep consistent with the original , we also apply Batch Normalization [ 1 5 ] and ReLU after each of the convolution layers .", "ner": [["Batch Normalization", "Method"], ["ReLU", "Method"], ["convolution layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "We compare our ResNet - GCN structure with the original ResNet model .", "ner": [["ResNet - GCN", "Method"], ["ResNet", "Method"]], "rel": [["ResNet - GCN", "Compare-With", "ResNet"]], "rel_plus": [["ResNet - GCN:Method", "Compare-With", "ResNet:Method"]]}
{"doc_id": "8238530", "sentence": "For fair comparison , sizes for ResNet - GCN are carefully selected so that both network have similar computation cost and number of parameters .", "ner": [["ResNet - GCN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "We first pretrain ResNet - GCN on ImageNet 2 0 1 5 [ 2 9 ] and fine tune on PASCAL VOC 2 0 1 2 segmentation dataset .", "ner": [["ResNet - GCN", "Method"], ["ImageNet 2 0 1 5", "Dataset"], ["PASCAL VOC 2 0 1 2 segmentation", "Dataset"]], "rel": [["ResNet - GCN", "Trained-With", "ImageNet 2 0 1 5"], ["ResNet - GCN", "Trained-With", "PASCAL VOC 2 0 1 2 segmentation"]], "rel_plus": [["ResNet - GCN:Method", "Trained-With", "ImageNet 2 0 1 5:Dataset"], ["ResNet - GCN:Method", "Trained-With", "PASCAL VOC 2 0 1 2 segmentation:Dataset"]]}
{"doc_id": "8238530", "sentence": "Note that we take ResNet 5 0 model ( with or without GCN ) for comparison because the training of large ResNet 1 5 2 is very costly .", "ner": [["ResNet 5 0", "Method"], ["GCN", "Method"], ["ResNet 1 5 2", "Method"]], "rel": [["GCN", "Part-Of", "ResNet 5 0"], ["ResNet 5 0", "Compare-With", "ResNet 1 5 2"]], "rel_plus": [["GCN:Method", "Part-Of", "ResNet 5 0:Method"], ["ResNet 5 0:Method", "Compare-With", "ResNet 1 5 2:Method"]]}
{"doc_id": "8238530", "sentence": "From the results we can see that our GCNbased ResNet is slightly poorer than original ResNet as an ImageNet classification model .", "ner": [["GCNbased ResNet", "Method"], ["ResNet", "Method"], ["ImageNet classification", "Task"]], "rel": [["GCNbased ResNet", "Compare-With", "ResNet"], ["GCNbased ResNet", "Used-For", "ImageNet classification"], ["ResNet", "Used-For", "ImageNet classification"]], "rel_plus": [["GCNbased ResNet:Method", "Compare-With", "ResNet:Method"], ["GCNbased ResNet:Method", "Used-For", "ImageNet classification:Task"], ["ResNet:Method", "Used-For", "ImageNet classification:Task"]]}
{"doc_id": "8238530", "sentence": "However , after finetuning on segmentation dataset ResNet - GCN model outperforms original ResNet significantly by 5. 5 % .", "ner": [["segmentation", "Task"], ["ResNet - GCN", "Method"], ["ResNet", "Method"]], "rel": [["ResNet - GCN", "Used-For", "segmentation"], ["ResNet", "Used-For", "segmentation"], ["ResNet - GCN", "Compare-With", "ResNet"]], "rel_plus": [["ResNet - GCN:Method", "Used-For", "segmentation:Task"], ["ResNet:Method", "Used-For", "segmentation:Task"], ["ResNet - GCN:Method", "Compare-With", "ResNet:Method"]]}
{"doc_id": "8238530", "sentence": "With the application of GCN and boundary refinement , the gain of GCNbased pretrained model becomes minor but still prevails .", "ner": [["GCN", "Method"], ["boundary refinement", "Method"], ["GCNbased", "Method"]], "rel": [["boundary refinement", "Part-Of", "GCNbased"], ["GCN", "Part-Of", "GCNbased"]], "rel_plus": [["boundary refinement:Method", "Part-Of", "GCNbased:Method"], ["GCN:Method", "Part-Of", "GCNbased:Method"]]}
{"doc_id": "8238530", "sentence": "We can safely conclude that GCN mainly helps to improve segmentation performance , no matter in pretrained model or segmentation - specific structures .", "ner": [["GCN", "Method"], ["segmentation", "Task"]], "rel": [["GCN", "Used-For", "segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "ResNet 5 0 ResNet 5 0 - GCN ImageNet cls err ( % ) 7. 7 7. 9 Seg .", "ner": [["ResNet 5 0", "Method"], ["ResNet 5 0 - GCN", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet 5 0 - GCN", "Evaluated-With", "ImageNet"]], "rel_plus": [["ResNet 5 0 - GCN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "8238530", "sentence": "Score ( GCN + BR ) 7 2 . 3 7 2 . 5   In this section we discuss our practice on PASCAL VOC 2 0 1 2 dataset .", "ner": [["GCN + BR", "Method"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "COCO has 8 0 classes and here we only retain the images including the same 2 0 classes in PASCAL VOC 2 0 1 2 .", "ner": [["COCO", "Dataset"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "The training phase is split into three stages : ( 1 ) Our GCN + BR model clearly prevails , meanwhile the post - processing multi - scale and denseCRF [ 1 8 ] also bring benefits .", "ner": [["GCN + BR", "Method"], ["denseCRF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Our work has outperformed all the previous state - of - the - arts . mean - IoU(% ) FCN - 8 s - heavy [ 3 0 ] 6 7 . 2 TTI zoomout v 2 [ 2 6 ] 6 9 . 6 MSRA BoxSup [ 9 ] 7 1 . 0 DeepLab - MSc - CRF - LargeFOV [ 6 ] 7 1 . 6 Oxford TVG CRF RNN COCO [ 3 8 ] 7 4 . 7 CUHK DPN COCO [ 2 4 ] 7 7 . 5 Oxford TVG HO CRF [ 2 ] 7 7 . 9 CASIA IVA OASeg [ 3 4 ] 7 8 . 3 Adelaide VeryDeep FCN VOC [ 3 5 ] 7 9 . 1 LRR 4x ResNet COCO [ 1 2 ] 7 9 . 3 Deeplabv 2 - CRF [ 7 ] 7 9 . 7 CentraleSupelec Deep G - CRF [ 5 ] 8 0 . 2 Our approach 8 2 . 2   Cityscapes [ 8 ] is a dataset collected for semantic segmentation on urban street scenes .", "ner": [["FCN - 8 s - heavy", "Method"], ["TTI zoomout v 2", "Method"], ["MSRA BoxSup", "Method"], ["DeepLab - MSc - CRF - LargeFOV", "Method"], ["Oxford TVG CRF RNN COCO", "Method"], ["CUHK DPN COCO", "Method"], ["Oxford TVG HO CRF", "Method"], ["CASIA IVA OASeg", "Method"], ["Adelaide VeryDeep FCN VOC", "Method"], ["LRR 4x ResNet COCO", "Method"], ["Deeplabv 2 - CRF", "Method"], ["CentraleSupelec Deep G - CRF", "Method"], ["Cityscapes", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "8238530", "sentence": "According to our analysis on classification and segmentation , we find that large kernels is crucial to relieve the contradiction between classification and localization .", "ner": [["classification", "Task"], ["segmentation", "Task"], ["classification", "Task"], ["localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "8238530", "sentence": "Our best model achieves state - of - the - art on two public benchmarks : PASCAL VOC 2 0 1 2 ( 8 2 . 2 % ) and Cityscapes ( 7 6 . 9 % ) .", "ner": [["PASCAL VOC 2 0 1 2", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Specifically , we extract four types of visual co - occurrences between object and attribute words from large - scale , textually - annotated visual databases like VisualGenome and ImageNet .", "ner": [["VisualGenome", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Through unsupervised clustering , supervised partitioning , and a zero - shot - like generalization analysis we show that our word embeddings complement text - only embeddings like GloVe by better representing similarities and differences between visual concepts that are difficult to obtain from text corpora alone .", "ner": [["unsupervised clustering", "Method"], ["supervised partitioning", "Method"], ["zero - shot - like generalization analysis", "Method"], ["word embeddings", "Method"], ["text - only embeddings", "Method"], ["GloVe", "Method"]], "rel": [["zero - shot - like generalization analysis", "Used-For", "word embeddings"], ["supervised partitioning", "Used-For", "word embeddings"], ["unsupervised clustering", "Used-For", "word embeddings"], ["GloVe", "SubClass-Of", "text - only embeddings"]], "rel_plus": [["zero - shot - like generalization analysis:Method", "Used-For", "word embeddings:Method"], ["supervised partitioning:Method", "Used-For", "word embeddings:Method"], ["unsupervised clustering:Method", "Used-For", "word embeddings:Method"], ["GloVe:Method", "SubClass-Of", "text - only embeddings:Method"]]}
{"doc_id": "201307511", "sentence": "These word embeddings , e.g. , GloVe and word 2 vec , are typically learned from large - scale text corpora by modeling textual co - occurrences .", "ner": [["word embeddings", "Method"], ["GloVe", "Method"], ["word 2 vec", "Method"]], "rel": [["GloVe", "SubClass-Of", "word embeddings"], ["word 2 vec", "SubClass-Of", "word embeddings"]], "rel_plus": [["GloVe:Method", "SubClass-Of", "word embeddings:Method"], ["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"]]}
{"doc_id": "201307511", "sentence": "While no visual dataset exists with such exhaus - tive annotations ( many non - annotated words may still be applicable to an image region ) , large scale datasets like Vi - sualGenome [ 1 7 ] and ImageNet [ 8 ] along with their Word - Net [ 3 2 ] synset annotations provide a good starting point .", "ner": [["Vi - sualGenome", "Dataset"], ["ImageNet", "Dataset"], ["Word - Net", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "We use ImageNet annotations augmented with WordNet hypernyms to compute Object - Hypernym co - occurrences while the remaining types of co - occurrence are computed from VisualGenome 's object and attribute annotations .", "ner": [["ImageNet", "Dataset"], ["WordNet", "Dataset"], ["VisualGenome", "Dataset"]], "rel": [["WordNet", "Used-For", "ImageNet"]], "rel_plus": [["WordNet:Dataset", "Used-For", "ImageNet:Dataset"]]}
{"doc_id": "201307511", "sentence": "To learn ViCo , i.e. , word embeddings from Visual Cooccurrences , we could concatenate GloVe - like embeddings trained separately for each co - occurrence type via a logbilinear model .", "ner": [["ViCo", "Method"], ["word embeddings", "Method"], ["Visual Cooccurrences", "Method"], ["GloVe", "Method"]], "rel": [["word embeddings", "Part-Of", "Visual Cooccurrences"]], "rel_plus": [["word embeddings:Method", "Part-Of", "Visual Cooccurrences:Method"]]}
{"doc_id": "201307511", "sentence": "To test ViCo 's ability to capture similarities and differences between visual concepts , we analyze performance in an unsupervised clustering , supervised partitioning ( see supplementary material ) , and a zero - shot - like visual generalization setting .", "ner": [["ViCo", "Method"], ["unsupervised clustering", "Method"], ["supervised partitioning", "Method"], ["zero - shot - like visual generalization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "In both cases , ViCo augmented GloVe outperforms GloVe , random vectors , vis - w 2 v , or their combinations .", "ner": [["ViCo augmented GloVe", "Method"], ["GloVe", "Method"], ["random vectors", "Method"], ["vis - w 2 v ,", "Method"]], "rel": [["ViCo augmented GloVe", "Compare-With", "GloVe"], ["ViCo augmented GloVe", "Compare-With", "random vectors"], ["ViCo augmented GloVe", "Compare-With", "vis - w 2 v ,"]], "rel_plus": [["ViCo augmented GloVe:Method", "Compare-With", "GloVe:Method"], ["ViCo augmented GloVe:Method", "Compare-With", "random vectors:Method"], ["ViCo augmented GloVe:Method", "Compare-With", "vis - w 2 v ,:Method"]]}
{"doc_id": "201307511", "sentence": "Through a qualitative analogy question answering evaluation , we also find ViCo embedding space to better capture relations between visual concepts than GloVe .", "ner": [["question answering", "Task"], ["ViCo", "Method"], ["GloVe", "Method"]], "rel": [["ViCo", "Used-For", "question answering"], ["GloVe", "Used-For", "question answering"], ["ViCo", "Compare-With", "GloVe"]], "rel_plus": [["ViCo:Method", "Used-For", "question answering:Task"], ["GloVe:Method", "Used-For", "question answering:Task"], ["ViCo:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "The latter includes Caption - Image Retrieval , VQA , Referring Expression Comprehension , and Image Captioning .", "ner": [["Caption - Image Retrieval", "Task"], ["VQA", "Task"], ["Referring Expression Comprehension", "Task"], ["Image Captioning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Systems using ViCo outperform those using GloVe for almost all tasks and metrics .", "ner": [["ViCo", "Method"], ["GloVe", "Method"]], "rel": [["ViCo", "Compare-With", "GloVe"]], "rel_plus": [["ViCo:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "To summarize our contributions : ( 1 ) We develop a multitask method to learn a word embedding from multiple types of co - occurrences ; ( 2 ) We show that the embeddings learned from multiple visual co - occurrences , when com - bined with GloVe , outperform GloVe alone in unsupervised clustering , supervised partitioning , and zero - shot - like analysis , as well as on multiple vision - language tasks ; ( 3 ) We find that performance of supervised vision - language models is relatively insensitive to word embeddings , with even random embeddings leading to nearly the same performance as learned embeddings .", "ner": [["word embedding", "Method"], ["com - bined with GloVe", "Method"], ["GloVe alone", "Method"], ["unsupervised clustering", "Method"], ["supervised partitioning", "Method"], ["zero - shot - like analysis", "Method"], ["vision - language", "Task"], ["word embeddings", "Method"]], "rel": [["supervised partitioning", "Used-For", "GloVe alone"], ["unsupervised clustering", "Used-For", "GloVe alone"], ["zero - shot - like analysis", "Used-For", "GloVe alone"], ["com - bined with GloVe", "Compare-With", "GloVe alone"], ["GloVe alone", "Used-For", "vision - language"]], "rel_plus": [["supervised partitioning:Method", "Used-For", "GloVe alone:Method"], ["unsupervised clustering:Method", "Used-For", "GloVe alone:Method"], ["zero - shot - like analysis:Method", "Used-For", "GloVe alone:Method"], ["com - bined with GloVe:Method", "Compare-With", "GloVe alone:Method"], ["GloVe alone:Method", "Used-For", "vision - language:Task"]]}
{"doc_id": "201307511", "sentence": "Semantic Differential ( SD ) [ 3 4 ] is among the earliest attempts to obtain vector representations of words .", "ner": [["Semantic Differential", "Method"], ["SD", "Method"]], "rel": [["SD", "Synonym-Of", "Semantic Differential"]], "rel_plus": [["SD:Method", "Synonym-Of", "Semantic Differential:Method"]]}
{"doc_id": "201307511", "sentence": "Another approach involved acquiring word similarity annotations followed by applying Multidimensional Scaling ( MDS ) [ 2 1 ] to obtain low dimensional ( typically 2 - 4 ) embeddings and then identifying meaningful clusters or interpretable dimensions [ 4 5 ] .", "ner": [["Multidimensional Scaling", "Method"], ["MDS", "Method"]], "rel": [["MDS", "Synonym-Of", "Multidimensional Scaling"]], "rel_plus": [["MDS:Method", "Synonym-Of", "Multidimensional Scaling:Method"]]}
{"doc_id": "201307511", "sentence": "Like SD , the MDS approach lacked representation power , and embeddings and their interpretations varied based on words ( e.g. , food names [ 4 5 ] , animals [ 4 4 ] , etc . ) to which MDS was applied .", "ner": [["SD", "Method"], ["MDS", "Method"], ["MDS", "Method"]], "rel": [["MDS", "Compare-With", "SD"]], "rel_plus": [["MDS:Method", "Compare-With", "SD:Method"]]}
{"doc_id": "201307511", "sentence": "Recent neural approaches like the Continuous Bag - of - Words ( CBOW ) and the Skip - Gram models [ 2 9 , 3 1 , 3 0 ] learn from co - occurrences in local context windows as opposed to global co - occurrence statistics .", "ner": [["Continuous Bag - of - Words", "Method"], ["CBOW", "Method"], ["Skip - Gram", "Method"]], "rel": [["CBOW", "Synonym-Of", "Continuous Bag - of - Words"]], "rel_plus": [["CBOW:Method", "Synonym-Of", "Continuous Bag - of - Words:Method"]]}
{"doc_id": "201307511", "sentence": "We show loss computation of different approaches for learning word embeddings wi and wj for words i and j. The embeddings are denoted by colored vertical bars . ( i ) shows GloVe 's log - bilinear model . ( ii ) is our multi - task extension to learn from multiple co - occurrence matrices .", "ner": [["learning word embeddings", "Task"], ["GloVe", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "There is some work on incorporating image representations into word embeddings . vis - w 2 v [ 1 8 ] uses abstract ( synthetic ) scenes to learn visual relatedness .", "ner": [["word embeddings", "Method"], ["vis - w 2 v", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "ViEW [ 1 3 ] is another approach to visually enhance existing word embeddings .", "ner": [["ViEW", "Method"], ["word embeddings", "Method"]], "rel": [["ViEW", "SubClass-Of", "word embeddings"]], "rel_plus": [["ViEW:Method", "SubClass-Of", "word embeddings:Method"]]}
{"doc_id": "201307511", "sentence": "An autoencoder is trained on pre - trained word embeddings while matching intermediate representations to visual features extracted from a convolutional network trained on ImageNet .", "ner": [["autoencoder", "Method"], ["word embeddings", "Method"], ["convolutional network", "Method"], ["ImageNet", "Dataset"]], "rel": [["word embeddings", "Used-For", "autoencoder"], ["convolutional network", "Used-For", "autoencoder"], ["convolutional network", "Trained-With", "ImageNet"]], "rel_plus": [["word embeddings:Method", "Used-For", "autoencoder:Method"], ["convolutional network:Method", "Used-For", "autoencoder:Method"], ["convolutional network:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "201307511", "sentence": "The past year has seen several advances in contextualized word representations through pre - training on language models such as ELMo [ 3 9 ] , OpenAI GPT [ 4 2 ] , and BERT [ 9 ] .", "ner": [["pre - training on language models", "Method"], ["ELMo", "Method"], ["GPT", "Method"], ["BERT", "Method"]], "rel": [["ELMo", "SubClass-Of", "pre - training on language models"], ["GPT", "SubClass-Of", "pre - training on language models"], ["BERT", "SubClass-Of", "pre - training on language models"]], "rel_plus": [["ELMo:Method", "SubClass-Of", "pre - training on language models:Method"], ["GPT:Method", "SubClass-Of", "pre - training on language models:Method"], ["BERT:Method", "SubClass-Of", "pre - training on language models:Method"]]}
{"doc_id": "201307511", "sentence": "GloVe learns d - dimensional embeddings w i \u2208 R d for all words i by optimizing where f : R \u2192 R is a weighting function that assigns lower weight to less frequent , noisy co - occurrences and b i is a learnable bias term for word i. Intuitively , the program in Eq. ( 1 ) learns word embeddings such that for any word pair with non - zero cooccurrence , the dot product w T i w j approximates the log co - occurrence count up to an additive constant .", "ner": [["GloVe", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Note the slight difference between the objective in Eq. ( 1 ) and the original GloVe objective : GloVe replaces w j and b j withw j ( context vector ) andb j which are also trainable .", "ner": [["GloVe", "Method"], ["GloVe", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "We learn ViCo embeddings w i \u2208 R d for all words i by minimizing the following loss function Here \u03c6 t : R d \u2192 R dt is a co - occurrence type - specific transformation function that maps ViCo embeddings to a type - specialized embedding space . b t i is a learned bias term for word i and type t. We set function f ( X ) in Eq. ( 1 ) to the constant 1 for all X. Next , we discuss the transformations \u03c6 t , benefits of capturing different types of co - occurrences , use of the second term in Eq. ( 2 ) , and training details .", "ner": [["ViCo", "Method"], ["ViCo", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Pennington et al. [ 3 7 ] report Adagrad to work best for GloVe .", "ner": [["Adagrad", "Method"], ["GloVe", "Method"]], "rel": [["Adagrad", "Part-Of", "GloVe"]], "rel_plus": [["Adagrad:Method", "Part-Of", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "We use Visual Genome and ImageNet for estimating visual co - occurrence counts .", "ner": [["Visual Genome", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "ImageNet synsets and their ancestors in WordNet are used to compute Object - Hypernym ( oh ) counts .", "ner": [["ImageNet", "Dataset"], ["WordNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "For each region in Vi - sualGenome , we increment X oa ij by 1 , for each word pair ( i , j ) \u2208 S o \u00d7S a , and for all synset pairs ( S o , S a ) \u2208 O \u00d7 A. X oa ji is also incremented unless i = j. \u2022 For each region in VisualGenome , we increment X aa ij by 1 , for each word pair ( i , j ) \u2208 S a 1 \u00d7 S a 2 , and for all synset pairs ( S a 1 , S a 2 ) \u2208 A \u00d7 A. \u2022 Let C be the union of all object synsets annotated in an image .", "ner": [["Vi - sualGenome", "Dataset"], ["VisualGenome", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "For each image in VisualGenome , X c ij is incremented by 1 , for each word pair ( i , j ) \u2208 S c 1 \u00d7 S c 2 , and for all synset pairs ( S c 1 , S c 2 ) \u2208 C \u00d7 C. \u2022 Let H be a set of object synsets annotated for an image in ImageNet and its ancestors in WordNet .", "ner": [["VisualGenome", "Dataset"], ["ImageNet", "Dataset"], ["WordNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "For each each image in ImageNet , X oh ij is incremented by 1 , for each word pair ( i , j ) \u2208 S h 1 \u00d7 S h 2 , and for all synset pairs ( S h 1 , S h 2 ) \u2208 H \u00d7 H. We analyze ViCo embeddings with respect to the following properties : ( 1 ) 1 Data for clustering analysis .", "ner": [["ImageNet", "Dataset"], ["ViCo", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "We hypothesize that ViCo represents similarities and differences between visual categories that are missing from GloVe .", "ner": [["ViCo", "Method"], ["GloVe", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Qualitative evidence to support this hypothesis can be found in t - SNE plots shown in Fig. 4 , where concatenation of GloVe and ViCo embeddings leads to tighter , more homogenous clusters of the 1 3 coarse categories than GloVe .", "ner": [["t - SNE", "Method"], ["GloVe", "Method"], ["ViCo", "Method"], ["GloVe", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "Plots ( c , d ) in Fig. 4 compare random vectors , GloVe , variants of ViCo and their combinations ( concatenation ) for different number of clusters using V - Measure .", "ner": [["random vectors", "Method"], ["GloVe", "Method"], ["variants of ViCo", "Method"]], "rel": [["random vectors", "Compare-With", "GloVe"], ["random vectors", "Compare-With", "variants of ViCo"], ["GloVe", "Compare-With", "variants of ViCo"]], "rel_plus": [["random vectors:Method", "Compare-With", "GloVe:Method"], ["random vectors:Method", "Compare-With", "variants of ViCo:Method"], ["GloVe:Method", "Compare-With", "variants of ViCo:Method"]]}
{"doc_id": "201307511", "sentence": "Tab . 3 shows that ViCo alone outperforms GloVe , random , and vis - w 2 v based embeddings .", "ner": [["ViCo", "Method"], ["GloVe", "Method"], ["random", "Method"], ["vis - w 2 v", "Method"]], "rel": [["ViCo", "Compare-With", "GloVe"], ["ViCo", "Compare-With", "random"], ["ViCo", "Compare-With", "vis - w 2 v"]], "rel_plus": [["ViCo:Method", "Compare-With", "GloVe:Method"], ["ViCo:Method", "Compare-With", "random:Method"], ["ViCo:Method", "Compare-With", "vis - w 2 v:Method"]]}
{"doc_id": "201307511", "sentence": "WordNet is not the sole contributor to strong performance of ViCo .", "ner": [["WordNet", "Dataset"], ["ViCo", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "To verify that ViCo 's gains are not simply due to the hierarchical nature of WordNet , we evaluate a version of ViCo trained on co - occurrences computed without using WordNet , i.e. , using raw word annotations in VisualGenome instead of synset annotations and without Object - Hypernym co - occurrences .", "ner": [["ViCo", "Method"], ["WordNet", "Dataset"], ["ViCo", "Method"], ["WordNet", "Dataset"], ["VisualGenome", "Dataset"]], "rel": [["WordNet", "Used-For", "ViCo"], ["ViCo", "Trained-With", "VisualGenome"]], "rel_plus": [["WordNet:Dataset", "Used-For", "ViCo:Method"], ["ViCo:Method", "Trained-With", "VisualGenome:Dataset"]]}
{"doc_id": "201307511", "sentence": "Tab . 3 shows that GloVe+ViCo(linear, 1 0 0 ,w/o WordNet ) outperforms GloVe for both coarse and fine categories on both metrics .", "ner": [["GloVe+ViCo(linear,", "Method"], ["WordNet", "Dataset"], ["GloVe", "Method"]], "rel": [["GloVe+ViCo(linear,", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+ViCo(linear,:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "ViCo outperforms existing visual word embeddings .", "ner": [["ViCo", "Method"], ["visual word embeddings", "Method"]], "rel": [["ViCo", "Compare-With", "visual word embeddings"]], "rel_plus": [["ViCo:Method", "Compare-With", "visual word embeddings:Method"]]}
{"doc_id": "201307511", "sentence": "Tab . 3 evaluates performance of existing visual word embeddings which are learned from abstract scenes [ 1 8 ] . wiki and coco are different versions of vis - w 2 v depending on the dataset ( Wikipedia or MS - COCO [ 2 5 , 5 ] ) used for training word 2 vec for initialization .", "ner": [["word embeddings", "Method"], ["wiki", "Dataset"], ["coco", "Dataset"], ["vis - w 2 v", "Method"], ["Wikipedia", "Dataset"], ["MS - COCO", "Dataset"], ["word 2 vec", "Method"]], "rel": [["vis - w 2 v", "Trained-With", "Wikipedia"], ["vis - w 2 v", "Trained-With", "MS - COCO"]], "rel_plus": [["vis - w 2 v:Method", "Trained-With", "Wikipedia:Dataset"], ["vis - w 2 v:Method", "Trained-With", "MS - COCO:Dataset"]]}
{"doc_id": "201307511", "sentence": "GloVe+vis - w 2 v - wiki performs similarly to GloVe and GloVe+vis - w 2 v - wiki - coco performs only slightly better than GloVe , showing that the majority of the information captured by vis - w 2 v may already be present in GloVe .", "ner": [["GloVe+vis - w 2 v - wiki", "Method"], ["GloVe", "Method"], ["GloVe+vis - w 2 v - wiki - coco", "Method"], ["GloVe", "Method"], ["vis - w 2 v", "Method"], ["GloVe", "Method"]], "rel": [["GloVe+vis - w 2 v - wiki", "Compare-With", "GloVe"], ["GloVe+vis - w 2 v - wiki - coco", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+vis - w 2 v - wiki:Method", "Compare-With", "GloVe:Method"], ["GloVe+vis - w 2 v - wiki - coco:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "GloVe+random performs similarly to GloVe or worse .", "ner": [["GloVe+random", "Method"], ["GloVe", "Method"]], "rel": [["GloVe+random", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+random:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "This implies that gains of GloVe+ViCo over GloVe are not just an artifact of increased dimensionality .", "ner": [["GloVe+ViCo", "Method"], ["GloVe", "Method"]], "rel": [["GloVe+ViCo", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+ViCo:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "To assess this ability , we evaluate embeddings on their zero - shot - like object classification performance using the CIFAR - 1 0 0 dataset .", "ner": [["object classification", "Task"], ["CIFAR - 1 0 0", "Dataset"]], "rel": [["CIFAR - 1 0 0", "Benchmark-For", "object classification"]], "rel_plus": [["CIFAR - 1 0 0:Dataset", "Benchmark-For", "object classification:Task"]]}
{"doc_id": "201307511", "sentence": "Note that our zero - shot - like setup is slightly different from a typical zero - shot setup because even though the visual classifier is not trained on unseen class images in CIFAR , annotations associated with images of unseen categories in VisualGenome or ImageNet may be used to compute word co - occurrences while learning word embeddings .", "ner": [["CIFAR", "Dataset"], ["VisualGenome", "Dataset"], ["ImageNet", "Dataset"], ["learning word embeddings", "Task"]], "rel": [["VisualGenome", "Used-For", "learning word embeddings"], ["ImageNet", "Used-For", "learning word embeddings"], ["CIFAR", "Used-For", "learning word embeddings"]], "rel_plus": [["VisualGenome:Dataset", "Used-For", "learning word embeddings:Task"], ["ImageNet:Dataset", "Used-For", "learning word embeddings:Task"], ["CIFAR:Dataset", "Used-For", "learning word embeddings:Task"]]}
{"doc_id": "201307511", "sentence": "Let f ( I ) \u2208 R n be the features extracted from image I using a CNN and let w c \u2208 R m denote the word embedding for class c \u2208 C. Let g : R m \u2192 R n denote a function that projects word embeddings into the space of image features .", "ner": [["CNN", "Method"], ["word embedding", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "In our experiments , f ( I ) is a 6 4 - dimensional feature vector produced by the last linear layer of a 3 4 - layer ResNet ( modified to accept 3 2 \u00d7 3 2 CIFAR images ) and g is a linear transformation .", "ner": [["linear layer", "Method"], ["ResNet", "Method"], ["CIFAR", "Dataset"]], "rel": [["linear layer", "Part-Of", "ResNet"]], "rel_plus": [["linear layer:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "201307511", "sentence": "Fig. 5 compares chance performance ( 1/|U| ) , random vectors , GloVe , and GloVe+ViCo on four seen/unseen splits .", "ner": [["random vectors", "Method"], ["GloVe", "Method"], ["GloVe+ViCo", "Method"]], "rel": [["random vectors", "Compare-With", "GloVe"], ["random vectors", "Compare-With", "GloVe+ViCo"], ["GloVe", "Compare-With", "GloVe+ViCo"]], "rel_plus": [["random vectors:Method", "Compare-With", "GloVe:Method"], ["random vectors:Method", "Compare-With", "GloVe+ViCo:Method"], ["GloVe:Method", "Compare-With", "GloVe+ViCo:Method"]]}
{"doc_id": "201307511", "sentence": "The key conclusions are as follows : ViCo generalizes to unseen classes better than GloVe .", "ner": [["ViCo", "Method"], ["GloVe", "Method"]], "rel": [["ViCo", "Compare-With", "GloVe"]], "rel_plus": [["ViCo:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "ViCo based embeddings , especially 2 0 0 - dim . select and linear variants show healthy gains over GloVe .", "ner": [["ViCo", "Method"], ["GloVe", "Method"]], "rel": [["ViCo", "Compare-With", "GloVe"]], "rel_plus": [["ViCo:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "Note that this is not just due to higher dimensions of the embeddings since GloVe+random( 2 0 0 ) performs worse than GloVe .", "ner": [["GloVe+random(", "Method"], ["GloVe", "Method"]], "rel": [["GloVe+random(", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+random(:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "However , GloVe+ViCo(linear, 1 0 0 ) still outperforms GloVe in 3 out of 4 splits .", "ner": [["GloVe+ViCo(linear,", "Method"], ["GloVe", "Method"]], "rel": [["GloVe+ViCo(linear,", "Compare-With", "GloVe"]], "rel_plus": [["GloVe+ViCo(linear,:Method", "Compare-With", "GloVe:Method"]]}
{"doc_id": "201307511", "sentence": "On all tasks GloVe+ViCo outpeforms GloVe and GloVe+random .", "ner": [["GloVe+ViCo", "Method"], ["GloVe", "Method"], ["GloVe+random", "Method"]], "rel": [["GloVe+ViCo", "Compare-With", "GloVe"], ["GloVe+ViCo", "Compare-With", "GloVe+random"]], "rel_plus": [["GloVe+ViCo:Method", "Compare-With", "GloVe:Method"], ["GloVe+ViCo:Method", "Compare-With", "GloVe+random:Method"]]}
{"doc_id": "201307511", "sentence": "Comparing ViCo to GloVe and random vectors .", "ner": [["ViCo", "Method"], ["GloVe", "Method"], ["random vectors", "Method"]], "rel": [["ViCo", "Compare-With", "GloVe"], ["ViCo", "Compare-With", "random vectors"]], "rel_plus": [["ViCo:Method", "Compare-With", "GloVe:Method"], ["ViCo:Method", "Compare-With", "random vectors:Method"]]}
{"doc_id": "201307511", "sentence": "GloVe+ViCo(linear ) outperforms GloVe and GloVe+random for all tasks and outperforms random for all tasks except Image Captioning .", "ner": [["GloVe+ViCo(linear )", "Method"], ["GloVe", "Method"], ["GloVe+random", "Method"], ["Image Captioning", "Task"]], "rel": [["GloVe+ViCo(linear )", "Compare-With", "GloVe"], ["GloVe+ViCo(linear )", "Compare-With", "GloVe+random"]], "rel_plus": [["GloVe+ViCo(linear ):Method", "Compare-With", "GloVe:Method"], ["GloVe+ViCo(linear ):Method", "Compare-With", "GloVe+random:Method"]]}
{"doc_id": "201307511", "sentence": "Let w 1 , w 2 , and a be the word embeddings ( GloVe or ViCo ) for the two concept words and the attribute word .", "ner": [["word embeddings", "Method"], ["GloVe", "Method"], ["ViCo", "Method"]], "rel": [["GloVe", "SubClass-Of", "word embeddings"], ["ViCo", "SubClass-Of", "word embeddings"]], "rel_plus": [["GloVe:Method", "SubClass-Of", "word embeddings:Method"], ["ViCo:Method", "SubClass-Of", "word embeddings:Method"]]}
{"doc_id": "201307511", "sentence": "We compute the scores s g and s v for GloVe and ViCo using function s(a , w 1 , w 2 ) = cosine(a , w 1 ) \u2212 cosine(a , w 2 ) , where cosine ( \u00b7 ) is the cosine similarity .", "ner": [["GloVe", "Method"], ["ViCo", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "We then learn a linear SVM over s g for the GloVe only model and over s g and s v for the GloVe+ViCo model .", "ner": [["linear SVM", "Method"], ["GloVe only model", "Method"], ["GloVe+ViCo", "Method"]], "rel": [["linear SVM", "Part-Of", "GloVe only model"], ["linear SVM", "Part-Of", "GloVe+ViCo"]], "rel_plus": [["linear SVM:Method", "Part-Of", "GloVe only model:Method"], ["linear SVM:Method", "Part-Of", "GloVe+ViCo:Method"]]}
{"doc_id": "201307511", "sentence": "Image features are then fused with a question representation using a GRU operating on word embeddings and fed into an answer classifier .", "ner": [["GRU", "Method"], ["word embeddings", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201307511", "sentence": "We use the open source implementation of MAt - tNet [ 5 4 ] to compare localization accuracy with different embeddings on the RefCOCO+ dataset using the UNC split .", "ner": [["MAt - tNet", "Method"], ["localization", "Task"], ["RefCOCO+", "Dataset"]], "rel": [["MAt - tNet", "Used-For", "localization"], ["RefCOCO+", "Benchmark-For", "localization"], ["MAt - tNet", "Evaluated-With", "RefCOCO+"]], "rel_plus": [["MAt - tNet:Method", "Used-For", "localization:Task"], ["RefCOCO+:Dataset", "Benchmark-For", "localization:Task"], ["MAt - tNet:Method", "Evaluated-With", "RefCOCO+:Dataset"]]}
{"doc_id": "201307511", "sentence": "Out of 3 0 analogy pairings tested , we found both GloVe and ViCo to be correct 1 9 times , only ViCo was correct 8 times , and only Glove was correct 3 times .", "ner": [["GloVe", "Method"], ["ViCo", "Method"], ["ViCo", "Method"], ["Glove", "Method"]], "rel": [], "rel_plus": []}
