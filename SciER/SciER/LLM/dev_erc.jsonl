{"doc_id": "ICCV_2003_158_abs", "sentence": "This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a rigidly-moving Lambertian object under distant illumination . The problem is formulated in a manner that subsumes structure from motion , multi-view stereo , and photo-metric stereo as special cases . The algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces . The algorithm works by iteratively estimating affine camera parameters , illumination , shape , and albedo in an alternating fashion . Results are demonstrated on videos of hand-held objects moving in front of a fixed light and camera .", "ner": [["algorithm", "Generic"], ["computing optical flow , shape , motion , lighting , and albedo", "Task"], ["image sequence", "Material"], ["rigidly-moving Lambertian object", "Material"], ["distant illumination", "OtherScientificTerm"], ["problem", "Generic"], ["motion", "Material"], ["multi-view stereo", "Material"], ["photo-metric stereo", "Material"], ["algorithm", "Generic"], ["spatial and temporal intensity variation", "OtherScientificTerm"], ["cues", "Generic"], ["former", "Generic"], ["flow", "OtherScientificTerm"], ["latter", "Generic"], ["surface orientation", "OtherScientificTerm"], ["cues", "Generic"], ["dense reconstruction of both textured and texture-less surfaces", "Task"], ["algorithm", "Generic"], ["estimating affine camera parameters , illumination , shape , and albedo", "Method"], ["videos of hand-held objects", "Material"]], "rel": [["algorithm", "Used-for", "computing optical flow , shape , motion , lighting , and albedo"], ["image sequence", "Used-for", "algorithm"], ["rigidly-moving Lambertian object", "Feature-of", "image sequence"], ["distant illumination", "Feature-of", "rigidly-moving Lambertian object"], ["motion", "Conjunction", "multi-view stereo"], ["multi-view stereo", "Conjunction", "photo-metric stereo"], ["spatial and temporal intensity variation", "Used-for", "algorithm"], ["former", "Hyponym-of", "cues"], ["former", "Used-for", "flow"], ["former", "Conjunction", "latter"], ["latter", "Hyponym-of", "cues"], ["latter", "Used-for", "surface orientation"], ["cues", "Used-for", "dense reconstruction of both textured and texture-less surfaces"], ["estimating affine camera parameters , illumination , shape , and albedo", "Used-for", "algorithm"]], "rel_plus": [["algorithm:Generic", "Used-for", "computing optical flow , shape , motion , lighting , and albedo:Task"], ["image sequence:Material", "Used-for", "algorithm:Generic"], ["rigidly-moving Lambertian object:Material", "Feature-of", "image sequence:Material"], ["distant illumination:OtherScientificTerm", "Feature-of", "rigidly-moving Lambertian object:Material"], ["motion:Material", "Conjunction", "multi-view stereo:Material"], ["multi-view stereo:Material", "Conjunction", "photo-metric stereo:Material"], ["spatial and temporal intensity variation:OtherScientificTerm", "Used-for", "algorithm:Generic"], ["former:Generic", "Hyponym-of", "cues:Generic"], ["former:Generic", "Used-for", "flow:OtherScientificTerm"], ["former:Generic", "Conjunction", "latter:Generic"], ["latter:Generic", "Hyponym-of", "cues:Generic"], ["latter:Generic", "Used-for", "surface orientation:OtherScientificTerm"], ["cues:Generic", "Used-for", "dense reconstruction of both textured and texture-less surfaces:Task"], ["estimating affine camera parameters , illumination , shape , and albedo:Method", "Used-for", "algorithm:Generic"]]}
{"doc_id": "C04-1096", "sentence": "Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects . However , such an approach does not work well when there is no distinctive attribute among objects . To overcome this limitation , this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them . The key is to identify groups of objects that are naturally recognized by humans . We conducted psychological experiments with 42 subjects to collect referring expressions in such situations , and built a generation algorithm based on the results . The evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions .", "ner": [["referring expressions", "OtherScientificTerm"], ["binary relations", "OtherScientificTerm"], ["n-ary relations", "OtherScientificTerm"], ["referring expressions", "OtherScientificTerm"], ["generation algorithm", "Method"], ["method", "Generic"], ["referring expressions", "OtherScientificTerm"]], "rel": [], "rel_plus": []}
{"doc_id": "P84-1047", "sentence": "An entity-oriented approach to restricted-domain parsing is proposed . In this approach , the definitions of the structure and surface representation of domain entities are grouped together . Like semantic grammar , this allows easy exploitation of limited domain semantics . In addition , it facilitates fragmentary recognition and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input . Several advantages from the point of view of language definition are also noted . Representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some parsing strategies that use the control structure , and worked examples of parses . A parser incorporating the control structure and the parsing strategies is currently under implementation .", "ner": [["entity-oriented approach", "Method"], ["restricted-domain parsing", "Task"], ["approach", "Generic"], ["structure and surface representation of domain entities", "OtherScientificTerm"], ["semantic grammar", "Method"], ["this", "Generic"], ["limited domain semantics", "OtherScientificTerm"], ["it", "Generic"], ["fragmentary recognition", "Task"], ["multiple parsing strategies", "Method"], ["recognition of extra-grammatical input", "OtherScientificTerm"], ["entity-oriented language definition", "OtherScientificTerm"], ["control structure", "OtherScientificTerm"], ["entity-oriented parser", "Method"], ["parsing strategies", "Method"], ["control structure", "OtherScientificTerm"], ["parser", "Method"], ["control structure", "OtherScientificTerm"], ["parsing strategies", "Method"]], "rel": [["entity-oriented approach", "Used-for", "restricted-domain parsing"], ["this", "Used-for", "limited domain semantics"], ["it", "Used-for", "fragmentary recognition"], ["it", "Used-for", "multiple parsing strategies"], ["multiple parsing strategies", "Used-for", "recognition of extra-grammatical input"], ["control structure", "Used-for", "entity-oriented parser"], ["control structure", "Used-for", "parsing strategies"], ["control structure", "Part-of", "parser"]], "rel_plus": [["entity-oriented approach:Method", "Used-for", "restricted-domain parsing:Task"], ["this:Generic", "Used-for", "limited domain semantics:OtherScientificTerm"], ["it:Generic", "Used-for", "fragmentary recognition:Task"], ["it:Generic", "Used-for", "multiple parsing strategies:Method"], ["multiple parsing strategies:Method", "Used-for", "recognition of extra-grammatical input:OtherScientificTerm"], ["control structure:OtherScientificTerm", "Used-for", "entity-oriented parser:Method"], ["control structure:OtherScientificTerm", "Used-for", "parsing strategies:Method"], ["control structure:OtherScientificTerm", "Part-of", "parser:Method"]]}
{"doc_id": "C88-1066", "sentence": "This paper summarizes the formalism of Category Cooccurrence Restrictions ( CCRs ) and describes two parsing algorithms that interpret it . CCRs are Boolean conditions on the cooccurrence of categories in local trees which allow the statement of generalizations which can not be captured in other current syntax formalisms . The use of CCRs leads to syntactic descriptions formulated entirely with restrictive statements . The paper shows how conventional algorithms for the analysis of context free languages can be adapted to the CCR formalism . Special attention is given to the part of the parser that checks the fulfillment of logical well-formedness conditions on trees .", "ner": [["formalism of Category Cooccurrence Restrictions ( CCRs )", "Task"], ["Category Cooccurrence Restrictions ( CCRs )", "OtherScientificTerm"], ["parsing algorithms", "Method"], ["it", "Generic"], ["CCRs", "OtherScientificTerm"], ["Boolean conditions", "OtherScientificTerm"], ["local trees", "OtherScientificTerm"], ["statement of generalizations", "OtherScientificTerm"], ["syntax formalisms", "Method"], ["CCRs", "OtherScientificTerm"], ["syntactic descriptions", "OtherScientificTerm"], ["restrictive statements", "OtherScientificTerm"], ["algorithms", "Generic"], ["context free languages", "Material"], ["CCR formalism", "Task"], ["parser", "Method"], ["logical well-formedness conditions", "OtherScientificTerm"], ["trees", "OtherScientificTerm"]], "rel": [["parsing algorithms", "Used-for", "it"], ["restrictive statements", "Feature-of", "syntactic descriptions"], ["algorithms", "Used-for", "CCR formalism"], ["context free languages", "Used-for", "algorithms"], ["logical well-formedness conditions", "Feature-of", "trees"]], "rel_plus": [["parsing algorithms:Method", "Used-for", "it:Generic"], ["restrictive statements:OtherScientificTerm", "Feature-of", "syntactic descriptions:OtherScientificTerm"], ["algorithms:Generic", "Used-for", "CCR formalism:Task"], ["context free languages:Material", "Used-for", "algorithms:Generic"], ["logical well-formedness conditions:OtherScientificTerm", "Feature-of", "trees:OtherScientificTerm"]]}
{"doc_id": "C04-1116", "sentence": "We present a text mining method for finding synonymous expressions based on the distributional hypothesis in a set of coherent corpora . This paper proposes a new methodology to improve the accuracy of a term aggregation system using each author 's text as a coherent corpus . Our approach is based on the idea that one person tends to use one expression for one meaning . According to our assumption , most of the words with similar context features in each author 's corpus tend not to be synonymous expressions . Our proposed method improves the accuracy of our term aggregation system , showing that our approach is successful .", "ner": [["text mining method", "Method"], ["synonymous expressions", "OtherScientificTerm"], ["distributional hypothesis", "OtherScientificTerm"], ["methodology", "Generic"], ["accuracy", "Metric"], ["term aggregation system", "Method"], ["approach", "Generic"], ["similar context features", "OtherScientificTerm"], ["synonymous expressions", "OtherScientificTerm"], ["method", "Generic"], ["accuracy", "Metric"], ["term aggregation system", "Method"], ["approach", "Generic"]], "rel": [["text mining method", "Used-for", "synonymous expressions"], ["distributional hypothesis", "Used-for", "text mining method"], ["accuracy", "Evaluate-for", "term aggregation system"], ["term aggregation system", "Evaluate-for", "methodology"], ["accuracy", "Evaluate-for", "term aggregation system"], ["term aggregation system", "Evaluate-for", "method"]], "rel_plus": [["text mining method:Method", "Used-for", "synonymous expressions:OtherScientificTerm"], ["distributional hypothesis:OtherScientificTerm", "Used-for", "text mining method:Method"], ["accuracy:Metric", "Evaluate-for", "term aggregation system:Method"], ["term aggregation system:Method", "Evaluate-for", "methodology:Generic"], ["accuracy:Metric", "Evaluate-for", "term aggregation system:Method"], ["term aggregation system:Method", "Evaluate-for", "method:Generic"]]}
{"doc_id": "ICCV_2009_47_abs", "sentence": "In this work , we present a technique for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more efficient robust estimation algorithm . In addition , we build on recent work in randomized model verification , and use this to characterize the ` non-randomness ' of a solution . The combination of these two strategies results in a robust estimation procedure that provides a significant speed-up over existing RANSAC techniques , while requiring no prior information to guide the sampling process . In particular , our algorithm requires , on average , 3-10 times fewer samples than standard RANSAC , which is in close agreement with theoretical predictions . The efficiency of the algorithm is demonstrated on a selection of geometric estimation problems .", "ner": [["technique", "Generic"], ["robust estimation", "Task"], ["inherent uncertainty of the estimation procedure", "OtherScientificTerm"], ["efficient robust estimation algorithm", "Method"], ["randomized model verification", "Task"], ["this", "Generic"], ["strategies", "Generic"], ["robust estimation procedure", "Method"], ["RANSAC techniques", "Method"], ["prior information", "OtherScientificTerm"], ["sampling process", "OtherScientificTerm"], ["algorithm", "Generic"], ["RANSAC", "Method"], ["theoretical predictions", "OtherScientificTerm"], ["algorithm", "Generic"], ["geometric estimation problems", "Task"]], "rel": [["technique", "Used-for", "robust estimation"], ["technique", "Used-for", "efficient robust estimation algorithm"], ["inherent uncertainty of the estimation procedure", "Used-for", "technique"], ["strategies", "Used-for", "robust estimation procedure"], ["RANSAC techniques", "Compare", "robust estimation procedure"], ["algorithm", "Compare", "RANSAC"], ["geometric estimation problems", "Evaluate-for", "algorithm"]], "rel_plus": [["technique:Generic", "Used-for", "robust estimation:Task"], ["technique:Generic", "Used-for", "efficient robust estimation algorithm:Method"], ["inherent uncertainty of the estimation procedure:OtherScientificTerm", "Used-for", "technique:Generic"], ["strategies:Generic", "Used-for", "robust estimation procedure:Method"], ["RANSAC techniques:Method", "Compare", "robust estimation procedure:Method"], ["algorithm:Generic", "Compare", "RANSAC:Method"], ["geometric estimation problems:Task", "Evaluate-for", "algorithm:Generic"]]}
{"doc_id": "C80-1073", "sentence": "An attempt has been made to use an Augmented Transition Network as a procedural dialog model . The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs . A standard ATN should be further developed in order to account for the verbal interactions of task-oriented dialogs .", "ner": [["Augmented Transition Network", "Method"], ["dialog model", "Method"], ["model", "Generic"], ["device", "Generic"], ["dialog schemata", "OtherScientificTerm"], ["conversation analysis", "Method"], ["device", "Generic"], ["models", "Generic"], ["verbal interaction", "OtherScientificTerm"], ["device", "Generic"], ["dialog schemata", "OtherScientificTerm"], ["verbal interaction", "OtherScientificTerm"], ["task-oriented and goal-directed dialogs", "Material"], ["ATN", "Method"], ["verbal interactions", "OtherScientificTerm"], ["task-oriented dialogs", "Material"]], "rel": [["Augmented Transition Network", "Hyponym-of", "dialog model"], ["dialog schemata", "Used-for", "device"], ["dialog schemata", "Used-for", "conversation analysis"], ["models", "Used-for", "device"], ["models", "Used-for", "verbal interaction"], ["dialog schemata", "Conjunction", "verbal interaction"], ["ATN", "Used-for", "verbal interactions"], ["verbal interactions", "Feature-of", "task-oriented dialogs"]], "rel_plus": [["Augmented Transition Network:Method", "Hyponym-of", "dialog model:Method"], ["dialog schemata:OtherScientificTerm", "Used-for", "device:Generic"], ["dialog schemata:OtherScientificTerm", "Used-for", "conversation analysis:Method"], ["models:Generic", "Used-for", "device:Generic"], ["models:Generic", "Used-for", "verbal interaction:OtherScientificTerm"], ["dialog schemata:OtherScientificTerm", "Conjunction", "verbal interaction:OtherScientificTerm"], ["ATN:Method", "Used-for", "verbal interactions:OtherScientificTerm"], ["verbal interactions:OtherScientificTerm", "Feature-of", "task-oriented dialogs:Material"]]}
{"doc_id": "H05-1041", "sentence": "We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines . The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes . We show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better .", "ner": [["unsupervised learning method", "Method"], ["single-snippet answers", "OtherScientificTerm"], ["question answering systems", "Method"], ["Web search engines", "Method"], ["method", "Generic"], ["on-line encyclopedias and dictionaries", "Material"], ["positive and negative definition examples", "Material"], ["svm", "Method"], ["method", "Generic"], ["it", "Generic"], ["alternative", "Generic"], ["system", "Generic"], ["news articles", "Material"], ["trec", "Material"], ["it", "Generic"], ["search engine", "Method"]], "rel": [["unsupervised learning method", "Used-for", "single-snippet answers"], ["question answering systems", "Used-for", "Web search engines"], ["method", "Used-for", "on-line encyclopedias and dictionaries"], ["on-line encyclopedias and dictionaries", "Used-for", "positive and negative definition examples"], ["positive and negative definition examples", "Used-for", "svm"], ["it", "Compare", "alternative"], ["news articles", "Used-for", "system"], ["news articles", "Part-of", "trec"], ["it", "Used-for", "search engine"]], "rel_plus": [["unsupervised learning method:Method", "Used-for", "single-snippet answers:OtherScientificTerm"], ["question answering systems:Method", "Used-for", "Web search engines:Method"], ["method:Generic", "Used-for", "on-line encyclopedias and dictionaries:Material"], ["on-line encyclopedias and dictionaries:Material", "Used-for", "positive and negative definition examples:Material"], ["positive and negative definition examples:Material", "Used-for", "svm:Method"], ["it:Generic", "Compare", "alternative:Generic"], ["news articles:Material", "Used-for", "system:Generic"], ["news articles:Material", "Part-of", "trec:Material"], ["it:Generic", "Used-for", "search engine:Method"]]}
{"doc_id": "NIPS_2014_18_abs", "sentence": "We revisit the classical decision-theoretic problem of weighted expert voting from a statistical learning perspective . In particular , we examine the consistency ( both asymptotic and finitary ) of the optimal Nitzan-Paroush weighted majority and related rules . In the case of known expert competence levels , we give sharp error estimates for the optimal rule . When the competence levels are unknown , they must be empirically estimated . We provide frequentist and Bayesian analyses for this situation . Some of our proof techniques are non-standard and may be of independent interest . The bounds we derive are nearly optimal , and several challenging open problems are posed . Experimental results are provided to illustrate the theory .", "ner": [["classical decision-theoretic problem of weighted expert voting", "Task"], ["statistical learning perspective", "Method"], ["Nitzan-Paroush weighted majority", "OtherScientificTerm"], ["expert competence levels", "OtherScientificTerm"], ["sharp error estimates", "Method"], ["optimal rule", "OtherScientificTerm"], ["competence levels", "OtherScientificTerm"], ["Bayesian analyses", "Method"]], "rel": [["statistical learning perspective", "Used-for", "classical decision-theoretic problem of weighted expert voting"], ["sharp error estimates", "Used-for", "optimal rule"]], "rel_plus": [["statistical learning perspective:Method", "Used-for", "classical decision-theoretic problem of weighted expert voting:Task"], ["sharp error estimates:Method", "Used-for", "optimal rule:OtherScientificTerm"]]}
{"doc_id": "NIPS_2014_10_abs", "sentence": "We analyze a reweighted version of the Kikuchi approximation for estimating the log partition function of a product distribution defined over a region graph . We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges . When the region graph has two layers , corresponding to a Bethe approximation , we show that our sufficient conditions for concavity are also necessary . Finally , we provide an explicit characterization of the polytope of concavity in terms of the cycle structure of the region graph . We conclude with simulations that demonstrate the advantages of the reweighted Kikuchi approach .", "ner": [["reweighted version of the Kikuchi approximation", "Method"], ["Kikuchi approximation", "Method"], ["log partition function of a product distribution", "Task"], ["region graph", "OtherScientificTerm"], ["concavity", "OtherScientificTerm"], ["reweighted objective function", "OtherScientificTerm"], ["weight assignments", "OtherScientificTerm"], ["Kikuchi expansion", "OtherScientificTerm"], ["reweighted version of the sum product algorithm", "Method"], ["Kikuchi region graph", "OtherScientificTerm"], ["global optima", "OtherScientificTerm"], ["Kikuchi approximation", "Method"], ["algorithm", "Generic"], ["region graph", "OtherScientificTerm"], ["Bethe approximation", "Method"], ["concavity", "OtherScientificTerm"], ["concavity", "OtherScientificTerm"], ["cycle structure", "OtherScientificTerm"], ["region graph", "OtherScientificTerm"], ["reweighted Kikuchi approach", "Method"]], "rel": [["reweighted version of the Kikuchi approximation", "Used-for", "log partition function of a product distribution"], ["log partition function of a product distribution", "Feature-of", "region graph"], ["concavity", "Feature-of", "reweighted objective function"], ["reweighted version of the sum product algorithm", "Used-for", "Kikuchi region graph"], ["global optima", "Feature-of", "Kikuchi approximation"], ["cycle structure", "Feature-of", "region graph"]], "rel_plus": [["reweighted version of the Kikuchi approximation:Method", "Used-for", "log partition function of a product distribution:Task"], ["log partition function of a product distribution:Task", "Feature-of", "region graph:OtherScientificTerm"], ["concavity:OtherScientificTerm", "Feature-of", "reweighted objective function:OtherScientificTerm"], ["reweighted version of the sum product algorithm:Method", "Used-for", "Kikuchi region graph:OtherScientificTerm"], ["global optima:OtherScientificTerm", "Feature-of", "Kikuchi approximation:Method"], ["cycle structure:OtherScientificTerm", "Feature-of", "region graph:OtherScientificTerm"]]}
{"doc_id": "P03-1022", "sentence": "We apply a decision tree based approach to pronoun resolution in spoken dialogue . Our system deals with pronouns with NP - and non-NP-antecedents . We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features . We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron 's ( 2002 ) manually tuned system .", "ner": [["decision tree based approach", "Method"], ["pronoun resolution", "Task"], ["spoken dialogue", "Task"], ["system", "Generic"], ["pronouns", "OtherScientificTerm"], ["NP - and non-NP-antecedents", "OtherScientificTerm"], ["features", "OtherScientificTerm"], ["pronoun resolution", "Task"], ["spoken dialogue", "Task"], ["features", "OtherScientificTerm"], ["system", "Generic"], ["Switchboard dialogues", "Material"], ["it", "Generic"], ["Byron 's ( 2002 ) manually tuned system", "Method"]], "rel": [["decision tree based approach", "Used-for", "pronoun resolution"], ["pronoun resolution", "Used-for", "spoken dialogue"], ["system", "Used-for", "pronouns"], ["NP - and non-NP-antecedents", "Used-for", "pronouns"], ["features", "Used-for", "pronoun resolution"], ["pronoun resolution", "Used-for", "spoken dialogue"], ["Switchboard dialogues", "Evaluate-for", "system"], ["it", "Compare", "Byron 's ( 2002 ) manually tuned system"]], "rel_plus": [["decision tree based approach:Method", "Used-for", "pronoun resolution:Task"], ["pronoun resolution:Task", "Used-for", "spoken dialogue:Task"], ["system:Generic", "Used-for", "pronouns:OtherScientificTerm"], ["NP - and non-NP-antecedents:OtherScientificTerm", "Used-for", "pronouns:OtherScientificTerm"], ["features:OtherScientificTerm", "Used-for", "pronoun resolution:Task"], ["pronoun resolution:Task", "Used-for", "spoken dialogue:Task"], ["Switchboard dialogues:Material", "Evaluate-for", "system:Generic"], ["it:Generic", "Compare", "Byron 's ( 2002 ) manually tuned system:Method"]]}
{"doc_id": "CVPR_2010_10_abs", "sentence": "We present a new approach for building an efficient and robust classifier for the two class problem , that localizes objects that may appear in the image under different orien-tations . In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an estimation stage and a classification stage . The estimator yields an initial set of potential object poses that are then validated by the classifier . This methodology allows reducing the time complexity of the algorithm while classification results remain high . The classifier we use in both stages is based on a boosted combination of Random Ferns over local histograms of oriented gradients ( HOGs ) , which we compute during a pre-processing step . Both the use of supervised learning and working on the gradient space makes our approach robust while being efficient at run-time . We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions .", "ner": [["approach", "Generic"], ["classifier", "Method"], ["class problem", "Task"], ["image", "Material"], ["orien-tations", "OtherScientificTerm"], ["problem", "Generic"], ["classifiers", "Method"], ["approach", "Generic"], ["estimation stage", "Method"], ["classification stage", "Method"], ["estimator", "Method"], ["object poses", "OtherScientificTerm"], ["classifier", "Method"], ["time complexity", "Metric"], ["algorithm", "Generic"], ["classification", "Task"], ["classifier", "Method"], ["boosted combination of Random Ferns", "Method"], ["local histograms of oriented gradients ( HOGs )", "OtherScientificTerm"], ["pre-processing step", "Method"], ["supervised learning", "Method"], ["gradient space", "OtherScientificTerm"], ["approach", "Generic"], ["database", "Generic"], ["motorbikes under planar rotations", "Material"], ["conditions", "Generic"], ["cluttered backgrounds", "OtherScientificTerm"], ["changing illumination conditions", "OtherScientificTerm"], ["partial occlusions", "OtherScientificTerm"]], "rel": [["approach", "Used-for", "classifier"], ["classifier", "Used-for", "class problem"], ["estimation stage", "Part-of", "approach"], ["estimation stage", "Conjunction", "classification stage"], ["classification stage", "Part-of", "approach"], ["classifier", "Used-for", "object poses"], ["time complexity", "Evaluate-for", "algorithm"], ["boosted combination of Random Ferns", "Used-for", "classifier"], ["local histograms of oriented gradients ( HOGs )", "Feature-of", "boosted combination of Random Ferns"], ["pre-processing step", "Used-for", "local histograms of oriented gradients ( HOGs )"], ["supervised learning", "Used-for", "approach"], ["gradient space", "Used-for", "approach"], ["motorbikes under planar rotations", "Feature-of", "database"], ["conditions", "Feature-of", "database"], ["cluttered backgrounds", "Hyponym-of", "conditions"], ["cluttered backgrounds", "Conjunction", "changing illumination conditions"], ["changing illumination conditions", "Hyponym-of", "conditions"], ["changing illumination conditions", "Conjunction", "partial occlusions"], ["partial occlusions", "Hyponym-of", "conditions"]], "rel_plus": [["approach:Generic", "Used-for", "classifier:Method"], ["classifier:Method", "Used-for", "class problem:Task"], ["estimation stage:Method", "Part-of", "approach:Generic"], ["estimation stage:Method", "Conjunction", "classification stage:Method"], ["classification stage:Method", "Part-of", "approach:Generic"], ["classifier:Method", "Used-for", "object poses:OtherScientificTerm"], ["time complexity:Metric", "Evaluate-for", "algorithm:Generic"], ["boosted combination of Random Ferns:Method", "Used-for", "classifier:Method"], ["local histograms of oriented gradients ( HOGs ):OtherScientificTerm", "Feature-of", "boosted combination of Random Ferns:Method"], ["pre-processing step:Method", "Used-for", "local histograms of oriented gradients ( HOGs ):OtherScientificTerm"], ["supervised learning:Method", "Used-for", "approach:Generic"], ["gradient space:OtherScientificTerm", "Used-for", "approach:Generic"], ["motorbikes under planar rotations:Material", "Feature-of", "database:Generic"], ["conditions:Generic", "Feature-of", "database:Generic"], ["cluttered backgrounds:OtherScientificTerm", "Hyponym-of", "conditions:Generic"], ["cluttered backgrounds:OtherScientificTerm", "Conjunction", "changing illumination conditions:OtherScientificTerm"], ["changing illumination conditions:OtherScientificTerm", "Hyponym-of", "conditions:Generic"], ["changing illumination conditions:OtherScientificTerm", "Conjunction", "partial occlusions:OtherScientificTerm"], ["partial occlusions:OtherScientificTerm", "Hyponym-of", "conditions:Generic"]]}
{"doc_id": "H91-1010", "sentence": "The following describes recent work on the Lincoln CSR system . Some new variations in semiphone modeling have been tested . A very simple improved duration model has reduced the error rate by about 10 % in both triphone and semiphone systems . A new training strategy has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related rapid adaptation technique . Finally , the recognizer has been modified to use bigram back-off language models . The system was then transferred from the RM task to the ATIS CSR task and a limited number of development tests performed . Evaluation test results are presented for both the RM and ATIS CSR tasks .", "ner": [["Lincoln CSR system", "Method"], ["semiphone modeling", "Method"], ["duration model", "Method"], ["error rate", "Metric"], ["triphone and semiphone systems", "Method"], ["training strategy", "Method"], ["rapid adaptation technique", "Method"], ["recognizer", "Method"], ["bigram back-off language models", "Method"], ["system", "Generic"], ["RM task", "Task"], ["ATIS CSR task", "Task"], ["RM and ATIS CSR tasks", "Task"]], "rel": [["duration model", "Used-for", "triphone and semiphone systems"], ["error rate", "Evaluate-for", "triphone and semiphone systems"], ["rapid adaptation technique", "Used-for", "training strategy"], ["bigram back-off language models", "Used-for", "recognizer"], ["system", "Used-for", "RM task"], ["system", "Used-for", "ATIS CSR task"], ["RM task", "Conjunction", "ATIS CSR task"]], "rel_plus": [["duration model:Method", "Used-for", "triphone and semiphone systems:Method"], ["error rate:Metric", "Evaluate-for", "triphone and semiphone systems:Method"], ["rapid adaptation technique:Method", "Used-for", "training strategy:Method"], ["bigram back-off language models:Method", "Used-for", "recognizer:Method"], ["system:Generic", "Used-for", "RM task:Task"], ["system:Generic", "Used-for", "ATIS CSR task:Task"], ["RM task:Task", "Conjunction", "ATIS CSR task:Task"]]}
{"doc_id": "C88-2160", "sentence": "A new approach for Interactive Machine Translation where the author interacts during the creation or the modification of the document is proposed . The explanation of an ambiguity or an error for the purposes of correction does not use any concepts of the underlying linguistic theory : it is a reformulation of the erroneous or ambiguous sentence . The interaction is limited to the analysis step of the translation process . This paper presents a new interactive disambiguation scheme based on the paraphrasing of a parser 's multiple output . Some examples of paraphrasing ambiguous sentences are presented .", "ner": [["approach", "Generic"], ["Interactive Machine Translation", "Task"], ["linguistic theory", "Method"], ["translation process", "Method"], ["interactive disambiguation scheme", "Method"], ["paraphrasing", "Method"]], "rel": [["approach", "Used-for", "Interactive Machine Translation"], ["paraphrasing", "Used-for", "interactive disambiguation scheme"]], "rel_plus": [["approach:Generic", "Used-for", "Interactive Machine Translation:Task"], ["paraphrasing:Method", "Used-for", "interactive disambiguation scheme:Method"]]}
{"doc_id": "P05-1034", "sentence": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation . This method requires a source-language dependency parser , target language word segmentation and an unsupervised word alignment component . We align a parallel corpus , project the source dependency parse onto the target sentence , extract dependency treelet translation pairs , and train a tree-based ordering model . We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser .", "ner": [["approach", "Generic"], ["statistical machine translation", "Task"], ["syntactic information", "OtherScientificTerm"], ["phrasal translation", "Task"], ["method", "Generic"], ["source-language dependency parser", "Method"], ["target language word segmentation", "Method"], ["unsupervised word alignment component", "Method"], ["parallel corpus", "Material"], ["source dependency parse", "OtherScientificTerm"], ["dependency treelet translation pairs", "OtherScientificTerm"], ["tree-based ordering model", "Method"], ["decoder", "Method"], ["tree-based models", "Method"], ["SMT models", "Method"], ["approach", "Generic"], ["phrasal SMT", "Method"], ["linguistic generality", "OtherScientificTerm"], ["parser", "Method"]], "rel": [["approach", "Used-for", "statistical machine translation"], ["syntactic information", "Part-of", "approach"], ["syntactic information", "Conjunction", "phrasal translation"], ["phrasal translation", "Part-of", "approach"], ["source-language dependency parser", "Used-for", "method"], ["source-language dependency parser", "Conjunction", "target language word segmentation"], ["target language word segmentation", "Used-for", "method"], ["target language word segmentation", "Conjunction", "unsupervised word alignment component"], ["unsupervised word alignment component", "Used-for", "method"], ["tree-based models", "Conjunction", "SMT models"], ["tree-based models", "Used-for", "approach"], ["SMT models", "Used-for", "approach"], ["phrasal SMT", "Conjunction", "linguistic generality"], ["phrasal SMT", "Used-for", "parser"], ["linguistic generality", "Feature-of", "parser"]], "rel_plus": [["approach:Generic", "Used-for", "statistical machine translation:Task"], ["syntactic information:OtherScientificTerm", "Part-of", "approach:Generic"], ["syntactic information:OtherScientificTerm", "Conjunction", "phrasal translation:Task"], ["phrasal translation:Task", "Part-of", "approach:Generic"], ["source-language dependency parser:Method", "Used-for", "method:Generic"], ["source-language dependency parser:Method", "Conjunction", "target language word segmentation:Method"], ["target language word segmentation:Method", "Used-for", "method:Generic"], ["target language word segmentation:Method", "Conjunction", "unsupervised word alignment component:Method"], ["unsupervised word alignment component:Method", "Used-for", "method:Generic"], ["tree-based models:Method", "Conjunction", "SMT models:Method"], ["tree-based models:Method", "Used-for", "approach:Generic"], ["SMT models:Method", "Used-for", "approach:Generic"], ["phrasal SMT:Method", "Conjunction", "linguistic generality:OtherScientificTerm"], ["phrasal SMT:Method", "Used-for", "parser:Method"], ["linguistic generality:OtherScientificTerm", "Feature-of", "parser:Method"]]}
{"doc_id": "CVPR_2011_292_abs", "sentence": "Video provides not only rich visual cues such as motion and appearance , but also much less explored long-range temporal interactions among objects . We aim to capture such interactions and to construct a powerful intermediate-level video representation for subsequent recognition . Motivated by this goal , we seek to obtain spatio-temporal over-segmentation of a video into regions that respect object boundaries and , at the same time , associate object pix-els over many video frames . The contributions of this paper are twofold . First , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates long-range motion cues from the past and future frames in the form of clusters of point tracks with coherent motion . Second , we devise a new track clustering cost function that includes occlusion reasoning , in the form of depth ordering constraints , as well as motion similarity along the tracks . We evaluate the proposed approach on a challenging set of video sequences of office scenes from feature length movies .", "ner": [["Video", "Material"], ["visual cues", "OtherScientificTerm"], ["motion", "OtherScientificTerm"], ["appearance", "OtherScientificTerm"], ["long-range temporal interactions", "OtherScientificTerm"], ["interactions", "Generic"], ["intermediate-level video representation", "Method"], ["recognition", "Task"], ["spatio-temporal over-segmentation", "OtherScientificTerm"], ["object boundaries", "OtherScientificTerm"], ["spatio-temporal video segmentation algorithm", "Method"], ["long-range motion cues", "OtherScientificTerm"], ["clusters of point tracks", "OtherScientificTerm"], ["track clustering cost function", "Method"], ["occlusion reasoning", "OtherScientificTerm"], ["depth ordering constraints", "OtherScientificTerm"], ["motion similarity", "OtherScientificTerm"], ["approach", "Generic"], ["video sequences of office scenes", "Material"], ["movies", "Material"]], "rel": [["visual cues", "Feature-of", "Video"], ["motion", "Hyponym-of", "visual cues"], ["motion", "Conjunction", "appearance"], ["appearance", "Hyponym-of", "visual cues"], ["intermediate-level video representation", "Used-for", "recognition"], ["long-range motion cues", "Used-for", "spatio-temporal video segmentation algorithm"], ["clusters of point tracks", "Used-for", "long-range motion cues"], ["occlusion reasoning", "Part-of", "track clustering cost function"], ["depth ordering constraints", "Feature-of", "occlusion reasoning"], ["motion similarity", "Part-of", "track clustering cost function"], ["video sequences of office scenes", "Evaluate-for", "approach"]], "rel_plus": [["visual cues:OtherScientificTerm", "Feature-of", "Video:Material"], ["motion:OtherScientificTerm", "Hyponym-of", "visual cues:OtherScientificTerm"], ["motion:OtherScientificTerm", "Conjunction", "appearance:OtherScientificTerm"], ["appearance:OtherScientificTerm", "Hyponym-of", "visual cues:OtherScientificTerm"], ["intermediate-level video representation:Method", "Used-for", "recognition:Task"], ["long-range motion cues:OtherScientificTerm", "Used-for", "spatio-temporal video segmentation algorithm:Method"], ["clusters of point tracks:OtherScientificTerm", "Used-for", "long-range motion cues:OtherScientificTerm"], ["occlusion reasoning:OtherScientificTerm", "Part-of", "track clustering cost function:Method"], ["depth ordering constraints:OtherScientificTerm", "Feature-of", "occlusion reasoning:OtherScientificTerm"], ["motion similarity:OtherScientificTerm", "Part-of", "track clustering cost function:Method"], ["video sequences of office scenes:Material", "Evaluate-for", "approach:Generic"]]}
{"doc_id": "ECCV_2012_40_abs", "sentence": "In this paper , we introduce KAZE features , a novel multiscale 2D feature detection and description algorithm in nonlinear scale spaces . Previous approaches detect and describe features at different scale levels by building or approximating the Gaussian scale space of an image . However , Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same degree both details and noise , reducing localization accuracy and distinctiveness . In contrast , we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering . In this way , we can make blurring locally adaptive to the image data , reducing noise but retaining object boundaries , obtaining superior localization accuracy and distinctiviness . The nonlinear scale space is built using efficient Additive Operator Splitting ( AOS ) techniques and variable con-ductance diffusion . We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces . Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods .", "ner": [["KAZE features", "Method"], ["multiscale 2D feature detection and description algorithm", "Method"], ["nonlinear scale spaces", "OtherScientificTerm"], ["Gaussian scale space", "OtherScientificTerm"], ["Gaussian blurring", "OtherScientificTerm"], ["boundaries of objects", "OtherScientificTerm"], ["localization accuracy and distinctiveness", "Metric"], ["2D features", "Method"], ["nonlinear scale space", "OtherScientificTerm"], ["nonlinear diffusion filtering", "Method"], ["image data", "Material"], ["object boundaries", "OtherScientificTerm"], ["localization accuracy and distinctiviness", "Metric"], ["nonlinear scale space", "OtherScientificTerm"], ["Additive Operator Splitting ( AOS ) techniques", "Method"], ["variable con-ductance diffusion", "Method"], ["benchmark datasets", "Material"], ["matching application on deformable surfaces", "Task"], ["features", "Generic"], ["SURF", "Method"], ["nonlinear scale space", "OtherScientificTerm"], ["SIFT", "Method"], ["results", "Generic"], ["detection", "Task"], ["description", "Task"], ["state-of-the-art methods", "Generic"]], "rel": [["KAZE features", "Hyponym-of", "multiscale 2D feature detection and description algorithm"], ["nonlinear scale spaces", "Feature-of", "multiscale 2D feature detection and description algorithm"], ["nonlinear scale space", "Feature-of", "2D features"], ["nonlinear diffusion filtering", "Used-for", "2D features"], ["Additive Operator Splitting ( AOS ) techniques", "Used-for", "nonlinear scale space"], ["Additive Operator Splitting ( AOS ) techniques", "Conjunction", "variable con-ductance diffusion"], ["variable con-ductance diffusion", "Used-for", "nonlinear scale space"], ["features", "Compare", "SURF"], ["features", "Compare", "SIFT"], ["results", "Compare", "state-of-the-art methods"], ["detection", "Evaluate-for", "results"], ["detection", "Conjunction", "description"], ["detection", "Evaluate-for", "state-of-the-art methods"], ["description", "Evaluate-for", "results"], ["description", "Evaluate-for", "state-of-the-art methods"]], "rel_plus": [["KAZE features:Method", "Hyponym-of", "multiscale 2D feature detection and description algorithm:Method"], ["nonlinear scale spaces:OtherScientificTerm", "Feature-of", "multiscale 2D feature detection and description algorithm:Method"], ["nonlinear scale space:OtherScientificTerm", "Feature-of", "2D features:Method"], ["nonlinear diffusion filtering:Method", "Used-for", "2D features:Method"], ["Additive Operator Splitting ( AOS ) techniques:Method", "Used-for", "nonlinear scale space:OtherScientificTerm"], ["Additive Operator Splitting ( AOS ) techniques:Method", "Conjunction", "variable con-ductance diffusion:Method"], ["variable con-ductance diffusion:Method", "Used-for", "nonlinear scale space:OtherScientificTerm"], ["features:Generic", "Compare", "SURF:Method"], ["features:Generic", "Compare", "SIFT:Method"], ["results:Generic", "Compare", "state-of-the-art methods:Generic"], ["detection:Task", "Evaluate-for", "results:Generic"], ["detection:Task", "Conjunction", "description:Task"], ["detection:Task", "Evaluate-for", "state-of-the-art methods:Generic"], ["description:Task", "Evaluate-for", "results:Generic"], ["description:Task", "Evaluate-for", "state-of-the-art methods:Generic"]]}
{"doc_id": "AAAI_2015_21_abs", "sentence": "Semantic Web documents that encode facts about entities on the Web have been growing rapidly in size and evolving over time . Creating summaries on lengthy Semantic Web documents for quick identification of the corresponding entity has been of great contemporary interest . In this paper , we explore automatic summa-rization techniques that characterize and enable identification of an entity and create summaries that are human friendly . Specifically , we highlight the importance of diversified ( faceted ) summaries by combining three dimensions : diversity , uniqueness , and popularity . Our novel diversity-aware entity summarization approach mimics human conceptual clustering techniques to group facts , and picks representative facts from each group to form concise ( i.e. , short ) and comprehensive ( i.e. , improved coverage through diversity ) summaries . We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of entity summarization .", "ner": [["Semantic Web documents", "Material"], ["Creating summaries", "Task"], ["lengthy Semantic Web documents", "Material"], ["identification of the corresponding entity", "Task"], ["automatic summa-rization techniques", "Method"], ["diversified ( faceted ) summaries", "OtherScientificTerm"], ["diversity", "OtherScientificTerm"], ["uniqueness", "OtherScientificTerm"], ["popularity", "OtherScientificTerm"], ["diversity-aware entity summarization approach", "Method"], ["human conceptual clustering techniques", "Method"], ["approach", "Generic"], ["state-of-the-art techniques", "Generic"], ["quality", "Metric"], ["efficiency", "Metric"], ["entity summarization", "Task"]], "rel": [["Creating summaries", "Used-for", "identification of the corresponding entity"], ["lengthy Semantic Web documents", "Used-for", "Creating summaries"], ["diversity", "Feature-of", "diversified ( faceted ) summaries"], ["diversity", "Conjunction", "uniqueness"], ["uniqueness", "Feature-of", "diversified ( faceted ) summaries"], ["uniqueness", "Conjunction", "popularity"], ["popularity", "Feature-of", "diversified ( faceted ) summaries"], ["human conceptual clustering techniques", "Used-for", "diversity-aware entity summarization approach"], ["approach", "Used-for", "entity summarization"], ["state-of-the-art techniques", "Compare", "approach"], ["state-of-the-art techniques", "Used-for", "entity summarization"], ["quality", "Evaluate-for", "entity summarization"], ["efficiency", "Evaluate-for", "entity summarization"]], "rel_plus": [["Creating summaries:Task", "Used-for", "identification of the corresponding entity:Task"], ["lengthy Semantic Web documents:Material", "Used-for", "Creating summaries:Task"], ["diversity:OtherScientificTerm", "Feature-of", "diversified ( faceted ) summaries:OtherScientificTerm"], ["diversity:OtherScientificTerm", "Conjunction", "uniqueness:OtherScientificTerm"], ["uniqueness:OtherScientificTerm", "Feature-of", "diversified ( faceted ) summaries:OtherScientificTerm"], ["uniqueness:OtherScientificTerm", "Conjunction", "popularity:OtherScientificTerm"], ["popularity:OtherScientificTerm", "Feature-of", "diversified ( faceted ) summaries:OtherScientificTerm"], ["human conceptual clustering techniques:Method", "Used-for", "diversity-aware entity summarization approach:Method"], ["approach:Generic", "Used-for", "entity summarization:Task"], ["state-of-the-art techniques:Generic", "Compare", "approach:Generic"], ["state-of-the-art techniques:Generic", "Used-for", "entity summarization:Task"], ["quality:Metric", "Evaluate-for", "entity summarization:Task"], ["efficiency:Metric", "Evaluate-for", "entity summarization:Task"]]}
{"doc_id": "C04-1147", "sentence": "We present a framework for the fast computation of lexical affinity models . The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model . In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus . The framework is flexible , allowing fast adaptation to applications and it is scalable . We apply it in combination with a terabyte corpus to answer natural language tests , achieving encouraging results .", "ner": [["framework", "Generic"], ["fast computation of lexical affinity models", "Task"], ["framework", "Generic"], ["algorithm", "Generic"], ["co-occurrence distribution", "OtherScientificTerm"], ["independence model", "Method"], ["parametric affinity model", "Method"], ["models", "Generic"], ["similarity", "OtherScientificTerm"], ["lexical affinity", "OtherScientificTerm"], ["sequential models", "Method"], ["models", "Generic"], ["co-occurrence patterns", "OtherScientificTerm"], ["framework", "Generic"], ["it", "Generic"], ["terabyte corpus", "Material"], ["natural language tests", "Task"]], "rel": [["framework", "Used-for", "fast computation of lexical affinity models"], ["algorithm", "Part-of", "framework"], ["algorithm", "Used-for", "co-occurrence distribution"], ["algorithm", "Conjunction", "independence model"], ["independence model", "Part-of", "framework"], ["independence model", "Conjunction", "parametric affinity model"], ["parametric affinity model", "Part-of", "framework"], ["lexical affinity", "Used-for", "sequential models"], ["models", "Compare", "models"], ["models", "Used-for", "co-occurrence patterns"], ["it", "Used-for", "natural language tests"], ["terabyte corpus", "Evaluate-for", "it"]], "rel_plus": [["framework:Generic", "Used-for", "fast computation of lexical affinity models:Task"], ["algorithm:Generic", "Part-of", "framework:Generic"], ["algorithm:Generic", "Used-for", "co-occurrence distribution:OtherScientificTerm"], ["algorithm:Generic", "Conjunction", "independence model:Method"], ["independence model:Method", "Part-of", "framework:Generic"], ["independence model:Method", "Conjunction", "parametric affinity model:Method"], ["parametric affinity model:Method", "Part-of", "framework:Generic"], ["lexical affinity:OtherScientificTerm", "Used-for", "sequential models:Method"], ["models:Generic", "Compare", "models:Generic"], ["models:Generic", "Used-for", "co-occurrence patterns:OtherScientificTerm"], ["it:Generic", "Used-for", "natural language tests:Task"], ["terabyte corpus:Material", "Evaluate-for", "it:Generic"]]}
{"doc_id": "A00-1024", "sentence": "This paper introduces a system for categorizing unknown words . The system is based on a multi-component architecture where each component is responsible for identifying one class of unknown words . The focus of this paper is the components that identify names and spelling errors . Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word . The system is evaluated using data from live closed captions - a genre replete with a wide variety of unknown words .", "ner": [["system", "Generic"], ["categorizing unknown words", "Task"], ["system", "Generic"], ["multi-component architecture", "Method"], ["component", "Generic"], ["unknown words", "OtherScientificTerm"], ["components", "Generic"], ["names", "OtherScientificTerm"], ["spelling errors", "OtherScientificTerm"], ["component", "Generic"], ["decision tree architecture", "Method"], ["unknown word", "OtherScientificTerm"], ["system", "Generic"], ["live closed captions", "Material"], ["unknown words", "OtherScientificTerm"]], "rel": [["system", "Used-for", "categorizing unknown words"], ["multi-component architecture", "Used-for", "system"], ["component", "Part-of", "multi-component architecture"], ["component", "Used-for", "unknown words"], ["components", "Used-for", "names"], ["components", "Used-for", "spelling errors"], ["names", "Conjunction", "spelling errors"], ["decision tree architecture", "Used-for", "component"], ["live closed captions", "Evaluate-for", "system"]], "rel_plus": [["system:Generic", "Used-for", "categorizing unknown words:Task"], ["multi-component architecture:Method", "Used-for", "system:Generic"], ["component:Generic", "Part-of", "multi-component architecture:Method"], ["component:Generic", "Used-for", "unknown words:OtherScientificTerm"], ["components:Generic", "Used-for", "names:OtherScientificTerm"], ["components:Generic", "Used-for", "spelling errors:OtherScientificTerm"], ["names:OtherScientificTerm", "Conjunction", "spelling errors:OtherScientificTerm"], ["decision tree architecture:Method", "Used-for", "component:Generic"], ["live closed captions:Material", "Evaluate-for", "system:Generic"]]}
{"doc_id": "H01-1041", "sentence": "At MIT Lincoln Laboratory , we have been developing a Korean-to-English machine translation system CCLINC ( Common Coalition Language System at Lincoln Laboratory ) . The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame . The key features of the system include : ( i ) Robust efficient parsing of Korean ( a verb final language with overt case markers , relatively free word order , and frequent omissions of arguments ) . ( ii ) High quality translation via word sense disambiguation and accurate word order generation of the target language . ( iii ) Rapid system development and porting to new domains via knowledge-based automated acquisition of grammars . Having been trained on Korean newspaper articles on missiles and chemical biological warfare , the system produces the translation output sufficient for content understanding of the original document .", "ner": [["Korean-to-English machine translation system", "Method"], ["CCLINC ( Common Coalition Language System at Lincoln Laboratory )", "Method"], ["CCLINC Korean-to-English translation system", "Method"], ["core modules", "Generic"], ["language understanding and generation modules", "Method"], ["language neutral meaning representation", "Method"], ["semantic frame", "OtherScientificTerm"], ["system", "Generic"], ["parsing of Korean", "Task"], ["Korean", "Material"], ["verb final language", "Material"], ["overt case markers", "OtherScientificTerm"], ["translation", "Task"], ["word sense disambiguation", "Method"], ["word order generation", "Task"], ["knowledge-based automated acquisition of grammars", "Method"], ["Korean newspaper articles", "Material"], ["missiles and chemical biological warfare", "Material"], ["system", "Generic"]], "rel": [["CCLINC ( Common Coalition Language System at Lincoln Laboratory )", "Hyponym-of", "Korean-to-English machine translation system"], ["core modules", "Part-of", "CCLINC Korean-to-English translation system"], ["language neutral meaning representation", "Used-for", "language understanding and generation modules"], ["semantic frame", "Hyponym-of", "language neutral meaning representation"], ["Korean", "Hyponym-of", "verb final language"], ["overt case markers", "Feature-of", "verb final language"], ["word sense disambiguation", "Used-for", "translation"], ["word sense disambiguation", "Conjunction", "word order generation"], ["word order generation", "Used-for", "translation"], ["Korean newspaper articles", "Used-for", "system"], ["missiles and chemical biological warfare", "Feature-of", "Korean newspaper articles"]], "rel_plus": [["CCLINC ( Common Coalition Language System at Lincoln Laboratory ):Method", "Hyponym-of", "Korean-to-English machine translation system:Method"], ["core modules:Generic", "Part-of", "CCLINC Korean-to-English translation system:Method"], ["language neutral meaning representation:Method", "Used-for", "language understanding and generation modules:Method"], ["semantic frame:OtherScientificTerm", "Hyponym-of", "language neutral meaning representation:Method"], ["Korean:Material", "Hyponym-of", "verb final language:Material"], ["overt case markers:OtherScientificTerm", "Feature-of", "verb final language:Material"], ["word sense disambiguation:Method", "Used-for", "translation:Task"], ["word sense disambiguation:Method", "Conjunction", "word order generation:Task"], ["word order generation:Task", "Used-for", "translation:Task"], ["Korean newspaper articles:Material", "Used-for", "system:Generic"], ["missiles and chemical biological warfare:Material", "Feature-of", "Korean newspaper articles:Material"]]}
{"doc_id": "N03-4010", "sentence": "The JAVELIN system integrates a flexible , planning-based architecture with a variety of language processing modules to provide an open-domain question answering capability on free text . The demonstration will focus on how JAVELIN processes questions and retrieves the most likely answer candidates from the given text corpus . The operation of the system will be explained in depth through browsing the repository of data objects created by the system during each question answering session .", "ner": [["JAVELIN system", "Method"], ["planning-based architecture", "Method"], ["language processing modules", "Method"], ["open-domain question answering capability", "Task"], ["JAVELIN", "Method"], ["system", "Generic"], ["system", "Generic"], ["question answering", "Task"]], "rel": [["JAVELIN system", "Used-for", "open-domain question answering capability"], ["planning-based architecture", "Part-of", "JAVELIN system"], ["language processing modules", "Part-of", "JAVELIN system"], ["language processing modules", "Conjunction", "planning-based architecture"]], "rel_plus": [["JAVELIN system:Method", "Used-for", "open-domain question answering capability:Task"], ["planning-based architecture:Method", "Part-of", "JAVELIN system:Method"], ["language processing modules:Method", "Part-of", "JAVELIN system:Method"], ["language processing modules:Method", "Conjunction", "planning-based architecture:Method"]]}
{"doc_id": "P04-1030", "sentence": "We present the first application of the head-driven statistical parsing model of Collins ( 1999 ) as a simultaneous language model and parser for large-vocabulary speech recognition . The model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities . The parser uses structural and lexical dependencies not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships . Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding .", "ner": [["head-driven statistical parsing model", "Method"], ["simultaneous language model", "Method"], ["parser", "Method"], ["large-vocabulary speech recognition", "Task"], ["model", "Generic"], ["online left to right chart-parser", "Method"], ["word lattices", "OtherScientificTerm"], ["acoustic , n-gram , and parser probabilities", "OtherScientificTerm"], ["parser", "Method"], ["structural and lexical dependencies", "OtherScientificTerm"], ["n-gram models", "Method"], ["Wall Street Journal treebank", "Material"], ["lattice corpora", "Material"], ["word error rates", "Metric"], ["n-gram language model", "Method"], ["structural information", "OtherScientificTerm"], ["speech understanding", "Task"]], "rel": [["head-driven statistical parsing model", "Used-for", "simultaneous language model"], ["head-driven statistical parsing model", "Used-for", "parser"], ["simultaneous language model", "Conjunction", "parser"], ["simultaneous language model", "Used-for", "large-vocabulary speech recognition"], ["parser", "Used-for", "large-vocabulary speech recognition"], ["model", "Used-for", "online left to right chart-parser"], ["online left to right chart-parser", "Used-for", "word lattices"], ["acoustic , n-gram , and parser probabilities", "Part-of", "online left to right chart-parser"], ["structural and lexical dependencies", "Used-for", "parser"], ["Wall Street Journal treebank", "Conjunction", "lattice corpora"], ["Wall Street Journal treebank", "Evaluate-for", "n-gram language model"], ["lattice corpora", "Evaluate-for", "n-gram language model"], ["word error rates", "Evaluate-for", "n-gram language model"], ["structural information", "Used-for", "speech understanding"]], "rel_plus": [["head-driven statistical parsing model:Method", "Used-for", "simultaneous language model:Method"], ["head-driven statistical parsing model:Method", "Used-for", "parser:Method"], ["simultaneous language model:Method", "Conjunction", "parser:Method"], ["simultaneous language model:Method", "Used-for", "large-vocabulary speech recognition:Task"], ["parser:Method", "Used-for", "large-vocabulary speech recognition:Task"], ["model:Generic", "Used-for", "online left to right chart-parser:Method"], ["online left to right chart-parser:Method", "Used-for", "word lattices:OtherScientificTerm"], ["acoustic , n-gram , and parser probabilities:OtherScientificTerm", "Part-of", "online left to right chart-parser:Method"], ["structural and lexical dependencies:OtherScientificTerm", "Used-for", "parser:Method"], ["Wall Street Journal treebank:Material", "Conjunction", "lattice corpora:Material"], ["Wall Street Journal treebank:Material", "Evaluate-for", "n-gram language model:Method"], ["lattice corpora:Material", "Evaluate-for", "n-gram language model:Method"], ["word error rates:Metric", "Evaluate-for", "n-gram language model:Method"], ["structural information:OtherScientificTerm", "Used-for", "speech understanding:Task"]]}
{"doc_id": "CVPR_2004_10_abs", "sentence": "Image composition ( or mosaicing ) has attracted a growing attention in recent years as one of the main elements in video analysis and representation . In this paper we deal with the problem of global alignment and super-resolution . We also propose to evaluate the quality of the resulting mosaic by measuring the amount of blurring . Global registration is achieved by combining a graph-based technique-that exploits the topological structure of the sequence induced by the spatial overlap-with a bundle adjustment which uses only the homographies computed in the previous steps . Experimental comparison with other techniques shows the effectiveness of our approach .", "ner": [["Image composition ( or mosaicing )", "Task"], ["video analysis and representation", "Task"], ["global alignment", "Task"], ["super-resolution", "Task"], ["mosaic", "Task"], ["amount of blurring", "Metric"], ["Global registration", "Task"], ["graph-based technique", "Method"], ["topological structure", "OtherScientificTerm"], ["spatial overlap", "OtherScientificTerm"], ["bundle adjustment", "Method"], ["homographies", "OtherScientificTerm"], ["techniques", "Generic"], ["approach", "Generic"]], "rel": [["Image composition ( or mosaicing )", "Part-of", "video analysis and representation"], ["global alignment", "Conjunction", "super-resolution"], ["amount of blurring", "Evaluate-for", "mosaic"], ["graph-based technique", "Used-for", "Global registration"], ["graph-based technique", "Used-for", "topological structure"], ["graph-based technique", "Conjunction", "bundle adjustment"], ["bundle adjustment", "Used-for", "Global registration"], ["homographies", "Used-for", "bundle adjustment"], ["approach", "Compare", "techniques"]], "rel_plus": [["Image composition ( or mosaicing ):Task", "Part-of", "video analysis and representation:Task"], ["global alignment:Task", "Conjunction", "super-resolution:Task"], ["amount of blurring:Metric", "Evaluate-for", "mosaic:Task"], ["graph-based technique:Method", "Used-for", "Global registration:Task"], ["graph-based technique:Method", "Used-for", "topological structure:OtherScientificTerm"], ["graph-based technique:Method", "Conjunction", "bundle adjustment:Method"], ["bundle adjustment:Method", "Used-for", "Global registration:Task"], ["homographies:OtherScientificTerm", "Used-for", "bundle adjustment:Method"], ["approach:Generic", "Compare", "techniques:Generic"]]}
{"doc_id": "L08-1260", "sentence": "The project presented here is a part of a long term research program aiming at a full lexicon grammar for Polish ( SyntLex ) . The main of this project is computer-assisted acquisition and morpho-syntactic description of verb-noun collocations in Polish . We present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description . In this paper we focus on the results of the third phase . The presented here corpus-based approach permitted us to triple the size the verb-noun collocation dictionary for Polish . In the paper we describe the SyntLex Dictionary of Collocations and announce some future research intended to be a separate project continuation .", "ner": [["lexicon grammar for Polish ( SyntLex )", "Method"], ["computer-assisted acquisition and morpho-syntactic description of verb-noun collocations", "Task"], ["Polish", "Material"], ["phases", "Generic"], ["dictionary-based acquisition of collocation lexicon", "Task"], ["feasibility study", "Task"], ["corpus-based lexicon enlargement phase", "Task"], ["corpus-based lexicon enlargement and collocation description", "Task"], ["corpus-based approach", "Method"], ["verb-noun collocation dictionary", "Material"], ["Polish", "Material"], ["SyntLex Dictionary of Collocations", "Material"]], "rel": [["Polish", "Used-for", "computer-assisted acquisition and morpho-syntactic description of verb-noun collocations"], ["dictionary-based acquisition of collocation lexicon", "Hyponym-of", "phases"], ["dictionary-based acquisition of collocation lexicon", "Conjunction", "feasibility study"], ["feasibility study", "Hyponym-of", "phases"], ["feasibility study", "Used-for", "corpus-based lexicon enlargement phase"], ["corpus-based lexicon enlargement and collocation description", "Hyponym-of", "phases"], ["corpus-based lexicon enlargement and collocation description", "Conjunction", "feasibility study"], ["corpus-based approach", "Used-for", "verb-noun collocation dictionary"], ["Polish", "Feature-of", "verb-noun collocation dictionary"]], "rel_plus": [["Polish:Material", "Used-for", "computer-assisted acquisition and morpho-syntactic description of verb-noun collocations:Task"], ["dictionary-based acquisition of collocation lexicon:Task", "Hyponym-of", "phases:Generic"], ["dictionary-based acquisition of collocation lexicon:Task", "Conjunction", "feasibility study:Task"], ["feasibility study:Task", "Hyponym-of", "phases:Generic"], ["feasibility study:Task", "Used-for", "corpus-based lexicon enlargement phase:Task"], ["corpus-based lexicon enlargement and collocation description:Task", "Hyponym-of", "phases:Generic"], ["corpus-based lexicon enlargement and collocation description:Task", "Conjunction", "feasibility study:Task"], ["corpus-based approach:Method", "Used-for", "verb-noun collocation dictionary:Material"], ["Polish:Material", "Feature-of", "verb-noun collocation dictionary:Material"]]}
{"doc_id": "IJCAI_2016_412_abs", "sentence": "Along with the increasing requirements , the hash-tag recommendation task for microblogs has been receiving considerable attention in recent years . Various researchers have studied the problem from different aspects . However , most of these methods usually need handcrafted features . Motivated by the successful use of convolutional neural networks ( CNNs ) for many natural language processing tasks , in this paper , we adopt CNNs to perform the hashtag recommendation problem . To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel architecture with an attention mechanism . The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed model outperforms state-of-the-art methods . By incorporating trigger words into the consideration , the relative improvement of the proposed method over the state-of-the-art method is around 9.4 % in the F1-score .", "ner": [["hash-tag recommendation task", "Task"], ["microblogs", "Material"], ["handcrafted features", "OtherScientificTerm"], ["convolutional neural networks ( CNNs )", "Method"], ["natural language processing tasks", "Task"], ["CNNs", "Method"], ["hashtag recommendation problem", "Task"], ["trigger words", "OtherScientificTerm"], ["architecture", "Generic"], ["attention mechanism", "OtherScientificTerm"], ["data", "Generic"], ["model", "Generic"], ["state-of-the-art methods", "Generic"], ["trigger words", "OtherScientificTerm"], ["method", "Generic"], ["state-of-the-art method", "Generic"], ["F1-score", "Metric"]], "rel": [["hash-tag recommendation task", "Used-for", "microblogs"], ["convolutional neural networks ( CNNs )", "Used-for", "natural language processing tasks"], ["architecture", "Used-for", "trigger words"], ["attention mechanism", "Feature-of", "architecture"], ["data", "Evaluate-for", "model"], ["model", "Compare", "state-of-the-art methods"], ["method", "Compare", "state-of-the-art method"], ["F1-score", "Evaluate-for", "state-of-the-art method"]], "rel_plus": [["hash-tag recommendation task:Task", "Used-for", "microblogs:Material"], ["convolutional neural networks ( CNNs ):Method", "Used-for", "natural language processing tasks:Task"], ["architecture:Generic", "Used-for", "trigger words:OtherScientificTerm"], ["attention mechanism:OtherScientificTerm", "Feature-of", "architecture:Generic"], ["data:Generic", "Evaluate-for", "model:Generic"], ["model:Generic", "Compare", "state-of-the-art methods:Generic"], ["method:Generic", "Compare", "state-of-the-art method:Generic"], ["F1-score:Metric", "Evaluate-for", "state-of-the-art method:Generic"]]}
{"doc_id": "W03-0406", "sentence": "In this paper , we improve an unsupervised learning method using the Expectation-Maximization ( EM ) algorithm proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation ( WSD ) problems . The improved method stops the EM algorithm at the optimum iteration number . To estimate that number , we propose two methods . In experiments , we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2 . The score of our method is a match for the best public score of this task . Furthermore , our methods were confirmed to be effective also for verb WSD problems .", "ner": [["unsupervised learning method", "Method"], ["Expectation-Maximization ( EM ) algorithm", "Method"], ["text classification problems", "Task"], ["it", "Generic"], ["word sense disambiguation ( WSD ) problems", "Task"], ["method", "Generic"], ["EM algorithm", "Method"], ["optimum iteration number", "OtherScientificTerm"], ["number", "Generic"], ["noun WSD problems", "Task"], ["Japanese Dictionary Task", "Task"], ["SENSEVAL2", "Material"], ["method", "Generic"], ["task", "Generic"], ["methods", "Generic"], ["verb WSD problems", "Task"]], "rel": [["Expectation-Maximization ( EM ) algorithm", "Used-for", "unsupervised learning method"], ["Expectation-Maximization ( EM ) algorithm", "Used-for", "text classification problems"], ["it", "Used-for", "word sense disambiguation ( WSD ) problems"], ["Japanese Dictionary Task", "Feature-of", "SENSEVAL2"], ["methods", "Used-for", "verb WSD problems"]], "rel_plus": [["Expectation-Maximization ( EM ) algorithm:Method", "Used-for", "unsupervised learning method:Method"], ["Expectation-Maximization ( EM ) algorithm:Method", "Used-for", "text classification problems:Task"], ["it:Generic", "Used-for", "word sense disambiguation ( WSD ) problems:Task"], ["Japanese Dictionary Task:Task", "Feature-of", "SENSEVAL2:Material"], ["methods:Generic", "Used-for", "verb WSD problems:Task"]]}
{"doc_id": "E99-1023", "sentence": "Dividing sentences in chunks of words is a useful preprocessing step for parsing , information extraction and information retrieval . ( Ramshaw and Marcus , 1995 ) have introduced a `` convenient '' data representation for chunking by converting it to a tagging task . In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks . We will show that the data representation choice has a minor influence on chunking performance . However , equipped with the most suitable data representation , our memory-based learning chunker was able to improve the best published chunking results for a standard data set .", "ner": [["Dividing sentences in chunks of words", "Task"], ["parsing", "Task"], ["information extraction", "Task"], ["information retrieval", "Task"], ["data representation", "Method"], ["chunking", "Task"], ["it", "Generic"], ["tagging task", "Task"], ["data representations", "Method"], ["recognizing noun phrase chunks", "Task"], ["data representation", "Method"], ["chunking", "Task"], ["data representation", "Method"], ["memory-based learning chunker", "Method"], ["chunking", "Task"], ["data set", "Material"]], "rel": [["Dividing sentences in chunks of words", "Used-for", "parsing"], ["Dividing sentences in chunks of words", "Used-for", "information extraction"], ["Dividing sentences in chunks of words", "Used-for", "information retrieval"], ["parsing", "Conjunction", "information extraction"], ["information extraction", "Conjunction", "information retrieval"], ["data representation", "Used-for", "chunking"], ["data representations", "Used-for", "recognizing noun phrase chunks"], ["data representation", "Used-for", "memory-based learning chunker"], ["data set", "Evaluate-for", "memory-based learning chunker"]], "rel_plus": [["Dividing sentences in chunks of words:Task", "Used-for", "parsing:Task"], ["Dividing sentences in chunks of words:Task", "Used-for", "information extraction:Task"], ["Dividing sentences in chunks of words:Task", "Used-for", "information retrieval:Task"], ["parsing:Task", "Conjunction", "information extraction:Task"], ["information extraction:Task", "Conjunction", "information retrieval:Task"], ["data representation:Method", "Used-for", "chunking:Task"], ["data representations:Method", "Used-for", "recognizing noun phrase chunks:Task"], ["data representation:Method", "Used-for", "memory-based learning chunker:Method"], ["data set:Material", "Evaluate-for", "memory-based learning chunker:Method"]]}
{"doc_id": "N04-1008", "sentence": "In this paper we describe and evaluate a Question Answering system that goes beyond answering factoid questions . We focus on FAQ-like questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web .", "ner": [["Question Answering system", "Method"], ["FAQ-like questions and answers", "Material"], ["system", "Generic"], ["noisy-channel architecture", "Method"], ["language model", "Method"], ["transformation model", "Method"], ["Web", "Material"]], "rel": [["system", "Used-for", "FAQ-like questions and answers"], ["noisy-channel architecture", "Used-for", "system"], ["noisy-channel architecture", "Used-for", "language model"], ["noisy-channel architecture", "Used-for", "transformation model"]], "rel_plus": [["system:Generic", "Used-for", "FAQ-like questions and answers:Material"], ["noisy-channel architecture:Method", "Used-for", "system:Generic"], ["noisy-channel architecture:Method", "Used-for", "language model:Method"], ["noisy-channel architecture:Method", "Used-for", "transformation model:Method"]]}
{"doc_id": "ICASSP_2011_598_abs", "sentence": "In this paper we evaluate four objective measures of speech with regards to intelligibility prediction of synthesized speech in diverse noisy situations . We evaluated three intel-ligibility measures , the Dau measure , the glimpse proportion and the Speech Intelligibility Index ( SII ) and a quality measure , the Perceptual Evaluation of Speech Quality ( PESQ ) . For the generation of synthesized speech we used a state of the art HMM-based speech synthesis system . The noisy conditions comprised four additive noises . The measures were compared with subjective intelligibility scores obtained in listening tests . The results show the Dau and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores . All measures gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech ; in particular the SII measure . In additional experiments , we processed the synthesized speech by an ideal binary mask before adding noise . The Glimpse measure gave the most accurate intelligibility predictions in this situation .", "ner": [["measures of speech", "Metric"], ["intelligibility prediction", "Task"], ["synthesized speech", "Material"], ["diverse noisy situations", "OtherScientificTerm"], ["intel-ligibility measures", "Metric"], ["Dau measure", "Metric"], ["glimpse proportion", "Metric"], ["Speech Intelligibility Index ( SII )", "Metric"], ["quality measure", "Metric"], ["Perceptual Evaluation of Speech Quality ( PESQ )", "Metric"], ["generation of synthesized speech", "Task"], ["HMM-based speech synthesis system", "Method"], ["noisy conditions", "OtherScientificTerm"], ["additive noises", "OtherScientificTerm"], ["measures", "Generic"], ["subjective intelligibility scores", "Metric"], ["Dau", "Metric"], ["glimpse measures", "Metric"], ["predictors of intelligibility", "Method"], ["correlations", "Metric"], ["subjective scores", "OtherScientificTerm"], ["measures", "Generic"], ["predictions of intelligibility", "Task"], ["synthetic speech", "Material"], ["natural speech", "Material"], ["SII measure", "Metric"], ["synthesized speech", "Material"], ["ideal binary mask", "Method"], ["Glimpse measure", "Metric"], ["intelligibility predictions", "Task"]], "rel": [["measures of speech", "Evaluate-for", "intelligibility prediction"], ["synthesized speech", "Used-for", "intelligibility prediction"], ["diverse noisy situations", "Feature-of", "synthesized speech"], ["intel-ligibility measures", "Conjunction", "quality measure"], ["Dau measure", "Hyponym-of", "intel-ligibility measures"], ["Dau measure", "Conjunction", "glimpse proportion"], ["glimpse proportion", "Hyponym-of", "intel-ligibility measures"], ["glimpse proportion", "Conjunction", "Speech Intelligibility Index ( SII )"], ["Speech Intelligibility Index ( SII )", "Hyponym-of", "intel-ligibility measures"], ["Perceptual Evaluation of Speech Quality ( PESQ )", "Hyponym-of", "quality measure"], ["HMM-based speech synthesis system", "Used-for", "generation of synthesized speech"], ["additive noises", "Part-of", "noisy conditions"], ["measures", "Compare", "subjective intelligibility scores"], ["Dau", "Conjunction", "glimpse measures"], ["Dau", "Hyponym-of", "predictors of intelligibility"], ["Dau", "Compare", "subjective scores"], ["glimpse measures", "Hyponym-of", "predictors of intelligibility"], ["glimpse measures", "Compare", "subjective scores"], ["correlations", "Evaluate-for", "Dau"], ["correlations", "Evaluate-for", "glimpse measures"], ["measures", "Evaluate-for", "predictions of intelligibility"], ["synthetic speech", "Used-for", "predictions of intelligibility"], ["synthetic speech", "Compare", "natural speech"], ["SII measure", "Hyponym-of", "measures"], ["ideal binary mask", "Used-for", "synthesized speech"], ["Glimpse measure", "Used-for", "intelligibility predictions"]], "rel_plus": [["measures of speech:Metric", "Evaluate-for", "intelligibility prediction:Task"], ["synthesized speech:Material", "Used-for", "intelligibility prediction:Task"], ["diverse noisy situations:OtherScientificTerm", "Feature-of", "synthesized speech:Material"], ["intel-ligibility measures:Metric", "Conjunction", "quality measure:Metric"], ["Dau measure:Metric", "Hyponym-of", "intel-ligibility measures:Metric"], ["Dau measure:Metric", "Conjunction", "glimpse proportion:Metric"], ["glimpse proportion:Metric", "Hyponym-of", "intel-ligibility measures:Metric"], ["glimpse proportion:Metric", "Conjunction", "Speech Intelligibility Index ( SII ):Metric"], ["Speech Intelligibility Index ( SII ):Metric", "Hyponym-of", "intel-ligibility measures:Metric"], ["Perceptual Evaluation of Speech Quality ( PESQ ):Metric", "Hyponym-of", "quality measure:Metric"], ["HMM-based speech synthesis system:Method", "Used-for", "generation of synthesized speech:Task"], ["additive noises:OtherScientificTerm", "Part-of", "noisy conditions:OtherScientificTerm"], ["measures:Generic", "Compare", "subjective intelligibility scores:Metric"], ["Dau:Metric", "Conjunction", "glimpse measures:Metric"], ["Dau:Metric", "Hyponym-of", "predictors of intelligibility:Method"], ["Dau:Metric", "Compare", "subjective scores:OtherScientificTerm"], ["glimpse measures:Metric", "Hyponym-of", "predictors of intelligibility:Method"], ["glimpse measures:Metric", "Compare", "subjective scores:OtherScientificTerm"], ["correlations:Metric", "Evaluate-for", "Dau:Metric"], ["correlations:Metric", "Evaluate-for", "glimpse measures:Metric"], ["measures:Generic", "Evaluate-for", "predictions of intelligibility:Task"], ["synthetic speech:Material", "Used-for", "predictions of intelligibility:Task"], ["synthetic speech:Material", "Compare", "natural speech:Material"], ["SII measure:Metric", "Hyponym-of", "measures:Generic"], ["ideal binary mask:Method", "Used-for", "synthesized speech:Material"], ["Glimpse measure:Metric", "Used-for", "intelligibility predictions:Task"]]}
{"doc_id": "CVPR_2003_10_abs", "sentence": "A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : reconstruction on demand by tensor voting , or ROD-TV . ROD-TV simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail ( LOD ) . Locally inferred surface elements are robust to noise and better capture local shapes . By inferring per-vertex normals at sub-voxel precision on the fly , we can achieve interpolative shading . Since these missing details can be recovered at the current level of detail , our result is not upper bounded by the scanning resolution . By relaxing the mesh connectivity requirement , we extend ROD-TV and propose a simple but effective multiscale feature extraction algorithm . ROD-TV consists of a hierarchical data structure that encodes different levels of detail . The local reconstruction algorithm is tensor voting . It is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and collecting tensorial support in a neighborhood . We compare our approach and present encouraging results .", "ner": [["'' graphics for vision '' approach", "Method"], ["reconstruction", "Task"], ["large and imperfect data set", "Material"], ["reconstruction", "Task"], ["tensor voting", "Method"], ["ROD-TV", "Method"], ["ROD-TV", "Method"], ["efficiency", "Metric"], ["robust-ness", "Metric"], ["primitive connectivity", "OtherScientificTerm"], ["view dependence", "OtherScientificTerm"], ["levels of detail ( LOD )", "OtherScientificTerm"], ["Locally inferred surface elements", "OtherScientificTerm"], ["noise", "OtherScientificTerm"], ["local shapes", "OtherScientificTerm"], ["per-vertex normals", "OtherScientificTerm"], ["sub-voxel precision", "Metric"], ["interpolative shading", "Task"], ["scanning resolution", "OtherScientificTerm"], ["mesh connectivity requirement", "OtherScientificTerm"], ["ROD-TV", "Method"], ["multiscale feature extraction algorithm", "Method"], ["ROD-TV", "Method"], ["hierarchical data structure", "Method"], ["local reconstruction algorithm", "Method"], ["tensor voting", "Method"], ["It", "Generic"], ["traversing the data hierarchy", "Method"], ["collecting tensorial support", "Method"], ["approach", "Generic"]], "rel": [["'' graphics for vision '' approach", "Used-for", "reconstruction"], ["large and imperfect data set", "Used-for", "reconstruction"], ["tensor voting", "Used-for", "reconstruction"], ["tensor voting", "Conjunction", "ROD-TV"], ["ROD-TV", "Used-for", "reconstruction"], ["efficiency", "Evaluate-for", "ROD-TV"], ["robust-ness", "Evaluate-for", "ROD-TV"], ["robust-ness", "Conjunction", "efficiency"], ["view dependence", "Conjunction", "primitive connectivity"], ["levels of detail ( LOD )", "Conjunction", "view dependence"], ["Locally inferred surface elements", "Used-for", "local shapes"], ["per-vertex normals", "Used-for", "interpolative shading"], ["sub-voxel precision", "Feature-of", "per-vertex normals"], ["mesh connectivity requirement", "Used-for", "multiscale feature extraction algorithm"], ["ROD-TV", "Used-for", "multiscale feature extraction algorithm"], ["hierarchical data structure", "Part-of", "ROD-TV"], ["tensor voting", "Hyponym-of", "local reconstruction algorithm"], ["traversing the data hierarchy", "Used-for", "It"], ["traversing the data hierarchy", "Conjunction", "collecting tensorial support"], ["collecting tensorial support", "Used-for", "It"]], "rel_plus": [["'' graphics for vision '' approach:Method", "Used-for", "reconstruction:Task"], ["large and imperfect data set:Material", "Used-for", "reconstruction:Task"], ["tensor voting:Method", "Used-for", "reconstruction:Task"], ["tensor voting:Method", "Conjunction", "ROD-TV:Method"], ["ROD-TV:Method", "Used-for", "reconstruction:Task"], ["efficiency:Metric", "Evaluate-for", "ROD-TV:Method"], ["robust-ness:Metric", "Evaluate-for", "ROD-TV:Method"], ["robust-ness:Metric", "Conjunction", "efficiency:Metric"], ["view dependence:OtherScientificTerm", "Conjunction", "primitive connectivity:OtherScientificTerm"], ["levels of detail ( LOD ):OtherScientificTerm", "Conjunction", "view dependence:OtherScientificTerm"], ["Locally inferred surface elements:OtherScientificTerm", "Used-for", "local shapes:OtherScientificTerm"], ["per-vertex normals:OtherScientificTerm", "Used-for", "interpolative shading:Task"], ["sub-voxel precision:Metric", "Feature-of", "per-vertex normals:OtherScientificTerm"], ["mesh connectivity requirement:OtherScientificTerm", "Used-for", "multiscale feature extraction algorithm:Method"], ["ROD-TV:Method", "Used-for", "multiscale feature extraction algorithm:Method"], ["hierarchical data structure:Method", "Part-of", "ROD-TV:Method"], ["tensor voting:Method", "Hyponym-of", "local reconstruction algorithm:Method"], ["traversing the data hierarchy:Method", "Used-for", "It:Generic"], ["traversing the data hierarchy:Method", "Conjunction", "collecting tensorial support:Method"], ["collecting tensorial support:Method", "Used-for", "It:Generic"]]}
{"doc_id": "P06-3008", "sentence": "Both rhetorical structure and punctuation have been helpful in discourse processing . Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon . The rhetorical patterns of these marks are compared against patterns around cue phrases in general . Results show that these Chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in Chinese texts .", "ner": [["rhetorical structure", "OtherScientificTerm"], ["punctuation", "OtherScientificTerm"], ["discourse processing", "Task"], ["corpus annotation project", "Task"], ["discursive usage of 6 Chinese punctuation marks", "Task"], ["Chinese punctuation marks", "OtherScientificTerm"], ["news commentary texts", "Material"], ["Colon", "OtherScientificTerm"], ["Dash", "OtherScientificTerm"], ["Ellipsis", "OtherScientificTerm"], ["Exclamation Mark", "OtherScientificTerm"], ["Question Mark", "OtherScientificTerm"], ["Semicolon", "OtherScientificTerm"], ["rhetorical patterns", "OtherScientificTerm"], ["marks", "Generic"], ["patterns around cue phrases", "OtherScientificTerm"], ["Chinese punctuation marks", "OtherScientificTerm"], ["cue phrases", "OtherScientificTerm"], ["indicators of nuclearity", "OtherScientificTerm"], ["Chinese texts", "Material"]], "rel": [["rhetorical structure", "Conjunction", "punctuation"], ["rhetorical structure", "Used-for", "discourse processing"], ["punctuation", "Used-for", "discourse processing"], ["Chinese punctuation marks", "Part-of", "news commentary texts"], ["Colon", "Hyponym-of", "Chinese punctuation marks"], ["Colon", "Conjunction", "Dash"], ["Dash", "Hyponym-of", "Chinese punctuation marks"], ["Dash", "Conjunction", "Ellipsis"], ["Ellipsis", "Hyponym-of", "Chinese punctuation marks"], ["Ellipsis", "Conjunction", "Exclamation Mark"], ["Exclamation Mark", "Hyponym-of", "Chinese punctuation marks"], ["Exclamation Mark", "Conjunction", "Question Mark"], ["Question Mark", "Hyponym-of", "Chinese punctuation marks"], ["Question Mark", "Conjunction", "Semicolon"], ["Semicolon", "Hyponym-of", "Chinese punctuation marks"], ["rhetorical patterns", "Feature-of", "marks"], ["rhetorical patterns", "Compare", "patterns around cue phrases"], ["Chinese punctuation marks", "Compare", "cue phrases"], ["Chinese punctuation marks", "Used-for", "indicators of nuclearity"], ["Chinese texts", "Feature-of", "indicators of nuclearity"]], "rel_plus": [["rhetorical structure:OtherScientificTerm", "Conjunction", "punctuation:OtherScientificTerm"], ["rhetorical structure:OtherScientificTerm", "Used-for", "discourse processing:Task"], ["punctuation:OtherScientificTerm", "Used-for", "discourse processing:Task"], ["Chinese punctuation marks:OtherScientificTerm", "Part-of", "news commentary texts:Material"], ["Colon:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["Colon:OtherScientificTerm", "Conjunction", "Dash:OtherScientificTerm"], ["Dash:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["Dash:OtherScientificTerm", "Conjunction", "Ellipsis:OtherScientificTerm"], ["Ellipsis:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["Ellipsis:OtherScientificTerm", "Conjunction", "Exclamation Mark:OtherScientificTerm"], ["Exclamation Mark:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["Exclamation Mark:OtherScientificTerm", "Conjunction", "Question Mark:OtherScientificTerm"], ["Question Mark:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["Question Mark:OtherScientificTerm", "Conjunction", "Semicolon:OtherScientificTerm"], ["Semicolon:OtherScientificTerm", "Hyponym-of", "Chinese punctuation marks:OtherScientificTerm"], ["rhetorical patterns:OtherScientificTerm", "Feature-of", "marks:Generic"], ["rhetorical patterns:OtherScientificTerm", "Compare", "patterns around cue phrases:OtherScientificTerm"], ["Chinese punctuation marks:OtherScientificTerm", "Compare", "cue phrases:OtherScientificTerm"], ["Chinese punctuation marks:OtherScientificTerm", "Used-for", "indicators of nuclearity:OtherScientificTerm"], ["Chinese texts:Material", "Feature-of", "indicators of nuclearity:OtherScientificTerm"]]}
{"doc_id": "CVPR_2003_11_abs", "sentence": "The features based on Markov random field ( MRF ) models are usually sensitive to the rotation of image textures . This paper develops an anisotropic circular Gaussian MRF ( ACGMRF ) model for modelling rotated image textures and retrieving rotation-invariant texture features . To overcome the singularity problem of the least squares estimate ( LSE ) method , an approximate least squares estimate ( ALSE ) method is proposed to estimate the parameters of the ACGMRF model . The rotation-invariant features can be obtained from the parameters of the ACGMRF model by the one-dimensional ( 1-D ) discrete Fourier transform ( DFT ) . Significantly improved accuracy can be achieved by applying the rotation-invariant features to classify SAR ( synthetic aperture radar ) sea ice and Brodatz imagery .", "ner": [["features", "OtherScientificTerm"], ["Markov random field ( MRF ) models", "Method"], ["rotation of image textures", "OtherScientificTerm"], ["anisotropic circular Gaussian MRF ( ACGMRF ) model", "Method"], ["modelling rotated image textures", "Task"], ["retrieving rotation-invariant texture features", "Task"], ["singularity problem", "OtherScientificTerm"], ["least squares estimate ( LSE ) method", "Method"], ["approximate least squares estimate ( ALSE ) method", "Method"], ["parameters of the ACGMRF model", "OtherScientificTerm"], ["ACGMRF model", "Method"], ["rotation-invariant features", "OtherScientificTerm"], ["parameters of the ACGMRF model", "OtherScientificTerm"], ["ACGMRF model", "Method"], ["one-dimensional ( 1-D ) discrete Fourier transform ( DFT )", "Method"], ["accuracy", "Metric"], ["rotation-invariant features", "OtherScientificTerm"], ["SAR ( synthetic aperture radar", "OtherScientificTerm"]], "rel": [["Markov random field ( MRF ) models", "Used-for", "features"], ["anisotropic circular Gaussian MRF ( ACGMRF ) model", "Used-for", "modelling rotated image textures"], ["anisotropic circular Gaussian MRF ( ACGMRF ) model", "Used-for", "retrieving rotation-invariant texture features"], ["modelling rotated image textures", "Conjunction", "retrieving rotation-invariant texture features"], ["singularity problem", "Feature-of", "least squares estimate ( LSE ) method"], ["approximate least squares estimate ( ALSE ) method", "Used-for", "parameters of the ACGMRF model"], ["parameters of the ACGMRF model", "Used-for", "rotation-invariant features"], ["one-dimensional ( 1-D ) discrete Fourier transform ( DFT )", "Used-for", "rotation-invariant features"], ["rotation-invariant features", "Used-for", "SAR ( synthetic aperture radar"]], "rel_plus": [["Markov random field ( MRF ) models:Method", "Used-for", "features:OtherScientificTerm"], ["anisotropic circular Gaussian MRF ( ACGMRF ) model:Method", "Used-for", "modelling rotated image textures:Task"], ["anisotropic circular Gaussian MRF ( ACGMRF ) model:Method", "Used-for", "retrieving rotation-invariant texture features:Task"], ["modelling rotated image textures:Task", "Conjunction", "retrieving rotation-invariant texture features:Task"], ["singularity problem:OtherScientificTerm", "Feature-of", "least squares estimate ( LSE ) method:Method"], ["approximate least squares estimate ( ALSE ) method:Method", "Used-for", "parameters of the ACGMRF model:OtherScientificTerm"], ["parameters of the ACGMRF model:OtherScientificTerm", "Used-for", "rotation-invariant features:OtherScientificTerm"], ["one-dimensional ( 1-D ) discrete Fourier transform ( DFT ):Method", "Used-for", "rotation-invariant features:OtherScientificTerm"], ["rotation-invariant features:OtherScientificTerm", "Used-for", "SAR ( synthetic aperture radar:OtherScientificTerm"]]}
{"doc_id": "P05-1073", "sentence": "Despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate label sequence models via Viterbi decoding . This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure , with strong dependencies between arguments . We show how to build a joint model of argument frames , incorporating novel features that model these interactions into discriminative log-linear models . This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank .", "ner": [["semantic role labeling", "Task"], ["independent classifiers", "Method"], ["label sequence models", "Method"], ["Viterbi decoding", "Method"], ["core argument frame", "OtherScientificTerm"], ["joint model of argument frames", "Method"], ["features", "OtherScientificTerm"], ["discriminative log-linear models", "Method"], ["system", "Generic"], ["error reduction", "Metric"], ["independent classifier", "Method"], ["gold-standard parse trees", "OtherScientificTerm"], ["PropBank", "Material"]], "rel": [["independent classifiers", "Used-for", "semantic role labeling"], ["independent classifiers", "Conjunction", "label sequence models"], ["Viterbi decoding", "Used-for", "label sequence models"], ["features", "Part-of", "discriminative log-linear models"], ["error reduction", "Evaluate-for", "system"], ["error reduction", "Evaluate-for", "independent classifier"], ["independent classifier", "Compare", "system"], ["gold-standard parse trees", "Evaluate-for", "system"], ["gold-standard parse trees", "Evaluate-for", "independent classifier"], ["gold-standard parse trees", "Part-of", "PropBank"]], "rel_plus": [["independent classifiers:Method", "Used-for", "semantic role labeling:Task"], ["independent classifiers:Method", "Conjunction", "label sequence models:Method"], ["Viterbi decoding:Method", "Used-for", "label sequence models:Method"], ["features:OtherScientificTerm", "Part-of", "discriminative log-linear models:Method"], ["error reduction:Metric", "Evaluate-for", "system:Generic"], ["error reduction:Metric", "Evaluate-for", "independent classifier:Method"], ["independent classifier:Method", "Compare", "system:Generic"], ["gold-standard parse trees:OtherScientificTerm", "Evaluate-for", "system:Generic"], ["gold-standard parse trees:OtherScientificTerm", "Evaluate-for", "independent classifier:Method"], ["gold-standard parse trees:OtherScientificTerm", "Part-of", "PropBank:Material"]]}
{"doc_id": "E93-1023", "sentence": "One of the major problems one is faced with when decomposing words into their constituent parts is ambiguity : the generation of multiple analyses for one input word , many of which are implausible . In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar ( PCFG ) , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse . Consequently , remaining analyses can be ordered along a scale of plausibility . Test performance data will show that a PCFG yields good results in morphological parsing . MORPA is a fully implemented parser developed for use in a text-to-speech conversion system .", "ner": [["ambiguity", "OtherScientificTerm"], ["generation", "Task"], ["ambiguity", "OtherScientificTerm"], ["MORphological PArser MORPA", "Method"], ["probabilistic context-free grammar ( PCFG )", "Method"], ["it", "Generic"], ["`` conventional '' context-free morphological grammar", "Method"], ["ungrammatical segmentations", "OtherScientificTerm"], ["probability-based scoring function", "Method"], ["parse", "Task"], ["PCFG", "Method"], ["morphological parsing", "Task"], ["MORPA", "Method"], ["parser", "Method"], ["text-to-speech conversion system", "Task"]], "rel": [["MORphological PArser MORPA", "Used-for", "ambiguity"], ["probabilistic context-free grammar ( PCFG )", "Used-for", "MORphological PArser MORPA"], ["`` conventional '' context-free morphological grammar", "Used-for", "it"], ["`` conventional '' context-free morphological grammar", "Used-for", "ungrammatical segmentations"], ["probability-based scoring function", "Used-for", "it"], ["probability-based scoring function", "Conjunction", "`` conventional '' context-free morphological grammar"], ["probability-based scoring function", "Used-for", "parse"], ["PCFG", "Used-for", "morphological parsing"], ["MORPA", "Hyponym-of", "parser"], ["MORPA", "Used-for", "text-to-speech conversion system"], ["parser", "Used-for", "text-to-speech conversion system"]], "rel_plus": [["MORphological PArser MORPA:Method", "Used-for", "ambiguity:OtherScientificTerm"], ["probabilistic context-free grammar ( PCFG ):Method", "Used-for", "MORphological PArser MORPA:Method"], ["`` conventional '' context-free morphological grammar:Method", "Used-for", "it:Generic"], ["`` conventional '' context-free morphological grammar:Method", "Used-for", "ungrammatical segmentations:OtherScientificTerm"], ["probability-based scoring function:Method", "Used-for", "it:Generic"], ["probability-based scoring function:Method", "Conjunction", "`` conventional '' context-free morphological grammar:Method"], ["probability-based scoring function:Method", "Used-for", "parse:Task"], ["PCFG:Method", "Used-for", "morphological parsing:Task"], ["MORPA:Method", "Hyponym-of", "parser:Method"], ["MORPA:Method", "Used-for", "text-to-speech conversion system:Task"], ["parser:Method", "Used-for", "text-to-speech conversion system:Task"]]}
{"doc_id": "C90-3014", "sentence": "This paper describes the framework of a Korean phonological knowledge base system using the unification-based grammar formalism : Korean Phonology Structure Grammar ( KPSG ) . The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system . We show that the proposed approach is more describable than other approaches such as those employing a traditional generative phonological approach .", "ner": [["Korean phonological knowledge base system", "Task"], ["unification-based grammar formalism", "Method"], ["Korean Phonology Structure Grammar ( KPSG )", "Method"], ["approach", "Generic"], ["KPSG", "Method"], ["phonological system", "Task"], ["speech recognition and synthesis system", "Task"], ["approach", "Generic"], ["approaches", "Generic"], ["those", "Generic"], ["generative phonological approach", "Method"]], "rel": [["unification-based grammar formalism", "Used-for", "Korean phonological knowledge base system"], ["Korean Phonology Structure Grammar ( KPSG )", "Hyponym-of", "unification-based grammar formalism"], ["approach", "Used-for", "KPSG"], ["KPSG", "Used-for", "phonological system"], ["approach", "Compare", "approaches"], ["generative phonological approach", "Used-for", "those"]], "rel_plus": [["unification-based grammar formalism:Method", "Used-for", "Korean phonological knowledge base system:Task"], ["Korean Phonology Structure Grammar ( KPSG ):Method", "Hyponym-of", "unification-based grammar formalism:Method"], ["approach:Generic", "Used-for", "KPSG:Method"], ["KPSG:Method", "Used-for", "phonological system:Task"], ["approach:Generic", "Compare", "approaches:Generic"], ["generative phonological approach:Method", "Used-for", "those:Generic"]]}
{"doc_id": "IJCAI_2016_413_abs", "sentence": "In some auction domains , there is uncertainty regarding the final availability of the goods being auctioned off . For example , a government may auction off spectrum from its public safety network , but it may need this spectrum back in times of emergency . In such a domain , standard combinatorial auctions perform poorly because they lead to violations of individual rationality ( IR ) , even in expectation , and to very low efficiency . In this paper , we study the design of core-selecting payment rules for such domains . Surprisingly , we show that in this new domain , there does not exist a payment rule with is guaranteed to be ex-post core-selecting . However , we show that by designing rules that are '' execution-contingent , '' i.e. , by charging payments that are conditioned on the realization of the availability of the goods , we can reduce IR violations . We design two core-selecting rules that always satisfy IR in expectation . To study the performance of our rules we perform a computational Bayes-Nash equilibrium analysis . We show that , in equilibrium , our new rules have better incentives , higher efficiency , and a lower rate of ex-post IR violations than standard core-selecting rules .", "ner": [["auction domains", "Task"], ["domain", "Generic"], ["combinatorial auctions", "Method"], ["they", "Generic"], ["violations of individual rationality ( IR )", "OtherScientificTerm"], ["individual rationality ( IR )", "OtherScientificTerm"], ["design of core-selecting payment rules", "Task"], ["domains", "Generic"], ["domain", "Generic"], ["payment rule", "OtherScientificTerm"], ["rules", "OtherScientificTerm"], ["IR violations", "OtherScientificTerm"], ["core-selecting rules", "OtherScientificTerm"], ["IR", "OtherScientificTerm"], ["rules", "Generic"], ["computational Bayes-Nash equilibrium analysis", "Method"], ["rules", "Generic"], ["rate of ex-post IR violations", "Metric"], ["core-selecting rules", "OtherScientificTerm"]], "rel": [["design of core-selecting payment rules", "Used-for", "domains"], ["core-selecting rules", "Used-for", "IR"], ["computational Bayes-Nash equilibrium analysis", "Used-for", "rules"], ["rules", "Compare", "core-selecting rules"], ["rate of ex-post IR violations", "Evaluate-for", "rules"], ["rate of ex-post IR violations", "Evaluate-for", "core-selecting rules"]], "rel_plus": [["design of core-selecting payment rules:Task", "Used-for", "domains:Generic"], ["core-selecting rules:OtherScientificTerm", "Used-for", "IR:OtherScientificTerm"], ["computational Bayes-Nash equilibrium analysis:Method", "Used-for", "rules:Generic"], ["rules:Generic", "Compare", "core-selecting rules:OtherScientificTerm"], ["rate of ex-post IR violations:Metric", "Evaluate-for", "rules:Generic"], ["rate of ex-post IR violations:Metric", "Evaluate-for", "core-selecting rules:OtherScientificTerm"]]}
{"doc_id": "C08-3010", "sentence": "In this paper , we will describe a search tool for a huge set of ngrams . The tool supports queries with an arbitrary number of wildcards . It takes a fraction of a second for a search , and can provide the fillers of the wildcards . The system runs on a single Linux PC with reasonable size memory ( less than 4GB ) and disk space ( less than 400GB ) . This system can be a very useful tool for linguistic knowledge discovery and other NLP tasks .", "ner": [["search tool", "Method"], ["ngrams", "OtherScientificTerm"], ["tool", "Generic"], ["It", "Generic"], ["system", "Generic"], ["memory", "OtherScientificTerm"], ["disk space", "OtherScientificTerm"], ["system", "Generic"], ["tool", "Generic"], ["linguistic knowledge discovery", "Task"], ["NLP tasks", "Task"]], "rel": [["search tool", "Used-for", "ngrams"], ["tool", "Used-for", "linguistic knowledge discovery"], ["tool", "Used-for", "NLP tasks"], ["linguistic knowledge discovery", "Conjunction", "NLP tasks"]], "rel_plus": [["search tool:Method", "Used-for", "ngrams:OtherScientificTerm"], ["tool:Generic", "Used-for", "linguistic knowledge discovery:Task"], ["tool:Generic", "Used-for", "NLP tasks:Task"], ["linguistic knowledge discovery:Task", "Conjunction", "NLP tasks:Task"]]}
{"doc_id": "J88-3002", "sentence": "For intelligent interactive systems to communicate with humans in a natural manner , they must have knowledge about the system users . This paper explores the role of user modeling in such systems . It begins with a characterization of what a user model is and how it can be used . The types of information that a user model may be required to keep about a user are then identified and discussed . User models themselves can vary greatly depending on the requirements of the situation and the implementation , so several dimensions along which they can be classified are presented . Since acquiring the knowledge for a user model is a fundamental problem in user modeling , a section is devoted to this topic . Next , the benefits and costs of implementing a user modeling component for a system are weighed in light of several aspects of the interaction requirements that may be imposed by the system . Finally , the current state of research in user modeling is summarized , and future research topics that must be addressed in order to achieve powerful , general user modeling systems are assessed .", "ner": [["intelligent interactive systems", "Method"], ["they", "Generic"], ["user modeling", "Task"], ["systems", "Method"], ["user model", "Method"], ["it", "Generic"], ["user model", "Method"], ["User models", "Method"], ["they", "Generic"], ["user model", "Method"], ["user modeling", "Task"], ["user modeling component", "Method"], ["system", "Generic"], ["system", "Generic"], ["user modeling", "Task"], ["user modeling systems", "Method"]], "rel": [["user modeling", "Part-of", "systems"], ["user model", "Used-for", "user modeling"], ["user modeling component", "Part-of", "system"]], "rel_plus": [["user modeling:Task", "Part-of", "systems:Method"], ["user model:Method", "Used-for", "user modeling:Task"], ["user modeling component:Method", "Part-of", "system:Generic"]]}
{"doc_id": "N04-4028", "sentence": "Information extraction techniques automatically create structured databases from unstructured data sources , such as the Web or newswire documents . Despite the successes of these systems , accuracy will always be imperfect . For many reasons , it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field . The information extraction system we evaluate is based on a linear-chain conditional random field ( CRF ) , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model . We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records .", "ner": [["Information extraction techniques", "Method"], ["structured databases", "Material"], ["unstructured data sources", "Material"], ["Web", "Material"], ["newswire documents", "Material"], ["systems", "Generic"], ["accuracy", "Metric"], ["system", "Generic"], ["information extraction system", "Method"], ["linear-chain conditional random field ( CRF )", "Method"], ["probabilistic model", "Method"], ["information extraction tasks", "Task"], ["arbitrary , overlapping features", "OtherScientificTerm"], ["input", "Generic"], ["Markov model", "Method"], ["techniques", "Generic"], ["extracted fields", "Generic"], ["multi-field records", "Material"], ["average precision", "Metric"], ["multi-field records", "Material"]], "rel": [["Information extraction techniques", "Used-for", "structured databases"], ["unstructured data sources", "Used-for", "Information extraction techniques"], ["Web", "Hyponym-of", "unstructured data sources"], ["Web", "Conjunction", "newswire documents"], ["newswire documents", "Hyponym-of", "unstructured data sources"], ["accuracy", "Evaluate-for", "systems"], ["linear-chain conditional random field ( CRF )", "Used-for", "information extraction system"], ["linear-chain conditional random field ( CRF )", "Hyponym-of", "probabilistic model"], ["probabilistic model", "Used-for", "information extraction tasks"], ["probabilistic model", "Used-for", "arbitrary , overlapping features"], ["arbitrary , overlapping features", "Feature-of", "input"], ["arbitrary , overlapping features", "Part-of", "Markov model"], ["extracted fields", "Conjunction", "multi-field records"], ["average precision", "Evaluate-for", "techniques"]], "rel_plus": [["Information extraction techniques:Method", "Used-for", "structured databases:Material"], ["unstructured data sources:Material", "Used-for", "Information extraction techniques:Method"], ["Web:Material", "Hyponym-of", "unstructured data sources:Material"], ["Web:Material", "Conjunction", "newswire documents:Material"], ["newswire documents:Material", "Hyponym-of", "unstructured data sources:Material"], ["accuracy:Metric", "Evaluate-for", "systems:Generic"], ["linear-chain conditional random field ( CRF ):Method", "Used-for", "information extraction system:Method"], ["linear-chain conditional random field ( CRF ):Method", "Hyponym-of", "probabilistic model:Method"], ["probabilistic model:Method", "Used-for", "information extraction tasks:Task"], ["probabilistic model:Method", "Used-for", "arbitrary , overlapping features:OtherScientificTerm"], ["arbitrary , overlapping features:OtherScientificTerm", "Feature-of", "input:Generic"], ["arbitrary , overlapping features:OtherScientificTerm", "Part-of", "Markov model:Method"], ["extracted fields:Generic", "Conjunction", "multi-field records:Material"], ["average precision:Metric", "Evaluate-for", "techniques:Generic"]]}
{"doc_id": "H05-1005", "sentence": "In this paper , we use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries . We consider the case of multi-document summarization , where the input documents are in Arabic , and the output summary is in English . Typically , information that makes it to a summary appears in many different lexical-syntactic forms in the input documents . Further , the use of multiple machine translation systems provides yet more redundancy , yielding different ways to realize that information in English . We demonstrate how errors in the machine translations of the input Arabic documents can be corrected by identifying and generating from such redundancy , focusing on noun phrases .", "ner": [["information redundancy in multilingual input", "OtherScientificTerm"], ["machine translation", "Task"], ["multilingual summaries", "Task"], ["multi-document summarization", "Task"], ["Arabic", "Material"], ["English", "Material"], ["lexical-syntactic forms", "OtherScientificTerm"], ["machine translation systems", "Method"], ["English", "Material"], ["machine translations", "Task"], ["Arabic documents", "Material"]], "rel": [["information redundancy in multilingual input", "Used-for", "machine translation"], ["information redundancy in multilingual input", "Used-for", "multilingual summaries"], ["Arabic documents", "Used-for", "machine translations"]], "rel_plus": [["information redundancy in multilingual input:OtherScientificTerm", "Used-for", "machine translation:Task"], ["information redundancy in multilingual input:OtherScientificTerm", "Used-for", "multilingual summaries:Task"], ["Arabic documents:Material", "Used-for", "machine translations:Task"]]}
{"doc_id": "ICCV_2015_392_abs", "sentence": "In this paper , we propose a new approach to generate oriented object proposals ( OOPs ) to reduce the detection error caused by various orientations of the object . To this end , we propose to efficiently locate object regions according to pixelwise object probability , rather than measuring the objectness from a set of sampled windows . We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes ( i.e. , sizes and orientations ) can be produced by locating the local maximum likelihoods . The new approach has three main advantages . First , it helps the object detector handle objects of different orientations . Second , as the shapes of the proposals may vary to fit the objects , the resulting proposals are tighter than the sampling windows with fixed sizes . Third , it avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high recall . Experiments on the PASCAL VOC 2007 dataset show that the proposed OOP outperforms the state-of-the-art fast methods . Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios . Generating OOPs is very fast and takes only 0.5 s per image .", "ner": [["approach", "Generic"], ["oriented object proposals ( OOPs )", "Task"], ["detection error", "Metric"], ["orientations of the object", "OtherScientificTerm"], ["object regions", "OtherScientificTerm"], ["pixelwise object probability", "OtherScientificTerm"], ["objectness", "OtherScientificTerm"], ["proposal generation problem", "Task"], ["generative proba-bilistic model", "Method"], ["object proposals", "OtherScientificTerm"], ["shapes", "OtherScientificTerm"], ["sizes", "OtherScientificTerm"], ["orientations", "OtherScientificTerm"], ["local maximum likelihoods", "OtherScientificTerm"], ["approach", "Generic"], ["it", "Generic"], ["object detector", "Method"], ["orientations", "OtherScientificTerm"], ["shapes of the proposals", "OtherScientificTerm"], ["sampling windows", "OtherScientificTerm"], ["it", "Generic"], ["massive window sampling", "Method"], ["number of proposals", "OtherScientificTerm"], ["recall", "Metric"], ["PASCAL VOC 2007 dataset", "Material"], ["OOP", "Method"], ["state-of-the-art fast methods", "Generic"], ["rotation invariant property", "OtherScientificTerm"], ["class-specific object detector", "Method"], ["proposal generation methods", "Method"], ["object rotation scenarios", "Material"], ["general scenarios", "Material"], ["OOPs", "Task"]], "rel": [["approach", "Used-for", "oriented object proposals ( OOPs )"], ["detection error", "Evaluate-for", "oriented object proposals ( OOPs )"], ["pixelwise object probability", "Used-for", "object regions"], ["pixelwise object probability", "Compare", "objectness"], ["generative proba-bilistic model", "Used-for", "proposal generation problem"], ["shapes", "Feature-of", "object proposals"], ["sizes", "Hyponym-of", "shapes"], ["sizes", "Conjunction", "orientations"], ["orientations", "Hyponym-of", "shapes"], ["local maximum likelihoods", "Used-for", "object proposals"], ["object detector", "Used-for", "orientations"], ["it", "Used-for", "number of proposals"], ["recall", "Evaluate-for", "it"], ["PASCAL VOC 2007 dataset", "Evaluate-for", "OOP"], ["OOP", "Compare", "state-of-the-art fast methods"], ["rotation invariant property", "Used-for", "class-specific object detector"], ["class-specific object detector", "Compare", "proposal generation methods"], ["object rotation scenarios", "Evaluate-for", "class-specific object detector"], ["object rotation scenarios", "Evaluate-for", "proposal generation methods"], ["object rotation scenarios", "Conjunction", "general scenarios"], ["general scenarios", "Evaluate-for", "class-specific object detector"], ["general scenarios", "Evaluate-for", "proposal generation methods"]], "rel_plus": [["approach:Generic", "Used-for", "oriented object proposals ( OOPs ):Task"], ["detection error:Metric", "Evaluate-for", "oriented object proposals ( OOPs ):Task"], ["pixelwise object probability:OtherScientificTerm", "Used-for", "object regions:OtherScientificTerm"], ["pixelwise object probability:OtherScientificTerm", "Compare", "objectness:OtherScientificTerm"], ["generative proba-bilistic model:Method", "Used-for", "proposal generation problem:Task"], ["shapes:OtherScientificTerm", "Feature-of", "object proposals:OtherScientificTerm"], ["sizes:OtherScientificTerm", "Hyponym-of", "shapes:OtherScientificTerm"], ["sizes:OtherScientificTerm", "Conjunction", "orientations:OtherScientificTerm"], ["orientations:OtherScientificTerm", "Hyponym-of", "shapes:OtherScientificTerm"], ["local maximum likelihoods:OtherScientificTerm", "Used-for", "object proposals:OtherScientificTerm"], ["object detector:Method", "Used-for", "orientations:OtherScientificTerm"], ["it:Generic", "Used-for", "number of proposals:OtherScientificTerm"], ["recall:Metric", "Evaluate-for", "it:Generic"], ["PASCAL VOC 2007 dataset:Material", "Evaluate-for", "OOP:Method"], ["OOP:Method", "Compare", "state-of-the-art fast methods:Generic"], ["rotation invariant property:OtherScientificTerm", "Used-for", "class-specific object detector:Method"], ["class-specific object detector:Method", "Compare", "proposal generation methods:Method"], ["object rotation scenarios:Material", "Evaluate-for", "class-specific object detector:Method"], ["object rotation scenarios:Material", "Evaluate-for", "proposal generation methods:Method"], ["object rotation scenarios:Material", "Conjunction", "general scenarios:Material"], ["general scenarios:Material", "Evaluate-for", "class-specific object detector:Method"], ["general scenarios:Material", "Evaluate-for", "proposal generation methods:Method"]]}
{"doc_id": "H92-1017", "sentence": "This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and database query paraphrase . In addition , we discuss the results of the February 1992 ATIS benchmark tests . We describe a variation on the standard evaluation metric which provides a more tightly controlled measure of progress . Finally , we briefly describe an experiment which we have done in extending the n-best speech/language integration architecture to improving OCR accuracy .", "ner": [["domain-independent capabilities", "Generic"], ["Paramax spoken language understanding system", "Method"], ["non-monotonic reasoning", "Task"], ["implicit reference resolution", "Task"], ["database query paraphrase", "Task"], ["February 1992 ATIS benchmark tests", "Material"], ["n-best speech/language integration architecture", "Method"], ["OCR accuracy", "Metric"]], "rel": [["domain-independent capabilities", "Part-of", "Paramax spoken language understanding system"], ["non-monotonic reasoning", "Hyponym-of", "domain-independent capabilities"], ["implicit reference resolution", "Hyponym-of", "domain-independent capabilities"], ["database query paraphrase", "Hyponym-of", "domain-independent capabilities"], ["OCR accuracy", "Evaluate-for", "n-best speech/language integration architecture"]], "rel_plus": [["domain-independent capabilities:Generic", "Part-of", "Paramax spoken language understanding system:Method"], ["non-monotonic reasoning:Task", "Hyponym-of", "domain-independent capabilities:Generic"], ["implicit reference resolution:Task", "Hyponym-of", "domain-independent capabilities:Generic"], ["database query paraphrase:Task", "Hyponym-of", "domain-independent capabilities:Generic"], ["OCR accuracy:Metric", "Evaluate-for", "n-best speech/language integration architecture:Method"]]}
{"doc_id": "CVPR_2016_416_abs", "sentence": "We investigate the problem of fine-grained sketch-based image retrieval ( SBIR ) , where free-hand human sketches are used as queries to perform instance-level retrieval of images . This is an extremely challenging task because ( i ) visual comparisons not only need to be fine-grained but also executed cross-domain , ( ii ) free-hand ( finger ) sketches are highly abstract , making fine-grained matching harder , and most importantly ( iii ) annotated cross-domain sketch-photo datasets required for training are scarce , challenging many state-of-the-art machine learning techniques . In this paper , for the first time , we address all these challenges , providing a step towards the capabilities that would underpin a commercial sketch-based image retrieval application . We introduce a new database of 1,432 sketch-photo pairs from two categories with 32,000 fine-grained triplet ranking annotations . We then develop a deep triplet-ranking model for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data . Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training deep networks for fine-grained cross-domain ranking tasks .", "ner": [["fine-grained sketch-based image retrieval ( SBIR )", "Task"], ["free-hand human sketches", "OtherScientificTerm"], ["instance-level retrieval of images", "Task"], ["visual comparisons", "OtherScientificTerm"], ["free-hand ( finger ) sketches", "OtherScientificTerm"], ["fine-grained matching", "Task"], ["annotated cross-domain sketch-photo datasets", "Material"], ["machine learning techniques", "Method"], ["sketch-based image retrieval application", "Task"], ["fine-grained triplet ranking annotations", "OtherScientificTerm"], ["deep triplet-ranking model", "Method"], ["instance-level SBIR", "Task"], ["data augmentation", "Method"], ["staged pre-training strategy", "Method"], ["insufficient fine-grained training data", "Material"], ["data sufficiency", "OtherScientificTerm"], ["over-fitting avoidance", "OtherScientificTerm"], ["deep networks", "Method"], ["fine-grained cross-domain ranking tasks", "Task"]], "rel": [["free-hand human sketches", "Used-for", "instance-level retrieval of images"], ["annotated cross-domain sketch-photo datasets", "Used-for", "machine learning techniques"], ["deep triplet-ranking model", "Used-for", "instance-level SBIR"], ["deep triplet-ranking model", "Used-for", "insufficient fine-grained training data"], ["data augmentation", "Used-for", "deep triplet-ranking model"], ["data augmentation", "Conjunction", "staged pre-training strategy"], ["staged pre-training strategy", "Used-for", "deep triplet-ranking model"], ["data sufficiency", "Conjunction", "over-fitting avoidance"], ["deep networks", "Used-for", "fine-grained cross-domain ranking tasks"]], "rel_plus": [["free-hand human sketches:OtherScientificTerm", "Used-for", "instance-level retrieval of images:Task"], ["annotated cross-domain sketch-photo datasets:Material", "Used-for", "machine learning techniques:Method"], ["deep triplet-ranking model:Method", "Used-for", "instance-level SBIR:Task"], ["deep triplet-ranking model:Method", "Used-for", "insufficient fine-grained training data:Material"], ["data augmentation:Method", "Used-for", "deep triplet-ranking model:Method"], ["data augmentation:Method", "Conjunction", "staged pre-training strategy:Method"], ["staged pre-training strategy:Method", "Used-for", "deep triplet-ranking model:Method"], ["data sufficiency:OtherScientificTerm", "Conjunction", "over-fitting avoidance:OtherScientificTerm"], ["deep networks:Method", "Used-for", "fine-grained cross-domain ranking tasks:Task"]]}
{"doc_id": "CVPR_2015_303_abs", "sentence": "In this paper we target at generating generic action proposals in unconstrained videos . Each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , a spatio-temporal video tube , which has a good potential to locate one human action . Assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the ac-tionness of the video tubes . After picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score . Compared with existing action proposal approaches , our action proposals do not rely on video segmentation and can be generated in nearly real-time . Experimental results on two challenging datasets , MSRII and UCF 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search .", "ner": [["generic action proposals", "OtherScientificTerm"], ["unconstrained videos", "Material"], ["action proposal", "OtherScientificTerm"], ["temporal series of spatial bounding boxes", "OtherScientificTerm"], ["spatio-temporal video tube", "OtherScientificTerm"], ["human action", "OtherScientificTerm"], ["appearance and motion cues", "OtherScientificTerm"], ["ac-tionness", "Metric"], ["video tubes", "Material"], ["actionness scores", "Metric"], ["action proposal generation", "Task"], ["maximum set coverage problem", "Task"], ["greedy search", "Method"], ["action proposals", "OtherScientificTerm"], ["actionness score", "Metric"], ["action proposal approaches", "Method"], ["action proposals", "OtherScientificTerm"], ["video segmentation", "Method"], ["datasets", "Generic"], ["MSRII", "Material"], ["UCF 101", "Material"], ["action proposals", "OtherScientificTerm"], ["action detection and search", "Task"]], "rel": [["unconstrained videos", "Used-for", "generic action proposals"], ["spatio-temporal video tube", "Hyponym-of", "temporal series of spatial bounding boxes"], ["spatio-temporal video tube", "Used-for", "human action"], ["appearance and motion cues", "Used-for", "ac-tionness"], ["ac-tionness", "Evaluate-for", "video tubes"], ["maximum set coverage problem", "Used-for", "action proposal generation"], ["greedy search", "Used-for", "action proposals"], ["actionness score", "Evaluate-for", "action proposals"], ["action proposal approaches", "Compare", "action proposals"], ["datasets", "Evaluate-for", "action proposals"], ["MSRII", "Hyponym-of", "datasets"], ["MSRII", "Conjunction", "UCF 101"], ["UCF 101", "Hyponym-of", "datasets"], ["action detection and search", "Evaluate-for", "action proposals"]], "rel_plus": [["unconstrained videos:Material", "Used-for", "generic action proposals:OtherScientificTerm"], ["spatio-temporal video tube:OtherScientificTerm", "Hyponym-of", "temporal series of spatial bounding boxes:OtherScientificTerm"], ["spatio-temporal video tube:OtherScientificTerm", "Used-for", "human action:OtherScientificTerm"], ["appearance and motion cues:OtherScientificTerm", "Used-for", "ac-tionness:Metric"], ["ac-tionness:Metric", "Evaluate-for", "video tubes:Material"], ["maximum set coverage problem:Task", "Used-for", "action proposal generation:Task"], ["greedy search:Method", "Used-for", "action proposals:OtherScientificTerm"], ["actionness score:Metric", "Evaluate-for", "action proposals:OtherScientificTerm"], ["action proposal approaches:Method", "Compare", "action proposals:OtherScientificTerm"], ["datasets:Generic", "Evaluate-for", "action proposals:OtherScientificTerm"], ["MSRII:Material", "Hyponym-of", "datasets:Generic"], ["MSRII:Material", "Conjunction", "UCF 101:Material"], ["UCF 101:Material", "Hyponym-of", "datasets:Generic"], ["action detection and search:Task", "Evaluate-for", "action proposals:OtherScientificTerm"]]}
{"doc_id": "J81-1002", "sentence": "This paper reports recent research into methods for creating natural language text . A new processing paradigm called Fragment-and-Compose has been created and an experimental system implemented in it . The knowledge to be expressed in text is first divided into small propositional units , which are then composed into appropriate combinations and converted into text . KDS ( Knowledge Delivery System ) , which embodies this paradigm , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text . The Fragment-and-Compose paradigm and the computational methods of KDS are described .", "ner": [["methods", "Generic"], ["creating natural language text", "Task"], ["processing paradigm", "Generic"], ["Fragment-and-Compose", "Method"], ["KDS ( Knowledge Delivery System )", "Method"], ["paradigm", "Generic"], ["Fragment-and-Compose paradigm", "Method"], ["computational methods", "Generic"], ["KDS", "Method"]], "rel": [["methods", "Used-for", "creating natural language text"], ["paradigm", "Part-of", "KDS ( Knowledge Delivery System )"], ["computational methods", "Used-for", "KDS"]], "rel_plus": [["methods:Generic", "Used-for", "creating natural language text:Task"], ["paradigm:Generic", "Part-of", "KDS ( Knowledge Delivery System ):Method"], ["computational methods:Generic", "Used-for", "KDS:Method"]]}
{"doc_id": "E99-1034", "sentence": "This paper explores the issue of using different co-occurrence similarities between terms for separating query terms that are useful for retrieval from those that are harmful . The hypothesis under examination is that useful terms tend to be more similar to each other than to other query terms . Preliminary experiments with similarities computed using first-order and second-order co-occurrence seem to confirm the hypothesis . Term similarities could then be used for determining which query terms are useful and best reflect the user 's information need . A possible application would be to use this source of evidence for tuning the weights of the query terms .", "ner": [["co-occurrence similarities", "OtherScientificTerm"], ["query terms", "Generic"], ["retrieval", "Task"], ["those", "Generic"], ["useful terms", "Generic"], ["query terms", "Generic"], ["similarities", "Generic"], ["first-order and second-order co-occurrence", "OtherScientificTerm"], ["Term similarities", "OtherScientificTerm"]], "rel": [["co-occurrence similarities", "Used-for", "query terms"], ["query terms", "Used-for", "retrieval"], ["those", "Compare", "query terms"], ["useful terms", "Compare", "query terms"], ["first-order and second-order co-occurrence", "Used-for", "similarities"]], "rel_plus": [["co-occurrence similarities:OtherScientificTerm", "Used-for", "query terms:Generic"], ["query terms:Generic", "Used-for", "retrieval:Task"], ["those:Generic", "Compare", "query terms:Generic"], ["useful terms:Generic", "Compare", "query terms:Generic"], ["first-order and second-order co-occurrence:OtherScientificTerm", "Used-for", "similarities:Generic"]]}
{"doc_id": "N03-1017", "sentence": "We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several , previously proposed phrase-based translation models . Within our framework , we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models . Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations . Surprisingly , learning phrases longer than three words and learning phrases from high-accuracy word-level alignment models does not have a strong impact on performance . Learning only syntactically motivated phrases degrades the performance of our systems .", "ner": [["phrase-based translation model", "Method"], ["decoding algorithm", "Method"], ["phrase-based translation models", "Method"], ["framework", "Generic"], ["phrase-based models", "Method"], ["word-based models", "Method"], ["means", "Generic"], ["heuristic learning of phrase translations", "Method"], ["word-based alignments", "Method"], ["lexical weighting of phrase translations", "Method"], ["high-accuracy word-level alignment models", "Method"], ["systems", "Generic"]], "rel": [["phrase-based translation model", "Conjunction", "decoding algorithm"], ["phrase-based models", "Compare", "word-based models"], ["heuristic learning of phrase translations", "Hyponym-of", "means"], ["word-based alignments", "Used-for", "heuristic learning of phrase translations"], ["lexical weighting of phrase translations", "Hyponym-of", "means"]], "rel_plus": [["phrase-based translation model:Method", "Conjunction", "decoding algorithm:Method"], ["phrase-based models:Method", "Compare", "word-based models:Method"], ["heuristic learning of phrase translations:Method", "Hyponym-of", "means:Generic"], ["word-based alignments:Method", "Used-for", "heuristic learning of phrase translations:Method"], ["lexical weighting of phrase translations:Method", "Hyponym-of", "means:Generic"]]}
{"doc_id": "CVPR_2011_293_abs", "sentence": "Color is known to be highly discriminative for many object recognition tasks , but is difficult to infer from uncontrolled images in which the illuminant is not known . Traditional methods for color constancy can improve surface re-flectance estimates from such uncalibrated images , but their output depends significantly on the background scene . In many recognition and retrieval applications , we have access to image sets that contain multiple views of the same object in different environments ; we show in this paper that correspondences between these images provide important constraints that can improve color constancy . We introduce the multi-view color constancy problem , and present a method to recover estimates of underlying surface re-flectance based on joint estimation of these surface properties and the illuminants present in multiple images . The method can exploit image correspondences obtained by various alignment techniques , and we show examples based on matching local region features . Our results show that multi-view constraints can significantly improve estimates of both scene illuminants and object color ( surface reflectance ) when compared to a baseline single-view method .", "ner": [["object recognition tasks", "Task"], ["uncontrolled images", "Material"], ["illuminant", "OtherScientificTerm"], ["methods", "Generic"], ["color constancy", "Task"], ["surface re-flectance estimates", "Method"], ["uncalibrated images", "Material"], ["background scene", "OtherScientificTerm"], ["recognition and retrieval applications", "Task"], ["color constancy", "Task"], ["multi-view color constancy problem", "Task"], ["method", "Generic"], ["estimates of underlying surface re-flectance", "Task"], ["surface properties", "OtherScientificTerm"], ["illuminants", "OtherScientificTerm"], ["method", "Generic"], ["image correspondences", "OtherScientificTerm"], ["alignment techniques", "Method"], ["matching local region features", "OtherScientificTerm"], ["multi-view constraints", "OtherScientificTerm"], ["estimates of both scene illuminants and object color ( surface reflectance )", "Task"], ["baseline single-view method", "Method"]], "rel": [["methods", "Used-for", "color constancy"], ["methods", "Used-for", "surface re-flectance estimates"], ["uncalibrated images", "Used-for", "surface re-flectance estimates"], ["method", "Used-for", "estimates of underlying surface re-flectance"], ["method", "Used-for", "image correspondences"], ["alignment techniques", "Used-for", "image correspondences"], ["multi-view constraints", "Used-for", "estimates of both scene illuminants and object color ( surface reflectance )"], ["baseline single-view method", "Compare", "multi-view constraints"]], "rel_plus": [["methods:Generic", "Used-for", "color constancy:Task"], ["methods:Generic", "Used-for", "surface re-flectance estimates:Method"], ["uncalibrated images:Material", "Used-for", "surface re-flectance estimates:Method"], ["method:Generic", "Used-for", "estimates of underlying surface re-flectance:Task"], ["method:Generic", "Used-for", "image correspondences:OtherScientificTerm"], ["alignment techniques:Method", "Used-for", "image correspondences:OtherScientificTerm"], ["multi-view constraints:OtherScientificTerm", "Used-for", "estimates of both scene illuminants and object color ( surface reflectance ):Task"], ["baseline single-view method:Method", "Compare", "multi-view constraints:OtherScientificTerm"]]}
{"doc_id": "P05-3001", "sentence": "We describe a dialogue system that works with its interlocutor to identify objects . Our contributions include a concise , modular architecture with reversible processes of understanding and generation , an information-state model of reference , and flexible links between semantics and collaborative problem solving .", "ner": [["dialogue system", "Method"], ["concise , modular architecture", "Method"], ["understanding", "Task"], ["generation", "Task"], ["information-state model of reference", "Method"], ["semantics", "OtherScientificTerm"], ["collaborative problem solving", "Task"]], "rel": [["concise , modular architecture", "Used-for", "understanding"], ["concise , modular architecture", "Used-for", "generation"], ["understanding", "Conjunction", "generation"]], "rel_plus": [["concise , modular architecture:Method", "Used-for", "understanding:Task"], ["concise , modular architecture:Method", "Used-for", "generation:Task"], ["understanding:Task", "Conjunction", "generation:Task"]]}
