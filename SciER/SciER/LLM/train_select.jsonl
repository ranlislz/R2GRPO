{"doc_id": "208202241", "sentence": "However , the experiment on Charades exhibits a contrary result , KSSNet ( knowledge graph ) outperforms KSSNet ( statistical graph ) by a mAP of 0. 4 .", "ner": [["Charades", "Dataset"], ["KSSNet", "Method"], ["knowledge graph", "Method"], ["KSSNet", "Method"], ["statistical graph", "Method"]], "rel": [["KSSNet", "Evaluated-With", "Charades"], ["KSSNet", "Evaluated-With", "Charades"], ["knowledge graph", "Part-Of", "KSSNet"], ["statistical graph", "Part-Of", "KSSNet"]], "rel_plus": [["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["knowledge graph:Method", "Part-Of", "KSSNet:Method"], ["statistical graph:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "6116678", "sentence": "The loss function for fine - tuning the deep contrast network ( the first stream ) and the fusing weights is the cross entropy between the ground truth and the fused saliency map ( S ): where G is the groundtruth label , W denotes the collection of all network parameters in MS - FCN and the fusion layer , \u03b2 i is a weight balancing the number of salient pixels and unsalient ones , and |I| , |I| and |I| + denote the total number of pixels , unsalient pixels and salient pixels in image I , respectively .", "ner": [["deep contrast network", "Method"], ["cross entropy", "Method"], ["fused saliency map", "Method"], ["MS - FCN", "Method"], ["fusion layer", "Method"]], "rel": [["cross entropy", "Part-Of", "deep contrast network"], ["fused saliency map", "Part-Of", "deep contrast network"]], "rel_plus": [["cross entropy:Method", "Part-Of", "deep contrast network:Method"], ["fused saliency map:Method", "Part-Of", "deep contrast network:Method"]]}
{"doc_id": "52910494", "sentence": "In this paper , we set compression factor as 1. 0 in standard DenseNet while set as 0. 5 in the structure of DenseNet with bottleneck and compression ( DenseNet - BC ) .", "ner": [["DenseNet", "Method"], ["DenseNet with bottleneck and compression", "Method"], ["DenseNet - BC", "Method"]], "rel": [["DenseNet - BC", "Synonym-Of", "DenseNet with bottleneck and compression"]], "rel_plus": [["DenseNet - BC:Method", "Synonym-Of", "DenseNet with bottleneck and compression:Method"]]}
{"doc_id": "146120936", "sentence": "Future directions include exploring the applicability of CARAFE in low - level vision tasks such as image restoration and super - resolution .", "ner": [["CARAFE", "Method"], ["image restoration", "Task"], ["super - resolution", "Task"]], "rel": [["CARAFE", "Used-For", "image restoration"], ["CARAFE", "Used-For", "super - resolution"]], "rel_plus": [["CARAFE:Method", "Used-For", "image restoration:Task"], ["CARAFE:Method", "Used-For", "super - resolution:Task"]]}
{"doc_id": "210164920", "sentence": "In [ 7 0 ] , the authors have used multi - scale CNN for scene labeling and achieve state - of - the - art results in the Sift flow [ 7 1 ] , the Bercelona dataset [ 7 2 ] and the Standford background dataset [ 7 3 ] .", "ner": [["CNN", "Method"], ["scene labeling", "Task"], ["Sift flow", "Dataset"], ["Bercelona", "Dataset"], ["Standford background", "Dataset"]], "rel": [["CNN", "Used-For", "scene labeling"], ["Sift flow", "Benchmark-For", "scene labeling"], ["Bercelona", "Benchmark-For", "scene labeling"], ["Standford background", "Benchmark-For", "scene labeling"], ["CNN", "Evaluated-With", "Sift flow"], ["CNN", "Evaluated-With", "Bercelona"], ["CNN", "Evaluated-With", "Standford background"]], "rel_plus": [["CNN:Method", "Used-For", "scene labeling:Task"], ["Sift flow:Dataset", "Benchmark-For", "scene labeling:Task"], ["Bercelona:Dataset", "Benchmark-For", "scene labeling:Task"], ["Standford background:Dataset", "Benchmark-For", "scene labeling:Task"], ["CNN:Method", "Evaluated-With", "Sift flow:Dataset"], ["CNN:Method", "Evaluated-With", "Bercelona:Dataset"], ["CNN:Method", "Evaluated-With", "Standford background:Dataset"]]}
{"doc_id": "195347056", "sentence": "Caltech - UCSD Birds - 2 0 0 - 2 0 1 1 ( CUB - 2 0 0 ) [ 2 1 ] is an image dataset with 2 0 0 bird species and a total of 1 1 , 7 8 8 images .", "ner": [["Caltech - UCSD Birds - 2 0 0 - 2 0 1 1", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "Second , we analyze the results of the often adopted domain adaptation strategy denoted as SGAN - S Uncond . ( for unconditioned discriminator ) in Table VI - B and Figure 1 1 .", "ner": [["domain adaptation", "Method"], ["SGAN - S Uncond .", "Method"], ["unconditioned discriminator", "Method"]], "rel": [["unconditioned discriminator", "Part-Of", "SGAN - S Uncond ."], ["domain adaptation", "Used-For", "SGAN - S Uncond ."]], "rel_plus": [["unconditioned discriminator:Method", "Part-Of", "SGAN - S Uncond .:Method"], ["domain adaptation:Method", "Used-For", "SGAN - S Uncond .:Method"]]}
{"doc_id": "209532167", "sentence": "BERT ( Devlin et al. , 2 0 1 9 ) improved natural - language pre - training by using a denoising autoencoder .", "ner": [["BERT", "Method"], ["denoising autoencoder", "Method"]], "rel": [["denoising autoencoder", "Part-Of", "BERT"]], "rel_plus": [["denoising autoencoder:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "211010520", "sentence": "For example , RoBERTa trained on D RoBERTa achieves 7 1 . 4 , 5 3 . 5 , 4 8 . 6 and 3 8 . 9 F 1 when evaluated on D SQuAD , D BiDAF , D BERT and D RoBERTa , respectively .", "ner": [["RoBERTa", "Method"], ["D RoBERTa", "Dataset"], ["D SQuAD", "Dataset"], ["D BiDAF", "Dataset"], ["D BERT", "Dataset"], ["D RoBERTa", "Dataset"]], "rel": [["RoBERTa", "Trained-With", "D RoBERTa"], ["RoBERTa", "Evaluated-With", "D SQuAD"], ["RoBERTa", "Evaluated-With", "D BiDAF"], ["RoBERTa", "Evaluated-With", "D BERT"], ["RoBERTa", "Evaluated-With", "D RoBERTa"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D SQuAD:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D BERT:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"]]}
{"doc_id": "210164920", "sentence": "Chen et al. has brought together methods from Deep Convolutional Neural Network(DCNN ) and probabilistic graphical model , and produced DeepLab [ 8 2 ] to address semantic segmentation .", "ner": [["Deep Convolutional Neural Network(DCNN )", "Method"], ["probabilistic graphical model", "Method"], ["DeepLab", "Method"], ["semantic segmentation", "Task"]], "rel": [["Deep Convolutional Neural Network(DCNN )", "Used-For", "semantic segmentation"], ["probabilistic graphical model", "Used-For", "semantic segmentation"], ["DeepLab", "Used-For", "semantic segmentation"]], "rel_plus": [["Deep Convolutional Neural Network(DCNN ):Method", "Used-For", "semantic segmentation:Task"], ["probabilistic graphical model:Method", "Used-For", "semantic segmentation:Task"], ["DeepLab:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Large LR - ASPP is 3 0 \\% faster than MobileNetV 2 R - ASPP at similar accuracy for Cityscapes segmentation .", "ner": [["MobileNetV 3 - Large LR - ASPP", "Method"], ["MobileNetV 2 R - ASPP", "Method"], ["Cityscapes", "Dataset"], ["segmentation", "Task"]], "rel": [["MobileNetV 3 - Large LR - ASPP", "Compare-With", "MobileNetV 2 R - ASPP"], ["MobileNetV 2 R - ASPP", "Evaluated-With", "Cityscapes"], ["MobileNetV 3 - Large LR - ASPP", "Evaluated-With", "Cityscapes"], ["Cityscapes", "Benchmark-For", "segmentation"]], "rel_plus": [["MobileNetV 3 - Large LR - ASPP:Method", "Compare-With", "MobileNetV 2 R - ASPP:Method"], ["MobileNetV 2 R - ASPP:Method", "Evaluated-With", "Cityscapes:Dataset"], ["MobileNetV 3 - Large LR - ASPP:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "198897554", "sentence": "As , in this case , the decoder of the segmentation network is simply an up - sampling layer without any learnable parameter , the Final layer is actually the prediction layer of the segmentation network .", "ner": [["segmentation network", "Method"], ["up - sampling layer", "Method"], ["prediction layer", "Method"], ["segmentation network", "Method"]], "rel": [["up - sampling layer", "Part-Of", "segmentation network"], ["prediction layer", "Part-Of", "segmentation network"]], "rel_plus": [["up - sampling layer:Method", "Part-Of", "segmentation network:Method"], ["prediction layer:Method", "Part-Of", "segmentation network:Method"]]}
{"doc_id": "56657874", "sentence": "Zhu et al. [ 4 5 ] proposed CycleGAN that learned a mapping between an input image and an output image using both adversarial and cycle consistency loss where unpaired data were used for image generation .", "ner": [["CycleGAN", "Method"], ["cycle consistency loss", "Method"], ["image generation", "Task"]], "rel": [["cycle consistency loss", "Part-Of", "CycleGAN"], ["CycleGAN", "Used-For", "image generation"]], "rel_plus": [["cycle consistency loss:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Used-For", "image generation:Task"]]}
{"doc_id": "67855714", "sentence": "Table 4 : Comparison of our methods P sisr /adv , rec and P sisr /dis , rec with baselines and the SRGAN method [ 1 ] on the datasets ImageNet ( a subset of 2 0 0 , 0 0 0 randomely selected images ) and DTD , in terms of classical metrics ( L 2 and SSIM ) and perceptual metrics [ 4 4 ] . in figure 7 .", "ner": [["P sisr /adv", "Method"], ["rec and P sisr /dis", "Method"], ["SRGAN", "Method"], ["ImageNet", "Dataset"], ["DTD", "Dataset"]], "rel": [["P sisr /adv", "Compare-With", "rec and P sisr /dis"], ["P sisr /adv", "Compare-With", "SRGAN"], ["rec and P sisr /dis", "Compare-With", "SRGAN"], ["SRGAN", "Evaluated-With", "ImageNet"], ["rec and P sisr /dis", "Evaluated-With", "ImageNet"], ["P sisr /adv", "Evaluated-With", "ImageNet"], ["SRGAN", "Evaluated-With", "DTD"], ["rec and P sisr /dis", "Evaluated-With", "DTD"], ["P sisr /adv", "Evaluated-With", "DTD"]], "rel_plus": [["P sisr /adv:Method", "Compare-With", "rec and P sisr /dis:Method"], ["P sisr /adv:Method", "Compare-With", "SRGAN:Method"], ["rec and P sisr /dis:Method", "Compare-With", "SRGAN:Method"], ["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["rec and P sisr /dis:Method", "Evaluated-With", "ImageNet:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "ImageNet:Dataset"], ["SRGAN:Method", "Evaluated-With", "DTD:Dataset"], ["rec and P sisr /dis:Method", "Evaluated-With", "DTD:Dataset"], ["P sisr /adv:Method", "Evaluated-With", "DTD:Dataset"]]}
{"doc_id": "198147921", "sentence": "Lahoud and Ghanem [ 4 5 ] , on the other hand , use the 2D Faster R - CNN [ 6 9 ] and VGG - 1 6 [ 7 7 ] , pre - trained on the 2D ImageNet database [ 7 2 ] , to position 2D bounding boxes around possible objects with high accuracy and efficiency .", "ner": [["2D Faster R - CNN", "Method"], ["VGG - 1 6", "Method"], ["2D ImageNet database", "Dataset"]], "rel": [["2D Faster R - CNN", "Trained-With", "2D ImageNet database"], ["VGG - 1 6", "Trained-With", "2D ImageNet database"]], "rel_plus": [["2D Faster R - CNN:Method", "Trained-With", "2D ImageNet database:Dataset"], ["VGG - 1 6:Method", "Trained-With", "2D ImageNet database:Dataset"]]}
{"doc_id": "203593581", "sentence": "As for 3 bit , APoT resembles PoT , therefore we evaluate APoT quantization and uniform quantization .", "ner": [["APoT", "Method"], ["PoT", "Method"], ["APoT quantization", "Method"], ["uniform quantization", "Method"]], "rel": [["PoT", "Part-Of", "APoT"]], "rel_plus": [["PoT:Method", "Part-Of", "APoT:Method"]]}
{"doc_id": "102351044", "sentence": "We attribute the failure of the standard dropout to the incorrect placement of drop - operations and propose general convolutional building blocks with drop - operations incorporated right before each convolutional layer in Figure 5b , BOTH FOR drop - channel and drop - neuron .", "ner": [["dropout", "Method"], ["drop - operations", "Method"], ["convolutional building blocks", "Method"], ["drop - operations", "Method"], ["convolutional layer", "Method"], ["drop - channel", "Method"], ["drop - neuron", "Method"]], "rel": [["drop - operations", "Part-Of", "convolutional building blocks"], ["drop - channel", "Part-Of", "convolutional building blocks"], ["drop - neuron", "Part-Of", "convolutional building blocks"], ["convolutional layer", "Part-Of", "convolutional building blocks"], ["drop - neuron", "SubClass-Of", "drop - operations"], ["drop - channel", "SubClass-Of", "drop - operations"]], "rel_plus": [["drop - operations:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - channel:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["convolutional layer:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operations:Method"], ["drop - channel:Method", "SubClass-Of", "drop - operations:Method"]]}
{"doc_id": "44148233", "sentence": "For activity detection , features like Spatiotemporal Interest Points such as Histogram of Oriented Optical Flow ( HOOF ) [ 3 2 ] , Bayesian Networks ( BN ) [ 7 2 ] , Dynamic Bayesian Networks ( DBNs ) [ 5 9 ] , Hidden Markov Models ( HMM ) [ 2 7 ] , state machines [ 8 5 ] , and PNF Networks [ 1 2 1 ] have been used by SVO approaches . \u2022 Integrated Approaches : Instead of detecting the description - relevant entities separately , Stochastic Attribute Image Grammar ( SAIG ) [ 1 9 2 ] and Stochastic Context Free Grammars ( SCFG ) [ 1 1 0 ] , allow for compositional representation of visual entities present in a video , an image or a scene based on their spatial and functional relations .", "ner": [["activity detection", "Task"], ["Spatiotemporal Interest Points", "Method"], ["Histogram of Oriented Optical Flow", "Method"], ["HOOF", "Method"], ["Bayesian Networks", "Method"], ["BN", "Method"], ["Dynamic Bayesian Networks", "Method"], ["DBNs", "Method"], ["Hidden Markov Models", "Method"], ["HMM", "Method"], ["state machines", "Method"], ["PNF Networks", "Method"], ["SVO", "Method"], ["Stochastic Attribute Image Grammar", "Method"], ["SAIG", "Method"], ["Stochastic Context Free Grammars", "Method"], ["SCFG", "Method"]], "rel": [["Histogram of Oriented Optical Flow", "SubClass-Of", "Spatiotemporal Interest Points"], ["Bayesian Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["Dynamic Bayesian Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["Hidden Markov Models", "SubClass-Of", "Spatiotemporal Interest Points"], ["state machines", "SubClass-Of", "Spatiotemporal Interest Points"], ["PNF Networks", "SubClass-Of", "Spatiotemporal Interest Points"], ["HOOF", "Synonym-Of", "Histogram of Oriented Optical Flow"], ["BN", "Synonym-Of", "Bayesian Networks"], ["DBNs", "Synonym-Of", "Dynamic Bayesian Networks"], ["HMM", "Synonym-Of", "Hidden Markov Models"], ["PNF Networks", "Part-Of", "SVO"], ["state machines", "Part-Of", "SVO"], ["Hidden Markov Models", "Part-Of", "SVO"], ["Dynamic Bayesian Networks", "Part-Of", "SVO"], ["Bayesian Networks", "Part-Of", "SVO"], ["Histogram of Oriented Optical Flow", "Part-Of", "SVO"], ["SAIG", "Synonym-Of", "Stochastic Attribute Image Grammar"], ["SCFG", "Synonym-Of", "Stochastic Context Free Grammars"]], "rel_plus": [["Histogram of Oriented Optical Flow:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Bayesian Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Dynamic Bayesian Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["Hidden Markov Models:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["state machines:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["PNF Networks:Method", "SubClass-Of", "Spatiotemporal Interest Points:Method"], ["HOOF:Method", "Synonym-Of", "Histogram of Oriented Optical Flow:Method"], ["BN:Method", "Synonym-Of", "Bayesian Networks:Method"], ["DBNs:Method", "Synonym-Of", "Dynamic Bayesian Networks:Method"], ["HMM:Method", "Synonym-Of", "Hidden Markov Models:Method"], ["PNF Networks:Method", "Part-Of", "SVO:Method"], ["state machines:Method", "Part-Of", "SVO:Method"], ["Hidden Markov Models:Method", "Part-Of", "SVO:Method"], ["Dynamic Bayesian Networks:Method", "Part-Of", "SVO:Method"], ["Bayesian Networks:Method", "Part-Of", "SVO:Method"], ["Histogram of Oriented Optical Flow:Method", "Part-Of", "SVO:Method"], ["SAIG:Method", "Synonym-Of", "Stochastic Attribute Image Grammar:Method"], ["SCFG:Method", "Synonym-Of", "Stochastic Context Free Grammars:Method"]]}
{"doc_id": "201124533", "sentence": "We perform the ablation study for the components in HRNet over two tasks : human pose estimation on COCO validation and semantic segmentation on Cityscapes validation .", "ner": [["HRNet", "Method"], ["human pose estimation", "Task"], ["COCO validation", "Dataset"], ["semantic segmentation", "Task"], ["Cityscapes validation", "Dataset"]], "rel": [["HRNet", "Used-For", "human pose estimation"], ["COCO validation", "Benchmark-For", "human pose estimation"], ["HRNet", "Evaluated-With", "COCO validation"], ["HRNet", "Used-For", "semantic segmentation"], ["Cityscapes validation", "Benchmark-For", "semantic segmentation"], ["HRNet", "Evaluated-With", "Cityscapes validation"]], "rel_plus": [["HRNet:Method", "Used-For", "human pose estimation:Task"], ["COCO validation:Dataset", "Benchmark-For", "human pose estimation:Task"], ["HRNet:Method", "Evaluated-With", "COCO validation:Dataset"], ["HRNet:Method", "Used-For", "semantic segmentation:Task"], ["Cityscapes validation:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["HRNet:Method", "Evaluated-With", "Cityscapes validation:Dataset"]]}
{"doc_id": "4319457", "sentence": "We could even extend this , using the discriminator in a GAN as a measure for the reconstruction objective , as introduced by Larsen et al. [ 2 0 1 6 ] .", "ner": [["discriminator", "Method"], ["GAN", "Method"], ["reconstruction", "Task"]], "rel": [["discriminator", "Part-Of", "GAN"], ["discriminator", "Used-For", "reconstruction"]], "rel_plus": [["discriminator:Method", "Part-Of", "GAN:Method"], ["discriminator:Method", "Used-For", "reconstruction:Task"]]}
{"doc_id": "210164920", "sentence": "The authors have used Faster R - CNN [ 1 9 ] ( ResNet - 1 0 1 [ 1 0 ] based ) for predicting bounding boxes for object instances .", "ner": [["Faster R - CNN", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "Faster R - CNN"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "Faster R - CNN:Method"]]}
{"doc_id": "210713911", "sentence": "R 5 0 is a simple notation for ResNet - 5 0 .", "ner": [["R 5 0", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["R 5 0", "Synonym-Of", "ResNet - 5 0"]], "rel_plus": [["R 5 0:Method", "Synonym-Of", "ResNet - 5 0:Method"]]}
{"doc_id": "4246700", "sentence": "This section comprehensively reviews the existing image captioning including natural image captioning and remote sensing image captioning .", "ner": [["image captioning", "Task"], ["natural image captioning", "Task"], ["remote sensing image captioning", "Task"]], "rel": [["natural image captioning", "SubTask-Of", "image captioning"], ["remote sensing image captioning", "SubTask-Of", "image captioning"]], "rel_plus": [["natural image captioning:Task", "SubTask-Of", "image captioning:Task"], ["remote sensing image captioning:Task", "SubTask-Of", "image captioning:Task"]]}
{"doc_id": "195347056", "sentence": "In addition , we also trained the model on Oxford - 1 0 2 [ 2 0 ] and CUB - 2 0 0 [ 2 1 ] for additional performance assessments .", "ner": [["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "208202241", "sentence": "The backbone of KSSNet is ResNet 1 0 1 and Inception - I 3 d for MS - COCO and Charades , respectively .", "ner": [["KSSNet", "Method"], ["ResNet 1 0 1", "Method"], ["Inception - I 3 d", "Method"], ["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [["ResNet 1 0 1", "Part-Of", "KSSNet"], ["Inception - I 3 d", "Part-Of", "KSSNet"], ["KSSNet", "Trained-With", "MS - COCO"], ["KSSNet", "Evaluated-With", "Charades"]], "rel_plus": [["ResNet 1 0 1:Method", "Part-Of", "KSSNet:Method"], ["Inception - I 3 d:Method", "Part-Of", "KSSNet:Method"], ["KSSNet:Method", "Trained-With", "MS - COCO:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"]]}
{"doc_id": "199668978", "sentence": "Tuan et al [ 3 4 ] apply the o -policy gradient method to the sequence generation task and shows that PPO surpass policy gradient on stability and performance .", "ner": [["o -policy gradient method", "Method"], ["sequence generation", "Task"], ["PPO", "Method"], ["policy gradient", "Method"]], "rel": [["o -policy gradient method", "Used-For", "sequence generation"], ["PPO", "Compare-With", "policy gradient"]], "rel_plus": [["o -policy gradient method:Method", "Used-For", "sequence generation:Task"], ["PPO:Method", "Compare-With", "policy gradient:Method"]]}
{"doc_id": "102351044", "sentence": "Dropout [ 2 8 , 9 ] is initially introduced in fully connected layers [ 1 9 ] of neural networks .", "ner": [["Dropout", "Method"], ["fully connected layers", "Method"], ["neural networks", "Method"]], "rel": [["Dropout", "Part-Of", "fully connected layers"], ["fully connected layers", "Part-Of", "neural networks"]], "rel_plus": [["Dropout:Method", "Part-Of", "fully connected layers:Method"], ["fully connected layers:Method", "Part-Of", "neural networks:Method"]]}
{"doc_id": "44148233", "sentence": "However , the advent of deep learning and the tremendous advancements in CV and NLP have equally affected the area of video captioning .", "ner": [["deep learning", "Method"], ["CV", "Task"], ["NLP", "Task"], ["video captioning", "Task"]], "rel": [["deep learning", "Used-For", "CV"], ["deep learning", "Used-For", "NLP"], ["deep learning", "Used-For", "video captioning"]], "rel_plus": [["deep learning:Method", "Used-For", "CV:Task"], ["deep learning:Method", "Used-For", "NLP:Task"], ["deep learning:Method", "Used-For", "video captioning:Task"]]}
{"doc_id": "210860962", "sentence": "As mentioned in Section II - B , a common approach for tackling the semantic segmentation problem is the use of FCNs that allow to obtain a segmentation mask directly from an image in an end - to - end way .", "ner": [["semantic segmentation", "Task"], ["FCNs", "Method"]], "rel": [["FCNs", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "Video Object Segmentation ( VOS ) aims at automatically generating accurate pixel masks for objects in each frame of a video , then associating those proposed object pixel masks in the successive frames to obtain temporally consistent tracks .", "ner": [["Video Object Segmentation", "Task"], ["VOS", "Task"]], "rel": [["VOS", "Synonym-Of", "Video Object Segmentation"]], "rel_plus": [["VOS:Task", "Synonym-Of", "Video Object Segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "COCO : To further verify the generality of GALD , we conduct the experiments on instance segmentation task on MS COCO based on the state - of - the - art method Mask R - CNN .", "ner": [["COCO", "Dataset"], ["GALD", "Method"], ["instance segmentation", "Task"], ["MS COCO", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["MS COCO", "Benchmark-For", "instance segmentation"], ["Mask R - CNN", "Evaluated-With", "MS COCO"], ["GALD", "Part-Of", "Mask R - CNN"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "146120936", "sentence": "On the one hand , for the decoders in dense prediction tasks ( e.g. super resolution [ 6 , 1 7 ] , inpainting [ 1 1 , 2 9 ] and semantic segmentation [ 3 9 , 4 ] ) , the high - level/low - res feature map is upsampled to match the high - resolution supervision .", "ner": [["dense prediction tasks", "Task"], ["super resolution", "Task"], ["inpainting", "Task"], ["semantic segmentation", "Task"]], "rel": [["super resolution", "SubTask-Of", "dense prediction tasks"], ["inpainting", "SubTask-Of", "dense prediction tasks"], ["semantic segmentation", "SubTask-Of", "dense prediction tasks"]], "rel_plus": [["super resolution:Task", "SubTask-Of", "dense prediction tasks:Task"], ["inpainting:Task", "SubTask-Of", "dense prediction tasks:Task"], ["semantic segmentation:Task", "SubTask-Of", "dense prediction tasks:Task"]]}
{"doc_id": "202577400", "sentence": "With GALD , Mask R - CNN can find objects that are missed in baseline ( e.g. , the \" light \" in the third column ) , resolve ambiguity in region classification ( e.g. , the \" bed \" in the first column ) and help to better estimate the spatial contents for objects ( e.g. , \" bear \" in last column ) .", "ner": [["GALD", "Method"], ["Mask R - CNN", "Method"], ["region classification", "Task"]], "rel": [["GALD", "Part-Of", "Mask R - CNN"], ["Mask R - CNN", "Used-For", "region classification"], ["GALD", "Used-For", "region classification"]], "rel_plus": [["GALD:Method", "Part-Of", "Mask R - CNN:Method"], ["Mask R - CNN:Method", "Used-For", "region classification:Task"], ["GALD:Method", "Used-For", "region classification:Task"]]}
{"doc_id": "204402755", "sentence": "The STL - 1 0 dataset presents a different challenge with a significantly smaller number of images than the CIFAR datasets , but in higher resolution .", "ner": [["STL - 1 0", "Dataset"], ["CIFAR", "Dataset"]], "rel": [["STL - 1 0", "Compare-With", "CIFAR"]], "rel_plus": [["STL - 1 0:Dataset", "Compare-With", "CIFAR:Dataset"]]}
{"doc_id": "28984897", "sentence": "The idea of the proposed approach is to use the maxout units and their model selection abilities for pruning entire neurons from an architecture without expensive processing .", "ner": [["maxout units", "Method"], ["model selection", "Task"]], "rel": [["maxout units", "Used-For", "model selection"]], "rel_plus": [["maxout units:Method", "Used-For", "model selection:Task"]]}
{"doc_id": "202540590", "sentence": "All automatic metric scores are averaged from 1 0 sets of sample output .   There have been many exciting new datasets developed for reading comprehension , such as SQuAD ( Rajpurkar et al. , 2 0 1 6 ) , NEWSQA ( Trischler et al. , 2 0 1 7 ) , SearchQA ( Dunn et al. , 2 0 1 7 ) , NarrativeQA ( Ko\u010disk\u1ef3 et al. , 2 0 1 8) , ProPara ( Mishra et al. , 2 0 1 8) , CoQA ( Reddy et al. , 2 0 1 8) , ReCoRD ( Zhang et al. , 2 0 1 8) , MCTest ( Richardson et al. , 2 0 1 3 ) , RACE ( Lai et al. , 2 0 1 7 ) , CNN/Daily Mail ( Hermann et al. , 2 0 1 5 ) , Children 's Book Test ( Hill et al. , 2 0 1 5 ) , and MCScript ( Ostermann et al. , 2 0 1 8) .", "ner": [["reading comprehension", "Task"], ["SQuAD", "Dataset"], ["NEWSQA", "Dataset"], ["SearchQA", "Dataset"], ["NarrativeQA", "Dataset"], ["ProPara", "Dataset"], ["CoQA", "Dataset"], ["ReCoRD", "Dataset"], ["MCTest", "Dataset"], ["RACE", "Dataset"], ["CNN/Daily Mail", "Dataset"], ["Children 's Book Test", "Dataset"], ["MCScript", "Dataset"]], "rel": [["SQuAD", "Benchmark-For", "reading comprehension"], ["NEWSQA", "Benchmark-For", "reading comprehension"], ["SearchQA", "Benchmark-For", "reading comprehension"], ["NarrativeQA", "Benchmark-For", "reading comprehension"], ["ProPara", "Benchmark-For", "reading comprehension"], ["CoQA", "Benchmark-For", "reading comprehension"], ["ReCoRD", "Benchmark-For", "reading comprehension"], ["MCTest", "Benchmark-For", "reading comprehension"], ["RACE", "Benchmark-For", "reading comprehension"], ["CNN/Daily Mail", "Benchmark-For", "reading comprehension"], ["Children 's Book Test", "Benchmark-For", "reading comprehension"], ["MCScript", "Benchmark-For", "reading comprehension"]], "rel_plus": [["SQuAD:Dataset", "Benchmark-For", "reading comprehension:Task"], ["NEWSQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["SearchQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["NarrativeQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["ProPara:Dataset", "Benchmark-For", "reading comprehension:Task"], ["CoQA:Dataset", "Benchmark-For", "reading comprehension:Task"], ["ReCoRD:Dataset", "Benchmark-For", "reading comprehension:Task"], ["MCTest:Dataset", "Benchmark-For", "reading comprehension:Task"], ["RACE:Dataset", "Benchmark-For", "reading comprehension:Task"], ["CNN/Daily Mail:Dataset", "Benchmark-For", "reading comprehension:Task"], ["Children 's Book Test:Dataset", "Benchmark-For", "reading comprehension:Task"], ["MCScript:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "4246700", "sentence": "The \" hard \" attention mechanism based on the convolutional features extracted by GoogLeNet gets the best result on UCM - captions dataset and RSICD dataset .", "ner": [["\" hard \" attention mechanism", "Method"], ["convolutional features", "Method"], ["GoogLeNet", "Method"], ["UCM - captions", "Dataset"], ["RSICD", "Dataset"]], "rel": [["convolutional features", "Used-For", "\" hard \" attention mechanism"], ["GoogLeNet", "Used-For", "convolutional features"], ["\" hard \" attention mechanism", "Evaluated-With", "UCM - captions"], ["\" hard \" attention mechanism", "Evaluated-With", "RSICD"]], "rel_plus": [["convolutional features:Method", "Used-For", "\" hard \" attention mechanism:Method"], ["GoogLeNet:Method", "Used-For", "convolutional features:Method"], ["\" hard \" attention mechanism:Method", "Evaluated-With", "UCM - captions:Dataset"], ["\" hard \" attention mechanism:Method", "Evaluated-With", "RSICD:Dataset"]]}
{"doc_id": "102351044", "sentence": "Third , the introduction of different levels of dropouts to convolutional layers of CNNs , especially drop - channel , provides a more general and effective regularization for CNNs which achieves stateof - the - art performance in a wide range of tasks , e.g. CIFAR - 1 0 , CIFAR - 1 0 0 and SVHN .", "ner": [["dropouts", "Method"], ["convolutional layers", "Method"], ["CNNs", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["drop - channel", "SubClass-Of", "dropouts"], ["dropouts", "Part-Of", "convolutional layers"], ["convolutional layers", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"], ["CNNs", "Evaluated-With", "CIFAR - 1 0"], ["CNNs", "Evaluated-With", "CIFAR - 1 0 0"], ["CNNs", "Evaluated-With", "SVHN"]], "rel_plus": [["drop - channel:Method", "SubClass-Of", "dropouts:Method"], ["dropouts:Method", "Part-Of", "convolutional layers:Method"], ["convolutional layers:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["CNNs:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["CNNs:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "146120936", "sentence": "To demonstrate the universal effectiveness of CARAFE , we conduct comprehensive evaluations across a wide range of dense prediction tasks , i.e. , object detection , instance segmentation , semantic segmentation , image inpainting , with mainstream architectures .", "ner": [["CARAFE", "Method"], ["dense prediction tasks", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"], ["semantic segmentation", "Task"], ["image inpainting", "Task"]], "rel": [["object detection", "Used-For", "dense prediction tasks"], ["instance segmentation", "Used-For", "dense prediction tasks"], ["semantic segmentation", "Used-For", "dense prediction tasks"], ["image inpainting", "Used-For", "dense prediction tasks"]], "rel_plus": [["object detection:Task", "Used-For", "dense prediction tasks:Task"], ["instance segmentation:Task", "Used-For", "dense prediction tasks:Task"], ["semantic segmentation:Task", "Used-For", "dense prediction tasks:Task"], ["image inpainting:Task", "Used-For", "dense prediction tasks:Task"]]}
{"doc_id": "199543700", "sentence": "We also compare the proposed SGGAN approach with the leading supervised deep learning methods on facial attribute recognition problem with the CelebFaces Attributes Dataset ( CelebA ) dataset [ 4 0 ] and the Labeled Faces in the Wild - a ( LFW - a ) dataset [ 2 4 ] .", "ner": [["SGGAN", "Method"], ["supervised deep learning", "Method"], ["facial attribute recognition", "Task"], ["CelebFaces Attributes Dataset", "Dataset"], ["CelebA", "Dataset"], ["Labeled Faces in the Wild - a", "Dataset"], ["LFW - a", "Dataset"]], "rel": [["SGGAN", "Compare-With", "supervised deep learning"], ["SGGAN", "Used-For", "facial attribute recognition"], ["CelebFaces Attributes Dataset", "Benchmark-For", "facial attribute recognition"], ["Labeled Faces in the Wild - a", "Benchmark-For", "facial attribute recognition"], ["supervised deep learning", "Used-For", "facial attribute recognition"], ["CelebA", "Synonym-Of", "CelebFaces Attributes Dataset"], ["SGGAN", "Evaluated-With", "CelebFaces Attributes Dataset"], ["supervised deep learning", "Evaluated-With", "CelebFaces Attributes Dataset"], ["LFW - a", "Synonym-Of", "Labeled Faces in the Wild - a"], ["SGGAN", "Evaluated-With", "Labeled Faces in the Wild - a"], ["supervised deep learning", "Evaluated-With", "Labeled Faces in the Wild - a"]], "rel_plus": [["SGGAN:Method", "Compare-With", "supervised deep learning:Method"], ["SGGAN:Method", "Used-For", "facial attribute recognition:Task"], ["CelebFaces Attributes Dataset:Dataset", "Benchmark-For", "facial attribute recognition:Task"], ["Labeled Faces in the Wild - a:Dataset", "Benchmark-For", "facial attribute recognition:Task"], ["supervised deep learning:Method", "Used-For", "facial attribute recognition:Task"], ["CelebA:Dataset", "Synonym-Of", "CelebFaces Attributes Dataset:Dataset"], ["SGGAN:Method", "Evaluated-With", "CelebFaces Attributes Dataset:Dataset"], ["supervised deep learning:Method", "Evaluated-With", "CelebFaces Attributes Dataset:Dataset"], ["LFW - a:Dataset", "Synonym-Of", "Labeled Faces in the Wild - a:Dataset"], ["SGGAN:Method", "Evaluated-With", "Labeled Faces in the Wild - a:Dataset"], ["supervised deep learning:Method", "Evaluated-With", "Labeled Faces in the Wild - a:Dataset"]]}
{"doc_id": "AUG150", "sentence": "ImageNet Large Scale Visual Recognition Challenge, known as ImageNet, is improved by VGG for image classification.", "ner": [["ImageNet Large Scale Visual Recognition Challenge", "Dataset"], ["ImageNet", "Dataset"], ["VGG", "Method"], ["image classification", "Task"]], "rel": [["ImageNet", "Synonym-Of", "ImageNet Large Scale Visual Recognition Challenge"], ["VGG", "Used-For", "image classification"], ["VGG", "Evaluated-With", "ImageNet"]], "rel_plus": [["ImageNet:Dataset", "Synonym-Of", "ImageNet Large Scale Visual Recognition Challenge:Dataset"], ["VGG:Method", "Used-For", "image classification:Task"], ["VGG:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "208202241", "sentence": "KSSNet ( 3 layers ) achieves better performance than KSSNet ( 2 layers ) by absolute mAP improvements of 0. 6 % and 1. 9 % in MS - COCO and Charades .", "ner": [["KSSNet ( 3 layers )", "Method"], ["KSSNet ( 2 layers )", "Method"], ["MS - COCO", "Dataset"], ["Charades", "Dataset"]], "rel": [["KSSNet ( 3 layers )", "Compare-With", "KSSNet ( 2 layers )"], ["KSSNet ( 3 layers )", "Evaluated-With", "MS - COCO"], ["KSSNet ( 2 layers )", "Evaluated-With", "Charades"]], "rel_plus": [["KSSNet ( 3 layers ):Method", "Compare-With", "KSSNet ( 2 layers ):Method"], ["KSSNet ( 3 layers ):Method", "Evaluated-With", "MS - COCO:Dataset"], ["KSSNet ( 2 layers ):Method", "Evaluated-With", "Charades:Dataset"]]}
{"doc_id": "210860760", "sentence": "At the end , we conduct extensive experiments to validate the effectiveness of ExEm through assessing its performance over the multi - label classification , link prediction , and recommendation tasks on common datasets and our collected data formed by crawling the vast author Scopus profiles .", "ner": [["ExEm", "Method"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [["ExEm", "Used-For", "multi - label classification"], ["ExEm", "Used-For", "link prediction"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Used-For", "multi - label classification:Task"], ["ExEm:Method", "Used-For", "link prediction:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "44148233", "sentence": "Long Short - Term Memory ( LSTMs ) [ 7 1 ] and the more general deep Recurrent Neural Networks ( RNNs ) , on the other hand , are now dominating the area of sequence modeling , setting new benchmarks in machine translation [ 3 7 ] , [ 1 5 0 ] , speech recognition [ 6 2 ] and the closely related task of image captioning [ 4 5 ] , [ 1 6 3 ] .", "ner": [["Long Short - Term Memory", "Method"], ["LSTMs", "Method"], ["Recurrent Neural Networks", "Method"], ["RNNs", "Method"], ["sequence modeling", "Task"], ["machine translation", "Task"], ["speech recognition", "Task"], ["image captioning", "Task"]], "rel": [["LSTMs", "Synonym-Of", "Long Short - Term Memory"], ["RNNs", "Synonym-Of", "Recurrent Neural Networks"], ["Recurrent Neural Networks", "Used-For", "sequence modeling"], ["Long Short - Term Memory", "Used-For", "sequence modeling"], ["Long Short - Term Memory", "Used-For", "machine translation"], ["Recurrent Neural Networks", "Used-For", "machine translation"], ["Long Short - Term Memory", "Used-For", "speech recognition"], ["Recurrent Neural Networks", "Used-For", "speech recognition"], ["Long Short - Term Memory", "Used-For", "image captioning"], ["Recurrent Neural Networks", "Used-For", "image captioning"]], "rel_plus": [["LSTMs:Method", "Synonym-Of", "Long Short - Term Memory:Method"], ["RNNs:Method", "Synonym-Of", "Recurrent Neural Networks:Method"], ["Recurrent Neural Networks:Method", "Used-For", "sequence modeling:Task"], ["Long Short - Term Memory:Method", "Used-For", "sequence modeling:Task"], ["Long Short - Term Memory:Method", "Used-For", "machine translation:Task"], ["Recurrent Neural Networks:Method", "Used-For", "machine translation:Task"], ["Long Short - Term Memory:Method", "Used-For", "speech recognition:Task"], ["Recurrent Neural Networks:Method", "Used-For", "speech recognition:Task"], ["Long Short - Term Memory:Method", "Used-For", "image captioning:Task"], ["Recurrent Neural Networks:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "4539700", "sentence": "In the last couple of years , ActivityNet [ 5 ] , which is a somewhat larger video dataset , has become available , and its use has make it possible to accomplish additional tasks such as untrimmed action classification and detection , but the number of action instances it contains is still limited .", "ner": [["ActivityNet", "Dataset"], ["action classification", "Task"], ["detection", "Task"]], "rel": [["ActivityNet", "Benchmark-For", "action classification"], ["ActivityNet", "Benchmark-For", "detection"]], "rel_plus": [["ActivityNet:Dataset", "Benchmark-For", "action classification:Task"], ["ActivityNet:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "201646309", "sentence": "RoBERTa showed , that the performance of BERT can further improved by small adaptations to the pre - training process .", "ner": [["RoBERTa", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Send X and Y to the BERT mechine and pretrained word embedding model to calculate the word - level reward 6 : Send X , Y and Y to the qa - lstm model to calculate the sentence - level reward , based on Eq. 7 7 : Calculate the advantage function of each time step according to Eq. 1 5 8 : repeat 9 : Update the policy p \u03b8 using Eq. 1 6 1 0 : until convergence 1 1 : Set old policy p \u03b8 ol d to policy p \u03b8 1 2 : end for A t is the expected advantage function ( the expected rewards minus a baseline like value function V ( k t ) of time k t ) which can be calculated as : To improve the exploration of our model for generating diverse yet coherent words that could constitute a better well - formed question , we use entropy regularization .", "ner": [["BERT", "Method"], ["pretrained word embedding model", "Method"], ["calculate the word - level reward", "Task"], ["entropy regularization", "Method"]], "rel": [["BERT", "Used-For", "calculate the word - level reward"], ["pretrained word embedding model", "Used-For", "calculate the word - level reward"]], "rel_plus": [["BERT:Method", "Used-For", "calculate the word - level reward:Task"], ["pretrained word embedding model:Method", "Used-For", "calculate the word - level reward:Task"]]}
{"doc_id": "211004033", "sentence": "Index Terms - Ellipse regression , occlusion handling , 3D object localization , object detection , convolutional neural networks .", "ner": [["Index Terms - Ellipse regression", "Task"], ["occlusion handling", "Task"], ["3D object localization", "Task"], ["object detection", "Task"], ["convolutional neural networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "Other works include : light upsample process [ 5 ] , [ 1 9 ] , [ 7 2 ] , [ 1 2 4 ] , possibly with dilated convolutions used in the backbone [ 4 7 ] , [ 6 9 ] , [ 9 1 ] ; light downsample and heavy upsample processes [ 1 1 5 ] , recombinator networks [ 4 0 ] ; improving skip connections with more or complicated convolutional units [ 4 8 ] , [ 8 9 ] , [ 1 4 3 ] , as well as sending information from low - resolution skip connections to highresolution skip connections [ 1 5 1 ] or exchanging information between them [ 3 4 ] ; studying the details of the upsample process [ 1 2 0 ] ; combining multi - scale pyramid representations [ 1 8 ] , [ 1 2 5 ] ; stacking multiple DeconvNets/UNets/Hourglass [ 3 1 ] , [ 1 2 2 ] with dense connections [ 1 1 0 ] .", "ner": [["light upsample process", "Method"], ["dilated convolutions", "Method"], ["light downsample and heavy upsample processes", "Method"], ["recombinator networks", "Method"], ["skip connections", "Method"], ["convolutional units", "Method"], ["low - resolution skip connections", "Method"], ["highresolution skip connections", "Method"], ["upsample process", "Method"], ["multi - scale pyramid representations", "Method"], ["DeconvNets/UNets/Hourglass", "Method"], ["dense connections", "Method"]], "rel": [["dilated convolutions", "Part-Of", "light upsample process"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "light upsample process:Method"]]}
{"doc_id": "202676714", "sentence": "Although SNC is more robust than Absum on CIFAR 1 0 and CIFAR 1 0 0 , clean accuracies of SNC are less than those of Absum and the computation time of SNC is larger than that of Absum as discussed below .", "ner": [["SNC", "Method"], ["Absum", "Method"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SNC", "Method"], ["Absum", "Method"], ["SNC", "Method"], ["Absum", "Method"]], "rel": [["SNC", "Compare-With", "Absum"], ["SNC", "Evaluated-With", "CIFAR 1 0"], ["Absum", "Evaluated-With", "CIFAR 1 0"], ["SNC", "Evaluated-With", "CIFAR 1 0 0"], ["Absum", "Evaluated-With", "CIFAR 1 0 0"], ["SNC", "Compare-With", "Absum"], ["SNC", "Compare-With", "Absum"]], "rel_plus": [["SNC:Method", "Compare-With", "Absum:Method"], ["SNC:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["Absum:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["SNC:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["Absum:Method", "Evaluated-With", "CIFAR 1 0 0:Dataset"], ["SNC:Method", "Compare-With", "Absum:Method"], ["SNC:Method", "Compare-With", "Absum:Method"]]}
{"doc_id": "210164920", "sentence": "FCN has used only local information for semantic segmentation but only local information makes semantic segmentation quite ambiguous .", "ner": [["FCN", "Method"], ["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [["FCN", "Used-For", "semantic segmentation"]], "rel_plus": [["FCN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210713911", "sentence": "Food - 1 0 1 1 0 0 Stanford Cars 1, 0 0 0 Oxford - Flowers 1, 0 0 0 FGVC Aircraft 8 0 0 Oxford - IIIT Pets 1, 3 0 0 We use the same hyperparameters for as all datasets as possible for transfer learning .", "ner": [["Food - 1 0 1", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford - Flowers", "Dataset"], ["FGVC Aircraft", "Dataset"], ["Oxford - IIIT Pets", "Dataset"], ["transfer learning", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "147703932", "sentence": "The best overall model is found by performing 2D pose and depth estimation along with segmentation . 2 ) 2D pose estimation : The results in Table III show the performance of the different multi - task models on 2D human pose estimation .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"], ["2D pose estimation", "Task"], ["2D human pose estimation", "Task"]], "rel": [["2D human pose estimation", "Synonym-Of", "2D pose estimation"]], "rel_plus": [["2D human pose estimation:Task", "Synonym-Of", "2D pose estimation:Task"]]}
{"doc_id": "202888986", "sentence": "That is , for ALBERT , we use a sentence - order prediction ( SOP ) loss , which avoids topic prediction and instead focuses on modeling inter - sentence coherence .", "ner": [["ALBERT", "Method"], ["sentence - order prediction", "Task"], ["SOP", "Task"]], "rel": [["SOP", "Synonym-Of", "sentence - order prediction"], ["ALBERT", "Trained-With", "sentence - order prediction"]], "rel_plus": [["SOP:Task", "Synonym-Of", "sentence - order prediction:Task"], ["ALBERT:Method", "Trained-With", "sentence - order prediction:Task"]]}
{"doc_id": "146808333", "sentence": "We were able to reduce the number of filters to 1 6 while maintaining the same accuracy as 3 2 filters using either ReLU or swish .", "ner": [["ReLU", "Method"], ["swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "We perform experiments using from - scratch , Word 2 Vec [ 3 5 ] , WordNet retrofitted Word 2 Vec [ 1 3 ] , Fast - Text [ 4 ] , Visual Word 2 Vec [ 2 6 ] , HGLMM ( 3 0 0 - D , 6K - D ) [ 2 4 ] , InferSent [ 8 ] , and BERT [ 9 ] representations in addition to a new embedding , GrOVLE , on five visionlanguage tasks : image - sentence retrieval , visual question answering , phrase grounding , image captioning , and textto - clip retrieval .", "ner": [["Word 2 Vec", "Method"], ["WordNet", "Dataset"], ["Word 2 Vec", "Method"], ["Fast - Text", "Method"], ["Visual Word 2 Vec", "Method"], ["HGLMM", "Method"], ["InferSent", "Method"], ["BERT", "Method"], ["GrOVLE", "Method"], ["image - sentence retrieval", "Task"], ["visual question answering", "Task"], ["phrase grounding", "Task"], ["image captioning", "Task"], ["textto - clip retrieval", "Task"]], "rel": [["Word 2 Vec", "Trained-With", "WordNet"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Fast - Text", "Used-For", "image - sentence retrieval"], ["InferSent", "Used-For", "image - sentence retrieval"], ["BERT", "Used-For", "image - sentence retrieval"], ["Visual Word 2 Vec", "Used-For", "image - sentence retrieval"], ["HGLMM", "Used-For", "image - sentence retrieval"], ["GrOVLE", "Used-For", "image - sentence retrieval"], ["Visual Word 2 Vec", "Used-For", "visual question answering"], ["Word 2 Vec", "Used-For", "visual question answering"], ["Word 2 Vec", "Used-For", "visual question answering"], ["Fast - Text", "Used-For", "visual question answering"], ["HGLMM", "Used-For", "visual question answering"], ["InferSent", "Used-For", "visual question answering"], ["BERT", "Used-For", "visual question answering"], ["HGLMM", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "phrase grounding"], ["Fast - Text", "Used-For", "phrase grounding"], ["InferSent", "Used-For", "phrase grounding"], ["BERT", "Used-For", "phrase grounding"], ["Word 2 Vec", "Used-For", "image captioning"], ["Word 2 Vec", "Used-For", "image captioning"], ["Fast - Text", "Used-For", "image captioning"], ["Visual Word 2 Vec", "Used-For", "image captioning"], ["HGLMM", "Used-For", "image captioning"], ["InferSent", "Used-For", "image captioning"], ["BERT", "Used-For", "image captioning"], ["Word 2 Vec", "Used-For", "textto - clip retrieval"], ["Word 2 Vec", "Used-For", "textto - clip retrieval"], ["Fast - Text", "Used-For", "textto - clip retrieval"], ["Visual Word 2 Vec", "Used-For", "textto - clip retrieval"], ["HGLMM", "Used-For", "textto - clip retrieval"], ["InferSent", "Used-For", "textto - clip retrieval"], ["BERT", "Used-For", "textto - clip retrieval"]], "rel_plus": [["Word 2 Vec:Method", "Trained-With", "WordNet:Dataset"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Fast - Text:Method", "Used-For", "image - sentence retrieval:Task"], ["InferSent:Method", "Used-For", "image - sentence retrieval:Task"], ["BERT:Method", "Used-For", "image - sentence retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["HGLMM:Method", "Used-For", "image - sentence retrieval:Task"], ["GrOVLE:Method", "Used-For", "image - sentence retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Word 2 Vec:Method", "Used-For", "visual question answering:Task"], ["Fast - Text:Method", "Used-For", "visual question answering:Task"], ["HGLMM:Method", "Used-For", "visual question answering:Task"], ["InferSent:Method", "Used-For", "visual question answering:Task"], ["BERT:Method", "Used-For", "visual question answering:Task"], ["HGLMM:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "phrase grounding:Task"], ["Fast - Text:Method", "Used-For", "phrase grounding:Task"], ["InferSent:Method", "Used-For", "phrase grounding:Task"], ["BERT:Method", "Used-For", "phrase grounding:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["Fast - Text:Method", "Used-For", "image captioning:Task"], ["Visual Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["HGLMM:Method", "Used-For", "image captioning:Task"], ["InferSent:Method", "Used-For", "image captioning:Task"], ["BERT:Method", "Used-For", "image captioning:Task"], ["Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["Fast - Text:Method", "Used-For", "textto - clip retrieval:Task"], ["Visual Word 2 Vec:Method", "Used-For", "textto - clip retrieval:Task"], ["HGLMM:Method", "Used-For", "textto - clip retrieval:Task"], ["InferSent:Method", "Used-For", "textto - clip retrieval:Task"], ["BERT:Method", "Used-For", "textto - clip retrieval:Task"]]}
{"doc_id": "210860760", "sentence": "Furthermore , the DeepWalk and Node 2 Vec approaches were used in order to compare them with the ExEm method .", "ner": [["DeepWalk", "Method"], ["Node 2 Vec", "Method"], ["ExEm", "Method"]], "rel": [["DeepWalk", "Compare-With", "ExEm"], ["Node 2 Vec", "Compare-With", "ExEm"]], "rel_plus": [["DeepWalk:Method", "Compare-With", "ExEm:Method"], ["Node 2 Vec:Method", "Compare-With", "ExEm:Method"]]}
{"doc_id": "6423078", "sentence": "Broadly speaking , one may categorize works into a few groups such as I : early pioneering methods like the Sobel detector ( Kittler 1 9 8 3 ) , zero - crossing ( Marr and Hildreth 1 9 8 0 ; Torre and Poggio 1 9 8 6 ) , and the widely adopted Canny detector ( Canny 1 9 8 6 ) ; methods driven by II : information theory on top of features arrived at through careful manual design , such as Statistical Edges ( Konishi et al. 2 0 0 3 ) , Pb ( Martin et al. 2 0 0 4 ) , and gPb ( Arbelaez et al. 2 0 1 1 ) ; and III : learning - based methods that remain reliant on features of human design , such as BEL ( Doll\u00e1r et al. 2 0 0 6 ) , Multi - scale ( Ren 2 0 0 8) , Sketch Tokens ( Lim et al. 2 0 1 3 ) , and Structured Edges ( Doll\u00e1r and Zitnick 2 0 1 5 ) .", "ner": [["Sobel detector", "Method"], ["zero - crossing", "Method"], ["Canny detector", "Method"], ["Statistical Edges", "Method"], ["Pb", "Method"], ["gPb", "Method"], ["BEL", "Method"], ["Multi - scale", "Method"], ["Sketch Tokens", "Method"], ["Structured Edges", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "198147921", "sentence": "It extends the HOG descriptor , which was originally designed for 2D images [ 1 0 , 2 4 ] , to 3D data . \u2022 Histogram of Control Points ( HOCP ) descriptor [ 7 3 , 7 4 ] .", "ner": [["HOG", "Method"], ["Histogram of Control Points", "Method"], ["HOCP", "Method"]], "rel": [["HOCP", "Synonym-Of", "Histogram of Control Points"]], "rel_plus": [["HOCP:Method", "Synonym-Of", "Histogram of Control Points:Method"]]}
{"doc_id": "150374036", "sentence": "We further explore each components of RAN on FERPlus [ 5 ] , AffectNet [ 4 8 ] , and SFEW [ 1 5 ] .", "ner": [["RAN", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["SFEW", "Dataset"]], "rel": [["RAN", "Evaluated-With", "FERPlus"], ["RAN", "Evaluated-With", "AffectNet"], ["RAN", "Evaluated-With", "SFEW"]], "rel_plus": [["RAN:Method", "Evaluated-With", "FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "SFEW:Dataset"]]}
{"doc_id": "202540251", "sentence": "After that , the problem subsequently became one of the tracks in the Visual Domain Adaptation Challenge ( VisDA ) 2 0 1 7 [ 3 5 ] and started receiving increasing attention .", "ner": [["Visual Domain Adaptation Challenge", "Task"], ["VisDA", "Task"]], "rel": [["VisDA", "Synonym-Of", "Visual Domain Adaptation Challenge"]], "rel_plus": [["VisDA:Task", "Synonym-Of", "Visual Domain Adaptation Challenge:Task"]]}
{"doc_id": "6423078", "sentence": "This problem is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency , segmentation , object detection/recognition , tracking and motion analysis , medical imaging , structurefrom - motion and 3D reconstruction , to modern applications like autonomous driving , mobile computing , and image - totext analysis .", "ner": [["computer vision", "Task"], ["visual saliency", "Task"], ["segmentation", "Task"], ["object detection/recognition", "Task"], ["tracking", "Task"], ["motion analysis", "Task"], ["medical imaging", "Task"], ["structurefrom - motion", "Task"], ["3D reconstruction", "Task"], ["autonomous driving", "Task"], ["mobile computing", "Task"], ["image - totext analysis", "Task"]], "rel": [["object detection/recognition", "SubTask-Of", "computer vision"], ["visual saliency", "SubTask-Of", "computer vision"], ["segmentation", "SubTask-Of", "computer vision"], ["tracking", "SubTask-Of", "computer vision"], ["motion analysis", "SubTask-Of", "computer vision"], ["medical imaging", "SubTask-Of", "computer vision"], ["structurefrom - motion", "SubTask-Of", "computer vision"], ["3D reconstruction", "SubTask-Of", "computer vision"], ["autonomous driving", "SubTask-Of", "computer vision"], ["mobile computing", "SubTask-Of", "computer vision"], ["image - totext analysis", "SubTask-Of", "computer vision"]], "rel_plus": [["object detection/recognition:Task", "SubTask-Of", "computer vision:Task"], ["visual saliency:Task", "SubTask-Of", "computer vision:Task"], ["segmentation:Task", "SubTask-Of", "computer vision:Task"], ["tracking:Task", "SubTask-Of", "computer vision:Task"], ["motion analysis:Task", "SubTask-Of", "computer vision:Task"], ["medical imaging:Task", "SubTask-Of", "computer vision:Task"], ["structurefrom - motion:Task", "SubTask-Of", "computer vision:Task"], ["3D reconstruction:Task", "SubTask-Of", "computer vision:Task"], ["autonomous driving:Task", "SubTask-Of", "computer vision:Task"], ["mobile computing:Task", "SubTask-Of", "computer vision:Task"], ["image - totext analysis:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "202577400", "sentence": "We also extensively verify GALD on three vision benchmarks , including Cityscapes for semantic segmentation , Pascal VOC 2 0 0 7 for object detection , and MS COCO for both object detection and instance segmentation , and all achieve notable improvement .", "ner": [["GALD", "Method"], ["Cityscapes", "Dataset"], ["semantic segmentation", "Task"], ["Pascal VOC 2 0 0 7", "Dataset"], ["object detection", "Task"], ["MS COCO", "Dataset"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Evaluated-With", "Cityscapes"], ["Cityscapes", "Benchmark-For", "semantic segmentation"], ["GALD", "Used-For", "semantic segmentation"], ["GALD", "Evaluated-With", "Pascal VOC 2 0 0 7"], ["Pascal VOC 2 0 0 7", "Benchmark-For", "object detection"], ["GALD", "Used-For", "object detection"], ["GALD", "Evaluated-With", "MS COCO"], ["MS COCO", "Benchmark-For", "object detection"], ["GALD", "Used-For", "object detection"], ["MS COCO", "Benchmark-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["GALD:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Evaluated-With", "Pascal VOC 2 0 0 7:Dataset"], ["Pascal VOC 2 0 0 7:Dataset", "Benchmark-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Evaluated-With", "MS COCO:Dataset"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Pyramid Attention Network ( PAN ) [ 9 2 ] , ParseNet [ 8 9 ] , PSPNet [ 9 0 ] and GCN [ 9 1 ] have used global context information with local feature to have better segmentation .", "ner": [["Pyramid Attention Network", "Method"], ["PAN", "Method"], ["ParseNet", "Method"], ["PSPNet", "Method"], ["GCN", "Method"], ["segmentation", "Task"]], "rel": [["PAN", "Synonym-Of", "Pyramid Attention Network"], ["GCN", "Used-For", "segmentation"], ["PSPNet", "Used-For", "segmentation"], ["ParseNet", "Used-For", "segmentation"], ["Pyramid Attention Network", "Used-For", "segmentation"]], "rel_plus": [["PAN:Method", "Synonym-Of", "Pyramid Attention Network:Method"], ["GCN:Method", "Used-For", "segmentation:Task"], ["PSPNet:Method", "Used-For", "segmentation:Task"], ["ParseNet:Method", "Used-For", "segmentation:Task"], ["Pyramid Attention Network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "4539700", "sentence": "However , can 3D CNNs retrace the successful history of 2D CNNs and ImageNet ?", "ner": [["3D CNNs", "Method"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "In the existence of missing data , the value of forecasting can be distracted far from real values in LSTM neural network , Fig. 1 1 b .   This paper illustrates a new framework for spatial time series forecasting problem and its application on traffic flow data .", "ner": [["forecasting", "Task"], ["LSTM neural network", "Method"], ["spatial time series forecasting", "Task"]], "rel": [["LSTM neural network", "Used-For", "forecasting"]], "rel_plus": [["LSTM neural network:Method", "Used-For", "forecasting:Task"]]}
{"doc_id": "23569888", "sentence": "Deep Convolutional Neural Networks ( CNNs ) are hierarchical models organized as the concatenation of multiple processing layers .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "211004033", "sentence": "Ellipse detection : traditional ellipse detection in 2D [ 1 ] .", "ner": [["Ellipse detection", "Task"], ["traditional ellipse detection in 2D", "Task"]], "rel": [["Ellipse detection", "Synonym-Of", "traditional ellipse detection in 2D"]], "rel_plus": [["Ellipse detection:Task", "Synonym-Of", "traditional ellipse detection in 2D:Task"]]}
{"doc_id": "153312532", "sentence": "One kind of pre - trained models is the word embeddings , such as word 2 vec ( Mikolov et al. , 2 0 1 3 ) and GloVe ( Pennington et al. , 2 0 1 4 ) , or the contextualized word embeddings , such as CoVe ( Mc - Cann et al. , 2 0 1 7 ) and ELMo ( Peters et al. , 2 0 1 8) .", "ner": [["word embeddings", "Method"], ["word 2 vec", "Method"], ["GloVe", "Method"], ["contextualized word embeddings", "Method"], ["CoVe", "Method"], ["ELMo", "Method"]], "rel": [["word 2 vec", "SubClass-Of", "word embeddings"], ["GloVe", "SubClass-Of", "word embeddings"], ["CoVe", "SubClass-Of", "word embeddings"], ["ELMo", "SubClass-Of", "word embeddings"], ["contextualized word embeddings", "SubClass-Of", "word embeddings"], ["CoVe", "SubClass-Of", "contextualized word embeddings"], ["ELMo", "SubClass-Of", "contextualized word embeddings"]], "rel_plus": [["word 2 vec:Method", "SubClass-Of", "word embeddings:Method"], ["GloVe:Method", "SubClass-Of", "word embeddings:Method"], ["CoVe:Method", "SubClass-Of", "word embeddings:Method"], ["ELMo:Method", "SubClass-Of", "word embeddings:Method"], ["contextualized word embeddings:Method", "SubClass-Of", "word embeddings:Method"], ["CoVe:Method", "SubClass-Of", "contextualized word embeddings:Method"], ["ELMo:Method", "SubClass-Of", "contextualized word embeddings:Method"]]}
{"doc_id": "202719032", "sentence": "Modern CNN based object detectors can be roughly categorized into two categories : the two - stage detectors , such as Faster R - CNN [ 2 4 ] , Mask R - CNN [ 1 0 ] , and R - FCN [ 4 ] , and the one - stage detectors , such as YOLO [ 2 2 ] , YOLOv 2 [ 2 3 ] , SSD [ 1 8 ] and RetinaNet [ 1 6 ] .", "ner": [["CNN based object detectors", "Method"], ["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["YOLO", "Method"], ["YOLOv 2", "Method"], ["SSD", "Method"], ["RetinaNet", "Method"]], "rel": [["Faster R - CNN", "SubClass-Of", "CNN based object detectors"], ["Mask R - CNN", "SubClass-Of", "CNN based object detectors"], ["R - FCN", "SubClass-Of", "CNN based object detectors"], ["YOLO", "SubClass-Of", "CNN based object detectors"], ["YOLOv 2", "SubClass-Of", "CNN based object detectors"], ["SSD", "SubClass-Of", "CNN based object detectors"], ["RetinaNet", "SubClass-Of", "CNN based object detectors"]], "rel_plus": [["Faster R - CNN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["Mask R - CNN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["R - FCN:Method", "SubClass-Of", "CNN based object detectors:Method"], ["YOLO:Method", "SubClass-Of", "CNN based object detectors:Method"], ["YOLOv 2:Method", "SubClass-Of", "CNN based object detectors:Method"], ["SSD:Method", "SubClass-Of", "CNN based object detectors:Method"], ["RetinaNet:Method", "SubClass-Of", "CNN based object detectors:Method"]]}
{"doc_id": "4246700", "sentence": "In CNNs features , AlexNet gets the best result on ROUGE L and CIDEr and VGG 1 9 gets the best result on other objective metrics with a little superiority than others .", "ner": [["CNNs", "Method"], ["AlexNet", "Method"], ["ROUGE L", "Method"], ["CIDEr", "Method"], ["VGG 1 9", "Method"]], "rel": [["AlexNet", "SubClass-Of", "CNNs"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNNs:Method"]]}
{"doc_id": "210861282", "sentence": "We used \" Detectron 2 \" [ 1 7 ] for segmentation and detection of all the individuals in the image , followed by the UniPose method to detect the pose of the selected individual .", "ner": [["Detectron 2", "Method"], ["segmentation", "Task"], ["detection", "Task"], ["UniPose", "Method"]], "rel": [["Detectron 2", "Used-For", "segmentation"], ["Detectron 2", "Used-For", "detection"]], "rel_plus": [["Detectron 2:Method", "Used-For", "segmentation:Task"], ["Detectron 2:Method", "Used-For", "detection:Task"]]}
{"doc_id": "6423078", "sentence": "HED shows a clear advantage in consistency over Canny The task of edge and object boundary detection is inherently challenging .", "ner": [["Canny", "Method"], ["edge and object boundary detection", "Task"]], "rel": [["Canny", "Used-For", "edge and object boundary detection"]], "rel_plus": [["Canny:Method", "Used-For", "edge and object boundary detection:Task"]]}
{"doc_id": "24972096", "sentence": "Using the quad high definition ( QHD ) data from Kinect 2 , the runtime performances of our system are 5. 6 8 Hz ( VGG 1 6 ) and 3. 2 3 Hz ( ResNet 1 0 1 ) when the RGB is resized to 5 1 2 \u00d7 5 1 2 and the point cloud is down - sampled to three scales , 1 6 , 3 8 4 \u00d7 1 , 4 0 9 6 \u00d7 1 and 1 0 2 4 \u00d7 1 .", "ner": [["Kinect 2", "Dataset"], ["VGG 1 6", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "We compare PointRend to the default 4 \u00d7 conv head in Mask R - CNN in [ 1 9 ] .", "ner": [["PointRend", "Method"], ["4 \u00d7 conv head", "Method"], ["Mask R - CNN", "Method"]], "rel": [["4 \u00d7 conv head", "Part-Of", "Mask R - CNN"], ["PointRend", "Part-Of", "Mask R - CNN"]], "rel_plus": [["4 \u00d7 conv head:Method", "Part-Of", "Mask R - CNN:Method"], ["PointRend:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "201070697", "sentence": "To overcome this , Cycle consistency loss is proposed in CycleGAN [ 5 ] to measure the discrepancy between the input image x and the image F(G(x ) ) generated by the inverse mapping that translates the input image back to the original domain space .", "ner": [["Cycle consistency loss", "Method"], ["CycleGAN", "Method"]], "rel": [["Cycle consistency loss", "Part-Of", "CycleGAN"]], "rel_plus": [["Cycle consistency loss:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "3920676", "sentence": "Once the model is trained , we use the output of the fc 7 layer as the image descriptor , giving a 4 0 9 6 - dimensional feature vector in the case of IDE - VGGNet and IDE - CaffeNet , and a 2 0 4 8 dimensional feature vector in the case of IDE - ResNet .", "ner": [["fc 7", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"]], "rel": [["fc 7", "Part-Of", "IDE - VGGNet"], ["fc 7", "Part-Of", "IDE - CaffeNet"], ["fc 7", "Part-Of", "IDE - ResNet"]], "rel_plus": [["fc 7:Method", "Part-Of", "IDE - VGGNet:Method"], ["fc 7:Method", "Part-Of", "IDE - CaffeNet:Method"], ["fc 7:Method", "Part-Of", "IDE - ResNet:Method"]]}
{"doc_id": "AUG064", "sentence": "Task-specific feature engineering improves sentiment analysis on IMDb.", "ner": [["task-specific feature engineering", "Method"], ["sentiment analysis", "Task"], ["IMDb", "Dataset"]], "rel": [["task-specific feature engineering", "Used-For", "sentiment analysis"], ["task-specific feature engineering", "Trained-With", "IMDb"]], "rel_plus": [["task-specific feature engineering:Method", "Used-For", "sentiment analysis:Task"], ["task-specific feature engineering:Method", "Trained-With", "IMDb:Dataset"]]}
{"doc_id": "211010520", "sentence": "It may however also be due to a limitation of BERT and RoBERTa -similar to BiDAF -in learning from a data distribution designed to beat these models ; an even stronger model might learn more e.g. from D RoBERTa .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["BiDAF", "Method"], ["D RoBERTa", "Dataset"]], "rel": [["BERT", "Compare-With", "BiDAF"], ["RoBERTa", "Compare-With", "BiDAF"]], "rel_plus": [["BERT:Method", "Compare-With", "BiDAF:Method"], ["RoBERTa:Method", "Compare-With", "BiDAF:Method"]]}
{"doc_id": "AUG070", "sentence": "Context-sensitive relation extraction is improved by GloVe on SemEval 2010.", "ner": [["context-sensitive relation extraction", "Task"], ["GloVe", "Method"], ["SemEval 2010", "Dataset"]], "rel": [["GloVe", "Used-For", "context-sensitive relation extraction"], ["GloVe", "Evaluated-With", "SemEval 2010"]], "rel_plus": [["GloVe:Method", "Used-For", "context-sensitive relation extraction:Task"], ["GloVe:Method", "Evaluated-With", "SemEval 2010:Dataset"]]}
{"doc_id": "102351044", "sentence": "As summarized in Table 1 , drop - neuron and drop - channel are applicable to general CNNs while the applicability of drop - path and drop - layer are dependent on the detailed CNN architectures .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["CNNs", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"], ["CNN", "Method"]], "rel": [["drop - neuron", "Part-Of", "CNNs"], ["drop - channel", "Part-Of", "CNNs"], ["drop - neuron", "Compare-With", "drop - path"], ["drop - channel", "Compare-With", "drop - path"], ["drop - neuron", "Compare-With", "drop - layer"], ["drop - channel", "Compare-With", "drop - layer"], ["drop - path", "Part-Of", "CNN"], ["drop - layer", "Part-Of", "CNN"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNNs:Method"], ["drop - channel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Compare-With", "drop - path:Method"], ["drop - channel:Method", "Compare-With", "drop - path:Method"], ["drop - neuron:Method", "Compare-With", "drop - layer:Method"], ["drop - channel:Method", "Compare-With", "drop - layer:Method"], ["drop - path:Method", "Part-Of", "CNN:Method"], ["drop - layer:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "209532167", "sentence": "This shows that CuBERT can reach accuracies that are comparable to or better than those of BiLSTMs trained with Word 2 Vec embeddings within only a few epochs .", "ner": [["CuBERT", "Method"], ["BiLSTMs", "Method"], ["Word 2 Vec", "Method"]], "rel": [["Word 2 Vec", "Part-Of", "BiLSTMs"], ["CuBERT", "Compare-With", "BiLSTMs"]], "rel_plus": [["Word 2 Vec:Method", "Part-Of", "BiLSTMs:Method"], ["CuBERT:Method", "Compare-With", "BiLSTMs:Method"]]}
{"doc_id": "3920676", "sentence": "We noted that IDE - ResNet resulted in the best performance among all the evaluated feature extraction methods , with IDE - VGGNet and IDE - CaffeNet close behind .", "ner": [["IDE - ResNet", "Method"], ["feature extraction", "Method"], ["IDE - VGGNet", "Method"], ["IDE - CaffeNet", "Method"]], "rel": [["IDE - VGGNet", "SubClass-Of", "feature extraction"], ["IDE - CaffeNet", "SubClass-Of", "feature extraction"], ["IDE - ResNet", "SubClass-Of", "feature extraction"]], "rel_plus": [["IDE - VGGNet:Method", "SubClass-Of", "feature extraction:Method"], ["IDE - CaffeNet:Method", "SubClass-Of", "feature extraction:Method"], ["IDE - ResNet:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "210713911", "sentence": "The state - of - the - art Models ResNet - 5 0 -tuned [ 1 8 ] ResNet - 5 0 Assemble - ResNet - FGVC - 5 0 Table 9 .", "ner": [["ResNet - 5 0", "Method"], ["ResNet - 5 0 Assemble - ResNet - FGVC - 5 0", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "FCNs endow Convolutional Neural Networks ( CNNs ) with an upsampling path to recover the input resolution .", "ner": [["FCNs", "Method"], ["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["upsampling path", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["FCNs", "Part-Of", "Convolutional Neural Networks"], ["upsampling path", "Part-Of", "Convolutional Neural Networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["FCNs:Method", "Part-Of", "Convolutional Neural Networks:Method"], ["upsampling path:Method", "Part-Of", "Convolutional Neural Networks:Method"]]}
{"doc_id": "210920315", "sentence": "The focus of our design is the PSC module that is designed to capture intra - part and inter - part spatial co - occurrence of different body parts through a Graph Convolutional Network ( GCN ) .", "ner": [["PSC", "Method"], ["Graph Convolutional Network", "Method"], ["GCN", "Method"]], "rel": [["Graph Convolutional Network", "Part-Of", "PSC"], ["GCN", "Synonym-Of", "Graph Convolutional Network"]], "rel_plus": [["Graph Convolutional Network:Method", "Part-Of", "PSC:Method"], ["GCN:Method", "Synonym-Of", "Graph Convolutional Network:Method"]]}
{"doc_id": "147703932", "sentence": "Each hourglass module consists of an encoderdecoder architecture with residual connections from encoder layers to corresponding decoder ones .", "ner": [["hourglass module", "Method"], ["encoderdecoder", "Method"], ["residual connections", "Method"]], "rel": [["encoderdecoder", "Part-Of", "hourglass module"], ["residual connections", "Part-Of", "encoderdecoder"]], "rel_plus": [["encoderdecoder:Method", "Part-Of", "hourglass module:Method"], ["residual connections:Method", "Part-Of", "encoderdecoder:Method"]]}
{"doc_id": "195347056", "sentence": "However , PPGN differs to the other generative models discussed , where images are generated in one - shot from the latent codes in the traditional generative models .", "ner": [["PPGN", "Method"], ["generative models", "Method"], ["generative models", "Method"]], "rel": [["PPGN", "Compare-With", "generative models"]], "rel_plus": [["PPGN:Method", "Compare-With", "generative models:Method"]]}
{"doc_id": "21683040", "sentence": "The main trade - off of object detectors which are based on ConvNets is that the contradiction between object recognition and location .", "ner": [["ConvNets", "Method"], ["object recognition", "Task"]], "rel": [["ConvNets", "Used-For", "object recognition"]], "rel_plus": [["ConvNets:Method", "Used-For", "object recognition:Task"]]}
{"doc_id": "211020570", "sentence": "As shown in Fig. 2 , the proposed spatial transformer -generative adversarial network ( ST - GAN ) consists of two parts : a generative deep neural network ( GDNN ) and a discriminative deep neural network ( DDNN ) .", "ner": [["spatial transformer -generative adversarial network", "Method"], ["ST - GAN", "Method"], ["generative deep neural network", "Method"], ["GDNN", "Method"], ["discriminative deep neural network", "Method"], ["DDNN", "Method"]], "rel": [["ST - GAN", "Synonym-Of", "spatial transformer -generative adversarial network"], ["GDNN", "Synonym-Of", "generative deep neural network"], ["DDNN", "Synonym-Of", "discriminative deep neural network"]], "rel_plus": [["ST - GAN:Method", "Synonym-Of", "spatial transformer -generative adversarial network:Method"], ["GDNN:Method", "Synonym-Of", "generative deep neural network:Method"], ["DDNN:Method", "Synonym-Of", "discriminative deep neural network:Method"]]}
{"doc_id": "201124533", "sentence": "We empirically demonstrate the superior pose estimation performance on the COCO keypoint detection dataset [ 7 4 ] .", "ner": [["pose estimation", "Task"], ["COCO keypoint detection", "Dataset"]], "rel": [["COCO keypoint detection", "Benchmark-For", "pose estimation"]], "rel_plus": [["COCO keypoint detection:Dataset", "Benchmark-For", "pose estimation:Task"]]}
{"doc_id": "202719032", "sentence": "SSD used Mobilenet v 2 [ 2 6 ] as the backbone .", "ner": [["SSD", "Method"], ["Mobilenet v 2", "Method"]], "rel": [["Mobilenet v 2", "Part-Of", "SSD"]], "rel_plus": [["Mobilenet v 2:Method", "Part-Of", "SSD:Method"]]}
{"doc_id": "35249701", "sentence": "The deeper ResNet and DenseNet networks with 5 0 and 1 2 1 layers , respectively , are very robust to pruning , losing just 0. 4 5 % and 0.0 4 % top - 1 accuracy on ImageNet , respectively .", "ner": [["ResNet", "Method"], ["DenseNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["ResNet", "Evaluated-With", "ImageNet"], ["DenseNet", "Evaluated-With", "ImageNet"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["DenseNet:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "198147921", "sentence": "Examples include colour statistics , Histogram of Oriented Gradients ( HOG ) descriptor [ 1 3 ] , Scale - Invariant Feature Transform ( SIFT ) [ 1 4 ] , the Chamfer distance [ 7 ] , and Local Binary Patterns ( LBPs ) [ 3 1 ] .", "ner": [["Histogram of Oriented Gradients", "Method"], ["HOG", "Method"], ["Scale - Invariant Feature Transform", "Method"], ["SIFT", "Method"], ["Chamfer distance", "Method"], ["Local Binary Patterns", "Method"], ["LBPs", "Method"]], "rel": [["HOG", "Synonym-Of", "Histogram of Oriented Gradients"], ["SIFT", "Synonym-Of", "Scale - Invariant Feature Transform"], ["LBPs", "Synonym-Of", "Local Binary Patterns"]], "rel_plus": [["HOG:Method", "Synonym-Of", "Histogram of Oriented Gradients:Method"], ["SIFT:Method", "Synonym-Of", "Scale - Invariant Feature Transform:Method"], ["LBPs:Method", "Synonym-Of", "Local Binary Patterns:Method"]]}
{"doc_id": "208513596", "sentence": "Here , we compare XDC with state - of - the - art selfsupervised methods on action recognition in UCF 1 0 1 [ 5 8 ] and HMDB 5 1 [ 3 0 ] , and on audio event classification in ESC 5 0 [ 4 9 ] and DCASE [ 6 0 ] .", "ner": [["XDC", "Method"], ["action recognition", "Task"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["audio event classification", "Task"], ["ESC 5 0", "Dataset"], ["DCASE", "Dataset"]], "rel": [["UCF 1 0 1", "Benchmark-For", "action recognition"], ["HMDB 5 1", "Benchmark-For", "action recognition"], ["XDC", "Used-For", "action recognition"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["XDC", "Used-For", "audio event classification"], ["XDC", "Evaluated-With", "ESC 5 0"], ["XDC", "Evaluated-With", "DCASE"]], "rel_plus": [["UCF 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"], ["HMDB 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["XDC:Method", "Used-For", "action recognition:Task"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Used-For", "audio event classification:Task"], ["XDC:Method", "Evaluated-With", "ESC 5 0:Dataset"], ["XDC:Method", "Evaluated-With", "DCASE:Dataset"]]}
{"doc_id": "53719742", "sentence": "Although more effective , there still exists room for further improvement Then , for Fast R - CNN , the output features are globally average pooled before the final text/non - text classification and quadrilateral bounding box regression layers , while for the mask prediction network , the output features are followed by four consecutive 3 \u00d7 3 convolutional layers and then upsampled before the final mask prediction layers .", "ner": [["Fast R - CNN", "Method"], ["text/non - text classification", "Task"], ["quadrilateral bounding box regression", "Task"], ["mask prediction network", "Method"], ["3 \u00d7 3 convolutional layers", "Method"], ["mask prediction layers", "Method"]], "rel": [["Fast R - CNN", "Used-For", "text/non - text classification"], ["Fast R - CNN", "Used-For", "quadrilateral bounding box regression"], ["mask prediction layers", "Part-Of", "mask prediction network"], ["3 \u00d7 3 convolutional layers", "Part-Of", "mask prediction network"]], "rel_plus": [["Fast R - CNN:Method", "Used-For", "text/non - text classification:Task"], ["Fast R - CNN:Method", "Used-For", "quadrilateral bounding box regression:Task"], ["mask prediction layers:Method", "Part-Of", "mask prediction network:Method"], ["3 \u00d7 3 convolutional layers:Method", "Part-Of", "mask prediction network:Method"]]}
{"doc_id": "210861282", "sentence": "Pose estimation results and comparisons with other methods for the BBC Pose dataset Figure 9 shows examples of pose estimation and bounding box detections for subjects in the BBC dataset .", "ner": [["Pose estimation", "Task"], ["BBC Pose", "Dataset"], ["pose estimation", "Task"], ["BBC", "Dataset"]], "rel": [["BBC Pose", "Benchmark-For", "Pose estimation"]], "rel_plus": [["BBC Pose:Dataset", "Benchmark-For", "Pose estimation:Task"]]}
{"doc_id": "4246700", "sentence": "But according to [ 6 1 ] , LSTM should outperform RNN .", "ner": [["LSTM", "Method"], ["RNN", "Method"]], "rel": [["LSTM", "Compare-With", "RNN"]], "rel_plus": [["LSTM:Method", "Compare-With", "RNN:Method"]]}
{"doc_id": "23569888", "sentence": "The prototypical structure of a CNN ( see Fig. 4 ) performs at each layer ( usually referred to as convolution layer ) the following set of operations : \u2022 Convolutions : local convolution with respect to a bank of ( learned ) linear filters . \u2022 Spatial Downsampling for instance using strided convolutions . \u2022 Element - wise Non Linearity such as sigmoid functions ( Bishop 2 0 0 6 ) or , more recently Rectifying Linear Units ( He et al 2 0 1 5 ) . \u2022 Spatial Pooling to aggregate local responses to the signal in a single component , for instance by taking the maximum value observed ( max - pooling ) .", "ner": [["CNN", "Method"], ["convolution layer", "Method"], ["Convolutions", "Method"], ["convolution", "Method"], ["Spatial Downsampling", "Method"], ["strided convolutions", "Method"], ["Element - wise Non Linearity", "Method"], ["sigmoid", "Method"], ["Rectifying Linear Units", "Method"], ["Spatial Pooling", "Method"], ["max - pooling", "Method"]], "rel": [["convolution layer", "Part-Of", "CNN"], ["Convolutions", "Part-Of", "CNN"], ["strided convolutions", "Part-Of", "Spatial Downsampling"], ["sigmoid", "SubClass-Of", "Element - wise Non Linearity"], ["Rectifying Linear Units", "SubClass-Of", "Element - wise Non Linearity"], ["max - pooling", "SubClass-Of", "Spatial Pooling"]], "rel_plus": [["convolution layer:Method", "Part-Of", "CNN:Method"], ["Convolutions:Method", "Part-Of", "CNN:Method"], ["strided convolutions:Method", "Part-Of", "Spatial Downsampling:Method"], ["sigmoid:Method", "SubClass-Of", "Element - wise Non Linearity:Method"], ["Rectifying Linear Units:Method", "SubClass-Of", "Element - wise Non Linearity:Method"], ["max - pooling:Method", "SubClass-Of", "Spatial Pooling:Method"]]}
{"doc_id": "201070522", "sentence": "For the generation - based tasks ( i.e. captioning and VQA ) , the benefits of using adapted embeddings are less clear .", "ner": [["generation - based tasks", "Task"], ["captioning", "Task"], ["VQA", "Task"]], "rel": [["captioning", "SubTask-Of", "generation - based tasks"], ["VQA", "SubTask-Of", "generation - based tasks"]], "rel_plus": [["captioning:Task", "SubTask-Of", "generation - based tasks:Task"], ["VQA:Task", "SubTask-Of", "generation - based tasks:Task"]]}
{"doc_id": "59599694", "sentence": "In [ 2 6 ] , they studied image - like representation of spatial time series data using convolution layers and ensemble learning .", "ner": [["spatial time series", "Task"], ["convolution", "Method"], ["ensemble learning", "Method"]], "rel": [["ensemble learning", "Used-For", "spatial time series"], ["convolution", "Used-For", "spatial time series"]], "rel_plus": [["ensemble learning:Method", "Used-For", "spatial time series:Task"], ["convolution:Method", "Used-For", "spatial time series:Task"]]}
{"doc_id": "24972096", "sentence": "The improvement can be attributed to three parts : the hierarchical convolutional stack in PixelNet , the boundary refinement by VoxelNet and the softmax weighted fusion stack .", "ner": [["hierarchical convolutional stack", "Method"], ["PixelNet", "Method"], ["boundary refinement", "Task"], ["VoxelNet", "Method"], ["softmax weighted fusion stack", "Method"]], "rel": [["hierarchical convolutional stack", "Part-Of", "PixelNet"], ["VoxelNet", "Used-For", "boundary refinement"]], "rel_plus": [["hierarchical convolutional stack:Method", "Part-Of", "PixelNet:Method"], ["VoxelNet:Method", "Used-For", "boundary refinement:Task"]]}
{"doc_id": "59599694", "sentence": "A pretraining denoising stacked auto encoder decoder is applied on each cluster of sensors to generate a robust output .", "ner": [["denoising", "Task"], ["auto encoder", "Method"]], "rel": [["auto encoder", "Used-For", "denoising"]], "rel_plus": [["auto encoder:Method", "Used-For", "denoising:Task"]]}
{"doc_id": "210920315", "sentence": "Our PSC - Net consists of a standard pedestrian detection branch and a part spatial co - occurrence ( PSC ) module .", "ner": [["PSC - Net", "Method"], ["pedestrian detection branch", "Method"], ["part spatial co - occurrence", "Method"], ["PSC", "Method"]], "rel": [["pedestrian detection branch", "Part-Of", "PSC - Net"], ["part spatial co - occurrence", "Part-Of", "PSC - Net"], ["PSC", "Synonym-Of", "part spatial co - occurrence"]], "rel_plus": [["pedestrian detection branch:Method", "Part-Of", "PSC - Net:Method"], ["part spatial co - occurrence:Method", "Part-Of", "PSC - Net:Method"], ["PSC:Method", "Synonym-Of", "part spatial co - occurrence:Method"]]}
{"doc_id": "53719742", "sentence": "In this section , we focus on reviewing recently proposed CNN based text detection approaches and recent developments in instance segmentation tasks .", "ner": [["CNN", "Method"], ["text detection", "Task"], ["instance segmentation", "Task"]], "rel": [["CNN", "Used-For", "text detection"], ["CNN", "Used-For", "instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "text detection:Task"], ["CNN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211010786", "sentence": "Here , we propose the concept of variable - lag Granger causality , VL - Granger causality for short , which generalizes the Granger causal relation of De nition 4. 1 in a way that addresses the xed - lag limitation .", "ner": [["variable - lag Granger causality", "Method"], ["VL - Granger causality", "Method"]], "rel": [["VL - Granger causality", "Synonym-Of", "variable - lag Granger causality"]], "rel_plus": [["VL - Granger causality:Method", "Synonym-Of", "variable - lag Granger causality:Method"]]}
{"doc_id": "147703932", "sentence": "The work of [ 8 ] uses Mask - RCNN [ 1 8 ] in a multi - task cascade fashion , connecting several intermediate layers for pose estimation and body parts parsing , while in [ 1 9 ] Mask R - CNN tackles instance/mask segmentation and object/keypoint detection problems .", "ner": [["Mask - RCNN", "Method"], ["pose estimation", "Task"], ["body parts parsing", "Task"], ["Mask R - CNN", "Method"], ["instance/mask segmentation", "Task"], ["object/keypoint detection", "Task"]], "rel": [["Mask - RCNN", "Used-For", "pose estimation"], ["Mask - RCNN", "Used-For", "body parts parsing"], ["Mask R - CNN", "Used-For", "instance/mask segmentation"], ["Mask R - CNN", "Used-For", "object/keypoint detection"]], "rel_plus": [["Mask - RCNN:Method", "Used-For", "pose estimation:Task"], ["Mask - RCNN:Method", "Used-For", "body parts parsing:Task"], ["Mask R - CNN:Method", "Used-For", "instance/mask segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "object/keypoint detection:Task"]]}
{"doc_id": "201646309", "sentence": "We do not observe a significant difference between BERT and RoBERTa .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BERT", "Compare-With", "RoBERTa"]], "rel_plus": [["BERT:Method", "Compare-With", "RoBERTa:Method"]]}
{"doc_id": "202676714", "sentence": "Image recognition experiments on MNIST , Fashion - MNIST ( FMNIST ) , CIFAR 1 0 , CIFAR 1 0 0 , and SVHN demonstrate that Absum and SNC outperform L 1 and L 2 regularization methods in terms of improving robustness against SFA , and the computation time of Absum is about one - tenth that of SNC .", "ner": [["Image recognition", "Task"], ["MNIST", "Dataset"], ["Fashion - MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["Absum", "Method"], ["SNC", "Method"], ["L 1", "Method"], ["L 2 regularization", "Method"], ["robustness against SFA", "Task"], ["Absum", "Method"], ["SNC", "Method"]], "rel": [["MNIST", "Benchmark-For", "Image recognition"], ["Fashion - MNIST", "Benchmark-For", "Image recognition"], ["CIFAR 1 0", "Benchmark-For", "Image recognition"], ["CIFAR 1 0 0", "Benchmark-For", "Image recognition"], ["SVHN", "Benchmark-For", "Image recognition"], ["FMNIST", "Synonym-Of", "Fashion - MNIST"], ["Absum", "Compare-With", "L 1"], ["SNC", "Compare-With", "L 1"], ["Absum", "Compare-With", "L 2 regularization"], ["SNC", "Compare-With", "L 2 regularization"], ["SNC", "Used-For", "robustness against SFA"], ["Absum", "Used-For", "robustness against SFA"], ["L 1", "Used-For", "robustness against SFA"], ["L 2 regularization", "Used-For", "robustness against SFA"], ["Absum", "Compare-With", "SNC"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "Image recognition:Task"], ["Fashion - MNIST:Dataset", "Benchmark-For", "Image recognition:Task"], ["CIFAR 1 0:Dataset", "Benchmark-For", "Image recognition:Task"], ["CIFAR 1 0 0:Dataset", "Benchmark-For", "Image recognition:Task"], ["SVHN:Dataset", "Benchmark-For", "Image recognition:Task"], ["FMNIST:Dataset", "Synonym-Of", "Fashion - MNIST:Dataset"], ["Absum:Method", "Compare-With", "L 1:Method"], ["SNC:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Compare-With", "L 2 regularization:Method"], ["SNC:Method", "Compare-With", "L 2 regularization:Method"], ["SNC:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Used-For", "robustness against SFA:Task"], ["L 1:Method", "Used-For", "robustness against SFA:Task"], ["L 2 regularization:Method", "Used-For", "robustness against SFA:Task"], ["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "52009210", "sentence": "The final output of entity mapping , predicate mapping , and candidate refinement is our SIMPLEDBPEDIAQA dataset , which successfully migrates SIMPLEQUESTIONS from Freebase over to DBpedia .   To lay the foundation for future work on our new dataset , we provide simple yet strong baselines using recent work by Mohammed et al. ( 2 0 1 8) , who applied techniques with and without neural networks to SIMPLEQUESTIONS .", "ner": [["entity mapping", "Task"], ["predicate mapping", "Task"], ["candidate refinement", "Task"], ["SIMPLEDBPEDIAQA", "Dataset"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"], ["neural networks", "Method"], ["SIMPLEQUESTIONS", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Benchmark-For", "entity mapping"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "predicate mapping"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "candidate refinement"], ["neural networks", "Used-For", "SIMPLEQUESTIONS"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "entity mapping:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "predicate mapping:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "candidate refinement:Task"], ["neural networks:Method", "Used-For", "SIMPLEQUESTIONS:Dataset"]]}
{"doc_id": "6116678", "sentence": "Recently , machine learning and artificial intelligence have been revolutionized by deep convolutional neural networks , which have set new state of the art on a number of visual recognition tasks , including image classification [ 2 5 ] , object detection [ 1 6 ] , scene classification [ 4 8 ] and scene parsing [ 1 3 ] , closing the gap to human - level performance .", "ner": [["deep convolutional neural networks", "Method"], ["visual recognition", "Task"], ["image classification", "Task"], ["object detection", "Task"], ["scene classification", "Task"], ["scene parsing", "Task"]], "rel": [["deep convolutional neural networks", "Used-For", "visual recognition"], ["image classification", "SubTask-Of", "visual recognition"], ["object detection", "SubTask-Of", "visual recognition"], ["scene classification", "SubTask-Of", "visual recognition"], ["scene parsing", "SubTask-Of", "visual recognition"], ["deep convolutional neural networks", "Used-For", "image classification"], ["deep convolutional neural networks", "Used-For", "object detection"], ["deep convolutional neural networks", "Used-For", "scene classification"], ["deep convolutional neural networks", "Used-For", "scene parsing"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "visual recognition:Task"], ["image classification:Task", "SubTask-Of", "visual recognition:Task"], ["object detection:Task", "SubTask-Of", "visual recognition:Task"], ["scene classification:Task", "SubTask-Of", "visual recognition:Task"], ["scene parsing:Task", "SubTask-Of", "visual recognition:Task"], ["deep convolutional neural networks:Method", "Used-For", "image classification:Task"], ["deep convolutional neural networks:Method", "Used-For", "object detection:Task"], ["deep convolutional neural networks:Method", "Used-For", "scene classification:Task"], ["deep convolutional neural networks:Method", "Used-For", "scene parsing:Task"]]}
{"doc_id": "210713911", "sentence": "AutoAugment ( Autoaug ) AutoAugment [ 4 ] is a data augmentation procedure which learns augmentation strategies from data .", "ner": [["AutoAugment", "Method"], ["Autoaug", "Method"], ["AutoAugment", "Method"], ["data augmentation", "Method"]], "rel": [["Autoaug", "Synonym-Of", "AutoAugment"], ["AutoAugment", "SubClass-Of", "data augmentation"]], "rel_plus": [["Autoaug:Method", "Synonym-Of", "AutoAugment:Method"], ["AutoAugment:Method", "SubClass-Of", "data augmentation:Method"]]}
{"doc_id": "209386851", "sentence": "SemanticFPN [ 2 4 ] : We use SGD with 0. 9 momentum with 3 2 images per mini - batch cropped to a fixed 5 1 2 \u00d7 1 0 2 4 size ; the training schedule is 4 0 k / 1 5 k / 1 0 k updates at learning rates of 0.0 1 / 0.0 0 1 / 0.0 0 0 1 respectively ; a linear learning rate warmup [ 1 5 ] over 1 0 0 0 updates starting from a learning rate of 0.0 0 1 is applied ; weight decay 0.0 0 0 1 is applied ; horizontal flipping , color augmentation [ 3 3 ] , and crop bootstrapping [ 2 ] are used during training ; scale traintime data augmentation resizes an input image from 0. 5 \u00d7 to 2. 0 \u00d7 with a 3 2 pixel step ; BN layers are frozen ( i.e. , BN is not used ) ; no test - time augmentation is used .", "ner": [["SemanticFPN", "Method"], ["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"], ["horizontal flipping", "Method"], ["color augmentation", "Method"], ["crop bootstrapping", "Method"], ["data augmentation", "Method"], ["BN", "Method"], ["BN", "Method"]], "rel": [["SGD", "Part-Of", "SemanticFPN"], ["momentum", "Part-Of", "SGD"]], "rel_plus": [["SGD:Method", "Part-Of", "SemanticFPN:Method"], ["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "102351044", "sentence": "That is , they can co - exist in the same network whenever the CNN architecture allows for it , exploiting the benefits of dropout to the largest extent .", "ner": [["CNN", "Method"], ["dropout", "Method"]], "rel": [["dropout", "Part-Of", "CNN"]], "rel_plus": [["dropout:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "53719742", "sentence": "The multi - task loss function can be denoted as follows : where c and c * are predicted and ground - truth labels respectively , L R cls ( c , c * ) is a softmax loss for classification tasks ; r and r * represent the predicted and ground - truth 4 - dimensional parameterized regression targets as stated in [ 3 2 ] , L R loc ( r , r * ) is a smooth - L 1 loss [ 8 ] for regression tasks . \u03bb loc is a loss - balancing parameter , and we set \u03bb loc = 3 .", "ner": [["multi - task loss function", "Method"], ["softmax loss", "Method"], ["classification", "Task"]], "rel": [["softmax loss", "Used-For", "classification"]], "rel_plus": [["softmax loss:Method", "Used-For", "classification:Task"]]}
{"doc_id": "211020570", "sentence": "Recently , progresses have been made by convolutional neural networks ( CNNs ) in semantic segmentation [ 5 ] and in human pose estimation and face alignment based on heatmap regression [ 6 ] .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["semantic segmentation", "Task"], ["human pose estimation", "Task"], ["face alignment", "Task"], ["heatmap regression", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "semantic segmentation"], ["convolutional neural networks", "Used-For", "human pose estimation"], ["convolutional neural networks", "Used-For", "face alignment"], ["heatmap regression", "Used-For", "face alignment"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "semantic segmentation:Task"], ["convolutional neural networks:Method", "Used-For", "human pose estimation:Task"], ["convolutional neural networks:Method", "Used-For", "face alignment:Task"], ["heatmap regression:Method", "Used-For", "face alignment:Task"]]}
{"doc_id": "208513596", "sentence": "DeepCluster uses a 2D CNN ( e.g. ResNet - 5 0 ) as its image encoder E and clusters the features after each epoch using k - means .", "ner": [["DeepCluster", "Method"], ["2D CNN", "Method"], ["ResNet - 5 0", "Method"], ["k - means", "Method"]], "rel": [["k - means", "Part-Of", "DeepCluster"], ["2D CNN", "Part-Of", "DeepCluster"], ["ResNet - 5 0", "Synonym-Of", "2D CNN"]], "rel_plus": [["k - means:Method", "Part-Of", "DeepCluster:Method"], ["2D CNN:Method", "Part-Of", "DeepCluster:Method"], ["ResNet - 5 0:Method", "Synonym-Of", "2D CNN:Method"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 is tuned to mobile phone CPUs through a combination of hardware - aware network architecture search ( NAS ) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances .", "ner": [["MobileNetV 3", "Method"], ["network architecture search", "Method"], ["NAS", "Method"], ["NetAdapt", "Method"]], "rel": [["network architecture search", "Used-For", "MobileNetV 3"], ["NetAdapt", "Used-For", "MobileNetV 3"], ["NAS", "Synonym-Of", "network architecture search"]], "rel_plus": [["network architecture search:Method", "Used-For", "MobileNetV 3:Method"], ["NetAdapt:Method", "Used-For", "MobileNetV 3:Method"], ["NAS:Method", "Synonym-Of", "network architecture search:Method"]]}
{"doc_id": "147703932", "sentence": "Pyramid image decomposition is used as input to deal with semantic/boundary/object detection , normal estimation saliency/normal estimation , semantic/human part segmentation , semantic boundary detection , and region proposal generation .", "ner": [["Pyramid image decomposition", "Method"], ["semantic/boundary/object detection", "Task"], ["normal estimation saliency/normal estimation", "Task"], ["semantic/human part segmentation", "Task"], ["semantic boundary detection", "Task"], ["region proposal generation", "Task"]], "rel": [["Pyramid image decomposition", "Used-For", "semantic/boundary/object detection"], ["Pyramid image decomposition", "Used-For", "normal estimation saliency/normal estimation"], ["Pyramid image decomposition", "Used-For", "semantic/human part segmentation"], ["Pyramid image decomposition", "Used-For", "semantic boundary detection"], ["Pyramid image decomposition", "Used-For", "region proposal generation"]], "rel_plus": [["Pyramid image decomposition:Method", "Used-For", "semantic/boundary/object detection:Task"], ["Pyramid image decomposition:Method", "Used-For", "normal estimation saliency/normal estimation:Task"], ["Pyramid image decomposition:Method", "Used-For", "semantic/human part segmentation:Task"], ["Pyramid image decomposition:Method", "Used-For", "semantic boundary detection:Task"], ["Pyramid image decomposition:Method", "Used-For", "region proposal generation:Task"]]}
{"doc_id": "147703932", "sentence": "Other works [ 1 7 ] , [ 2 1 ] , [ 2 2 ] , [ 2 3 ] add additional tasks such as instance segmentation , multi - human parsing , and mask segmentation .", "ner": [["instance segmentation", "Task"], ["multi - human parsing", "Task"], ["mask segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The single - model ALBERT configuration incorporates the best - performing settings discussed : an ALBERT - xxlarge configuration ( Table 2 ) using combined MLM and SOP losses , and no dropout .", "ner": [["ALBERT", "Method"], ["ALBERT - xxlarge", "Method"], ["MLM", "Task"], ["SOP", "Task"], ["dropout", "Method"]], "rel": [["ALBERT - xxlarge", "Trained-With", "MLM"], ["ALBERT - xxlarge", "Trained-With", "SOP"]], "rel_plus": [["ALBERT - xxlarge:Method", "Trained-With", "MLM:Task"], ["ALBERT - xxlarge:Method", "Trained-With", "SOP:Task"]]}
{"doc_id": "7507210", "sentence": "Our approach is similar to the fully convolutional networks ( FCN ) [ 2 2 ] implemented in the first version of DeepLab [ 2 ] .", "ner": [["fully convolutional networks", "Method"], ["FCN", "Method"], ["DeepLab", "Method"]], "rel": [["FCN", "Synonym-Of", "fully convolutional networks"]], "rel_plus": [["FCN:Method", "Synonym-Of", "fully convolutional networks:Method"]]}
{"doc_id": "199543700", "sentence": "We demonstrate that MMD and CBT can separately and simultaneously stabilize the training of the proposed SGGAN .", "ner": [["MMD", "Method"], ["CBT", "Method"], ["SGGAN", "Method"]], "rel": [["MMD", "Part-Of", "SGGAN"], ["CBT", "Part-Of", "SGGAN"]], "rel_plus": [["MMD:Method", "Part-Of", "SGGAN:Method"], ["CBT:Method", "Part-Of", "SGGAN:Method"]]}
{"doc_id": "202734254", "sentence": "Though there is a high percentage of the questions containing pronouns in QuAC , they do not necessarily force the model to learn coreference resolution either .", "ner": [["QuAC", "Dataset"], ["coreference resolution", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "The top of Table 5a shows the entity linking results for the BiLSTM and the CRF .", "ner": [["entity linking", "Task"], ["BiLSTM", "Method"], ["CRF", "Method"]], "rel": [["BiLSTM", "Used-For", "entity linking"], ["CRF", "Used-For", "entity linking"]], "rel_plus": [["BiLSTM:Method", "Used-For", "entity linking:Task"], ["CRF:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "4539700", "sentence": "According to previous works [ 9 , 1 6 ] , 3D CNNs trained on UCF - 1 0 1 , HMDB - 5 1 , and ActivityNet do not achieve high accuracy whereas ones trained on Kinetics work well .", "ner": [["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"], ["ActivityNet", "Dataset"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "UCF - 1 0 1"], ["3D CNNs", "Trained-With", "HMDB - 5 1"], ["3D CNNs", "Trained-With", "ActivityNet"], ["3D CNNs", "Trained-With", "Kinetics"], ["UCF - 1 0 1", "Compare-With", "Kinetics"], ["HMDB - 5 1", "Compare-With", "Kinetics"], ["ActivityNet", "Compare-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Trained-With", "HMDB - 5 1:Dataset"], ["3D CNNs:Method", "Trained-With", "ActivityNet:Dataset"], ["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["UCF - 1 0 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["HMDB - 5 1:Dataset", "Compare-With", "Kinetics:Dataset"], ["ActivityNet:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "198231883", "sentence": "UVOS [ 5 ] was proposed as a challenge task for the 2 0 1 9 DAVIS Challenge on Video Object Segmentation . [ 5 ] evaluate the RVOS ( Recurrent Video Object Segmentation ) [ 3 2 ] method for the UVOS task .", "ner": [["UVOS", "Task"], ["2 0 1 9 DAVIS Challenge", "Dataset"], ["Video Object Segmentation", "Task"], ["RVOS", "Method"], ["Recurrent Video Object Segmentation", "Method"], ["UVOS", "Task"]], "rel": [["2 0 1 9 DAVIS Challenge", "Benchmark-For", "UVOS"], ["UVOS", "SubTask-Of", "Video Object Segmentation"], ["2 0 1 9 DAVIS Challenge", "Benchmark-For", "Video Object Segmentation"], ["RVOS", "Synonym-Of", "Recurrent Video Object Segmentation"], ["RVOS", "Used-For", "UVOS"]], "rel_plus": [["2 0 1 9 DAVIS Challenge:Dataset", "Benchmark-For", "UVOS:Task"], ["UVOS:Task", "SubTask-Of", "Video Object Segmentation:Task"], ["2 0 1 9 DAVIS Challenge:Dataset", "Benchmark-For", "Video Object Segmentation:Task"], ["RVOS:Method", "Synonym-Of", "Recurrent Video Object Segmentation:Method"], ["RVOS:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "146120936", "sentence": "By replacing the upsampling operators with CARAFE in two strong baselines Global&Local [ 1 1 ] and Partial Conv [ 2 0 ] , we observe significant improvements for both methods .", "ner": [["upsampling operators", "Method"], ["CARAFE", "Method"], ["Global&Local", "Method"], ["Partial Conv", "Method"]], "rel": [["upsampling operators", "Part-Of", "Global&Local"], ["CARAFE", "Part-Of", "Global&Local"], ["upsampling operators", "Part-Of", "Partial Conv"], ["CARAFE", "Part-Of", "Partial Conv"]], "rel_plus": [["upsampling operators:Method", "Part-Of", "Global&Local:Method"], ["CARAFE:Method", "Part-Of", "Global&Local:Method"], ["upsampling operators:Method", "Part-Of", "Partial Conv:Method"], ["CARAFE:Method", "Part-Of", "Partial Conv:Method"]]}
{"doc_id": "201124533", "sentence": "We compare HRNetV 2 and HRNetV 2 p , to HRNetV 1 on pose estimation , semantic segmentation and COCO object detection .", "ner": [["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["HRNetV 1", "Method"], ["pose estimation", "Task"], ["semantic segmentation", "Task"], ["COCO", "Dataset"], ["object detection", "Task"]], "rel": [["HRNetV 2", "Compare-With", "HRNetV 1"], ["HRNetV 2 p", "Compare-With", "HRNetV 1"], ["HRNetV 2", "Used-For", "pose estimation"], ["HRNetV 2 p", "Used-For", "pose estimation"], ["HRNetV 1", "Used-For", "pose estimation"], ["HRNetV 2 p", "Used-For", "semantic segmentation"], ["HRNetV 2", "Used-For", "semantic segmentation"], ["HRNetV 1", "Used-For", "semantic segmentation"], ["HRNetV 1", "Used-For", "object detection"], ["HRNetV 2 p", "Used-For", "object detection"], ["HRNetV 2", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2 p:Method", "Compare-With", "HRNetV 1:Method"], ["HRNetV 2:Method", "Used-For", "pose estimation:Task"], ["HRNetV 2 p:Method", "Used-For", "pose estimation:Task"], ["HRNetV 1:Method", "Used-For", "pose estimation:Task"], ["HRNetV 2 p:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 1:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 1:Method", "Used-For", "object detection:Task"], ["HRNetV 2 p:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "153312532", "sentence": "We also find that IMDb and Yelp do not help   We compare our model with the following a variety of different methods : CNN - based methods such as Char - level CNN ( Zhang et al. , 2 0 1 5 ) , VD - CNN ( Conneau et al. , 2 0 1 6 ) and DPCNN ( Johnson and Zhang , 2 0 1 7 ) ; RNN - based models such as D - LSTM ( Yogatama et al. , 2 0 1 7 ) , Skim - LSTM ( Seo et al. , 2 0 1 7 ) and hierarchical attention networks ( Yang et al. , 2 0 1 6 ) ; feature - based transfer learning methods such as rigion embedding ( Qiao et al. , 2 0 1 8) and CoVe ( McCann et al. , 2 0 1 7 ) ; and the language model fine - tuning method ( ULMFiT ) ( Howard and Ruder , 2 0 1 8) , which is the current state - of - the - art for text classification .", "ner": [["IMDb", "Dataset"], ["Yelp", "Dataset"], ["CNN", "Method"], ["Char - level CNN", "Method"], ["VD - CNN", "Method"], ["DPCNN", "Method"], ["RNN", "Method"], ["D - LSTM", "Method"], ["Skim - LSTM", "Method"], ["hierarchical attention networks", "Method"], ["feature - based transfer learning", "Method"], ["rigion embedding", "Method"], ["CoVe", "Method"], ["language model fine - tuning", "Method"], ["ULMFiT", "Method"], ["text classification", "Task"]], "rel": [["Char - level CNN", "SubClass-Of", "CNN"], ["VD - CNN", "SubClass-Of", "CNN"], ["DPCNN", "SubClass-Of", "CNN"], ["D - LSTM", "SubClass-Of", "RNN"], ["Skim - LSTM", "SubClass-Of", "RNN"], ["hierarchical attention networks", "SubClass-Of", "RNN"], ["rigion embedding", "SubClass-Of", "feature - based transfer learning"], ["CoVe", "SubClass-Of", "feature - based transfer learning"], ["ULMFiT", "Synonym-Of", "language model fine - tuning"], ["language model fine - tuning", "Used-For", "text classification"]], "rel_plus": [["Char - level CNN:Method", "SubClass-Of", "CNN:Method"], ["VD - CNN:Method", "SubClass-Of", "CNN:Method"], ["DPCNN:Method", "SubClass-Of", "CNN:Method"], ["D - LSTM:Method", "SubClass-Of", "RNN:Method"], ["Skim - LSTM:Method", "SubClass-Of", "RNN:Method"], ["hierarchical attention networks:Method", "SubClass-Of", "RNN:Method"], ["rigion embedding:Method", "SubClass-Of", "feature - based transfer learning:Method"], ["CoVe:Method", "SubClass-Of", "feature - based transfer learning:Method"], ["ULMFiT:Method", "Synonym-Of", "language model fine - tuning:Method"], ["language model fine - tuning:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "195347056", "sentence": "Since then , many extensions of GAN [ 5 ] - [ 1 1 ] have been introduced and shown significant promise in synthetically generating structural images using the MNIST [ 1 2 ] , CIFAR - 1 0 [ 1 3 ] and ImageNet [ 1 4 ] datasets .", "ner": [["GAN", "Method"], ["synthetically generating structural images", "Task"], ["MNIST", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "synthetically generating structural images"], ["GAN", "Used-For", "synthetically generating structural images"], ["CIFAR - 1 0", "Benchmark-For", "synthetically generating structural images"], ["ImageNet", "Benchmark-For", "synthetically generating structural images"], ["GAN", "Evaluated-With", "MNIST"], ["GAN", "Evaluated-With", "CIFAR - 1 0"], ["GAN", "Evaluated-With", "ImageNet"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["GAN:Method", "Used-For", "synthetically generating structural images:Task"], ["CIFAR - 1 0:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["ImageNet:Dataset", "Benchmark-For", "synthetically generating structural images:Task"], ["GAN:Method", "Evaluated-With", "MNIST:Dataset"], ["GAN:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["GAN:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "11241677", "sentence": "For the LSTM networks , we have two scenarios : 1 ) we use the PCA transformed features and learn a LSTM model from scratch using these features ; or 2 ) we use the LSTM layers pre - trained on the YouTube - 8 M task , and fine - tune them on the Sports - 1 M dataset ( along with a new softmax classifier ) .", "ner": [["LSTM", "Method"], ["PCA transformed features", "Method"], ["LSTM", "Method"], ["LSTM layers", "Method"], ["YouTube - 8 M", "Dataset"], ["Sports - 1 M", "Dataset"], ["softmax classifier", "Method"]], "rel": [["PCA transformed features", "Used-For", "LSTM"], ["softmax classifier", "Part-Of", "LSTM layers"], ["LSTM layers", "Trained-With", "YouTube - 8 M"], ["LSTM layers", "Trained-With", "Sports - 1 M"]], "rel_plus": [["PCA transformed features:Method", "Used-For", "LSTM:Method"], ["softmax classifier:Method", "Part-Of", "LSTM layers:Method"], ["LSTM layers:Method", "Trained-With", "YouTube - 8 M:Dataset"], ["LSTM layers:Method", "Trained-With", "Sports - 1 M:Dataset"]]}
{"doc_id": "59599694", "sentence": "Starting in the 1 9 7 0 's with the original work of Gazis and Knapp [ 1 4 ] there have been many studies applying time series forecasting techniques to traffic flow prediction problem , including parametric techniques , such as auto - regressive integrated moving average ( ARIMA ) [ 1 5 ] and Seasonal - ARIMA [ 1 6 ] , and statistical techniques , such as Bayesian analysis [ 1 7 ] , Markov chain [ 1 8 ] and Bayesian networks [ 1 9 ] .", "ner": [["time series forecasting techniques", "Method"], ["traffic flow prediction", "Task"], ["parametric techniques", "Method"], ["auto - regressive integrated moving average", "Method"], ["ARIMA", "Method"], ["Seasonal - ARIMA", "Method"], ["statistical techniques", "Method"], ["Bayesian analysis", "Method"], ["Markov chain", "Method"], ["Bayesian networks", "Method"]], "rel": [["time series forecasting techniques", "Used-For", "traffic flow prediction"], ["auto - regressive integrated moving average", "SubClass-Of", "parametric techniques"], ["Seasonal - ARIMA", "SubClass-Of", "parametric techniques"], ["ARIMA", "Synonym-Of", "auto - regressive integrated moving average"], ["Bayesian analysis", "SubClass-Of", "statistical techniques"], ["Markov chain", "SubClass-Of", "statistical techniques"], ["Bayesian networks", "SubClass-Of", "statistical techniques"]], "rel_plus": [["time series forecasting techniques:Method", "Used-For", "traffic flow prediction:Task"], ["auto - regressive integrated moving average:Method", "SubClass-Of", "parametric techniques:Method"], ["Seasonal - ARIMA:Method", "SubClass-Of", "parametric techniques:Method"], ["ARIMA:Method", "Synonym-Of", "auto - regressive integrated moving average:Method"], ["Bayesian analysis:Method", "SubClass-Of", "statistical techniques:Method"], ["Markov chain:Method", "SubClass-Of", "statistical techniques:Method"], ["Bayesian networks:Method", "SubClass-Of", "statistical techniques:Method"]]}
{"doc_id": "AUG098", "sentence": "Relation extraction is a task, not a dataset, improved by GloVe on SemEval 2010.", "ner": [["relation extraction", "Task"], ["GloVe", "Method"], ["SemEval 2010", "Dataset"]], "rel": [["GloVe", "Used-For", "relation extraction"], ["GloVe", "Evaluated-With", "SemEval 2010"]], "rel_plus": [["GloVe:Method", "Used-For", "relation extraction:Task"], ["GloVe:Method", "Evaluated-With", "SemEval 2010:Dataset"]]}
{"doc_id": "201646309", "sentence": "The paper is structured in the following way : Section 3 presents SBERT , section 4 evaluates SBERT on common STS tasks and on the challenging Argument Facet Similarity ( AFS ) corpus ( Misra et al. , 2 0 1 6 ) .", "ner": [["SBERT", "Method"], ["SBERT", "Method"], ["STS", "Task"], ["Argument Facet Similarity", "Dataset"], ["AFS", "Dataset"]], "rel": [["SBERT", "Used-For", "STS"], ["AFS", "Synonym-Of", "Argument Facet Similarity"], ["SBERT", "Evaluated-With", "Argument Facet Similarity"]], "rel_plus": [["SBERT:Method", "Used-For", "STS:Task"], ["AFS:Dataset", "Synonym-Of", "Argument Facet Similarity:Dataset"], ["SBERT:Method", "Evaluated-With", "Argument Facet Similarity:Dataset"]]}
{"doc_id": "AUG075", "sentence": "The 5-layer action recognition network enhances Kinetics performance.", "ner": [["5-layer action recognition network", "Method"], ["Kinetics", "Dataset"]], "rel": [["5-layer action recognition network", "Evaluated-With", "Kinetics"]], "rel_plus": [["5-layer action recognition network:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "102351044", "sentence": "Then we compare drop - neuron , drop - channel , drop - path and drop - layer together with their combinations on representative CNN architectures , based on which we propose enhancement for existing best models on CIFAR and SVHN datasets and achieve a significant better results .   The performance of dropout training mechanisms are evaluated on benchmark image classification datasets CIFAR [ 1 8 ] , SVHN [ 2 4 ] and ImageNet [ 4 ] .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"], ["CNN", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["dropout training mechanisms", "Method"], ["image classification", "Task"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["drop - neuron", "Part-Of", "CNN"], ["drop - channel", "Part-Of", "CNN"], ["drop - path", "Part-Of", "CNN"], ["drop - layer", "Part-Of", "CNN"], ["CNN", "Evaluated-With", "CIFAR"], ["CNN", "Evaluated-With", "SVHN"], ["dropout training mechanisms", "Used-For", "image classification"], ["CIFAR", "Benchmark-For", "image classification"], ["SVHN", "Benchmark-For", "image classification"], ["ImageNet", "Benchmark-For", "image classification"], ["dropout training mechanisms", "Evaluated-With", "CIFAR"], ["dropout training mechanisms", "Evaluated-With", "SVHN"], ["dropout training mechanisms", "Evaluated-With", "ImageNet"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "CNN:Method"], ["drop - channel:Method", "Part-Of", "CNN:Method"], ["drop - path:Method", "Part-Of", "CNN:Method"], ["drop - layer:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "Evaluated-With", "CIFAR:Dataset"], ["CNN:Method", "Evaluated-With", "SVHN:Dataset"], ["dropout training mechanisms:Method", "Used-For", "image classification:Task"], ["CIFAR:Dataset", "Benchmark-For", "image classification:Task"], ["SVHN:Dataset", "Benchmark-For", "image classification:Task"], ["ImageNet:Dataset", "Benchmark-For", "image classification:Task"], ["dropout training mechanisms:Method", "Evaluated-With", "CIFAR:Dataset"], ["dropout training mechanisms:Method", "Evaluated-With", "SVHN:Dataset"], ["dropout training mechanisms:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "208513596", "sentence": "Our experiments showed that XDC outperforms not only existing self - supervised representation learning methods but also fully - supervised ImageNet - and Kineticspretrained models in action recognition .", "ner": [["XDC", "Method"], ["ImageNet", "Dataset"], ["Kineticspretrained", "Dataset"], ["action recognition", "Task"]], "rel": [["ImageNet", "Used-For", "action recognition"], ["Kineticspretrained", "Used-For", "action recognition"], ["XDC", "Used-For", "action recognition"]], "rel_plus": [["ImageNet:Dataset", "Used-For", "action recognition:Task"], ["Kineticspretrained:Dataset", "Used-For", "action recognition:Task"], ["XDC:Method", "Used-For", "action recognition:Task"]]}
{"doc_id": "198147921", "sentence": "The approach simultaneously trains the 3D instance segmentation PointNet , the TNet , and the amodal box estimation PointNet , using a loss function that is defined as a weighted sum of the losses of the individual subnetworks .", "ner": [["3D instance segmentation", "Task"], ["PointNet", "Method"], ["TNet", "Method"], ["PointNet", "Method"]], "rel": [["PointNet", "Used-For", "3D instance segmentation"], ["TNet", "Used-For", "3D instance segmentation"]], "rel_plus": [["PointNet:Method", "Used-For", "3D instance segmentation:Task"], ["TNet:Method", "Used-For", "3D instance segmentation:Task"]]}
{"doc_id": "54447105", "sentence": "Classification datasets [ 1 8 , 6 1 ] with millions of images and numerous categories enabled the boost in accuracies of CNNs [ 5 9 ] .", "ner": [["Classification", "Task"], ["CNNs", "Method"]], "rel": [["CNNs", "Used-For", "Classification"]], "rel_plus": [["CNNs:Method", "Used-For", "Classification:Task"]]}
{"doc_id": "21683040", "sentence": "We adopt a SGD with momentum 0. 9 to optimize the FSSD which is initialized by a well pre - trained VGG 1 6 on ImageNet .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["FSSD", "Method"], ["VGG 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["momentum", "Part-Of", "SGD"], ["SGD", "Part-Of", "FSSD"], ["VGG 1 6", "Part-Of", "FSSD"], ["VGG 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["SGD:Method", "Part-Of", "FSSD:Method"], ["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["VGG 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "211020570", "sentence": "Training the ST - GAN and stacked hourglass network took around 8 hours and 6 hours respectively .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "As shown in Fig. 7 , this single scale of MS - FCN ( called SC MSFCN ) performs much worse than the complete version of MS - FCN in terms of the PR curve as well as the average precision , recall and F - measure .", "ner": [["single scale of MS - FCN", "Method"], ["SC MSFCN", "Method"], ["MS - FCN", "Method"]], "rel": [["SC MSFCN", "Synonym-Of", "single scale of MS - FCN"], ["single scale of MS - FCN", "Compare-With", "MS - FCN"]], "rel_plus": [["SC MSFCN:Method", "Synonym-Of", "single scale of MS - FCN:Method"], ["single scale of MS - FCN:Method", "Compare-With", "MS - FCN:Method"]]}
{"doc_id": "210164920", "sentence": "Among different deep learning algorithms , CNN got tremendous success in different fields of computer vision as well as grab the area of image segmentation [ 6 6 , 6 7 , 6 8 ] .", "ner": [["CNN", "Method"], ["computer vision", "Task"], ["image segmentation", "Task"]], "rel": [["CNN", "Used-For", "computer vision"], ["image segmentation", "SubTask-Of", "computer vision"], ["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "computer vision:Task"], ["image segmentation:Task", "SubTask-Of", "computer vision:Task"], ["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "147703932", "sentence": "Regarding parameter estimation , a root - mean - square - error ( RMSE ) loss is used for 2D ( L 2Dpose ) and 3D ( L 3Dpose ) pose estimation , while cross - entropy ( CE ) across the spatial dimension of the heatmaps is used for depth estimation ( L Depth ) and body part segmentation ( L BodyPart ) .", "ner": [["root - mean - square - error", "Method"], ["RMSE", "Method"], ["L 2Dpose", "Task"], ["L 3Dpose", "Task"], ["pose estimation", "Task"], ["cross - entropy", "Method"], ["CE", "Method"], ["depth estimation", "Task"], ["L Depth", "Task"], ["body part segmentation", "Task"], ["L BodyPart", "Task"]], "rel": [["RMSE", "Synonym-Of", "root - mean - square - error"], ["root - mean - square - error", "Used-For", "L 2Dpose"], ["root - mean - square - error", "Used-For", "L 3Dpose"], ["root - mean - square - error", "Used-For", "pose estimation"], ["CE", "Synonym-Of", "cross - entropy"], ["L Depth", "Synonym-Of", "depth estimation"], ["cross - entropy", "Used-For", "depth estimation"], ["L BodyPart", "Synonym-Of", "body part segmentation"], ["cross - entropy", "Used-For", "body part segmentation"]], "rel_plus": [["RMSE:Method", "Synonym-Of", "root - mean - square - error:Method"], ["root - mean - square - error:Method", "Used-For", "L 2Dpose:Task"], ["root - mean - square - error:Method", "Used-For", "L 3Dpose:Task"], ["root - mean - square - error:Method", "Used-For", "pose estimation:Task"], ["CE:Method", "Synonym-Of", "cross - entropy:Method"], ["L Depth:Task", "Synonym-Of", "depth estimation:Task"], ["cross - entropy:Method", "Used-For", "depth estimation:Task"], ["L BodyPart:Task", "Synonym-Of", "body part segmentation:Task"], ["cross - entropy:Method", "Used-For", "body part segmentation:Task"]]}
{"doc_id": "210839545", "sentence": "The soft gate masks M v , M h with the size of H \u00d7 W \u00d7 1 are predicted via a projection transformation F , where The transformation layer is defined with three consecutive operations : a 1 \u00d7 1 convolution , followed by a batch normalization ( BN ) and a rectified linear unit ( ReLU ) .", "ner": [["batch normalization", "Method"], ["BN", "Method"], ["rectified linear unit", "Method"], ["ReLU", "Method"]], "rel": [["BN", "Synonym-Of", "batch normalization"], ["ReLU", "Synonym-Of", "rectified linear unit"]], "rel_plus": [["BN:Method", "Synonym-Of", "batch normalization:Method"], ["ReLU:Method", "Synonym-Of", "rectified linear unit:Method"]]}
{"doc_id": "210164920", "sentence": "The architecture of SegNet is shown in figure 1 1 .   Like ParseNet , Global Convolution Network [ 9 1 ] has also used global features along with local features to make the pixel - wise prediction more accurate .", "ner": [["SegNet", "Method"], ["ParseNet", "Method"], ["Global Convolution Network", "Method"], ["pixel - wise prediction", "Task"]], "rel": [["ParseNet", "Used-For", "pixel - wise prediction"], ["Global Convolution Network", "Used-For", "pixel - wise prediction"]], "rel_plus": [["ParseNet:Method", "Used-For", "pixel - wise prediction:Task"], ["Global Convolution Network:Method", "Used-For", "pixel - wise prediction:Task"]]}
{"doc_id": "210713911", "sentence": "Replacing SE with SK improves performance by 1. 0 % and 4. 3 % for the top - 1 and mCE , respectively ( E 6 ) .", "ner": [["SE", "Method"], ["SK", "Method"]], "rel": [["SE", "Compare-With", "SK"]], "rel_plus": [["SE:Method", "Compare-With", "SK:Method"]]}
{"doc_id": "209532167", "sentence": "We compare fine - tuned CuBERT models to Transformer - based models trained from scratch on the classification tasks ( Section 4. 4 ) .", "ner": [["CuBERT", "Method"], ["Transformer - based models", "Method"], ["classification", "Task"]], "rel": [["CuBERT", "Compare-With", "Transformer - based models"], ["CuBERT", "Used-For", "classification"], ["Transformer - based models", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Compare-With", "Transformer - based models:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["Transformer - based models:Method", "Used-For", "classification:Task"]]}
{"doc_id": "21683040", "sentence": "We evaluate the FSSD in VOC PASCAL [ 6 ] dataset and MSCOCO [ 1 9 ] dataset .", "ner": [["FSSD", "Method"], ["VOC PASCAL", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["FSSD", "Evaluated-With", "VOC PASCAL"], ["FSSD", "Evaluated-With", "MSCOCO"]], "rel_plus": [["FSSD:Method", "Evaluated-With", "VOC PASCAL:Dataset"], ["FSSD:Method", "Evaluated-With", "MSCOCO:Dataset"]]}
{"doc_id": "203593581", "sentence": "DoReFa - Net ( Zhou et al. , 2 0 1 6 ) 8 8 . 2 8 9 . 9 9 0 . 5 PACT ( Choi et al. , 2 0 1 8 b ) 8 9 . 7 9 1 . 1 9 1 . 7 LQ - Net ( Zhang et al. , 2 0 1 8) 9 0 . 2 9 1 . 6 -PACT+SAWB+fpsc ( Choi et al. , 2 0 1 8 a )   Typically , quantization error is defined as the mean squared error between weightsW and and\u0174 before and after quantization respectively .", "ner": [["DoReFa - Net", "Method"], ["PACT", "Method"], ["LQ - Net", "Method"], ["-PACT+SAWB+fpsc", "Method"], ["quantization", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "104291983", "sentence": "Facial landmark detection results ( NME ) on WFLW test and 6 subsets : pose , expression ( expr . ) , illumination ( illu . ) , make - up ( mu . ) , occlusion ( occu . ) and blur .", "ner": [["Facial landmark detection", "Task"], ["WFLW", "Dataset"], ["pose", "Dataset"], ["expression", "Dataset"], ["expr .", "Dataset"], ["illumination", "Dataset"], ["illu .", "Dataset"], ["make - up", "Dataset"], ["mu .", "Dataset"], ["occlusion", "Dataset"], ["occu .", "Dataset"], ["blur", "Dataset"]], "rel": [["WFLW", "Benchmark-For", "Facial landmark detection"], ["pose", "SubClass-Of", "WFLW"], ["expression", "SubClass-Of", "WFLW"], ["illumination", "SubClass-Of", "WFLW"], ["make - up", "SubClass-Of", "WFLW"], ["occlusion", "SubClass-Of", "WFLW"], ["blur", "SubClass-Of", "WFLW"], ["expr .", "Synonym-Of", "expression"], ["illu .", "Synonym-Of", "illumination"], ["mu .", "Synonym-Of", "make - up"], ["occu .", "Synonym-Of", "occlusion"]], "rel_plus": [["WFLW:Dataset", "Benchmark-For", "Facial landmark detection:Task"], ["pose:Dataset", "SubClass-Of", "WFLW:Dataset"], ["expression:Dataset", "SubClass-Of", "WFLW:Dataset"], ["illumination:Dataset", "SubClass-Of", "WFLW:Dataset"], ["make - up:Dataset", "SubClass-Of", "WFLW:Dataset"], ["occlusion:Dataset", "SubClass-Of", "WFLW:Dataset"], ["blur:Dataset", "SubClass-Of", "WFLW:Dataset"], ["expr .:Dataset", "Synonym-Of", "expression:Dataset"], ["illu .:Dataset", "Synonym-Of", "illumination:Dataset"], ["mu .:Dataset", "Synonym-Of", "make - up:Dataset"], ["occu .:Dataset", "Synonym-Of", "occlusion:Dataset"]]}
{"doc_id": "201070697", "sentence": "CycleGAN was proposed by Zhu et al. [ 5 ] for image - to - image translation tasks in the absence of paired examples .", "ner": [["CycleGAN", "Method"], ["image - to - image translation", "Task"]], "rel": [["CycleGAN", "Used-For", "image - to - image translation"]], "rel_plus": [["CycleGAN:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "210860962", "sentence": "The downsampling blocks consist of a 4 \u00d7 4 convolution with stride 2 , duplicating the number of channels at its input .", "ner": [["downsampling blocks", "Method"], ["4 \u00d7 4 convolution", "Method"]], "rel": [["4 \u00d7 4 convolution", "Part-Of", "downsampling blocks"]], "rel_plus": [["4 \u00d7 4 convolution:Method", "Part-Of", "downsampling blocks:Method"]]}
{"doc_id": "209386851", "sentence": "We use ReLU inside the MLP and apply sigmoid to its output .", "ner": [["ReLU", "Method"], ["MLP", "Method"], ["sigmoid", "Method"]], "rel": [["ReLU", "Part-Of", "MLP"], ["sigmoid", "Part-Of", "MLP"]], "rel_plus": [["ReLU:Method", "Part-Of", "MLP:Method"], ["sigmoid:Method", "Part-Of", "MLP:Method"]]}
{"doc_id": "11241677", "sentence": "Towards this end , we provide extensive experiments comparing several state - of - the - art techniques for video representation learning , including Deep Networks [ 2 6 ] , and LSTMs ( Long Short - Term Memory Networks ) [ 1 3 ] on this dataset .", "ner": [["video representation learning", "Task"], ["Deep Networks", "Method"], ["LSTMs", "Method"], ["Long Short - Term Memory Networks", "Method"]], "rel": [["Deep Networks", "Used-For", "video representation learning"], ["LSTMs", "Used-For", "video representation learning"], ["LSTMs", "Synonym-Of", "Long Short - Term Memory Networks"]], "rel_plus": [["Deep Networks:Method", "Used-For", "video representation learning:Task"], ["LSTMs:Method", "Used-For", "video representation learning:Task"], ["LSTMs:Method", "Synonym-Of", "Long Short - Term Memory Networks:Method"]]}
{"doc_id": "51876625", "sentence": "T - CNN [ 1 8 ] uses 3D convolutions to estimate short tubes , micro - tubes rely on two successive frames [ 4 2 ] and pose - guided 3D convolutions add pose to a two - stream approach [ 6 5 ] .", "ner": [["T - CNN", "Method"], ["3D convolutions", "Method"], ["3D convolutions", "Method"]], "rel": [["3D convolutions", "Part-Of", "T - CNN"]], "rel_plus": [["3D convolutions:Method", "Part-Of", "T - CNN:Method"]]}
{"doc_id": "210860760", "sentence": "DNGR [ 2 0 ] is based on a deep learning approach that comprises three steps .", "ner": [["DNGR", "Method"], ["deep learning", "Method"]], "rel": [["deep learning", "Used-For", "DNGR"]], "rel_plus": [["deep learning:Method", "Used-For", "DNGR:Method"]]}
{"doc_id": "23569888", "sentence": "Fig. 1 reports a sample image for each category in iCWT : 1 1 categories ( in red in the figure ) are also in the ImageNet Large - Scale Visual Recognition Challenge ( ILSVRC ) 2 0 1 2 ( Russakovsky et al 2 0 1 5 ) , i.e. we found semantically and visually similar classes among the 1 0 0 0 of the classification challenge .", "ner": [["iCWT", "Dataset"], ["ImageNet Large - Scale Visual Recognition Challenge", "Dataset"], ["ILSVRC", "Dataset"], ["classification", "Task"]], "rel": [["ILSVRC", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge"], ["ImageNet Large - Scale Visual Recognition Challenge", "Benchmark-For", "classification"]], "rel_plus": [["ILSVRC:Dataset", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge:Dataset"], ["ImageNet Large - Scale Visual Recognition Challenge:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "208548469", "sentence": "Table 1 3 : The table shows the six state - of - the - art pretrained VQA models evaluation results on the GBQD and VQA dataset . \" - \" indicates the results are not available , \" -std \" represents the accuracy of VQA model evaluated on the complete testing set of GBQD and VQA dataset and \" -dev \" indicates the accuracy of VQA model evaluated on the partial testing set of GBQD and VQA dataset .", "ner": [["pretrained VQA models", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"], ["VQA model", "Method"], ["GBQD", "Dataset"], ["VQA dataset", "Dataset"]], "rel": [["pretrained VQA models", "Evaluated-With", "GBQD"], ["pretrained VQA models", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"], ["VQA model", "Evaluated-With", "GBQD"], ["VQA model", "Evaluated-With", "VQA dataset"]], "rel_plus": [["pretrained VQA models:Method", "Evaluated-With", "GBQD:Dataset"], ["pretrained VQA models:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"], ["VQA model:Method", "Evaluated-With", "GBQD:Dataset"], ["VQA model:Method", "Evaluated-With", "VQA dataset:Dataset"]]}
{"doc_id": "147703932", "sentence": "Overall , we can say that the cues of 2D pose and depth estimation help to improve the segmentation accuracy .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"]], "rel": [["2D pose and depth estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose and depth estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "210920315", "sentence": "When using same data visibility but 1. 3 \u00d7 input scale , Adaptive - NMS [ 1 9 ] and MGAN [ 3 3 ] achieve log - average miss rates of 5 4 . 0 and 4 9 . 6 , respectively on the HO set .", "ner": [["Adaptive - NMS", "Method"], ["MGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201070522", "sentence": "We provide a new embedding , GrOVLE , which adapts Word 2 Vec using two knowledge bases : WordNet and Visual Genome .", "ner": [["GrOVLE", "Method"], ["Word 2 Vec", "Method"], ["WordNet", "Dataset"], ["Visual Genome", "Dataset"]], "rel": [["GrOVLE", "SubClass-Of", "Word 2 Vec"], ["GrOVLE", "Trained-With", "WordNet"], ["GrOVLE", "Trained-With", "Visual Genome"]], "rel_plus": [["GrOVLE:Method", "SubClass-Of", "Word 2 Vec:Method"], ["GrOVLE:Method", "Trained-With", "WordNet:Dataset"], ["GrOVLE:Method", "Trained-With", "Visual Genome:Dataset"]]}
{"doc_id": "21683040", "sentence": "DSOD [ 3 0 ] investigates how to train a object detector from scratch and designs a DenseNet architecture to improve the parameter efficiency .", "ner": [["DSOD", "Method"], ["object detector", "Method"], ["DenseNet", "Method"]], "rel": [["DenseNet", "Part-Of", "DSOD"]], "rel_plus": [["DenseNet:Method", "Part-Of", "DSOD:Method"]]}
{"doc_id": "198147921", "sentence": "Another example is the object detection pipeline of Qi et al. [ 6 2 ] , which produces the full extents of an object 's bounding box in 3D from RGB - D images by using four sub - networks , namely : 3 6 3 . 7 MV 3 D [ 1 2 ] KITTI ( valid ) 3 5 5 . 1 MV 3 D [ 1 2 ] KITTI ( test ) 3 7 9 . 8 3D FCN [ 4 8 ] KITTI ( test ) 3 6 8 . 2 Using only 1 0 categories from the SUN RGB - D dataset . 3 Using the hard , cars subset of the KITTI dataset . 3 Using the hard , cars subset of the KITTI dataset . 4 Results taken from [ 3 0 ] . 5 Uses ResNet - 1 0 1 . 6 Using the PASCAL Visual Object Classes ( VOC ) evaluation . \u2022 A joint 2D RPN/ORN .", "ner": [["object detection", "Task"], ["MV 3 D", "Method"], ["KITTI ( valid )", "Dataset"], ["MV 3 D", "Method"], ["KITTI ( test )", "Dataset"], ["3D FCN", "Method"], ["KITTI ( test )", "Dataset"], ["SUN RGB - D", "Dataset"], ["KITTI", "Dataset"], ["KITTI", "Dataset"], ["ResNet - 1 0 1", "Method"], ["PASCAL Visual Object Classes", "Dataset"], ["VOC", "Dataset"]], "rel": [["MV 3 D", "Evaluated-With", "KITTI ( valid )"], ["MV 3 D", "Evaluated-With", "KITTI ( test )"], ["3D FCN", "Evaluated-With", "KITTI ( test )"], ["VOC", "Synonym-Of", "PASCAL Visual Object Classes"]], "rel_plus": [["MV 3 D:Method", "Evaluated-With", "KITTI ( valid ):Dataset"], ["MV 3 D:Method", "Evaluated-With", "KITTI ( test ):Dataset"], ["3D FCN:Method", "Evaluated-With", "KITTI ( test ):Dataset"], ["VOC:Dataset", "Synonym-Of", "PASCAL Visual Object Classes:Dataset"]]}
{"doc_id": "199668978", "sentence": "Table 4 : Question Generation Evaluation on Yahoo and CSU dataset . use the precision - based automatic metrics BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 [ 2 5 ] which measures the average n - gram precision on a set of reference sentences , with a penalty for overly short sentences , and ROUGE [ 1 7 ] based on recall and METEOR [ 3 ] that is based on both precision and recall to measure the generation results .", "ner": [["Question Generation", "Task"], ["Yahoo", "Dataset"], ["CSU", "Dataset"], ["precision - based automatic metrics", "Method"], ["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"], ["METEOR", "Method"]], "rel": [["Yahoo", "Benchmark-For", "Question Generation"], ["CSU", "Benchmark-For", "Question Generation"], ["BLEU - 1", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 2", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 3", "SubClass-Of", "precision - based automatic metrics"], ["BLEU - 4", "SubClass-Of", "precision - based automatic metrics"]], "rel_plus": [["Yahoo:Dataset", "Benchmark-For", "Question Generation:Task"], ["CSU:Dataset", "Benchmark-For", "Question Generation:Task"], ["BLEU - 1:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 2:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 3:Method", "SubClass-Of", "precision - based automatic metrics:Method"], ["BLEU - 4:Method", "SubClass-Of", "precision - based automatic metrics:Method"]]}
{"doc_id": "202734316", "sentence": "The former [ 2 3 ] investigate the effect of style augmentation on three tasks ( image classification , cross - domain classification and depth estimation ) whereas the latter [ 4 3 ] enforces the network to learn domain invariant features between source and restyled samples .", "ner": [["image classification", "Task"], ["cross - domain classification", "Task"], ["depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "VL - TE also performed be er than TE .", "ner": [["VL - TE", "Method"], ["TE", "Method"]], "rel": [["VL - TE", "Compare-With", "TE"]], "rel_plus": [["VL - TE:Method", "Compare-With", "TE:Method"]]}
{"doc_id": "4246700", "sentence": "We firstly test four handcrafted representations for captioning , and then use the different CNNs . 1 ) Results based on handcrafted representations : To evaluate the generated sentences based on handcrafted representations , four handcrafted representations are conducted including SIFT , BOW , FV and VLAD .", "ner": [["handcrafted representations", "Method"], ["captioning", "Task"], ["CNNs", "Method"], ["handcrafted representations", "Method"], ["handcrafted representations", "Method"], ["handcrafted representations", "Method"], ["SIFT", "Method"], ["BOW", "Method"], ["FV", "Method"], ["VLAD", "Method"]], "rel": [["handcrafted representations", "Used-For", "captioning"], ["CNNs", "Used-For", "captioning"], ["SIFT", "SubClass-Of", "handcrafted representations"], ["BOW", "SubClass-Of", "handcrafted representations"], ["FV", "SubClass-Of", "handcrafted representations"], ["VLAD", "SubClass-Of", "handcrafted representations"]], "rel_plus": [["handcrafted representations:Method", "Used-For", "captioning:Task"], ["CNNs:Method", "Used-For", "captioning:Task"], ["SIFT:Method", "SubClass-Of", "handcrafted representations:Method"], ["BOW:Method", "SubClass-Of", "handcrafted representations:Method"], ["FV:Method", "SubClass-Of", "handcrafted representations:Method"], ["VLAD:Method", "SubClass-Of", "handcrafted representations:Method"]]}
{"doc_id": "210861282", "sentence": "Additionally , our method is extended to UniPose - LSTM for multi - frame processing and achieves state - of - the - art results for temporal pose estimation in Video .", "ner": [["UniPose - LSTM", "Method"], ["multi - frame processing", "Task"], ["temporal pose estimation in Video", "Task"]], "rel": [["UniPose - LSTM", "Used-For", "multi - frame processing"], ["UniPose - LSTM", "Used-For", "temporal pose estimation in Video"]], "rel_plus": [["UniPose - LSTM:Method", "Used-For", "multi - frame processing:Task"], ["UniPose - LSTM:Method", "Used-For", "temporal pose estimation in Video:Task"]]}
{"doc_id": "35249701", "sentence": "In particular , we take a single ImageNet - trained VGG - 1 6 network [ 2 8 ] and add to it three fine - grained classification tasks -CUBS birds [ 2 9 ] , Stanford Cars [ 1 5 ] , and Oxford Flowers [ 2 1 ] -while achieving accuracies very close to those of separately trained networks for each individual task .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["fine - grained classification", "Task"], ["-CUBS birds", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford Flowers", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["-CUBS birds", "Benchmark-For", "fine - grained classification"], ["Stanford Cars", "Benchmark-For", "fine - grained classification"], ["Oxford Flowers", "Benchmark-For", "fine - grained classification"], ["VGG - 1 6", "Used-For", "fine - grained classification"], ["VGG - 1 6", "Evaluated-With", "-CUBS birds"], ["VGG - 1 6", "Evaluated-With", "Stanford Cars"], ["VGG - 1 6", "Evaluated-With", "Oxford Flowers"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["-CUBS birds:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Stanford Cars:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Oxford Flowers:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Used-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Evaluated-With", "-CUBS birds:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "Stanford Cars:Dataset"], ["VGG - 1 6:Method", "Evaluated-With", "Oxford Flowers:Dataset"]]}
{"doc_id": "21683040", "sentence": "Then we append some downsampling blocks to generate new feature pyramid , which are fed to multibox detectors to produce the final detection re - features are used to detect objects , which is used in some two stage detectors such as Faster R - CNN [ 2 7 ] and R - FCN [ 3 ] . ( c ) Feature fusion method adopted by [ 1 8 , 2 4 ]   Using the proposed architecture , our FSSD improves a lot in performance at a slight expense of speed compared with conventional SSD .", "ner": [["downsampling blocks", "Method"], ["generate new feature pyramid", "Task"], ["two stage detectors", "Method"], ["Faster R - CNN", "Method"], ["R - FCN", "Method"], ["FSSD", "Method"], ["SSD", "Method"]], "rel": [["downsampling blocks", "Used-For", "generate new feature pyramid"], ["Faster R - CNN", "SubClass-Of", "two stage detectors"], ["R - FCN", "SubClass-Of", "two stage detectors"], ["downsampling blocks", "Part-Of", "two stage detectors"], ["downsampling blocks", "Part-Of", "Faster R - CNN"], ["downsampling blocks", "Part-Of", "R - FCN"], ["FSSD", "Compare-With", "SSD"]], "rel_plus": [["downsampling blocks:Method", "Used-For", "generate new feature pyramid:Task"], ["Faster R - CNN:Method", "SubClass-Of", "two stage detectors:Method"], ["R - FCN:Method", "SubClass-Of", "two stage detectors:Method"], ["downsampling blocks:Method", "Part-Of", "two stage detectors:Method"], ["downsampling blocks:Method", "Part-Of", "Faster R - CNN:Method"], ["downsampling blocks:Method", "Part-Of", "R - FCN:Method"], ["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "51559", "sentence": "Quasi - recurrent neural networks are related to several such recently described models , especially the strongly - typed recurrent neural networks ( T - RNN ) introduced by Balduzzi & Ghifary ( 2 0 1 6 ) .", "ner": [["Quasi - recurrent neural networks", "Method"], ["strongly - typed recurrent neural networks", "Method"], ["T - RNN", "Method"]], "rel": [["T - RNN", "Synonym-Of", "strongly - typed recurrent neural networks"]], "rel_plus": [["T - RNN:Method", "Synonym-Of", "strongly - typed recurrent neural networks:Method"]]}
{"doc_id": "210713911", "sentence": "In addition , in the area of AutoML , network design was automatically decided to create models such as NASNet and MNASNet .", "ner": [["AutoML", "Task"], ["NASNet", "Method"], ["MNASNet", "Method"]], "rel": [["NASNet", "Used-For", "AutoML"], ["MNASNet", "Used-For", "AutoML"]], "rel_plus": [["NASNet:Method", "Used-For", "AutoML:Task"], ["MNASNet:Method", "Used-For", "AutoML:Task"]]}
{"doc_id": "23569888", "sentence": "For this analysis we selected recent architectures achieving the highest accuracy on the ImageNet Large - Scale Visual Recognition Challenge ( ILSVRC ) ( Russakovsky et al 2 0 1 5 ) between 2 0 1 2 and 2 0 1 5 .", "ner": [["ImageNet Large - Scale Visual Recognition Challenge", "Dataset"], ["ILSVRC", "Dataset"]], "rel": [["ILSVRC", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge"]], "rel_plus": [["ILSVRC:Dataset", "Synonym-Of", "ImageNet Large - Scale Visual Recognition Challenge:Dataset"]]}
{"doc_id": "209532167", "sentence": "For the rest of the tasks , CuBERT with only 2 finetuning epochs outperforms BiLSTM ( with the best task - wise Word 2 Vec configuration ) by a margin of 0. 7 - 1 2 % .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"]], "rel": [["CuBERT", "Compare-With", "BiLSTM"], ["Word 2 Vec", "Part-Of", "BiLSTM"]], "rel_plus": [["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"]]}
{"doc_id": "210920315", "sentence": "Convolutional neural networks ( CNNs ) have significantly advanced the state - of - the - art in numerous computer vision applications , such as image classification [ 1 2 ] , [ 4 0 ] , [ 3 8 ] , [ 4 2 ] , object detection [ 3 6 ] , [ 3 2 ] , [ 4 4 ] , [ 2 8 ] , [ 2 0 ] , object counting [ 4 7 ] , [ 9 ] , [ 8 ] , [ 1 6 ] , image retrieval [ 3 4 ] , [ 3 5 ] , [ 2 2 ] , [ 4 8 ] , action recognition [ 3 9 ] , [ 3 7 ] , [ 1 8 ] , [ 2 7 ] , and pedestrian detection [ 5 1 ] , [ 3 3 ] , [ 5 2 ] , [ 4 5 ] .", "ner": [["Convolutional neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["image classification", "Task"], ["object detection", "Task"], ["object counting", "Task"], ["image retrieval", "Task"], ["action recognition", "Task"], ["pedestrian detection", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional neural networks"], ["Convolutional neural networks", "Used-For", "computer vision"], ["image classification", "SubTask-Of", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["object counting", "SubTask-Of", "computer vision"], ["image retrieval", "SubTask-Of", "computer vision"], ["action recognition", "SubTask-Of", "computer vision"], ["pedestrian detection", "SubTask-Of", "computer vision"], ["Convolutional neural networks", "Used-For", "image classification"], ["Convolutional neural networks", "Used-For", "object detection"], ["Convolutional neural networks", "Used-For", "object counting"], ["Convolutional neural networks", "Used-For", "image retrieval"], ["Convolutional neural networks", "Used-For", "action recognition"], ["Convolutional neural networks", "Used-For", "pedestrian detection"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional neural networks:Method"], ["Convolutional neural networks:Method", "Used-For", "computer vision:Task"], ["image classification:Task", "SubTask-Of", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["object counting:Task", "SubTask-Of", "computer vision:Task"], ["image retrieval:Task", "SubTask-Of", "computer vision:Task"], ["action recognition:Task", "SubTask-Of", "computer vision:Task"], ["pedestrian detection:Task", "SubTask-Of", "computer vision:Task"], ["Convolutional neural networks:Method", "Used-For", "image classification:Task"], ["Convolutional neural networks:Method", "Used-For", "object detection:Task"], ["Convolutional neural networks:Method", "Used-For", "object counting:Task"], ["Convolutional neural networks:Method", "Used-For", "image retrieval:Task"], ["Convolutional neural networks:Method", "Used-For", "action recognition:Task"], ["Convolutional neural networks:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "210860760", "sentence": "In this paper , we propose a graph embedding method , called ExEm , that uses dominating - set theory and deep learning approaches to capture node representations .", "ner": [["graph embedding method", "Method"], ["ExEm", "Method"], ["deep learning", "Method"]], "rel": [["ExEm", "SubClass-Of", "graph embedding method"]], "rel_plus": [["ExEm:Method", "SubClass-Of", "graph embedding method:Method"]]}
{"doc_id": "202888751", "sentence": "In disease recognition , shallow learning approaches have been applied for classifying tomato powdery mildew against healthy leaves by means of thermal and stereo images ( Prince et al. , 2 0 1 5 ) , detecting yellow leaf curl virus in tomatoes by using a set of classic feature extraction steps , classified by SVM ( Mokhtar et al. , 2 0 1 5 ) , recognition of tomato diseases in a greenhouse ( Chai and Wang , 2 0 1 3 ) etc .", "ner": [["disease recognition", "Task"], ["shallow learning approaches", "Method"], ["classifying tomato powdery mildew", "Task"], ["detecting yellow leaf curl virus in tomatoes", "Task"], ["feature extraction", "Task"], ["SVM", "Method"], ["recognition of tomato diseases in a greenhouse", "Task"]], "rel": [["classifying tomato powdery mildew", "SubTask-Of", "disease recognition"], ["shallow learning approaches", "Used-For", "classifying tomato powdery mildew"], ["feature extraction", "Used-For", "detecting yellow leaf curl virus in tomatoes"], ["SVM", "Used-For", "detecting yellow leaf curl virus in tomatoes"], ["SVM", "Used-For", "recognition of tomato diseases in a greenhouse"]], "rel_plus": [["classifying tomato powdery mildew:Task", "SubTask-Of", "disease recognition:Task"], ["shallow learning approaches:Method", "Used-For", "classifying tomato powdery mildew:Task"], ["feature extraction:Task", "Used-For", "detecting yellow leaf curl virus in tomatoes:Task"], ["SVM:Method", "Used-For", "detecting yellow leaf curl virus in tomatoes:Task"], ["SVM:Method", "Used-For", "recognition of tomato diseases in a greenhouse:Task"]]}
{"doc_id": "201124533", "sentence": "The applications of the HRNet are not limited to the above that we have done , and are suitable to other positionsensitive vision applications , such as super - resolution , optical flow estimation , depth estimation , and so on .", "ner": [["HRNet", "Method"], ["super - resolution", "Task"], ["optical flow estimation", "Task"], ["depth estimation", "Task"]], "rel": [["HRNet", "Used-For", "super - resolution"], ["HRNet", "Used-For", "optical flow estimation"], ["HRNet", "Used-For", "depth estimation"]], "rel_plus": [["HRNet:Method", "Used-For", "super - resolution:Task"], ["HRNet:Method", "Used-For", "optical flow estimation:Task"], ["HRNet:Method", "Used-For", "depth estimation:Task"]]}
{"doc_id": "195347056", "sentence": "The first two types , namely as the ArtGAN - EB and the ArtGAN - AE are implemented using the pixel - level autoencoder , similar to the EBGAN [ 1 5 ] .", "ner": [["ArtGAN - EB", "Method"], ["ArtGAN - AE", "Method"], ["pixel - level autoencoder", "Method"], ["EBGAN", "Method"]], "rel": [["pixel - level autoencoder", "Part-Of", "ArtGAN - EB"], ["pixel - level autoencoder", "Part-Of", "ArtGAN - AE"], ["ArtGAN - AE", "Compare-With", "EBGAN"], ["ArtGAN - EB", "Compare-With", "EBGAN"]], "rel_plus": [["pixel - level autoencoder:Method", "Part-Of", "ArtGAN - EB:Method"], ["pixel - level autoencoder:Method", "Part-Of", "ArtGAN - AE:Method"], ["ArtGAN - AE:Method", "Compare-With", "EBGAN:Method"], ["ArtGAN - EB:Method", "Compare-With", "EBGAN:Method"]]}
{"doc_id": "AUG015", "sentence": "The multi-head attention mechanism optimizes machine comprehension on MS MARCO.", "ner": [["multi-head attention mechanism", "Method"], ["machine comprehension", "Task"], ["MS MARCO", "Dataset"]], "rel": [["multi-head attention mechanism", "Used-For", "machine comprehension"], ["multi-head attention mechanism", "Evaluated-With", "MS MARCO"]], "rel_plus": [["multi-head attention mechanism:Method", "Used-For", "machine comprehension:Task"], ["multi-head attention mechanism:Method", "Evaluated-With", "MS MARCO:Dataset"]]}
{"doc_id": "102351044", "sentence": "Lastly , with drop - path and drop - channel mechanisms introduced , DenseNet - L 1 6 9 - K 3 2 achieves 0. 4 3 % error rate drop .", "ner": [["drop - path", "Method"], ["drop - channel", "Method"], ["DenseNet - L 1 6 9 - K 3 2", "Method"]], "rel": [["drop - path", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"], ["drop - channel", "Part-Of", "DenseNet - L 1 6 9 - K 3 2"]], "rel_plus": [["drop - path:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"], ["drop - channel:Method", "Part-Of", "DenseNet - L 1 6 9 - K 3 2:Method"]]}
{"doc_id": "4539700", "sentence": "For action recognition , CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels ( 3D CNNs ) are recently more effective than CNNs with two - dimensional ( 2D ) kernels [ 2 ] .", "ner": [["action recognition", "Task"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Method"], ["3D CNNs", "Method"], ["CNNs with two - dimensional ( 2D ) kernels", "Method"]], "rel": [["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Used-For", "action recognition"], ["CNNs with two - dimensional ( 2D ) kernels", "Used-For", "action recognition"], ["3D CNNs", "Synonym-Of", "CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels", "Compare-With", "CNNs with two - dimensional ( 2D ) kernels"]], "rel_plus": [["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method", "Used-For", "action recognition:Task"], ["CNNs with two - dimensional ( 2D ) kernels:Method", "Used-For", "action recognition:Task"], ["3D CNNs:Method", "Synonym-Of", "CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method"], ["CNNs with spatio - temporal threedimensional ( 3D ) convolutional kernels:Method", "Compare-With", "CNNs with two - dimensional ( 2D ) kernels:Method"]]}
{"doc_id": "211020570", "sentence": "CNN - based methods can be generally classified into two categories : coordinate regression methods [ 2 7 ] - [ 2 9 ] and heatmap regression methods [ 3 0 ] - [ 3 5 ] .", "ner": [["CNN - based methods", "Method"], ["coordinate regression", "Method"], ["heatmap regression", "Method"]], "rel": [["coordinate regression", "SubClass-Of", "CNN - based methods"], ["heatmap regression", "SubClass-Of", "CNN - based methods"]], "rel_plus": [["coordinate regression:Method", "SubClass-Of", "CNN - based methods:Method"], ["heatmap regression:Method", "SubClass-Of", "CNN - based methods:Method"]]}
{"doc_id": "199543700", "sentence": "And the Improved GAN [ 5 2 ] , which propose a technique called feature matching to address the instability of GANs by specifying a new objective for the generator to prevents it from overtraining on the current discriminator .", "ner": [["GAN", "Method"], ["GANs", "Method"], ["generator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Table 7 compares XDC pretrained on three large - scale datasets against state - of - the - art self - supervised methods , after finetuning on the UCF 1 0 1 and HMDB 5 1 benchmarks .", "ner": [["XDC", "Method"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [["XDC", "Trained-With", "UCF 1 0 1"], ["XDC", "Trained-With", "HMDB 5 1"]], "rel_plus": [["XDC:Method", "Trained-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Trained-With", "HMDB 5 1:Dataset"]]}
{"doc_id": "202888986", "sentence": "In this section , we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture .", "ner": [["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "Compare-With", "BERT"]], "rel_plus": [["ALBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "44148233", "sentence": "For decoding , different flavours of RNNs are used , such as deep RNN , Bi - directional RNN , LSTM or Gated Recurrent Units ( GRU ) .", "ner": [["RNNs", "Method"], ["RNN", "Method"], ["Bi - directional RNN", "Method"], ["LSTM", "Method"], ["Gated Recurrent Units", "Method"], ["GRU", "Method"]], "rel": [["RNN", "SubClass-Of", "RNNs"], ["Bi - directional RNN", "SubClass-Of", "RNNs"], ["Gated Recurrent Units", "SubClass-Of", "RNNs"], ["LSTM", "SubClass-Of", "RNNs"], ["GRU", "Synonym-Of", "Gated Recurrent Units"]], "rel_plus": [["RNN:Method", "SubClass-Of", "RNNs:Method"], ["Bi - directional RNN:Method", "SubClass-Of", "RNNs:Method"], ["Gated Recurrent Units:Method", "SubClass-Of", "RNNs:Method"], ["LSTM:Method", "SubClass-Of", "RNNs:Method"], ["GRU:Method", "Synonym-Of", "Gated Recurrent Units:Method"]]}
{"doc_id": "202888751", "sentence": "Compared to shallow learning methods which include support vector machines ( SVM ) , decision trees and na\u00efve bayes , deep learning models exhibit more representative power by passing input data through several non - linearity functions to produce robust descriptive features and perform recognition based on those features .", "ner": [["support vector machines", "Method"], ["SVM", "Method"], ["decision trees", "Method"], ["na\u00efve bayes", "Method"], ["deep learning models", "Method"], ["recognition", "Task"]], "rel": [["SVM", "Synonym-Of", "support vector machines"], ["deep learning models", "Compare-With", "support vector machines"], ["deep learning models", "Compare-With", "decision trees"], ["deep learning models", "Compare-With", "na\u00efve bayes"], ["deep learning models", "Used-For", "recognition"], ["support vector machines", "Used-For", "recognition"], ["decision trees", "Used-For", "recognition"], ["na\u00efve bayes", "Used-For", "recognition"], ["deep learning models", "Used-For", "recognition"]], "rel_plus": [["SVM:Method", "Synonym-Of", "support vector machines:Method"], ["deep learning models:Method", "Compare-With", "support vector machines:Method"], ["deep learning models:Method", "Compare-With", "decision trees:Method"], ["deep learning models:Method", "Compare-With", "na\u00efve bayes:Method"], ["deep learning models:Method", "Used-For", "recognition:Task"], ["support vector machines:Method", "Used-For", "recognition:Task"], ["decision trees:Method", "Used-For", "recognition:Task"], ["na\u00efve bayes:Method", "Used-For", "recognition:Task"], ["deep learning models:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "28984897", "sentence": "The VGG 1 6 network , configuration D in [ 1 6 ] , is a deep CNN with 1 6 layers : 1 3 convolutional layer , two fully connected layers , and a softmax layer .", "ner": [["VGG 1 6", "Method"], ["CNN", "Method"], ["convolutional layer", "Method"], ["fully connected layers", "Method"], ["softmax layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "VGG 1 6"], ["fully connected layers", "Part-Of", "VGG 1 6"], ["softmax layer", "Part-Of", "VGG 1 6"], ["VGG 1 6", "SubClass-Of", "CNN"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "VGG 1 6:Method"], ["fully connected layers:Method", "Part-Of", "VGG 1 6:Method"], ["softmax layer:Method", "Part-Of", "VGG 1 6:Method"], ["VGG 1 6:Method", "SubClass-Of", "CNN:Method"]]}
{"doc_id": "202577400", "sentence": "Extensive experiments verify the universality of GALD in improving the performance of semantic segmentation , object detection and instance segmentation .", "ner": [["GALD", "Method"], ["semantic segmentation", "Task"], ["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [["GALD", "Used-For", "semantic segmentation"], ["GALD", "Used-For", "object detection"], ["GALD", "Used-For", "instance segmentation"]], "rel_plus": [["GALD:Method", "Used-For", "semantic segmentation:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Mask R - CNN contains three stages : ( 1 ) Feature extraction : a \" backbone \" network , such as ResNet [ 1 5 ] , is used to extract features from an image . ( 2 ) Region proposal : A region proposal layer uses these features to selects regions likely to contain an object .", "ner": [["Mask R - CNN", "Method"], ["Feature extraction", "Method"], ["ResNet", "Method"], ["Region proposal", "Method"], ["region proposal layer", "Method"]], "rel": [["Feature extraction", "Part-Of", "Mask R - CNN"], ["ResNet", "SubClass-Of", "Feature extraction"]], "rel_plus": [["Feature extraction:Method", "Part-Of", "Mask R - CNN:Method"], ["ResNet:Method", "SubClass-Of", "Feature extraction:Method"]]}
{"doc_id": "201070522", "sentence": "Lastly , sentence level embeddings InferSent and BERT are compared in Table 1 ( d ) ; results are without fine - tuning .", "ner": [["InferSent", "Method"], ["BERT", "Method"]], "rel": [["InferSent", "Compare-With", "BERT"]], "rel_plus": [["InferSent:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "6423078", "sentence": "We significantly advance the state - of - the - art on the BSD 5 0 0 dataset ( ODS F - score of . 7 8 2 ) and the NYU Depth dataset ( ODS F - score of . 7 4 6 ) , and do so with an improved speed ( 0. 4 second per image ) that is orders of magnitude faster than some recent CNN - based edge detection algorithms .", "ner": [["BSD 5 0 0", "Dataset"], ["NYU Depth", "Dataset"], ["CNN", "Method"], ["edge detection", "Task"]], "rel": [["CNN", "Used-For", "edge detection"]], "rel_plus": [["CNN:Method", "Used-For", "edge detection:Task"]]}
{"doc_id": "208513596", "sentence": "While Kinetics and AudioSet are supervised benchmarks for action recognition and audio classification , IG 6 5 M is a large - scale weaklysupervised dataset collected from a social media website .", "ner": [["Kinetics", "Dataset"], ["AudioSet", "Dataset"], ["action recognition", "Task"], ["audio classification", "Task"], ["IG 6 5 M", "Dataset"]], "rel": [["AudioSet", "Benchmark-For", "action recognition"], ["Kinetics", "Benchmark-For", "action recognition"], ["AudioSet", "Benchmark-For", "audio classification"], ["Kinetics", "Benchmark-For", "audio classification"]], "rel_plus": [["AudioSet:Dataset", "Benchmark-For", "action recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"], ["AudioSet:Dataset", "Benchmark-For", "audio classification:Task"], ["Kinetics:Dataset", "Benchmark-For", "audio classification:Task"]]}
{"doc_id": "198231883", "sentence": "The previous state - of - the - art VIS method is MaskTrack R - CNN [ 4 3 ] , which achieves a mAP scores of 3 0 . 3 and 3 2 . 2 on the YouTube - VIS validation and test set respectively .", "ner": [["VIS", "Task"], ["MaskTrack R - CNN", "Method"], ["YouTube - VIS validation", "Dataset"]], "rel": [["MaskTrack R - CNN", "Used-For", "VIS"], ["YouTube - VIS validation", "Benchmark-For", "VIS"], ["YouTube - VIS validation", "Evaluated-With", "MaskTrack R - CNN"]], "rel_plus": [["MaskTrack R - CNN:Method", "Used-For", "VIS:Task"], ["YouTube - VIS validation:Dataset", "Benchmark-For", "VIS:Task"], ["YouTube - VIS validation:Dataset", "Evaluated-With", "MaskTrack R - CNN:Method"]]}
{"doc_id": "201646309", "sentence": "We fine - tune SBERT on NLI data , which creates sentence embeddings that significantly outperform other state - of - the - art sentence embedding methods like InferSent ( Conneau et al. , 2 0 1 7 ) and Universal Sentence Encoder .", "ner": [["SBERT", "Method"], ["NLI", "Task"], ["sentence embeddings", "Task"], ["sentence embedding", "Task"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["SBERT", "Used-For", "NLI"], ["SBERT", "Used-For", "sentence embeddings"], ["InferSent", "Used-For", "sentence embedding"], ["Universal Sentence Encoder", "Used-For", "sentence embedding"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["SBERT:Method", "Used-For", "NLI:Task"], ["SBERT:Method", "Used-For", "sentence embeddings:Task"], ["InferSent:Method", "Used-For", "sentence embedding:Task"], ["Universal Sentence Encoder:Method", "Used-For", "sentence embedding:Task"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "201646309", "sentence": "In contrast , SentEval fits a logistic regression classifier to the sentence embeddings .", "ner": [["SentEval", "Dataset"], ["logistic regression", "Method"], ["sentence embeddings", "Task"]], "rel": [["logistic regression", "Trained-With", "SentEval"], ["logistic regression", "Used-For", "sentence embeddings"], ["SentEval", "Benchmark-For", "sentence embeddings"]], "rel_plus": [["logistic regression:Method", "Trained-With", "SentEval:Dataset"], ["logistic regression:Method", "Used-For", "sentence embeddings:Task"], ["SentEval:Dataset", "Benchmark-For", "sentence embeddings:Task"]]}
{"doc_id": "146120936", "sentence": "The results of ' Nearest + Conv ' and ' Bilinear + Conv ' show that extra parameters do not lead to a significant gain . ' Deconv ' , ' Pixel Shuffle ' , ' GUM ' and ' Spatial Attention ' obtain inferior performance to CARAFE , indicating that the design of effective upsampling operators is critical .", "ner": [["Nearest + Conv", "Method"], ["Bilinear + Conv", "Method"], ["Deconv", "Method"], ["Pixel Shuffle", "Method"], ["GUM", "Method"], ["Spatial Attention", "Method"], ["CARAFE", "Method"], ["upsampling operators", "Method"]], "rel": [["Spatial Attention", "Compare-With", "CARAFE"], ["GUM", "Compare-With", "CARAFE"], ["Pixel Shuffle", "Compare-With", "CARAFE"], ["Deconv", "Compare-With", "CARAFE"], ["CARAFE", "SubClass-Of", "upsampling operators"]], "rel_plus": [["Spatial Attention:Method", "Compare-With", "CARAFE:Method"], ["GUM:Method", "Compare-With", "CARAFE:Method"], ["Pixel Shuffle:Method", "Compare-With", "CARAFE:Method"], ["Deconv:Method", "Compare-With", "CARAFE:Method"], ["CARAFE:Method", "SubClass-Of", "upsampling operators:Method"]]}
{"doc_id": "201070522", "sentence": "While Fast - Text is a more modern embedding , Word 2 Vec only falls behind within a point or two across all tasks , and even outperforms or performs equally as well as FastText for certain tasks ( e.g. text - to - clip , image captioning ) .", "ner": [["Fast - Text", "Method"], ["Word 2 Vec", "Method"], ["FastText", "Method"], ["text - to - clip", "Task"], ["image captioning", "Task"]], "rel": [["Word 2 Vec", "Compare-With", "FastText"], ["Word 2 Vec", "Used-For", "text - to - clip"], ["FastText", "Used-For", "text - to - clip"], ["Word 2 Vec", "Used-For", "image captioning"], ["FastText", "Used-For", "image captioning"]], "rel_plus": [["Word 2 Vec:Method", "Compare-With", "FastText:Method"], ["Word 2 Vec:Method", "Used-For", "text - to - clip:Task"], ["FastText:Method", "Used-For", "text - to - clip:Task"], ["Word 2 Vec:Method", "Used-For", "image captioning:Task"], ["FastText:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "207880647", "sentence": "This can largely be attributed to the self - attention mechanism in the Transformer that allows BERT facilitates generic applicability .", "ner": [["self - attention mechanism", "Method"], ["Transformer", "Method"], ["BERT", "Method"]], "rel": [["self - attention mechanism", "Part-Of", "Transformer"], ["Transformer", "Part-Of", "BERT"]], "rel_plus": [["self - attention mechanism:Method", "Part-Of", "Transformer:Method"], ["Transformer:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "6116678", "sentence": "A mask is computed for every segment in the feature map generated from the last true convolutional layer ( Conv 5 3 ) of MS - FCN as follows .", "ner": [["convolutional layer", "Method"], ["Conv 5 3", "Method"], ["MS - FCN", "Method"]], "rel": [["Conv 5 3", "Synonym-Of", "convolutional layer"], ["convolutional layer", "Part-Of", "MS - FCN"]], "rel_plus": [["Conv 5 3:Method", "Synonym-Of", "convolutional layer:Method"], ["convolutional layer:Method", "Part-Of", "MS - FCN:Method"]]}
{"doc_id": "207880647", "sentence": "BERT as QA module in the feedback loop model shows best performance , which may be attributed to the bi - directional context specific em - beddings leveraging a powerful feedback mechanism .", "ner": [["BERT", "Method"], ["QA", "Task"]], "rel": [["BERT", "Used-For", "QA"]], "rel_plus": [["BERT:Method", "Used-For", "QA:Task"]]}
{"doc_id": "59599694", "sentence": "Given X l as the input of layer l , a layer l + 1 obtains by X l+ 1 = \u03c3(X l * W l + b ) for an activation function \u03c3 ( . ) and bias vector b. Pooling layers X l+ 1 = maxPool(X l ) among successive convolution layers reduces size of hidden layers , while extract features in locally connected layers , which selects the maximum value in a matrix of sizeW \u2208 R m \u00d7 n , and reduce the dimension of layers divided by m and n. A Long - Short Term Memory ( LSTM ) is a special recurrent neural network cell with powerful modelling of long - term dependencies [ 3 1 ] .", "ner": [["Pooling", "Method"], ["convolution", "Method"], ["Long - Short Term Memory", "Method"], ["LSTM", "Method"], ["recurrent neural network", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long - Short Term Memory"], ["Long - Short Term Memory", "SubClass-Of", "recurrent neural network"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long - Short Term Memory:Method"], ["Long - Short Term Memory:Method", "SubClass-Of", "recurrent neural network:Method"]]}
{"doc_id": "52910494", "sentence": "Compared to the Inception module that simply concatenates different groups of convolutions , the aggregation layer we used can significantly reduces the number of parameters .", "ner": [["Inception module", "Method"], ["convolutions", "Method"], ["aggregation layer", "Method"]], "rel": [["convolutions", "Part-Of", "Inception module"], ["aggregation layer", "Part-Of", "Inception module"]], "rel_plus": [["convolutions:Method", "Part-Of", "Inception module:Method"], ["aggregation layer:Method", "Part-Of", "Inception module:Method"]]}
{"doc_id": "24972096", "sentence": "It utilizes the visual odometry trajectory from RGB - D SLAM [ 8 ] to wrap semantic segmentation between two viewpoints .", "ner": [["visual odometry trajectory", "Task"], ["RGB - D SLAM", "Method"], ["semantic segmentation", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "visual odometry trajectory"], ["visual odometry trajectory", "Used-For", "semantic segmentation"], ["RGB - D SLAM", "Used-For", "semantic segmentation"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "visual odometry trajectory:Task"], ["visual odometry trajectory:Task", "Used-For", "semantic segmentation:Task"], ["RGB - D SLAM:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "204402755", "sentence": "We select vanilla Adam as the baseline algorithm and include more recent state - of - the - art adaptive learning rate methods Quasi - Hyperbolic Adam ( QHAdam ) ( Ma & Yarats , 2 0 1 8) and AMSGrad ( Reddi et al. , 2 0 1 9 ) in our comparison .", "ner": [["Adam", "Method"], ["Quasi - Hyperbolic Adam", "Method"], ["QHAdam", "Method"], ["AMSGrad", "Method"]], "rel": [["QHAdam", "Synonym-Of", "Quasi - Hyperbolic Adam"], ["Adam", "Compare-With", "Quasi - Hyperbolic Adam"], ["Adam", "Compare-With", "AMSGrad"]], "rel_plus": [["QHAdam:Method", "Synonym-Of", "Quasi - Hyperbolic Adam:Method"], ["Adam:Method", "Compare-With", "Quasi - Hyperbolic Adam:Method"], ["Adam:Method", "Compare-With", "AMSGrad:Method"]]}
{"doc_id": "150374036", "sentence": "The texture - based features mainly include SIFT [ 4 9 ] , HOG [ 1 4 ] , Histograms of LBP [ 5 2 ] , Gabor wavelet coefficients [ 3 9 ] , etc .", "ner": [["texture - based features", "Method"], ["SIFT", "Method"], ["HOG", "Method"], ["Histograms of LBP", "Method"], ["Gabor wavelet coefficients", "Method"]], "rel": [["SIFT", "SubClass-Of", "texture - based features"], ["HOG", "SubClass-Of", "texture - based features"], ["Histograms of LBP", "SubClass-Of", "texture - based features"], ["Gabor wavelet coefficients", "SubClass-Of", "texture - based features"]], "rel_plus": [["SIFT:Method", "SubClass-Of", "texture - based features:Method"], ["HOG:Method", "SubClass-Of", "texture - based features:Method"], ["Histograms of LBP:Method", "SubClass-Of", "texture - based features:Method"], ["Gabor wavelet coefficients:Method", "SubClass-Of", "texture - based features:Method"]]}
{"doc_id": "210164920", "sentence": "From the year 2 0 1 2 , different CNN based semantic segmentation models have emerged in successive years to date .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202676714", "sentence": "Since SFA and Absum are based on a circulant matrix for convolution operation , we show that the convolution can be expressed as a product of a vector and doubly block circulant matrix .", "ner": [["SFA", "Method"], ["Absum", "Method"], ["circulant matrix", "Method"], ["convolution operation", "Method"], ["convolution", "Method"], ["block circulant matrix", "Method"]], "rel": [["circulant matrix", "Part-Of", "SFA"], ["circulant matrix", "Part-Of", "Absum"], ["circulant matrix", "Part-Of", "convolution operation"], ["block circulant matrix", "Part-Of", "convolution"]], "rel_plus": [["circulant matrix:Method", "Part-Of", "SFA:Method"], ["circulant matrix:Method", "Part-Of", "Absum:Method"], ["circulant matrix:Method", "Part-Of", "convolution operation:Method"], ["block circulant matrix:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "210164920", "sentence": "Previous object detection and instance segmentation modules such as [ 7 4 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] [ 1 2 7 ] , [ 1 2 8 ] , [ 1 3 4 ] etc . have used computationally expensive external methods for generating object level or mask level proposals like Selective Search , MCG , CPMC [ 7 7 ] , RPN etc .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["Selective Search", "Method"], ["MCG", "Method"], ["CPMC", "Method"], ["RPN", "Method"]], "rel": [["Selective Search", "Used-For", "object detection"], ["MCG", "Used-For", "object detection"], ["CPMC", "Used-For", "object detection"], ["RPN", "Used-For", "object detection"], ["Selective Search", "Used-For", "instance segmentation"], ["MCG", "Used-For", "instance segmentation"], ["CPMC", "Used-For", "instance segmentation"], ["RPN", "Used-For", "instance segmentation"]], "rel_plus": [["Selective Search:Method", "Used-For", "object detection:Task"], ["MCG:Method", "Used-For", "object detection:Task"], ["CPMC:Method", "Used-For", "object detection:Task"], ["RPN:Method", "Used-For", "object detection:Task"], ["Selective Search:Method", "Used-For", "instance segmentation:Task"], ["MCG:Method", "Used-For", "instance segmentation:Task"], ["CPMC:Method", "Used-For", "instance segmentation:Task"], ["RPN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "23569888", "sentence": "To this end , in the following we deepen our analysis on the performance of CNNs on iCWT , with particular focus on the concept of invariance .", "ner": [["CNNs", "Method"], ["iCWT", "Dataset"]], "rel": [["CNNs", "Evaluated-With", "iCWT"]], "rel_plus": [["CNNs:Method", "Evaluated-With", "iCWT:Dataset"]]}
{"doc_id": "201070697", "sentence": "It has been recently used for performance evaluation of imageto - image translation and image generation models [ 7 ] , [ 3 4 ] .", "ner": [["imageto - image translation", "Task"], ["image generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "This can be interpreted as layer - wise dropout , which achieves an ensemble of ResNets implicitly [ 3 3 ] .", "ner": [["layer - wise dropout", "Method"], ["ResNets", "Method"]], "rel": [["layer - wise dropout", "Part-Of", "ResNets"]], "rel_plus": [["layer - wise dropout:Method", "Part-Of", "ResNets:Method"]]}
{"doc_id": "35249701", "sentence": "In the case of Flowers classification , we perform better than the individual network , probably because training the full network causes it to overfit to the Flowers dataset , which is the smallest .", "ner": [["Flowers", "Dataset"], ["Flowers", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The methods of the natural image captioning can be divided to three categories , including retrieved - based method , object detection based method and encoder - decoder method .", "ner": [["natural image captioning", "Task"], ["retrieved - based method", "Method"], ["object detection based method", "Method"], ["encoder - decoder method", "Method"]], "rel": [["encoder - decoder method", "Used-For", "natural image captioning"], ["object detection based method", "Used-For", "natural image captioning"], ["retrieved - based method", "Used-For", "natural image captioning"]], "rel_plus": [["encoder - decoder method:Method", "Used-For", "natural image captioning:Task"], ["object detection based method:Method", "Used-For", "natural image captioning:Task"], ["retrieved - based method:Method", "Used-For", "natural image captioning:Task"]]}
{"doc_id": "24972096", "sentence": "Locality - sensitive deconvolution networks ( LS - DeconvNets ) [ 1 6 ] involve a locality - sensitive DeconvNet to refine the boundary segmentation and also a gated fusion layer for combining modalities ( RGB and HHA ) ; however the number of input modalities is limited to two .", "ner": [["Locality - sensitive deconvolution networks", "Method"], ["LS - DeconvNets", "Method"], ["locality - sensitive DeconvNet", "Method"], ["boundary segmentation", "Task"]], "rel": [["LS - DeconvNets", "Synonym-Of", "Locality - sensitive deconvolution networks"], ["locality - sensitive DeconvNet", "Part-Of", "Locality - sensitive deconvolution networks"], ["locality - sensitive DeconvNet", "Used-For", "boundary segmentation"], ["Locality - sensitive deconvolution networks", "Used-For", "boundary segmentation"]], "rel_plus": [["LS - DeconvNets:Method", "Synonym-Of", "Locality - sensitive deconvolution networks:Method"], ["locality - sensitive DeconvNet:Method", "Part-Of", "Locality - sensitive deconvolution networks:Method"], ["locality - sensitive DeconvNet:Method", "Used-For", "boundary segmentation:Task"], ["Locality - sensitive deconvolution networks:Method", "Used-For", "boundary segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "In this paper , we have tried to give a survey of different image segmentation models based on CNN .", "ner": [["image segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "image segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "104291983", "sentence": "We use the SGD optimizer with the base learning rate of 0.0 1 , the momentum of 0. 9 and the weight decay of 0.0 0 0 5 .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD"], ["weight decay", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"], ["weight decay:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "44148233", "sentence": "Textually Annotated Cooking Scenes ( TACoS ) is a subset of MP - II Composites [ 1 3 6 ] .", "ner": [["Textually Annotated Cooking Scenes", "Dataset"], ["TACoS", "Dataset"], ["MP - II Composites", "Dataset"]], "rel": [["TACoS", "Synonym-Of", "Textually Annotated Cooking Scenes"], ["Textually Annotated Cooking Scenes", "SubClass-Of", "MP - II Composites"]], "rel_plus": [["TACoS:Dataset", "Synonym-Of", "Textually Annotated Cooking Scenes:Dataset"], ["Textually Annotated Cooking Scenes:Dataset", "SubClass-Of", "MP - II Composites:Dataset"]]}
{"doc_id": "21683040", "sentence": "Another way to train the FSSD is the same as the conventional SSD algorithm .", "ner": [["FSSD", "Method"], ["SSD", "Method"]], "rel": [["FSSD", "Compare-With", "SSD"]], "rel_plus": [["FSSD:Method", "Compare-With", "SSD:Method"]]}
{"doc_id": "202676714", "sentence": "Therefore , the maxnorm regularization on convolution is ||A i || 2 = l m k 2 l , m \u2264 c and is similar to L 2 regularization .", "ner": [["maxnorm regularization", "Method"], ["convolution", "Method"], ["L 2 regularization", "Method"]], "rel": [["maxnorm regularization", "Part-Of", "convolution"], ["maxnorm regularization", "Compare-With", "L 2 regularization"]], "rel_plus": [["maxnorm regularization:Method", "Part-Of", "convolution:Method"], ["maxnorm regularization:Method", "Compare-With", "L 2 regularization:Method"]]}
{"doc_id": "199543700", "sentence": "We compare the proposed SGGAN approach with the fine - tuned VGG - 1 6 and ResNet - 5 0 networks with different numbers of labeled training images .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "3920676", "sentence": "To this end , we perform experiments with 6 , 9 , and 1 5 horizontal strips in the best handcrafted feature extraction algorithm , GOG , with Euclidean distance as the metric in the single - shot case and Euclidean distance as the metric and AVER as the ranking strategy in the multi - shot case .", "ner": [["handcrafted feature extraction algorithm", "Method"], ["GOG", "Method"], ["Euclidean distance", "Method"], ["Euclidean distance", "Method"]], "rel": [["GOG", "SubClass-Of", "handcrafted feature extraction algorithm"], ["Euclidean distance", "Part-Of", "GOG"]], "rel_plus": [["GOG:Method", "SubClass-Of", "handcrafted feature extraction algorithm:Method"], ["Euclidean distance:Method", "Part-Of", "GOG:Method"]]}
{"doc_id": "210713911", "sentence": "Channel Attention ( SE , SK ) We examine two tweaks in relation to channel attention .", "ner": [["Channel Attention", "Method"], ["SE", "Method"], ["SK", "Method"], ["channel attention", "Method"]], "rel": [["SE", "SubClass-Of", "Channel Attention"], ["SK", "SubClass-Of", "Channel Attention"]], "rel_plus": [["SE:Method", "SubClass-Of", "Channel Attention:Method"], ["SK:Method", "SubClass-Of", "Channel Attention:Method"]]}
{"doc_id": "210164920", "sentence": "ParseNet , PSPNet , and GCN have addressed semantic segmentation with respect to contextual information .", "ner": [["ParseNet", "Method"], ["PSPNet", "Method"], ["GCN", "Method"], ["semantic segmentation", "Task"]], "rel": [["GCN", "Used-For", "semantic segmentation"], ["PSPNet", "Used-For", "semantic segmentation"], ["ParseNet", "Used-For", "semantic segmentation"]], "rel_plus": [["GCN:Method", "Used-For", "semantic segmentation:Task"], ["PSPNet:Method", "Used-For", "semantic segmentation:Task"], ["ParseNet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "150374036", "sentence": "The ResNet - 1 8 is pre - trained on MS - Celeb - 1 M face recognition dataset and VGG 1 6 is downloaded from website 3 .", "ner": [["ResNet - 1 8", "Method"], ["MS - Celeb - 1 M", "Dataset"], ["face recognition", "Task"], ["VGG 1 6", "Method"]], "rel": [["ResNet - 1 8", "Trained-With", "MS - Celeb - 1 M"], ["ResNet - 1 8", "Used-For", "face recognition"], ["MS - Celeb - 1 M", "Benchmark-For", "face recognition"]], "rel_plus": [["ResNet - 1 8:Method", "Trained-With", "MS - Celeb - 1 M:Dataset"], ["ResNet - 1 8:Method", "Used-For", "face recognition:Task"], ["MS - Celeb - 1 M:Dataset", "Benchmark-For", "face recognition:Task"]]}
{"doc_id": "210164920", "sentence": "Every step of the contracting path consists of two consecutive 3 \u00d7 3 convolutions followed by ReLU nonlinearity and max - pooling using 2 \u00d7 2 window with stride 2 .", "ner": [["3 \u00d7 3 convolutions", "Method"], ["ReLU", "Method"], ["max - pooling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "A similar concept was adopted by the LSTM Pose Machine [ 2 7 ] approach , where the LSTM was utilized as the memory augmentation of the network .", "ner": [["LSTM Pose Machine", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Part-Of", "LSTM Pose Machine"]], "rel_plus": [["LSTM:Method", "Part-Of", "LSTM Pose Machine:Method"]]}
{"doc_id": "209862890", "sentence": "We conduct experiment analysis to answer the following questions : \u2022 Do the entity embeddings from BERT better capture latent entity type information than that of Ganea and Hofmann ( 2 0 1 7 ) ? \u2022 Does the proposed model correct the type errors in the baseline ( Ganea and Hofmann 2 0 1 7 ) ? \u2022 Can straightforward integration of state - of - the - art fine grained entity typing systems improve entity linking performance ? \u2022 Can better global model further boost the performance of the proposed model ?", "ner": [["entity embeddings", "Method"], ["BERT", "Method"], ["Ganea and Hofmann", "Method"], ["fine grained entity typing systems", "Method"], ["entity linking", "Task"]], "rel": [["BERT", "Used-For", "entity embeddings"], ["entity embeddings", "Compare-With", "Ganea and Hofmann"], ["fine grained entity typing systems", "Used-For", "entity linking"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Method"], ["entity embeddings:Method", "Compare-With", "Ganea and Hofmann:Method"], ["fine grained entity typing systems:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "146808333", "sentence": "Note that placing h - swish at all layers with 8 0 channels or more ( V 3 ) provides the best trade - offs for both optimized hswish and non - optimized h - swish .", "ner": [["h - swish", "Method"], ["h - swish", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "146120936", "sentence": "Thanks to the content information , CARAFE can use an adaptive and optimized reassembly kernel in different locations and achieve better performance than mainstream upsampling operators , e.g. interpolations or deconvolution .", "ner": [["CARAFE", "Method"], ["upsampling operators", "Method"], ["interpolations", "Method"], ["deconvolution", "Method"]], "rel": [["interpolations", "SubClass-Of", "upsampling operators"], ["deconvolution", "SubClass-Of", "upsampling operators"], ["CARAFE", "Compare-With", "upsampling operators"], ["CARAFE", "Compare-With", "interpolations"], ["CARAFE", "Compare-With", "deconvolution"]], "rel_plus": [["interpolations:Method", "SubClass-Of", "upsampling operators:Method"], ["deconvolution:Method", "SubClass-Of", "upsampling operators:Method"], ["CARAFE:Method", "Compare-With", "upsampling operators:Method"], ["CARAFE:Method", "Compare-With", "interpolations:Method"], ["CARAFE:Method", "Compare-With", "deconvolution:Method"]]}
{"doc_id": "202676714", "sentence": "Finally , as the further investigation , we discuss the evaluation of the performance of Absum and SNC in terms of robustness against transferred attacks , vulnerability in frequency domain , and robustness against PGD when used with adversarial training .", "ner": [["Absum", "Method"], ["SNC", "Method"], ["robustness against transferred attacks", "Task"], ["vulnerability in frequency domain", "Task"], ["robustness against PGD", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Used-For", "robustness against transferred attacks"], ["SNC", "Used-For", "robustness against transferred attacks"], ["SNC", "Used-For", "vulnerability in frequency domain"], ["Absum", "Used-For", "vulnerability in frequency domain"], ["Absum", "Used-For", "robustness against PGD"], ["SNC", "Used-For", "robustness against PGD"], ["Absum", "Part-Of", "adversarial training"], ["SNC", "Part-Of", "adversarial training"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against transferred attacks:Task"], ["SNC:Method", "Used-For", "robustness against transferred attacks:Task"], ["SNC:Method", "Used-For", "vulnerability in frequency domain:Task"], ["Absum:Method", "Used-For", "vulnerability in frequency domain:Task"], ["Absum:Method", "Used-For", "robustness against PGD:Task"], ["SNC:Method", "Used-For", "robustness against PGD:Task"], ["Absum:Method", "Part-Of", "adversarial training:Method"], ["SNC:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "199668978", "sentence": "The compared methods are summarized as follows : Seq 2 Seq is a basic encoder - decoder sequence learning system with Luong attention [ 1 8 ] and Bi - direction LSTM on encoder model . [ 8 ] is a NMT - based question paraphrasing method which assigns higher weights to those linguistic expressions likely to yield correct answers .", "ner": [["Seq 2 Seq", "Method"], ["encoder - decoder sequence learning system", "Method"], ["Luong attention", "Method"], ["Bi - direction LSTM", "Method"], ["NMT - based question paraphrasing method", "Method"]], "rel": [["Luong attention", "Part-Of", "Seq 2 Seq"], ["Seq 2 Seq", "SubClass-Of", "encoder - decoder sequence learning system"], ["Luong attention", "Part-Of", "encoder - decoder sequence learning system"], ["Bi - direction LSTM", "Part-Of", "encoder - decoder sequence learning system"]], "rel_plus": [["Luong attention:Method", "Part-Of", "Seq 2 Seq:Method"], ["Seq 2 Seq:Method", "SubClass-Of", "encoder - decoder sequence learning system:Method"], ["Luong attention:Method", "Part-Of", "encoder - decoder sequence learning system:Method"], ["Bi - direction LSTM:Method", "Part-Of", "encoder - decoder sequence learning system:Method"]]}
{"doc_id": "210860962", "sentence": "Finally , the model is trained by making the generators G ab and G ba minimize the objective where \u03bb is an hyper - parameter controlling the relative importance of the two objectives , and making the discriminators Da and D b minimize the objective C. StarGAN StarGAN [ 1 0 ] is a recently proposed alternative to CycleGAN to address image translation , and is also the model that we choose as backbone for the unsupervised domain adaptation pipeline introduced in Section IV .", "ner": [["generators G", "Method"], ["discriminators Da", "Method"], ["StarGAN", "Method"], ["StarGAN", "Method"], ["CycleGAN", "Method"], ["image translation", "Task"], ["unsupervised domain adaptation", "Method"]], "rel": [["CycleGAN", "Used-For", "image translation"], ["CycleGAN", "Part-Of", "unsupervised domain adaptation"]], "rel_plus": [["CycleGAN:Method", "Used-For", "image translation:Task"], ["CycleGAN:Method", "Part-Of", "unsupervised domain adaptation:Method"]]}
{"doc_id": "4246700", "sentence": "Then , human subjective evaluation is performed to compare the generalization capabilities of models trained on different datasets . 1 ) The imbalance of Sydney - captions : To verify the influence of unbalance of different kinds image numbers , we present the result of FV using different numbers of cluster center as the results of the FV is related to the number of cluster centers to construct a Gaussian Mixture Model ( GMM ) .", "ner": [["Sydney - captions", "Dataset"], ["FV", "Method"], ["FV", "Method"], ["Gaussian Mixture Model", "Method"], ["GMM", "Method"]], "rel": [["GMM", "Synonym-Of", "Gaussian Mixture Model"], ["FV", "Part-Of", "Gaussian Mixture Model"]], "rel_plus": [["GMM:Method", "Synonym-Of", "Gaussian Mixture Model:Method"], ["FV:Method", "Part-Of", "Gaussian Mixture Model:Method"]]}
{"doc_id": "210164517", "sentence": "Representational autoencoders and CNN model is built in [ 1 5 ] to predict human emotion .", "ner": [["autoencoders", "Method"], ["CNN", "Method"], ["predict human emotion", "Task"]], "rel": [["autoencoders", "Used-For", "predict human emotion"], ["CNN", "Used-For", "predict human emotion"]], "rel_plus": [["autoencoders:Method", "Used-For", "predict human emotion:Task"], ["CNN:Method", "Used-For", "predict human emotion:Task"]]}
{"doc_id": "208548469", "sentence": "In Ren et al ( 2 0 1 5 a ) , the authors exploit Recurrent Neural Networks ( RNN ) and Convolutional Neural Networks ( CNN ) to build a question generation algorithm , but it sometimes generates questions with invalid grammar .", "ner": [["Recurrent Neural Networks", "Method"], ["RNN", "Method"], ["Convolutional Neural Networks", "Method"], ["CNN", "Method"], ["question generation", "Task"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Networks"], ["CNN", "Synonym-Of", "Convolutional Neural Networks"], ["Recurrent Neural Networks", "Used-For", "question generation"], ["Convolutional Neural Networks", "Used-For", "question generation"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Networks:Method"], ["CNN:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Recurrent Neural Networks:Method", "Used-For", "question generation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "210860962", "sentence": "First , we will show qualitative results comparing CycleGAN and StarGAN as outlined in Section V - C 1 .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [["CycleGAN", "Compare-With", "StarGAN"]], "rel_plus": [["CycleGAN:Method", "Compare-With", "StarGAN:Method"]]}
{"doc_id": "146120936", "sentence": "After upsampled by CARAFE , a feature map can represent the shape of an object more accurately , so that the model can predict better instance segmentation results .", "ner": [["CARAFE", "Method"], ["instance segmentation", "Task"]], "rel": [["CARAFE", "Used-For", "instance segmentation"]], "rel_plus": [["CARAFE:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "102351044", "sentence": "For CIFAR datasets , DenseNet - L 1 9 0 - K 4 0 2 is the stateof - the - art model obtaining 4. 3 6 % and 1 7 . 1 8 % error rates on CIFAR - 1 0 and CIFAR - 1 0 0 respectively .", "ner": [["CIFAR", "Dataset"], ["DenseNet - L 1 9 0 - K 4 0", "Method"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"]], "rel": [["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR"], ["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR - 1 0"], ["DenseNet - L 1 9 0 - K 4 0", "Evaluated-With", "CIFAR - 1 0 0"]], "rel_plus": [["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR:Dataset"], ["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["DenseNet - L 1 9 0 - K 4 0:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "204901567", "sentence": "Pre - train a monolingual BERT ( i.e. a transformer ) in L 1 with masked language modeling ( MLM ) and next sentence prediction ( NSP ) objectives on an unlabeled L 1 corpus . 2 .", "ner": [["monolingual BERT", "Method"], ["transformer", "Method"], ["masked language modeling", "Task"], ["MLM", "Task"], ["next sentence prediction", "Task"], ["NSP", "Task"]], "rel": [["monolingual BERT", "SubClass-Of", "transformer"], ["MLM", "Synonym-Of", "masked language modeling"], ["monolingual BERT", "Trained-With", "masked language modeling"], ["NSP", "Synonym-Of", "next sentence prediction"], ["monolingual BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["monolingual BERT:Method", "SubClass-Of", "transformer:Method"], ["MLM:Task", "Synonym-Of", "masked language modeling:Task"], ["monolingual BERT:Method", "Trained-With", "masked language modeling:Task"], ["NSP:Task", "Synonym-Of", "next sentence prediction:Task"], ["monolingual BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "59599694", "sentence": "In existence of long - term patterns , an LSTM layers shows great performance in forecasting problems because it can separately capture detrending data .", "ner": [["LSTM", "Method"], ["forecasting", "Task"]], "rel": [["LSTM", "Used-For", "forecasting"]], "rel_plus": [["LSTM:Method", "Used-For", "forecasting:Task"]]}
{"doc_id": "204402755", "sentence": "Trained with a batch size of 2 0 . \u2022 MNIST -VAE MNIST contains 6 0 , 0 0 0 3 2 x 3 2 x 1 grayscale images with a 5 0 , 0 0 0 training set , 1 0 , 0 0 0 test set split .", "ner": [["MNIST -VAE", "Method"], ["MNIST", "Dataset"]], "rel": [["MNIST -VAE", "Trained-With", "MNIST"]], "rel_plus": [["MNIST -VAE:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "102351044", "sentence": "CIFAR - 1 0 ( C 1 0 ) consists of images drawn from 1 0 classes and CIFAR - 1 0 0 ( C 1 0 0 ) from 1 0 0 classes .", "ner": [["CIFAR - 1 0", "Dataset"], ["C 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["C 1 0 0", "Dataset"]], "rel": [["C 1 0", "Synonym-Of", "CIFAR - 1 0"], ["C 1 0 0", "Synonym-Of", "CIFAR - 1 0 0"]], "rel_plus": [["C 1 0:Dataset", "Synonym-Of", "CIFAR - 1 0:Dataset"], ["C 1 0 0:Dataset", "Synonym-Of", "CIFAR - 1 0 0:Dataset"]]}
{"doc_id": "195347056", "sentence": "It can be noticed that comparison to other stateof - the - art generative models , ArtGAN - AEMT obtains stateof - the - art result with a score of 8. 8 1 \u00b1 0. 1 4 , outperforming two lastest methods -SGAN [ 5 3 ] ( 8. 5 9 \u00b1 0. 1 2 ) and AC - GAN [ 9 ] ( 8. 2 5 \u00b1 0.0 7 ) .", "ner": [["generative models", "Method"], ["ArtGAN - AEMT", "Method"], ["-SGAN", "Method"], ["AC - GAN", "Method"]], "rel": [["-SGAN", "SubClass-Of", "generative models"], ["AC - GAN", "SubClass-Of", "generative models"], ["ArtGAN - AEMT", "Compare-With", "generative models"], ["ArtGAN - AEMT", "Compare-With", "-SGAN"], ["ArtGAN - AEMT", "Compare-With", "AC - GAN"]], "rel_plus": [["-SGAN:Method", "SubClass-Of", "generative models:Method"], ["AC - GAN:Method", "SubClass-Of", "generative models:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "generative models:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "-SGAN:Method"], ["ArtGAN - AEMT:Method", "Compare-With", "AC - GAN:Method"]]}
{"doc_id": "198147921", "sentence": "To leverage this sparsity , Engelcke et al. [ 1 8 ] extended the approach of Song et al. [ 8 0 ] by replacing the SVM ensemble with a 3D CNN , which operates on voxelized 3D grids .", "ner": [["SVM", "Method"], ["3D CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "The top branch which is the CNN based object proposal method of DeepMask predicts a class - agnostic segmentation mask and bottom branch assigns a score for estimating the likelihood of patch being centered on the full object .", "ner": [["CNN based object proposal method", "Method"], ["DeepMask", "Method"]], "rel": [["CNN based object proposal method", "Part-Of", "DeepMask"]], "rel_plus": [["CNN based object proposal method:Method", "Part-Of", "DeepMask:Method"]]}
{"doc_id": "202676714", "sentence": "The \u03bb of Absum tends to be higher than the \u03bb of WD and L 1 ; thus , Absum can also improve robustness against PGD without deteriorating classification performance due to its looseness .", "ner": [["Absum", "Method"], ["WD", "Method"], ["L 1", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"], ["classification", "Task"]], "rel": [["Absum", "Compare-With", "WD"], ["Absum", "Compare-With", "L 1"], ["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Compare-With", "WD:Method"], ["Absum:Method", "Compare-With", "L 1:Method"], ["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "202676714", "sentence": "The fast gradient sign method ( FGSM ) and PGD are popular as simple and sophisticated white - box attacks , respectively ( Goodfellow , Shlens , and Szegedy 2 0 1 4 ; Kurakin , Goodfellow , and Bengio 2 0 1 6 ; Madry et al. 2 0 1 8) .", "ner": [["fast gradient sign method", "Method"], ["FGSM", "Method"], ["PGD", "Method"], ["white - box attacks", "Method"]], "rel": [["FGSM", "Synonym-Of", "fast gradient sign method"], ["fast gradient sign method", "SubClass-Of", "white - box attacks"], ["PGD", "SubClass-Of", "white - box attacks"]], "rel_plus": [["FGSM:Method", "Synonym-Of", "fast gradient sign method:Method"], ["fast gradient sign method:Method", "SubClass-Of", "white - box attacks:Method"], ["PGD:Method", "SubClass-Of", "white - box attacks:Method"]]}
{"doc_id": "52910494", "sentence": "SFR w. SFR Improve SFR(k = 1 2 ) 1 7 1 9 6 6. 9 3 6. 8 0 0. 1 3 SFR(k = 2 4 ) 3 4 3 9 2 6. 4 5 6. 0 8 0. 3 7 SFR(WIL ) 3 4 5 3 6 6. 0 9 5. 7 6 0. 3 3 SFR(k = 4 0 ) 5 7 3 2 0 6. 5 3 6. 3 2 0. 2 1 Table 3 .", "ner": [["SFR", "Method"], ["SFR", "Method"], ["SFR(k = 1 2 )", "Method"], ["SFR(k", "Method"], ["SFR(WIL )", "Method"], ["SFR(k = 4 0 )", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "Consequently , the large number of unlabeled face images available on the Internet have attracted increasing interests of researchers to tackle facial attribute recognition problem by semi - supervised learning ( SSL ) [ 6 ] methods . 2 ) Comparisons methods : The proposed method is compared with four competitive fully - supervised approaches including FaceTracer [ 3 0 ] , PANDA - w [ 7 4 ] , LNet+ANet(w/o ) and LNet+ANet [ 7 4 ] on the two datasets mentioned above .", "ner": [["facial attribute recognition", "Task"], ["semi - supervised learning", "Method"], ["SSL", "Method"], ["fully - supervised approaches", "Method"], ["FaceTracer", "Method"], ["PANDA - w", "Method"], ["LNet+ANet(w/o", "Method"], ["LNet+ANet", "Method"]], "rel": [["semi - supervised learning", "Used-For", "facial attribute recognition"], ["SSL", "Synonym-Of", "semi - supervised learning"], ["FaceTracer", "SubClass-Of", "fully - supervised approaches"], ["PANDA - w", "SubClass-Of", "fully - supervised approaches"], ["LNet+ANet(w/o", "SubClass-Of", "fully - supervised approaches"], ["LNet+ANet", "SubClass-Of", "fully - supervised approaches"]], "rel_plus": [["semi - supervised learning:Method", "Used-For", "facial attribute recognition:Task"], ["SSL:Method", "Synonym-Of", "semi - supervised learning:Method"], ["FaceTracer:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["PANDA - w:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["LNet+ANet(w/o:Method", "SubClass-Of", "fully - supervised approaches:Method"], ["LNet+ANet:Method", "SubClass-Of", "fully - supervised approaches:Method"]]}
{"doc_id": "211020570", "sentence": "In [ 3 8 ] , an STN was embedded in cascaded CNNs , to jointly learn spatial transformation and landmark localization for face detection .", "ner": [["STN", "Method"], ["cascaded CNNs", "Method"], ["spatial transformation", "Task"], ["landmark localization", "Task"], ["face detection", "Task"]], "rel": [["STN", "Part-Of", "cascaded CNNs"], ["cascaded CNNs", "Used-For", "spatial transformation"], ["cascaded CNNs", "Used-For", "landmark localization"], ["cascaded CNNs", "Used-For", "face detection"]], "rel_plus": [["STN:Method", "Part-Of", "cascaded CNNs:Method"], ["cascaded CNNs:Method", "Used-For", "spatial transformation:Task"], ["cascaded CNNs:Method", "Used-For", "landmark localization:Task"], ["cascaded CNNs:Method", "Used-For", "face detection:Task"]]}
{"doc_id": "51876625", "sentence": "To stabilize training , the batch - norm updates are disabled during training and we apply a gradient multiplier of 0.0 1 to gradients from RN to the feature map .   We perform a number of ablation experiments to better understand the properties of our actor - centric relation network and its impact on the action detection performance .", "ner": [["RN", "Method"], ["actor - centric relation network", "Method"], ["action detection", "Task"]], "rel": [["actor - centric relation network", "Used-For", "action detection"]], "rel_plus": [["actor - centric relation network:Method", "Used-For", "action detection:Task"]]}
{"doc_id": "202676714", "sentence": "In addition , we used PGD to evaluate robustness against transferred attacks and white - box attacks since PGD is a sophisticated white - box attack .", "ner": [["PGD", "Method"], ["robustness against transferred attacks", "Task"], ["white - box attacks", "Task"], ["PGD", "Method"], ["white - box attack", "Task"]], "rel": [["PGD", "Used-For", "robustness against transferred attacks"], ["PGD", "Used-For", "white - box attacks"]], "rel_plus": [["PGD:Method", "Used-For", "robustness against transferred attacks:Task"], ["PGD:Method", "Used-For", "white - box attacks:Task"]]}
{"doc_id": "210860962", "sentence": "In order to compare both models , we provide a small set of qualitative examples in Figure 1 0 Along with the translation samples , we detail the number of parameters required by a standard implementation of both models , see Table VI -A. As shown in the table , CycleGAN and StarGAN use similar architectures for their generators and discriminators .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"], ["generators", "Method"], ["discriminators", "Method"]], "rel": [["generators", "Part-Of", "CycleGAN"], ["discriminators", "Part-Of", "CycleGAN"], ["CycleGAN", "Compare-With", "StarGAN"], ["discriminators", "Part-Of", "StarGAN"], ["generators", "Part-Of", "StarGAN"]], "rel_plus": [["generators:Method", "Part-Of", "CycleGAN:Method"], ["discriminators:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Compare-With", "StarGAN:Method"], ["discriminators:Method", "Part-Of", "StarGAN:Method"], ["generators:Method", "Part-Of", "StarGAN:Method"]]}
{"doc_id": "201124533", "sentence": "Table 3 provides the comparison with several representative methods on the Cityscapes val set in terms of parameter and computation complexity and mIoU class . ( i ) HRNetV 2 - W 4 0 ( 4 0 indicates the width of the high - resolution convolution ) , with similar model size to DeepLabv 3 + and much lower computation complexity , gets better performance : 4. 7 points gain over UNet++ , 1. 7 points gain over DeepLabv 3 and about 0. 5 points gain over PSPNet , DeepLabv 3 + . ( ii ) HRNetV 2 - W 4 8 , with similar model size to PSPNet and much lower computation complexity , achieves much significant improvement : 5. 6 points gain over UNet++ , 2. 6 points gain over DeepLabv 3 and about 1. 4 points gain over PSPNet , DeepLabv 3 + .", "ner": [["Cityscapes", "Dataset"], ["HRNetV 2 - W 4 0", "Method"], ["high - resolution convolution", "Method"], ["DeepLabv 3 +", "Method"], ["UNet++", "Method"], ["DeepLabv 3", "Method"], ["PSPNet", "Method"], ["DeepLabv 3 +", "Method"], ["HRNetV 2 - W 4 8", "Method"], ["PSPNet", "Method"], ["UNet++", "Method"], ["DeepLabv 3", "Method"], ["PSPNet", "Method"], ["DeepLabv 3 +", "Method"]], "rel": [["high - resolution convolution", "Part-Of", "HRNetV 2 - W 4 0"], ["HRNetV 2 - W 4 0", "Compare-With", "UNet++"], ["HRNetV 2 - W 4 0", "Compare-With", "DeepLabv 3"], ["HRNetV 2 - W 4 0", "Compare-With", "PSPNet"], ["HRNetV 2 - W 4 0", "Compare-With", "DeepLabv 3 +"], ["HRNetV 2 - W 4 8", "Compare-With", "UNet++"], ["HRNetV 2 - W 4 8", "Compare-With", "DeepLabv 3"], ["HRNetV 2 - W 4 8", "Compare-With", "PSPNet"], ["HRNetV 2 - W 4 8", "Compare-With", "DeepLabv 3 +"]], "rel_plus": [["high - resolution convolution:Method", "Part-Of", "HRNetV 2 - W 4 0:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "UNet++:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "DeepLabv 3:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "PSPNet:Method"], ["HRNetV 2 - W 4 0:Method", "Compare-With", "DeepLabv 3 +:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "UNet++:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "DeepLabv 3:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "PSPNet:Method"], ["HRNetV 2 - W 4 8:Method", "Compare-With", "DeepLabv 3 +:Method"]]}
{"doc_id": "6423078", "sentence": "Figure 5 shows the precision - recall of the previous edge detection methods and HED on the BSDS 5 0 0 dataset ; Table 5 shows a detailed quantitative measures between these competing approaches .", "ner": [["edge detection", "Task"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"]], "rel": [["HED", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"], ["HED", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["HED:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "199543700", "sentence": "Then we compare SGGAN with other state - of - the - art semisupervised GAN based approaches on image recognition problem on two widely employed datasets .", "ner": [["SGGAN", "Method"], ["semisupervised GAN based approaches", "Method"], ["image recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "semisupervised GAN based approaches"], ["SGGAN", "Used-For", "image recognition"], ["semisupervised GAN based approaches", "Used-For", "image recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "semisupervised GAN based approaches:Method"], ["SGGAN:Method", "Used-For", "image recognition:Task"], ["semisupervised GAN based approaches:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "22825560", "sentence": "Convolutional Neural Networks ( CNNs ) have been successfully applied to almost every problem in computer vision , from interest point description [ 1 ] to stereo [ 2 ] , object detection [ 3 ] , semantic segmentation [ 4 ] , action recognition [ 5 ] and Structure - from - Motion [ 6 ] .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["object detection", "Task"], ["semantic segmentation", "Task"], ["action recognition", "Task"], ["Structure - from - Motion", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["Convolutional Neural Networks", "Used-For", "computer vision"], ["object detection", "SubTask-Of", "computer vision"], ["semantic segmentation", "SubTask-Of", "computer vision"], ["action recognition", "SubTask-Of", "computer vision"], ["Structure - from - Motion", "SubTask-Of", "computer vision"], ["Convolutional Neural Networks", "Used-For", "object detection"], ["Convolutional Neural Networks", "Used-For", "semantic segmentation"], ["Convolutional Neural Networks", "Used-For", "action recognition"], ["Convolutional Neural Networks", "Used-For", "Structure - from - Motion"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Convolutional Neural Networks:Method", "Used-For", "computer vision:Task"], ["object detection:Task", "SubTask-Of", "computer vision:Task"], ["semantic segmentation:Task", "SubTask-Of", "computer vision:Task"], ["action recognition:Task", "SubTask-Of", "computer vision:Task"], ["Structure - from - Motion:Task", "SubTask-Of", "computer vision:Task"], ["Convolutional Neural Networks:Method", "Used-For", "object detection:Task"], ["Convolutional Neural Networks:Method", "Used-For", "semantic segmentation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "action recognition:Task"], ["Convolutional Neural Networks:Method", "Used-For", "Structure - from - Motion:Task"]]}
{"doc_id": "54447105", "sentence": "The problem of training CNNs for classification on small datasets is usually tackled by transfer learning methods [ 5 8 ] : the network is pretrained on a large labeled dataset such as Imagenet [ 1 8 ] , and fine - tuned on the task at hand while preserving the original capabilities [ 3 6 ] .", "ner": [["CNNs", "Method"], ["classification", "Task"], ["transfer learning methods", "Method"], ["Imagenet", "Dataset"]], "rel": [["CNNs", "Used-For", "classification"], ["transfer learning methods", "Used-For", "classification"]], "rel_plus": [["CNNs:Method", "Used-For", "classification:Task"], ["transfer learning methods:Method", "Used-For", "classification:Task"]]}
{"doc_id": "AUG072", "sentence": "Waveform-based speech recognition is enhanced by WaveNet on LibriSpeech.", "ner": [["waveform-based speech recognition", "Task"], ["WaveNet", "Method"], ["LibriSpeech", "Dataset"]], "rel": [["WaveNet", "Used-For", "waveform-based speech recognition"], ["WaveNet", "Evaluated-With", "LibriSpeech"]], "rel_plus": [["WaveNet:Method", "Used-For", "waveform-based speech recognition:Task"], ["WaveNet:Method", "Evaluated-With", "LibriSpeech:Dataset"]]}
{"doc_id": "24972096", "sentence": "A variety of well - known methods such as RGB - D SLAM [ 8 ] , Kinect Fusion [ 9 ] and ElasticFusion [ 1 0 ] can generate a dense or semi - dense 3D map from RGB - D videos .", "ner": [["RGB - D SLAM", "Method"], ["Kinect Fusion", "Method"], ["ElasticFusion", "Method"], ["generate a dense or semi - dense 3D map", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "generate a dense or semi - dense 3D map"], ["Kinect Fusion", "Used-For", "generate a dense or semi - dense 3D map"], ["ElasticFusion", "Used-For", "generate a dense or semi - dense 3D map"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"], ["Kinect Fusion:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"], ["ElasticFusion:Method", "Used-For", "generate a dense or semi - dense 3D map:Task"]]}
{"doc_id": "67855714", "sentence": "First methods used generative adversarial networks ( GANs ) for generating high perceptual quality images [ 3 6 , 3 7 ] , style transfer [ 3 8 ] and inpainting [ 3 9 ] , namely the class of methods P/adv with \u03bb 1 = 0 .", "ner": [["generative adversarial networks", "Method"], ["GANs", "Method"], ["generating high perceptual quality images", "Task"], ["style transfer", "Task"], ["inpainting", "Task"]], "rel": [["GANs", "Synonym-Of", "generative adversarial networks"], ["generative adversarial networks", "Used-For", "generating high perceptual quality images"], ["generative adversarial networks", "Used-For", "style transfer"], ["generative adversarial networks", "Used-For", "inpainting"]], "rel_plus": [["GANs:Method", "Synonym-Of", "generative adversarial networks:Method"], ["generative adversarial networks:Method", "Used-For", "generating high perceptual quality images:Task"], ["generative adversarial networks:Method", "Used-For", "style transfer:Task"], ["generative adversarial networks:Method", "Used-For", "inpainting:Task"]]}
{"doc_id": "211004033", "sentence": "All three variants with different mechanisms of occlusion handling show some improvements to the baseline ( i.e. , Ellipse R - CNN - with R ) , ranging from 2. 1 to 7. 2 on MR and from 4. 3 to 8. 6 on MR \u0398 .", "ner": [["occlusion handling", "Task"], ["Ellipse R - CNN - with R", "Method"]], "rel": [["Ellipse R - CNN - with R", "Used-For", "occlusion handling"]], "rel_plus": [["Ellipse R - CNN - with R:Method", "Used-For", "occlusion handling:Task"]]}
{"doc_id": "202676714", "sentence": "Figure 1 shows examples of CIFAR 1 0 perturbed by SFA .", "ner": [["CIFAR 1 0", "Dataset"], ["SFA", "Method"]], "rel": [["SFA", "Used-For", "CIFAR 1 0"]], "rel_plus": [["SFA:Method", "Used-For", "CIFAR 1 0:Dataset"]]}
{"doc_id": "210164920", "sentence": "SDS , DeepMask , Hyper - columns have used feature maps from top layers of the network for object instance detection which leads to coarse object mask generation .", "ner": [["SDS", "Method"], ["DeepMask", "Method"], ["Hyper - columns", "Method"], ["object instance detection", "Task"]], "rel": [["Hyper - columns", "Used-For", "object instance detection"], ["DeepMask", "Used-For", "object instance detection"], ["SDS", "Used-For", "object instance detection"]], "rel_plus": [["Hyper - columns:Method", "Used-For", "object instance detection:Task"], ["DeepMask:Method", "Used-For", "object instance detection:Task"], ["SDS:Method", "Used-For", "object instance detection:Task"]]}
{"doc_id": "211010758", "sentence": "Our method is also related to recent gradient - based meta learning methods : MAML [ 1 0 ] and Reptile [ 3 7 ] , which are designed to learn a good initialization for few shot learning and have demonstrated good within - task generalization .", "ner": [["gradient - based meta learning methods", "Method"], ["MAML", "Method"], ["Reptile", "Method"], ["few shot learning", "Task"], ["within - task generalization", "Task"]], "rel": [["MAML", "Part-Of", "gradient - based meta learning methods"], ["Reptile", "Part-Of", "gradient - based meta learning methods"], ["gradient - based meta learning methods", "Used-For", "few shot learning"], ["MAML", "Used-For", "few shot learning"], ["Reptile", "Used-For", "few shot learning"], ["gradient - based meta learning methods", "Used-For", "within - task generalization"], ["MAML", "Used-For", "within - task generalization"], ["Reptile", "Used-For", "within - task generalization"]], "rel_plus": [["MAML:Method", "Part-Of", "gradient - based meta learning methods:Method"], ["Reptile:Method", "Part-Of", "gradient - based meta learning methods:Method"], ["gradient - based meta learning methods:Method", "Used-For", "few shot learning:Task"], ["MAML:Method", "Used-For", "few shot learning:Task"], ["Reptile:Method", "Used-For", "few shot learning:Task"], ["gradient - based meta learning methods:Method", "Used-For", "within - task generalization:Task"], ["MAML:Method", "Used-For", "within - task generalization:Task"], ["Reptile:Method", "Used-For", "within - task generalization:Task"]]}
{"doc_id": "201646309", "sentence": "We experimented with two setups : Only training on STSb , and first training on NLI , then training on STSb .", "ner": [["STSb", "Dataset"], ["NLI", "Dataset"], ["STSb", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "150374036", "sentence": "In all the following experiments , we use the CNN detector and the ERT [ 3 1 ] based face alignment method in Dlib toolbox 1 to crop and align faces , and then resize them to the size of 2 2 4 \u00d7 2 2 4 .", "ner": [["CNN detector", "Method"], ["ERT", "Method"], ["face alignment", "Task"], ["Dlib", "Method"]], "rel": [["ERT", "Used-For", "face alignment"], ["CNN detector", "Part-Of", "Dlib"], ["ERT", "Part-Of", "Dlib"]], "rel_plus": [["ERT:Method", "Used-For", "face alignment:Task"], ["CNN detector:Method", "Part-Of", "Dlib:Method"], ["ERT:Method", "Part-Of", "Dlib:Method"]]}
{"doc_id": "208513596", "sentence": "In two of our approachesMulti - Head Deep Clustering ( MDC ) and Concatenation Deep Clustering (CDC) - the pseudo - labels from the second modality are supplementary , i.e. , they complement the pseudo - labels generated on the first modality .", "ner": [["approachesMulti - Head Deep Clustering", "Method"], ["MDC", "Method"], ["Concatenation Deep Clustering", "Method"], ["(CDC)", "Method"]], "rel": [["MDC", "Synonym-Of", "approachesMulti - Head Deep Clustering"], ["(CDC)", "Synonym-Of", "Concatenation Deep Clustering"]], "rel_plus": [["MDC:Method", "Synonym-Of", "approachesMulti - Head Deep Clustering:Method"], ["(CDC):Method", "Synonym-Of", "Concatenation Deep Clustering:Method"]]}
{"doc_id": "210164920", "sentence": "The invention of LeNet - 5 paved the way for the continuous success of CNN in various high - level computer vision tasks as well as motivated researchers to explore the capabilities of such networks for pixel - level classification problems like image segmentation .", "ner": [["LeNet - 5", "Method"], ["CNN", "Method"], ["computer vision", "Task"], ["pixel - level classification", "Task"], ["image segmentation", "Task"]], "rel": [["LeNet - 5", "SubClass-Of", "CNN"], ["CNN", "Used-For", "computer vision"], ["LeNet - 5", "Used-For", "computer vision"], ["image segmentation", "SubTask-Of", "pixel - level classification"]], "rel_plus": [["LeNet - 5:Method", "SubClass-Of", "CNN:Method"], ["CNN:Method", "Used-For", "computer vision:Task"], ["LeNet - 5:Method", "Used-For", "computer vision:Task"], ["image segmentation:Task", "SubTask-Of", "pixel - level classification:Task"]]}
{"doc_id": "201124533", "sentence": "Existing state - of - the - art frameworks first encode the input image as a low - resolution representation through a subnetwork that is formed by connecting high - to - low resolution convolutions \\emph{in series } ( e.g. , ResNet , VGGNet ) , and then recover the high - resolution representation from the encoded low - resolution representation .", "ner": [["high - to - low resolution convolutions", "Method"], ["ResNet", "Method"], ["VGGNet", "Method"]], "rel": [["ResNet", "SubClass-Of", "high - to - low resolution convolutions"], ["VGGNet", "SubClass-Of", "high - to - low resolution convolutions"]], "rel_plus": [["ResNet:Method", "SubClass-Of", "high - to - low resolution convolutions:Method"], ["VGGNet:Method", "SubClass-Of", "high - to - low resolution convolutions:Method"]]}
{"doc_id": "21683040", "sentence": "In the future , it is worth enhancing our FSSD with much stronger backbone networks such as ResNet [ 1 2 ] and DenseNet [ 1 3 ] to get better performance on the MSCOCO dataset and replacing the FPN in Mask RCNN [ 1 0 ] with our feature fusion module is also an interesting research field .", "ner": [["FSSD", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"], ["MSCOCO", "Dataset"], ["FPN", "Method"], ["Mask RCNN", "Method"], ["feature fusion module", "Method"]], "rel": [["ResNet", "Part-Of", "FSSD"], ["DenseNet", "Part-Of", "FSSD"], ["FSSD", "Evaluated-With", "MSCOCO"], ["FPN", "Part-Of", "Mask RCNN"], ["feature fusion module", "Part-Of", "Mask RCNN"]], "rel_plus": [["ResNet:Method", "Part-Of", "FSSD:Method"], ["DenseNet:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Evaluated-With", "MSCOCO:Dataset"], ["FPN:Method", "Part-Of", "Mask RCNN:Method"], ["feature fusion module:Method", "Part-Of", "Mask RCNN:Method"]]}
{"doc_id": "195347056", "sentence": "Denoising Feature Matching ( DFM ) [ 3 8 ] keeps the traditional GAN adversarial loss , while an additional complementary information to the generator is computed using a denoising autoencoder in the feature space learnt by the discriminator .", "ner": [["Denoising Feature Matching", "Method"], ["DFM", "Method"], ["GAN adversarial loss", "Method"], ["generator", "Method"], ["denoising autoencoder", "Method"], ["discriminator", "Method"]], "rel": [["DFM", "Synonym-Of", "Denoising Feature Matching"], ["GAN adversarial loss", "Part-Of", "Denoising Feature Matching"], ["generator", "Part-Of", "Denoising Feature Matching"], ["discriminator", "Part-Of", "Denoising Feature Matching"], ["denoising autoencoder", "Part-Of", "generator"]], "rel_plus": [["DFM:Method", "Synonym-Of", "Denoising Feature Matching:Method"], ["GAN adversarial loss:Method", "Part-Of", "Denoising Feature Matching:Method"], ["generator:Method", "Part-Of", "Denoising Feature Matching:Method"], ["discriminator:Method", "Part-Of", "Denoising Feature Matching:Method"], ["denoising autoencoder:Method", "Part-Of", "generator:Method"]]}
{"doc_id": "208513596", "sentence": "To the best of our knowledge , XDC is the first method to demonstrate that self - supervision can outperform large - scale full - supervision in representation learning for action recognition . ( II ) XDC pretrained on IG 6 5 M sets new state - of - the - art performance for self - supervised methods on both datasets , as it outperforms the current state - of - the - art self - supervised method AVTS [ 2 9 ] by 5. 8 % on HMDB 5 1 and 5. 2 % on UCF 1 0 1 . ( III ) When constrained to the same pretraining dataset ( AudioSet ) , XDC outperforms AVTS by 2. 2 % on UCF 1 0 1 and is only slightly worse than AVTS on HMDB 5 1 ( by 0. 6 % ) . [ 2 9 ] 9 4 XDC ( AudioSet ) 9 3 ( b ) DCASE Table 8 : State - of - the - art on audio event classification .", "ner": [["XDC", "Method"], ["action recognition", "Task"], ["XDC", "Method"], ["IG 6 5 M", "Dataset"], ["AVTS", "Method"], ["HMDB 5 1", "Dataset"], ["UCF 1 0 1", "Dataset"], ["AudioSet", "Dataset"], ["XDC", "Method"], ["AVTS", "Method"], ["UCF 1 0 1", "Dataset"], ["AVTS", "Method"], ["HMDB 5 1", "Dataset"], ["XDC", "Method"], ["AudioSet", "Dataset"], ["DCASE", "Dataset"], ["audio event classification", "Task"]], "rel": [["XDC", "Used-For", "action recognition"], ["XDC", "Trained-With", "IG 6 5 M"], ["XDC", "Compare-With", "AVTS"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["AVTS", "Evaluated-With", "HMDB 5 1"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["AVTS", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Compare-With", "AVTS"], ["AVTS", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Evaluated-With", "UCF 1 0 1"], ["XDC", "Compare-With", "AVTS"], ["XDC", "Evaluated-With", "HMDB 5 1"], ["AVTS", "Evaluated-With", "HMDB 5 1"]], "rel_plus": [["XDC:Method", "Used-For", "action recognition:Task"], ["XDC:Method", "Trained-With", "IG 6 5 M:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["AVTS:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["AVTS:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["AVTS:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Evaluated-With", "UCF 1 0 1:Dataset"], ["XDC:Method", "Compare-With", "AVTS:Method"], ["XDC:Method", "Evaluated-With", "HMDB 5 1:Dataset"], ["AVTS:Method", "Evaluated-With", "HMDB 5 1:Dataset"]]}
{"doc_id": "202750230", "sentence": "The numerical values corresponding to the pruned 6 and 3 layer RoBERTa + LayerDrop models are shown in Table 7 .", "ner": [["RoBERTa + LayerDrop", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Inspired by the benefits of multi - scale convolutions [ 1 9 , 3 4 ] and features fusion for training deep networks , we design a novel module , referred as Multi - scale Convolution Aggregation ( MCA ) to work with DenseNets .", "ner": [["multi - scale convolutions", "Method"], ["features fusion", "Method"], ["deep networks", "Method"], ["Multi - scale Convolution Aggregation", "Method"], ["MCA", "Method"], ["DenseNets", "Method"]], "rel": [["multi - scale convolutions", "Part-Of", "deep networks"], ["features fusion", "Part-Of", "deep networks"], ["MCA", "Synonym-Of", "Multi - scale Convolution Aggregation"], ["Multi - scale Convolution Aggregation", "Part-Of", "DenseNets"]], "rel_plus": [["multi - scale convolutions:Method", "Part-Of", "deep networks:Method"], ["features fusion:Method", "Part-Of", "deep networks:Method"], ["MCA:Method", "Synonym-Of", "Multi - scale Convolution Aggregation:Method"], ["Multi - scale Convolution Aggregation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "198231883", "sentence": "With these scores UnOVOST also won the 2 0 1 9 YouTube - VIS Challenge on Video Instance Segmentation , outperforming 1 8 other methods .", "ner": [["UnOVOST", "Method"], ["2 0 1 9 YouTube - VIS Challenge", "Dataset"], ["Video Instance Segmentation", "Task"]], "rel": [["UnOVOST", "Evaluated-With", "2 0 1 9 YouTube - VIS Challenge"], ["UnOVOST", "Used-For", "Video Instance Segmentation"], ["2 0 1 9 YouTube - VIS Challenge", "Benchmark-For", "Video Instance Segmentation"]], "rel_plus": [["UnOVOST:Method", "Evaluated-With", "2 0 1 9 YouTube - VIS Challenge:Dataset"], ["UnOVOST:Method", "Used-For", "Video Instance Segmentation:Task"], ["2 0 1 9 YouTube - VIS Challenge:Dataset", "Benchmark-For", "Video Instance Segmentation:Task"]]}
{"doc_id": "202540251", "sentence": "Recently , an unsupervised domain adaptation method has been proposed for semantic segmentation via classbalanced self - training [ 6 3 ] .", "ner": [["unsupervised domain adaptation", "Method"], ["semantic segmentation", "Task"], ["classbalanced self - training", "Method"]], "rel": [["classbalanced self - training", "Used-For", "semantic segmentation"], ["unsupervised domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["classbalanced self - training:Method", "Used-For", "semantic segmentation:Task"], ["unsupervised domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "208513596", "sentence": "We refer to the DeepCluster baseline in our paper as SingleModality Deep Clustering ( SDC ) .", "ner": [["DeepCluster", "Method"], ["SingleModality Deep Clustering", "Method"], ["SDC", "Method"]], "rel": [["SingleModality Deep Clustering", "SubClass-Of", "DeepCluster"], ["SDC", "Synonym-Of", "SingleModality Deep Clustering"]], "rel_plus": [["SingleModality Deep Clustering:Method", "SubClass-Of", "DeepCluster:Method"], ["SDC:Method", "Synonym-Of", "SingleModality Deep Clustering:Method"]]}
{"doc_id": "201070522", "sentence": "Extensions either use language enhancements , visual enhancements , or both ( e.g. WordNet retrofitting , HGLMM vs. Visual Word 2 Vec vs. GrOVLE , respectively ) .", "ner": [["WordNet retrofitting", "Method"], ["HGLMM", "Method"], ["Visual Word 2 Vec", "Method"], ["GrOVLE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Using this technique we can apply dimensionality reduction , similar to a variational autoencoder [ Kingma and Welling , 2 0 1 3 ] .", "ner": [["dimensionality reduction", "Method"], ["variational autoencoder", "Method"]], "rel": [["dimensionality reduction", "Compare-With", "variational autoencoder"]], "rel_plus": [["dimensionality reduction:Method", "Compare-With", "variational autoencoder:Method"]]}
{"doc_id": "210860760", "sentence": "This study introduced three different architectures using an autoencoder , LSTM , and combination of autoencoder and LSTM .", "ner": [["autoencoder", "Method"], ["LSTM", "Method"], ["autoencoder", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211010786", "sentence": "Group , then we measured the ability of methods to infer that X is a cause of Y . e results , which are in the \" Group : X \u227a Y \" column in Table 4 , show that VL - G , G , TE and VL - TE performed well in this task , while CG and SIC failed to infer causal relations .", "ner": [["VL - G", "Method"], ["G", "Method"], ["TE", "Method"], ["VL - TE", "Method"], ["CG", "Method"], ["SIC", "Method"]], "rel": [["VL - G", "Compare-With", "CG"], ["G", "Compare-With", "CG"], ["TE", "Compare-With", "CG"], ["VL - TE", "Compare-With", "CG"], ["VL - G", "Compare-With", "SIC"], ["G", "Compare-With", "SIC"], ["TE", "Compare-With", "SIC"], ["VL - TE", "Compare-With", "SIC"]], "rel_plus": [["VL - G:Method", "Compare-With", "CG:Method"], ["G:Method", "Compare-With", "CG:Method"], ["TE:Method", "Compare-With", "CG:Method"], ["VL - TE:Method", "Compare-With", "CG:Method"], ["VL - G:Method", "Compare-With", "SIC:Method"], ["G:Method", "Compare-With", "SIC:Method"], ["TE:Method", "Compare-With", "SIC:Method"], ["VL - TE:Method", "Compare-With", "SIC:Method"]]}
{"doc_id": "211020570", "sentence": "This result demonstrates that the proposed ST - GAN method improved the performance of the face alignment task because STN can remove the translation , scale and rotation variation in each face , which can further reduce the variance in the regression target .", "ner": [["ST - GAN", "Method"], ["face alignment", "Task"], ["STN", "Method"], ["translation", "Task"], ["scale", "Task"], ["rotation", "Task"]], "rel": [["STN", "Part-Of", "ST - GAN"], ["ST - GAN", "Used-For", "face alignment"], ["STN", "Used-For", "translation"], ["STN", "Used-For", "scale"], ["STN", "Used-For", "rotation"]], "rel_plus": [["STN:Method", "Part-Of", "ST - GAN:Method"], ["ST - GAN:Method", "Used-For", "face alignment:Task"], ["STN:Method", "Used-For", "translation:Task"], ["STN:Method", "Used-For", "scale:Task"], ["STN:Method", "Used-For", "rotation:Task"]]}
{"doc_id": "22825560", "sentence": "We also conduct a reverse experiment that performs domain adaptation task : Pascal 3D+ \u2192 ShapeNet .", "ner": [["domain adaptation task", "Task"], ["Pascal 3D+", "Dataset"], ["ShapeNet", "Dataset"]], "rel": [["Pascal 3D+", "Benchmark-For", "domain adaptation task"], ["ShapeNet", "Benchmark-For", "domain adaptation task"]], "rel_plus": [["Pascal 3D+:Dataset", "Benchmark-For", "domain adaptation task:Task"], ["ShapeNet:Dataset", "Benchmark-For", "domain adaptation task:Task"]]}
{"doc_id": "202676714", "sentence": "Absum is as simple as standard regularization methods such as weight decay , but it can reduce sensitivity to SFA .", "ner": [["Absum", "Method"], ["standard regularization methods", "Method"], ["weight decay", "Method"], ["SFA", "Method"]], "rel": [["Absum", "SubClass-Of", "standard regularization methods"], ["weight decay", "SubClass-Of", "standard regularization methods"], ["Absum", "Used-For", "SFA"]], "rel_plus": [["Absum:Method", "SubClass-Of", "standard regularization methods:Method"], ["weight decay:Method", "SubClass-Of", "standard regularization methods:Method"], ["Absum:Method", "Used-For", "SFA:Method"]]}
{"doc_id": "202888751", "sentence": "Further , we also investigate the effectiveness of the synthetic data augmentation by GAN over classical data augmentation for improved recognition .", "ner": [["synthetic data augmentation", "Method"], ["GAN", "Method"], ["classical data augmentation", "Method"], ["recognition", "Task"]], "rel": [["GAN", "Used-For", "synthetic data augmentation"], ["synthetic data augmentation", "Compare-With", "classical data augmentation"], ["synthetic data augmentation", "Used-For", "recognition"], ["classical data augmentation", "Used-For", "recognition"]], "rel_plus": [["GAN:Method", "Used-For", "synthetic data augmentation:Method"], ["synthetic data augmentation:Method", "Compare-With", "classical data augmentation:Method"], ["synthetic data augmentation:Method", "Used-For", "recognition:Task"], ["classical data augmentation:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "210164517", "sentence": "To create similar conditions for comparison , both the models were created with global average pooling for classification purpose hence we donnot need to compare fully connected layer parameters .", "ner": [["global average pooling", "Method"], ["classification", "Task"], ["fully connected layer", "Method"]], "rel": [["global average pooling", "Used-For", "classification"]], "rel_plus": [["global average pooling:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202888751", "sentence": "Our approach builds on unsupervised imageto - image translation GAN architectures where they propose a self - consistency or reconstruction loss that preserves the input image after the translation cycle Yi et al. , 2 0 1 7 ; Kim et al. , 2 0 1 7 ) .", "ner": [["unsupervised imageto - image translation GAN", "Method"], ["self - consistency", "Method"], ["reconstruction loss", "Method"]], "rel": [["self - consistency", "Part-Of", "unsupervised imageto - image translation GAN"], ["reconstruction loss", "Part-Of", "unsupervised imageto - image translation GAN"]], "rel_plus": [["self - consistency:Method", "Part-Of", "unsupervised imageto - image translation GAN:Method"], ["reconstruction loss:Method", "Part-Of", "unsupervised imageto - image translation GAN:Method"]]}
{"doc_id": "51876625", "sentence": "The classification head ( Mixed 5b , Mixed 5c ) are initialized from RGB stream pre - trained on Kinetics for both RN and actor classification , but they are updated separately ( weights are not shared ) .", "ner": [["Mixed 5b", "Method"], ["Mixed 5c", "Method"], ["Kinetics", "Dataset"], ["RN", "Task"], ["actor classification", "Task"]], "rel": [["Mixed 5b", "Trained-With", "Kinetics"], ["Mixed 5c", "Trained-With", "Kinetics"], ["Mixed 5b", "Used-For", "RN"], ["Mixed 5c", "Used-For", "RN"], ["Mixed 5b", "Used-For", "actor classification"], ["Mixed 5c", "Used-For", "actor classification"]], "rel_plus": [["Mixed 5b:Method", "Trained-With", "Kinetics:Dataset"], ["Mixed 5c:Method", "Trained-With", "Kinetics:Dataset"], ["Mixed 5b:Method", "Used-For", "RN:Task"], ["Mixed 5c:Method", "Used-For", "RN:Task"], ["Mixed 5b:Method", "Used-For", "actor classification:Task"], ["Mixed 5c:Method", "Used-For", "actor classification:Task"]]}
{"doc_id": "202888986", "sentence": "In addition to the masked language modeling ( MLM ) loss , BERT uses an additional loss called next - sentence prediction ( NSP ) .", "ner": [["masked language modeling", "Task"], ["MLM", "Task"], ["BERT", "Method"], ["next - sentence prediction", "Task"], ["NSP", "Task"]], "rel": [["MLM", "Synonym-Of", "masked language modeling"], ["BERT", "Trained-With", "masked language modeling"], ["NSP", "Synonym-Of", "next - sentence prediction"], ["BERT", "Trained-With", "next - sentence prediction"]], "rel_plus": [["MLM:Task", "Synonym-Of", "masked language modeling:Task"], ["BERT:Method", "Trained-With", "masked language modeling:Task"], ["NSP:Task", "Synonym-Of", "next - sentence prediction:Task"], ["BERT:Method", "Trained-With", "next - sentence prediction:Task"]]}
{"doc_id": "201070522", "sentence": "We find similar trends in performance improvements across tasks : larger gains occur for image - sentence retrieval with + 7. 9 mean recall for the Flickr 3 0 K dataset and + 6. 3 for MSCOCO .", "ner": [["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"], ["MSCOCO", "Benchmark-For", "image - sentence retrieval"]], "rel_plus": [["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"], ["MSCOCO:Dataset", "Benchmark-For", "image - sentence retrieval:Task"]]}
{"doc_id": "44148233", "sentence": "Methods that follow CNN - LSTM/GRU framework mainly differ from each other in the different types of CNNs and language models ( vanilla RNN , LSTM , and GRUs ) they employ and as well as how they pass the extracted visual features to the language model ( at the first time step only or all time steps ) .", "ner": [["CNN - LSTM/GRU", "Method"], ["CNNs", "Method"], ["language models", "Method"], ["RNN", "Method"], ["LSTM", "Method"], ["GRUs", "Method"]], "rel": [["CNNs", "Part-Of", "CNN - LSTM/GRU"], ["language models", "Part-Of", "CNN - LSTM/GRU"], ["RNN", "SubClass-Of", "language models"], ["LSTM", "SubClass-Of", "language models"], ["GRUs", "SubClass-Of", "language models"]], "rel_plus": [["CNNs:Method", "Part-Of", "CNN - LSTM/GRU:Method"], ["language models:Method", "Part-Of", "CNN - LSTM/GRU:Method"], ["RNN:Method", "SubClass-Of", "language models:Method"], ["LSTM:Method", "SubClass-Of", "language models:Method"], ["GRUs:Method", "SubClass-Of", "language models:Method"]]}
{"doc_id": "199543700", "sentence": "And some leading GANs based approaches such as CatGAN [ 5 4 ] , which is based on an objective function that trades - off mutual information between unlabeled examples and their predicted categorical class distribution , against robustness of the classifier to an adversarial generative model .", "ner": [["GANs", "Method"], ["CatGAN", "Method"]], "rel": [["CatGAN", "SubClass-Of", "GANs"]], "rel_plus": [["CatGAN:Method", "SubClass-Of", "GANs:Method"]]}
{"doc_id": "209862890", "sentence": "Such state - of - the - art entity linking models ( Ganea and Hofmann 2 0 1 7 ; Le and Titov 2 0 1 8) employ attention - based bag - of - words context model and pre - trained entity embeddings bootstrapped from word embeddings to assess topic level context compatibility .", "ner": [["entity linking models", "Method"], ["attention - based bag - of - words context model", "Method"], ["entity embeddings", "Task"], ["word embeddings", "Task"]], "rel": [["attention - based bag - of - words context model", "Used-For", "entity linking models"], ["word embeddings", "Used-For", "entity embeddings"]], "rel_plus": [["attention - based bag - of - words context model:Method", "Used-For", "entity linking models:Method"], ["word embeddings:Task", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "23569888", "sentence": "It can be observed that , while the SGD solver is more robust to different choices of base LR , Adam provides slightly better accuracies for mid - range values of base LR , both for the small and the large experiment .", "ner": [["SGD", "Method"], ["Adam", "Method"]], "rel": [["SGD", "Compare-With", "Adam"]], "rel_plus": [["SGD:Method", "Compare-With", "Adam:Method"]]}
{"doc_id": "51559", "sentence": "Regularization An important extension to the stacked QRNN is a robust regularization scheme inspired by recent work in regularizing LSTMs .", "ner": [["QRNN", "Method"], ["LSTMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "To the best of our knowledge , there are no previous works on one shot domain adaptation for per - son re - ID and existing one shot re - ID methods can hardly apply to the domain adaptation directly .", "ner": [["one shot domain adaptation", "Method"], ["per - son re - ID", "Task"], ["one shot re - ID", "Task"], ["domain adaptation", "Method"]], "rel": [["one shot domain adaptation", "Used-For", "per - son re - ID"], ["one shot domain adaptation", "Used-For", "one shot re - ID"], ["one shot domain adaptation", "SubClass-Of", "domain adaptation"]], "rel_plus": [["one shot domain adaptation:Method", "Used-For", "per - son re - ID:Task"], ["one shot domain adaptation:Method", "Used-For", "one shot re - ID:Task"], ["one shot domain adaptation:Method", "SubClass-Of", "domain adaptation:Method"]]}
{"doc_id": "198897554", "sentence": "Accordingly , and using semantic segmentation of urban scenes as challenging main - task use case , the main contributions of this work are three - fold : \u2022 We proposed a generic method for domain adaptation with self - supervised visual representation learning . \u2022 Focusing on the image rotation prediction pretext learning task , we proposed several variations and studied their domain adaptation performance . \u2022 We proposed additional strategies to further boost the self - supervised domain adaptation , including prediction layer alignment and batch normalization calibration .", "ner": [["semantic segmentation", "Task"], ["domain adaptation", "Task"], ["self - supervised visual representation learning", "Method"], ["image rotation prediction", "Task"], ["domain adaptation", "Method"], ["self - supervised domain adaptation", "Method"], ["prediction layer", "Method"], ["batch normalization calibration", "Method"]], "rel": [["self - supervised visual representation learning", "Used-For", "domain adaptation"], ["domain adaptation", "Used-For", "image rotation prediction"], ["prediction layer", "Part-Of", "self - supervised domain adaptation"], ["batch normalization calibration", "Part-Of", "self - supervised domain adaptation"]], "rel_plus": [["self - supervised visual representation learning:Method", "Used-For", "domain adaptation:Task"], ["domain adaptation:Method", "Used-For", "image rotation prediction:Task"], ["prediction layer:Method", "Part-Of", "self - supervised domain adaptation:Method"], ["batch normalization calibration:Method", "Part-Of", "self - supervised domain adaptation:Method"]]}
{"doc_id": "207880647", "sentence": "Our Method ( BERT ) denotes the proposed approach using GPT - 2 for question generation as well as BERT as question answering .", "ner": [["BERT", "Method"], ["GPT - 2", "Method"], ["question generation", "Task"], ["BERT", "Method"], ["question answering", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"], ["BERT", "Used-For", "question answering"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "147703932", "sentence": "In order to evaluate each modality , we make use of standard metrics : Intersection over Union ( IOU ) for body part segmentation , Percentage of Correct Keypoints thresholded at 5 0 % of the head length ( PCKh ) [ 2 ] for 2D pose estimation , root - mean - square - error ( RMSE ) for full body depth estimation and mean joint distance MJD in millimeters ( mm ) for 3D pose estimation .", "ner": [["body part segmentation", "Task"], ["2D pose estimation", "Task"], ["root - mean - square - error", "Method"], ["RMSE", "Method"], ["body depth estimation", "Task"], ["mean joint distance", "Method"], ["MJD", "Method"], ["3D pose estimation", "Task"]], "rel": [["RMSE", "Synonym-Of", "root - mean - square - error"], ["root - mean - square - error", "Used-For", "body depth estimation"], ["MJD", "Synonym-Of", "mean joint distance"], ["mean joint distance", "Used-For", "3D pose estimation"]], "rel_plus": [["RMSE:Method", "Synonym-Of", "root - mean - square - error:Method"], ["root - mean - square - error:Method", "Used-For", "body depth estimation:Task"], ["MJD:Method", "Synonym-Of", "mean joint distance:Method"], ["mean joint distance:Method", "Used-For", "3D pose estimation:Task"]]}
{"doc_id": "67855714", "sentence": "In particular , recent works proposed the use of a VGG loss which consists in minimizing the error between the generated high resolution images and ground - truth in the feature space of a Convolutional Neural Network ( VGG 1 9 ) , pre - trained on the very \" large \" ImageNet dataset .", "ner": [["VGG loss", "Method"], ["Convolutional Neural Network", "Method"], ["VGG 1 9", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG 1 9", "Synonym-Of", "Convolutional Neural Network"], ["VGG loss", "Part-Of", "Convolutional Neural Network"], ["Convolutional Neural Network", "Trained-With", "ImageNet"]], "rel_plus": [["VGG 1 9:Method", "Synonym-Of", "Convolutional Neural Network:Method"], ["VGG loss:Method", "Part-Of", "Convolutional Neural Network:Method"], ["Convolutional Neural Network:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "210164517", "sentence": "This review focusses on various hybrid deep learning approaches for emotion classification problem involving CNN , Recurrent Neural Network ( RNN ) especially LSTM to capture sequential frames of images to predict emotion .", "ner": [["deep learning", "Method"], ["emotion classification", "Task"], ["CNN", "Method"], ["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["LSTM", "Method"], ["predict emotion", "Task"]], "rel": [["CNN", "Used-For", "emotion classification"], ["Recurrent Neural Network", "Used-For", "emotion classification"], ["LSTM", "Used-For", "emotion classification"], ["deep learning", "Used-For", "emotion classification"], ["RNN", "Synonym-Of", "Recurrent Neural Network"], ["LSTM", "SubClass-Of", "Recurrent Neural Network"], ["LSTM", "Used-For", "predict emotion"], ["Recurrent Neural Network", "Used-For", "predict emotion"], ["CNN", "Used-For", "predict emotion"]], "rel_plus": [["CNN:Method", "Used-For", "emotion classification:Task"], ["Recurrent Neural Network:Method", "Used-For", "emotion classification:Task"], ["LSTM:Method", "Used-For", "emotion classification:Task"], ["deep learning:Method", "Used-For", "emotion classification:Task"], ["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "SubClass-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Used-For", "predict emotion:Task"], ["Recurrent Neural Network:Method", "Used-For", "predict emotion:Task"], ["CNN:Method", "Used-For", "predict emotion:Task"]]}
{"doc_id": "3920676", "sentence": "Unlike other datasets that primarily capture images of people in a university setup ( e.g. , Market 1 5 0 1 , CUHK , DukeMTMC 4 ReID ) , the Airport dataset captures images of people from an eclectic mix of professions , leading to a richer , more diversified set of images .", "ner": [["Market 1 5 0 1", "Dataset"], ["CUHK", "Dataset"], ["DukeMTMC 4 ReID", "Dataset"], ["Airport", "Dataset"]], "rel": [["Airport", "Compare-With", "Market 1 5 0 1"], ["Airport", "Compare-With", "CUHK"], ["Airport", "Compare-With", "DukeMTMC 4 ReID"]], "rel_plus": [["Airport:Dataset", "Compare-With", "Market 1 5 0 1:Dataset"], ["Airport:Dataset", "Compare-With", "CUHK:Dataset"], ["Airport:Dataset", "Compare-With", "DukeMTMC 4 ReID:Dataset"]]}
{"doc_id": "53719742", "sentence": "Lots of state - of - the - art convolutional neural network ( CNN ) based object detection and segmentation frameworks , such as Faster R - CNN [ 3 2 ] , SSD [ 2 3 ] and FCN [ 2 7 ] , have been borrowed to solve the text detection problem and substan - tially outperform traditional MSER [ 2 9 ] or SWT [ 7 ] based bottom - up text detection approaches .", "ner": [["convolutional neural network", "Method"], ["CNN", "Method"], ["object detection", "Task"], ["segmentation", "Task"], ["Faster R - CNN", "Method"], ["SSD", "Method"], ["FCN", "Method"], ["text detection", "Task"], ["MSER", "Method"], ["SWT", "Method"], ["text detection", "Task"]], "rel": [["CNN", "Synonym-Of", "convolutional neural network"], ["Faster R - CNN", "SubClass-Of", "convolutional neural network"], ["SSD", "SubClass-Of", "convolutional neural network"], ["FCN", "SubClass-Of", "convolutional neural network"], ["convolutional neural network", "Used-For", "object detection"], ["convolutional neural network", "Used-For", "segmentation"], ["Faster R - CNN", "Used-For", "text detection"], ["SSD", "Used-For", "text detection"], ["FCN", "Used-For", "text detection"], ["Faster R - CNN", "Compare-With", "MSER"], ["SSD", "Compare-With", "MSER"], ["FCN", "Compare-With", "MSER"], ["SWT", "Used-For", "text detection"], ["MSER", "Used-For", "text detection"]], "rel_plus": [["CNN:Method", "Synonym-Of", "convolutional neural network:Method"], ["Faster R - CNN:Method", "SubClass-Of", "convolutional neural network:Method"], ["SSD:Method", "SubClass-Of", "convolutional neural network:Method"], ["FCN:Method", "SubClass-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Used-For", "object detection:Task"], ["convolutional neural network:Method", "Used-For", "segmentation:Task"], ["Faster R - CNN:Method", "Used-For", "text detection:Task"], ["SSD:Method", "Used-For", "text detection:Task"], ["FCN:Method", "Used-For", "text detection:Task"], ["Faster R - CNN:Method", "Compare-With", "MSER:Method"], ["SSD:Method", "Compare-With", "MSER:Method"], ["FCN:Method", "Compare-With", "MSER:Method"], ["SWT:Method", "Used-For", "text detection:Task"], ["MSER:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "202577400", "sentence": "Table 1 ( a ) first reports the performances of adding four GA modules to the baseline FCN , where all methods are using the same backbone ResNet 5 0 for fair comparison .", "ner": [["GA", "Method"], ["FCN", "Method"], ["ResNet 5 0", "Method"]], "rel": [["GA", "Part-Of", "FCN"], ["ResNet 5 0", "Part-Of", "FCN"]], "rel_plus": [["GA:Method", "Part-Of", "FCN:Method"], ["ResNet 5 0:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "146808333", "sentence": "Section 6 presents extensive experiments for classification , detection and segmentation in order do demonstrate efficacy and understand the contributions of different elements .", "ner": [["classification", "Task"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "In order to look different parts of an image , attention based method uses a different image representing method which is introduced as follows . 1 ) Representing remote sensing images : The features of lower convolution layers of CNNs represent the local feature compared with the fully connected layer [ 5 6 ] .", "ner": [["convolution layers", "Method"], ["CNNs", "Method"], ["fully connected layer", "Method"]], "rel": [["convolution layers", "Part-Of", "CNNs"], ["convolution layers", "Compare-With", "fully connected layer"]], "rel_plus": [["convolution layers:Method", "Part-Of", "CNNs:Method"], ["convolution layers:Method", "Compare-With", "fully connected layer:Method"]]}
{"doc_id": "202577400", "sentence": "Obviously , all GA modules significantly improves the baseline FCN on semantic segmentation task , where CGNL performs better than other three GA modules .", "ner": [["GA", "Method"], ["FCN", "Method"], ["semantic segmentation", "Task"], ["CGNL", "Method"], ["GA", "Method"]], "rel": [["GA", "Part-Of", "FCN"], ["FCN", "Used-For", "semantic segmentation"], ["CGNL", "Compare-With", "GA"]], "rel_plus": [["GA:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "semantic segmentation:Task"], ["CGNL:Method", "Compare-With", "GA:Method"]]}
{"doc_id": "6423078", "sentence": "Recently , VGGNet ( Simonyan and Zisserman 2 0 1 5 ) has been seen to achieve state - of - the - art performance in the ImageNet challenge , with great depth ( 1 6 convolutional layers ) , great density ( stride - 1 convolutional kernels ) , and multiple stages The bolded convolutional layers are linked to additional side - output layers ( five 2 - stride downsampling layers ) .", "ner": [["VGGNet", "Method"], ["ImageNet", "Dataset"], ["convolutional layers", "Method"], ["stride - 1 convolutional kernels", "Method"], ["convolutional layers", "Method"], ["2 - stride downsampling layers", "Method"]], "rel": [["convolutional layers", "Part-Of", "VGGNet"], ["stride - 1 convolutional kernels", "Part-Of", "VGGNet"], ["VGGNet", "Evaluated-With", "ImageNet"], ["2 - stride downsampling layers", "Part-Of", "convolutional layers"]], "rel_plus": [["convolutional layers:Method", "Part-Of", "VGGNet:Method"], ["stride - 1 convolutional kernels:Method", "Part-Of", "VGGNet:Method"], ["VGGNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["2 - stride downsampling layers:Method", "Part-Of", "convolutional layers:Method"]]}
{"doc_id": "4539700", "sentence": "However , unlike the results on Kinetics , ResNet - 2 0 0 also improved the accuracies in HMDB - 5 1 .", "ner": [["Kinetics", "Dataset"], ["ResNet - 2 0 0", "Method"], ["HMDB - 5 1", "Dataset"]], "rel": [["ResNet - 2 0 0", "Evaluated-With", "Kinetics"], ["ResNet - 2 0 0", "Evaluated-With", "HMDB - 5 1"], ["Kinetics", "Compare-With", "HMDB - 5 1"]], "rel_plus": [["ResNet - 2 0 0:Method", "Evaluated-With", "Kinetics:Dataset"], ["ResNet - 2 0 0:Method", "Evaluated-With", "HMDB - 5 1:Dataset"], ["Kinetics:Dataset", "Compare-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "198897554", "sentence": "In this work , we have explored self - supervised learning for domain adaptation .", "ner": [["self - supervised learning", "Task"], ["domain adaptation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202734254", "sentence": "In the current state - of - the - art question answering models , most models leverage the benefits from BERT ( Devlin et al. , 2 0 1 9 ) to advance the task .", "ner": [["question answering", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "question answering"]], "rel_plus": [["BERT:Method", "Used-For", "question answering:Task"]]}
{"doc_id": "4539700", "sentence": "The pre - activation ResNet is similar to bottleneck ResNet architectures , but there are differences in the convolution , batch normalization , and ReLU order .", "ner": [["pre - activation ResNet", "Method"], ["bottleneck ResNet", "Method"], ["convolution", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["ReLU", "Part-Of", "pre - activation ResNet"], ["batch normalization", "Part-Of", "pre - activation ResNet"], ["convolution", "Part-Of", "pre - activation ResNet"], ["pre - activation ResNet", "Compare-With", "bottleneck ResNet"], ["convolution", "Part-Of", "bottleneck ResNet"], ["batch normalization", "Part-Of", "bottleneck ResNet"], ["ReLU", "Part-Of", "bottleneck ResNet"]], "rel_plus": [["ReLU:Method", "Part-Of", "pre - activation ResNet:Method"], ["batch normalization:Method", "Part-Of", "pre - activation ResNet:Method"], ["convolution:Method", "Part-Of", "pre - activation ResNet:Method"], ["pre - activation ResNet:Method", "Compare-With", "bottleneck ResNet:Method"], ["convolution:Method", "Part-Of", "bottleneck ResNet:Method"], ["batch normalization:Method", "Part-Of", "bottleneck ResNet:Method"], ["ReLU:Method", "Part-Of", "bottleneck ResNet:Method"]]}
{"doc_id": "102351044", "sentence": "We can notice that drop - neuron improves the performance mildly from 5. 1 0 % to 4. 8 7 % , and drop - channel outperforms dropneuron with better error rate 4. 7 2 % .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["dropneuron", "Method"]], "rel": [["drop - channel", "Compare-With", "dropneuron"]], "rel_plus": [["drop - channel:Method", "Compare-With", "dropneuron:Method"]]}
{"doc_id": "211010758", "sentence": "A representative of two - stage framework is the Faster R - CNN proposed by Ren et al. [ 4 2 ] , which consists of two subnetworks : a region proposal network that generates region proposals and a R - CNN that classifies the categories of the proposals .", "ner": [["Faster R - CNN", "Method"], ["region proposal network", "Method"], ["region proposals", "Task"], ["R - CNN", "Method"]], "rel": [["region proposal network", "Part-Of", "Faster R - CNN"], ["R - CNN", "Part-Of", "Faster R - CNN"], ["region proposal network", "Used-For", "region proposals"]], "rel_plus": [["region proposal network:Method", "Part-Of", "Faster R - CNN:Method"], ["R - CNN:Method", "Part-Of", "Faster R - CNN:Method"], ["region proposal network:Method", "Used-For", "region proposals:Task"]]}
{"doc_id": "23569888", "sentence": "It diverges from previous architectures in that it concatenates so called inception modules and uses just one FC layer at the very end , reducing the parameters number to \u223c 4M for 2 2 layers . 7 The name is short for residual networks , which won the ILSVRC 2 0 1 5 ( He et al 2 0 1 6 ) , of which ResNet - 5 0 is a smaller version stacking 5 0 layers in \u223c 2 0 M parameters .", "ner": [["FC layer", "Method"], ["residual networks", "Method"], ["ILSVRC 2 0 1 5", "Dataset"], ["ResNet - 5 0", "Method"]], "rel": [["residual networks", "Used-For", "ILSVRC 2 0 1 5"]], "rel_plus": [["residual networks:Method", "Used-For", "ILSVRC 2 0 1 5:Dataset"]]}
{"doc_id": "210164920", "sentence": "In parallel to this , instance segmentation algorithms such as SDS and Hyper column have used Multi - scale Combinatorial Grouping ( MCG ) [ 1 3 3 ] for region proposal generation .", "ner": [["instance segmentation", "Task"], ["SDS", "Method"], ["Hyper column", "Method"], ["Multi - scale Combinatorial Grouping", "Method"], ["MCG", "Method"], ["region proposal generation", "Task"]], "rel": [["SDS", "Used-For", "instance segmentation"], ["Hyper column", "Used-For", "instance segmentation"], ["Multi - scale Combinatorial Grouping", "Part-Of", "SDS"], ["Multi - scale Combinatorial Grouping", "Part-Of", "Hyper column"], ["MCG", "Synonym-Of", "Multi - scale Combinatorial Grouping"], ["Hyper column", "Used-For", "region proposal generation"]], "rel_plus": [["SDS:Method", "Used-For", "instance segmentation:Task"], ["Hyper column:Method", "Used-For", "instance segmentation:Task"], ["Multi - scale Combinatorial Grouping:Method", "Part-Of", "SDS:Method"], ["Multi - scale Combinatorial Grouping:Method", "Part-Of", "Hyper column:Method"], ["MCG:Method", "Synonym-Of", "Multi - scale Combinatorial Grouping:Method"], ["Hyper column:Method", "Used-For", "region proposal generation:Task"]]}
{"doc_id": "51923817", "sentence": "Code is available at https://github.com/ princeton - vl/CornerNet .   Two - stage approach was first introduced and popularized by R - CNN ( Girshick et al. , 2 0 1 4 ) .", "ner": [["vl/CornerNet", "Method"], ["R - CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "23569888", "sentence": "Dropout % : percentage of dropout in FC layers .", "ner": [["Dropout", "Method"], ["dropout", "Method"], ["FC layers", "Method"]], "rel": [["dropout", "Part-Of", "FC layers"]], "rel_plus": [["dropout:Method", "Part-Of", "FC layers:Method"]]}
{"doc_id": "146808333", "sentence": "Top - 1 accuracy is on ImageNet and latency is in ms .   We use MobileNetV 3 as a drop - in replacement for the backbone feature extractor in SSDLite [ 3 9 ] and compare with other backbone networks on COCO dataset [ 2 6 ] .", "ner": [["ImageNet", "Dataset"], ["MobileNetV 3", "Method"], ["SSDLite", "Method"], ["COCO", "Dataset"]], "rel": [["MobileNetV 3", "Part-Of", "SSDLite"], ["SSDLite", "Evaluated-With", "COCO"]], "rel_plus": [["MobileNetV 3:Method", "Part-Of", "SSDLite:Method"], ["SSDLite:Method", "Evaluated-With", "COCO:Dataset"]]}
{"doc_id": "202676714", "sentence": "The constraint of Absum is looser than L 1 regularization because a large element k Note that the search space of weight decay is also the point K = O when ||K|| F = 0 .", "ner": [["Absum", "Method"], ["L 1 regularization", "Method"], ["weight decay", "Method"]], "rel": [["Absum", "Compare-With", "L 1 regularization"]], "rel_plus": [["Absum:Method", "Compare-With", "L 1 regularization:Method"]]}
{"doc_id": "210164920", "sentence": "Deep - Mask [ 1 3 4 ] , as discussed in section 4. 2 . 2 , has also used CNN based RPN as Faster R - CNN to generate region proposals so that the model can be trained end to end .", "ner": [["Deep - Mask", "Method"], ["CNN based RPN", "Method"], ["Faster R - CNN", "Method"], ["generate region proposals", "Task"]], "rel": [["Faster R - CNN", "Part-Of", "Deep - Mask"], ["CNN based RPN", "Part-Of", "Faster R - CNN"], ["Deep - Mask", "Used-For", "generate region proposals"]], "rel_plus": [["Faster R - CNN:Method", "Part-Of", "Deep - Mask:Method"], ["CNN based RPN:Method", "Part-Of", "Faster R - CNN:Method"], ["Deep - Mask:Method", "Used-For", "generate region proposals:Task"]]}
{"doc_id": "202540590", "sentence": "Most of the frequent trigram prefixes in COS - MOS , e.g. , why , what may happen , what will happen are almost absent from SQuAD 2. 0 , which demonstrates the unique challenge our dataset contributes .", "ner": [["COS - MOS", "Dataset"], ["SQuAD 2. 0", "Dataset"]], "rel": [["COS - MOS", "Compare-With", "SQuAD 2. 0"]], "rel_plus": [["COS - MOS:Dataset", "Compare-With", "SQuAD 2. 0:Dataset"]]}
{"doc_id": "210860760", "sentence": "We have compared ExEm against well - known baselines that have the best results on link prediction .", "ner": [["ExEm", "Method"], ["link prediction", "Task"]], "rel": [["ExEm", "Used-For", "link prediction"]], "rel_plus": [["ExEm:Method", "Used-For", "link prediction:Task"]]}
{"doc_id": "147703932", "sentence": "In this work we analyze the contribution of multi - tasking on four common body pose analysis problems : 2D/ 3 D body pose recovery , full - body depth estimation and body parts segmentation .", "ner": [["body pose analysis", "Task"], ["2D/ 3 D body pose recovery", "Task"], ["full - body depth estimation", "Task"], ["body parts segmentation", "Task"]], "rel": [["2D/ 3 D body pose recovery", "SubTask-Of", "body pose analysis"], ["full - body depth estimation", "SubTask-Of", "body pose analysis"], ["body parts segmentation", "SubTask-Of", "body pose analysis"]], "rel_plus": [["2D/ 3 D body pose recovery:Task", "SubTask-Of", "body pose analysis:Task"], ["full - body depth estimation:Task", "SubTask-Of", "body pose analysis:Task"], ["body parts segmentation:Task", "SubTask-Of", "body pose analysis:Task"]]}
{"doc_id": "210164920", "sentence": "In this section , we are going to survey the evolution of CNN based semantic segmentation models .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "60440450", "sentence": "Here , we train on the MS COCO dataset [ 2 5 ] , which contains approximately 1 2 0 , 0 0 0 training images with instance segmentation masks for each object in 8 0 categories .", "ner": [["MS COCO", "Dataset"], ["instance segmentation", "Task"]], "rel": [["MS COCO", "Benchmark-For", "instance segmentation"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"]]}
{"doc_id": "4246700", "sentence": "The experimental setup in this section : the word embedding dimension and hidden state dimension of RNN are respectively set to 2 5 6 and 2 5 6 for multimodal method , and the learning rate of multimodal method is 0.0 0 0 1 .", "ner": [["word embedding", "Method"], ["RNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839714", "sentence": "The Transformer [ 1 1 ] was proposed for natural language processing , specifically machine translation .", "ner": [["Transformer", "Method"], ["natural language processing", "Task"], ["machine translation", "Task"]], "rel": [["machine translation", "SubTask-Of", "natural language processing"], ["Transformer", "Used-For", "natural language processing"], ["Transformer", "Used-For", "machine translation"]], "rel_plus": [["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["Transformer:Method", "Used-For", "natural language processing:Task"], ["Transformer:Method", "Used-For", "machine translation:Task"]]}
{"doc_id": "202888986", "sentence": "The improvement in parameter efficiency showcases the most important advantage of ALBERT 's design choices , as shown in Table 3 : with only around 7 0 % of BERT - large 's parameters , ALBERT - xxlarge achieves significant improvements over BERT - large , as measured by the difference on development set scores for several representative downstream tasks : SQuAD v 1 . 1 ( + 1. 7 % ) , SQuAD v 2 . 0 ( + 4. 2 % ) , MNLI ( + 2. 2 % ) , SST - 2 ( + 3. 0 % ) , and RACE ( + 8. 5 % ) .", "ner": [["ALBERT", "Method"], ["BERT - large", "Method"], ["ALBERT - xxlarge", "Method"], ["BERT - large", "Method"], ["SQuAD v 1 . 1", "Dataset"], ["SQuAD v 2 . 0", "Dataset"], ["MNLI", "Dataset"], ["SST - 2", "Dataset"], ["RACE", "Dataset"]], "rel": [["ALBERT - xxlarge", "Compare-With", "BERT - large"], ["ALBERT - xxlarge", "Compare-With", "BERT - large"], ["ALBERT - xxlarge", "Evaluated-With", "SQuAD v 1 . 1"], ["BERT - large", "Evaluated-With", "SQuAD v 1 . 1"], ["ALBERT - xxlarge", "Evaluated-With", "SQuAD v 2 . 0"], ["BERT - large", "Evaluated-With", "SQuAD v 2 . 0"], ["ALBERT - xxlarge", "Evaluated-With", "MNLI"], ["BERT - large", "Evaluated-With", "MNLI"], ["ALBERT - xxlarge", "Evaluated-With", "SST - 2"], ["BERT - large", "Evaluated-With", "SST - 2"], ["ALBERT - xxlarge", "Evaluated-With", "RACE"], ["BERT - large", "Evaluated-With", "RACE"]], "rel_plus": [["ALBERT - xxlarge:Method", "Compare-With", "BERT - large:Method"], ["ALBERT - xxlarge:Method", "Compare-With", "BERT - large:Method"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SQuAD v 1 . 1:Dataset"], ["BERT - large:Method", "Evaluated-With", "SQuAD v 1 . 1:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SQuAD v 2 . 0:Dataset"], ["BERT - large:Method", "Evaluated-With", "SQuAD v 2 . 0:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "MNLI:Dataset"], ["BERT - large:Method", "Evaluated-With", "MNLI:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "SST - 2:Dataset"], ["BERT - large:Method", "Evaluated-With", "SST - 2:Dataset"], ["ALBERT - xxlarge:Method", "Evaluated-With", "RACE:Dataset"], ["BERT - large:Method", "Evaluated-With", "RACE:Dataset"]]}
{"doc_id": "3920676", "sentence": "AlexNet [ 4 2 ] , ResNet [ 4 3 ] , and VGGNet [ 4 4 ] architectures are employed in IDE - CaffeNet , IDE - ResNet and IDE - VGGNet respectively .", "ner": [["AlexNet", "Method"], ["ResNet", "Method"], ["VGGNet", "Method"], ["IDE - CaffeNet", "Method"], ["IDE - ResNet", "Method"], ["IDE - VGGNet", "Method"]], "rel": [["AlexNet", "Part-Of", "IDE - CaffeNet"], ["ResNet", "Part-Of", "IDE - ResNet"], ["VGGNet", "Part-Of", "IDE - VGGNet"]], "rel_plus": [["AlexNet:Method", "Part-Of", "IDE - CaffeNet:Method"], ["ResNet:Method", "Part-Of", "IDE - ResNet:Method"], ["VGGNet:Method", "Part-Of", "IDE - VGGNet:Method"]]}
{"doc_id": "21683040", "sentence": "Besides , our FSSD also outperforms a lot of state - of - the - art object detectors based on VG - GNet including ION [ 1 ] and Faster RCNN [ 2 7 ] .", "ner": [["FSSD", "Method"], ["object detectors", "Method"], ["VG - GNet", "Method"], ["ION", "Method"], ["Faster RCNN", "Method"]], "rel": [["Faster RCNN", "SubClass-Of", "object detectors"], ["VG - GNet", "SubClass-Of", "object detectors"], ["FSSD", "Compare-With", "object detectors"], ["ION", "Part-Of", "VG - GNet"], ["FSSD", "Compare-With", "VG - GNet"], ["FSSD", "Compare-With", "Faster RCNN"]], "rel_plus": [["Faster RCNN:Method", "SubClass-Of", "object detectors:Method"], ["VG - GNet:Method", "SubClass-Of", "object detectors:Method"], ["FSSD:Method", "Compare-With", "object detectors:Method"], ["ION:Method", "Part-Of", "VG - GNet:Method"], ["FSSD:Method", "Compare-With", "VG - GNet:Method"], ["FSSD:Method", "Compare-With", "Faster RCNN:Method"]]}
{"doc_id": "203593581", "sentence": "The ResNet architectures for CIFAR 1 0 ( He et al. , 2 0 1 6 ) contains a convolutional layer followed by 3 residual blocks and a final FC layer .", "ner": [["ResNet", "Method"], ["CIFAR 1 0", "Dataset"], ["convolutional layer", "Method"], ["residual blocks", "Method"], ["FC layer", "Method"]], "rel": [["convolutional layer", "Part-Of", "ResNet"], ["residual blocks", "Part-Of", "ResNet"], ["FC layer", "Part-Of", "ResNet"], ["ResNet", "Used-For", "CIFAR 1 0"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "ResNet:Method"], ["residual blocks:Method", "Part-Of", "ResNet:Method"], ["FC layer:Method", "Part-Of", "ResNet:Method"], ["ResNet:Method", "Used-For", "CIFAR 1 0:Dataset"]]}
{"doc_id": "209532167", "sentence": "Finally , for our baseline pointer model ( referred to as LSTM+pointer below ) we searched over the following hyperparameter choices : hidden sizes of 5 1 2 and 1 0 2 4 , token embedding sizes of 5 1 2 and 1 0 2 4 , learning rates of 0. 1 , 0.0 1 , and 0.0 0 1 , and the AdaGrad and Gradient Descent optimizers .", "ner": [["LSTM+pointer", "Method"], ["token embedding", "Method"], ["AdaGrad", "Method"], ["Gradient Descent optimizers", "Method"]], "rel": [["token embedding", "Part-Of", "LSTM+pointer"], ["AdaGrad", "Part-Of", "LSTM+pointer"], ["Gradient Descent optimizers", "Part-Of", "LSTM+pointer"]], "rel_plus": [["token embedding:Method", "Part-Of", "LSTM+pointer:Method"], ["AdaGrad:Method", "Part-Of", "LSTM+pointer:Method"], ["Gradient Descent optimizers:Method", "Part-Of", "LSTM+pointer:Method"]]}
{"doc_id": "56657874", "sentence": "For domain generalization , we use DAN [ 2 3 ] , D - CORAL [ 3 9 ] , RTN [ 2 4 ] and JAN [ 2 5 ] architecture where Alexnet [ 2 1 ] is used , comprising of five convolution layers and three fully connected layers .", "ner": [["domain generalization", "Method"], ["DAN", "Method"], ["D - CORAL", "Method"], ["RTN", "Method"], ["JAN", "Method"], ["Alexnet", "Method"], ["convolution layers", "Method"], ["fully connected layers", "Method"]], "rel": [["DAN", "Part-Of", "domain generalization"], ["D - CORAL", "Part-Of", "domain generalization"], ["RTN", "Part-Of", "domain generalization"], ["JAN", "Part-Of", "domain generalization"], ["convolution layers", "Part-Of", "Alexnet"], ["fully connected layers", "Part-Of", "Alexnet"]], "rel_plus": [["DAN:Method", "Part-Of", "domain generalization:Method"], ["D - CORAL:Method", "Part-Of", "domain generalization:Method"], ["RTN:Method", "Part-Of", "domain generalization:Method"], ["JAN:Method", "Part-Of", "domain generalization:Method"], ["convolution layers:Method", "Part-Of", "Alexnet:Method"], ["fully connected layers:Method", "Part-Of", "Alexnet:Method"]]}
{"doc_id": "AUG009", "sentence": "The graph-based feature propagation method improves clustering on MNIST.", "ner": [["graph-based feature propagation", "Method"], ["clustering", "Task"], ["MNIST", "Dataset"]], "rel": [["graph-based feature propagation", "Used-For", "clustering"], ["graph-based feature propagation", "Trained-With", "MNIST"]], "rel_plus": [["graph-based feature propagation:Method", "Used-For", "clustering:Task"], ["graph-based feature propagation:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "202676714", "sentence": "By using the proximal operator after stochastic gradient descent ( SGD ) , we update the i - th convolution filter : where \u03b7 is a learning rate , and B is a minibatch size .", "ner": [["stochastic gradient descent", "Method"], ["SGD", "Method"], ["convolution", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "208202241", "sentence": "Three 2D version LC operations between GCN and the backbone ResNet 1 0 1 are used and the label embeddings of four graph convolution layers are injected to res 2 , res 3 , res 4 and res 5 of ResNet 1 0 1 .", "ner": [["LC", "Method"], ["GCN", "Method"], ["ResNet 1 0 1", "Method"], ["graph convolution", "Method"], ["ResNet 1 0 1", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Sharp - Mask [ 1 4 4 ] contains a bottom - up feed - forward network for producing coarse semantic segmentation mask and a top - down network to refine those masks using a refinement module .", "ner": [["Sharp - Mask", "Method"], ["bottom - up feed - forward network", "Method"], ["coarse semantic segmentation mask", "Task"], ["top - down network", "Method"]], "rel": [["bottom - up feed - forward network", "Part-Of", "Sharp - Mask"], ["top - down network", "Part-Of", "Sharp - Mask"], ["Sharp - Mask", "Used-For", "coarse semantic segmentation mask"]], "rel_plus": [["bottom - up feed - forward network:Method", "Part-Of", "Sharp - Mask:Method"], ["top - down network:Method", "Part-Of", "Sharp - Mask:Method"], ["Sharp - Mask:Method", "Used-For", "coarse semantic segmentation mask:Task"]]}
{"doc_id": "52910494", "sentence": "In particular , DenseNets have demonstrated that feature reuse via dense skip connections can effectively alleviate the difficulty of training very deep networks and that reusing features generated by the initial layers in all subsequent layers has strong impact on performance .", "ner": [["DenseNets", "Method"], ["dense skip connections", "Method"]], "rel": [["dense skip connections", "Part-Of", "DenseNets"]], "rel_plus": [["dense skip connections:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "211010520", "sentence": "But on the other hand , training on D W also leads to generalisation towards D S : for example , the baseline of RoBERTa trained on 1 0 , 0 0 0 SQuAD samples reaches 2 2 . 1 F 1 on D RoBERTa ( D S ) , whereas training RoBERTa on D BiDAF and D BERT ( D W ) bumps this number to 3 6 . 0 F 1 and 3 4 . 6 F 1 , respectively .", "ner": [["RoBERTa", "Method"], ["SQuAD", "Dataset"], ["D RoBERTa", "Dataset"], ["RoBERTa", "Method"], ["D BiDAF", "Method"], ["D BERT", "Method"]], "rel": [["RoBERTa", "Trained-With", "SQuAD"], ["RoBERTa", "Evaluated-With", "D RoBERTa"], ["RoBERTa", "Trained-With", "D BiDAF"], ["RoBERTa", "Trained-With", "D BERT"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D RoBERTa:Dataset"], ["RoBERTa:Method", "Trained-With", "D BiDAF:Method"], ["RoBERTa:Method", "Trained-With", "D BERT:Method"]]}
{"doc_id": "6116678", "sentence": "The segment - level spatial pooling stream generates another saliency map at the superpixel level by performing spatial pooling and saliency estimation over superpixels .", "ner": [["segment - level spatial pooling stream", "Method"], ["spatial pooling", "Task"], ["saliency estimation", "Task"]], "rel": [["segment - level spatial pooling stream", "Used-For", "spatial pooling"], ["segment - level spatial pooling stream", "Used-For", "saliency estimation"]], "rel_plus": [["segment - level spatial pooling stream:Method", "Used-For", "spatial pooling:Task"], ["segment - level spatial pooling stream:Method", "Used-For", "saliency estimation:Task"]]}
{"doc_id": "208513596", "sentence": "XDC shows competitive performance . pretrained on AudioSet with the state - of - the - art in selfsupervised methods for audio .", "ner": [["XDC", "Method"], ["AudioSet", "Dataset"]], "rel": [["XDC", "Trained-With", "AudioSet"]], "rel_plus": [["XDC:Method", "Trained-With", "AudioSet:Dataset"]]}
{"doc_id": "208202241", "sentence": "Influence of GCN Depth in KSSNet As we know , conventional GCN suffers from over - smoothing .", "ner": [["GCN", "Method"], ["KSSNet", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Part-Of", "KSSNet"]], "rel_plus": [["GCN:Method", "Part-Of", "KSSNet:Method"]]}
{"doc_id": "201124533", "sentence": "In the Faster R - CNN framework , our networks perform better than ResNets with similar parameter and computation complexity : HRNetV 2 p - W 3 2 vs. ResNet - 1 0 1 - FPN , HRNetV 2 p - W 4 0 vs. ResNet - 1 5 2 - FPN , HRNetV 2 p - W 4 8 vs. X - 1 0 1 - 6 4 \u00d7 4d - FPN .", "ner": [["Faster R - CNN", "Method"], ["ResNets", "Method"], ["HRNetV 2 p - W 3 2", "Method"], ["ResNet - 1 0 1 - FPN", "Method"], ["HRNetV 2 p - W 4 0", "Method"], ["ResNet - 1 5 2 - FPN", "Method"], ["HRNetV 2 p - W 4 8", "Method"], ["X - 1 0 1 - 6 4 \u00d7 4d - FPN", "Method"]], "rel": [["HRNetV 2 p - W 3 2", "Compare-With", "ResNet - 1 0 1 - FPN"], ["HRNetV 2 p - W 4 0", "Compare-With", "ResNet - 1 5 2 - FPN"], ["HRNetV 2 p - W 4 8", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4d - FPN"]], "rel_plus": [["HRNetV 2 p - W 3 2:Method", "Compare-With", "ResNet - 1 0 1 - FPN:Method"], ["HRNetV 2 p - W 4 0:Method", "Compare-With", "ResNet - 1 5 2 - FPN:Method"], ["HRNetV 2 p - W 4 8:Method", "Compare-With", "X - 1 0 1 - 6 4 \u00d7 4d - FPN:Method"]]}
{"doc_id": "204402755", "sentence": "The most widely used are learning rate adaptive methods , starting with AdaGrad ( Duchi et al. , 2 0 1 1 ) , AdaDelta ( Zeiler , 2 0 1 2 ) , RMSprop , and Adam ( Kingma & Ba , 2 0 1 4 ) .", "ner": [["AdaGrad", "Method"], ["AdaDelta", "Method"], ["RMSprop", "Method"], ["Adam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "The AR - GAN improves on CycleGAN , unsupervised image - to - image translator , by introducing an activation reconstruction module consisting of a feature extraction network to calculate Activation Reconstruction Loss ( ARL ) between an image and its translation , in addition to the cycle - consistency loss and adversarial loss of CycleGAN .", "ner": [["AR - GAN", "Method"], ["CycleGAN", "Method"], ["unsupervised image - to - image translator", "Method"], ["activation reconstruction module", "Method"], ["feature extraction network", "Method"], ["Activation Reconstruction Loss", "Method"], ["ARL", "Method"], ["cycle - consistency loss", "Method"], ["adversarial loss", "Method"], ["CycleGAN", "Method"]], "rel": [["activation reconstruction module", "Part-Of", "AR - GAN"], ["cycle - consistency loss", "Part-Of", "AR - GAN"], ["adversarial loss", "Part-Of", "AR - GAN"], ["AR - GAN", "Compare-With", "CycleGAN"], ["AR - GAN", "Compare-With", "unsupervised image - to - image translator"], ["feature extraction network", "Part-Of", "activation reconstruction module"], ["ARL", "Synonym-Of", "Activation Reconstruction Loss"], ["feature extraction network", "Used-For", "Activation Reconstruction Loss"], ["adversarial loss", "Part-Of", "CycleGAN"]], "rel_plus": [["activation reconstruction module:Method", "Part-Of", "AR - GAN:Method"], ["cycle - consistency loss:Method", "Part-Of", "AR - GAN:Method"], ["adversarial loss:Method", "Part-Of", "AR - GAN:Method"], ["AR - GAN:Method", "Compare-With", "CycleGAN:Method"], ["AR - GAN:Method", "Compare-With", "unsupervised image - to - image translator:Method"], ["feature extraction network:Method", "Part-Of", "activation reconstruction module:Method"], ["ARL:Method", "Synonym-Of", "Activation Reconstruction Loss:Method"], ["feature extraction network:Method", "Used-For", "Activation Reconstruction Loss:Method"], ["adversarial loss:Method", "Part-Of", "CycleGAN:Method"]]}
{"doc_id": "202888751", "sentence": "We introduce AR - GAN , where in addition to the adversarial loss , our synthetic image generator optimizes on Activation Reconstruction loss ( ARL ) function that optimizes feature activations against the natural image .", "ner": [["AR - GAN", "Method"], ["adversarial loss", "Method"], ["synthetic image generator", "Method"], ["Activation Reconstruction loss", "Method"], ["ARL", "Method"]], "rel": [["adversarial loss", "Part-Of", "AR - GAN"], ["Activation Reconstruction loss", "Part-Of", "synthetic image generator"], ["ARL", "Synonym-Of", "Activation Reconstruction loss"]], "rel_plus": [["adversarial loss:Method", "Part-Of", "AR - GAN:Method"], ["Activation Reconstruction loss:Method", "Part-Of", "synthetic image generator:Method"], ["ARL:Method", "Synonym-Of", "Activation Reconstruction loss:Method"]]}
{"doc_id": "210164920", "sentence": "SDS , Hypercolumn , CFM [ 1 4 6 ] , MNC [ 1 3 5 ] , MultiPathNet [ 1 4 0 ] used two different subnetworks for object detection and segmentation which prevent the models to become an end to end trainable .", "ner": [["SDS", "Method"], ["Hypercolumn", "Method"], ["CFM", "Method"], ["MNC", "Method"], ["MultiPathNet", "Method"], ["object detection", "Task"], ["segmentation", "Task"]], "rel": [["MultiPathNet", "Used-For", "object detection"], ["MNC", "Used-For", "object detection"], ["CFM", "Used-For", "object detection"], ["Hypercolumn", "Used-For", "object detection"], ["SDS", "Used-For", "object detection"], ["MultiPathNet", "Used-For", "segmentation"], ["MNC", "Used-For", "segmentation"], ["CFM", "Used-For", "segmentation"], ["Hypercolumn", "Used-For", "segmentation"], ["SDS", "Used-For", "segmentation"]], "rel_plus": [["MultiPathNet:Method", "Used-For", "object detection:Task"], ["MNC:Method", "Used-For", "object detection:Task"], ["CFM:Method", "Used-For", "object detection:Task"], ["Hypercolumn:Method", "Used-For", "object detection:Task"], ["SDS:Method", "Used-For", "object detection:Task"], ["MultiPathNet:Method", "Used-For", "segmentation:Task"], ["MNC:Method", "Used-For", "segmentation:Task"], ["CFM:Method", "Used-For", "segmentation:Task"], ["Hypercolumn:Method", "Used-For", "segmentation:Task"], ["SDS:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "28984897", "sentence": "Faces in images Table 3 : EER in [ % ] and pruned weight 's proportions for the VGG 1 6 with maxout layer ( k = 4 ) after the first fully connected layer ( VGG 1 6 - MFC ) and after the last convolutional layer ( VGG 1 6 - MC ) . were detected using the Viola - Jones face detector [ 9 ] .", "ner": [["VGG 1 6", "Method"], ["maxout layer", "Method"], ["fully connected layer", "Method"], ["VGG 1 6 - MFC", "Method"], ["convolutional layer", "Method"], ["VGG 1 6 - MC", "Method"], ["Viola - Jones face detector", "Method"]], "rel": [["VGG 1 6", "Part-Of", "VGG 1 6 - MFC"], ["maxout layer", "Part-Of", "VGG 1 6 - MFC"], ["fully connected layer", "Part-Of", "VGG 1 6 - MFC"], ["convolutional layer", "Part-Of", "VGG 1 6 - MC"], ["VGG 1 6", "Part-Of", "VGG 1 6 - MC"], ["maxout layer", "Part-Of", "VGG 1 6 - MC"], ["fully connected layer", "Part-Of", "VGG 1 6 - MC"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["maxout layer:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MFC:Method"], ["convolutional layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["VGG 1 6:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["maxout layer:Method", "Part-Of", "VGG 1 6 - MC:Method"], ["fully connected layer:Method", "Part-Of", "VGG 1 6 - MC:Method"]]}
{"doc_id": "4539700", "sentence": "In contrast , RGB - I 3 D trained on Kinetics from scratch [ 3 ] were found to outperform ResNeXt - 1 0 1 even though ResNeXt - 1 0 1 is a deeper architecture than I 3 D .", "ner": [["RGB - I 3 D", "Method"], ["Kinetics", "Dataset"], ["ResNeXt - 1 0 1", "Method"], ["ResNeXt - 1 0 1", "Method"], ["I 3 D", "Method"]], "rel": [["RGB - I 3 D", "Trained-With", "Kinetics"], ["RGB - I 3 D", "Compare-With", "ResNeXt - 1 0 1"], ["ResNeXt - 1 0 1", "Compare-With", "I 3 D"]], "rel_plus": [["RGB - I 3 D:Method", "Trained-With", "Kinetics:Dataset"], ["RGB - I 3 D:Method", "Compare-With", "ResNeXt - 1 0 1:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "I 3 D:Method"]]}
{"doc_id": "202888751", "sentence": "This effect is demonstrated in Fig. 6 , on horse\u2192zebra translation task on unpaired Horse 2 Zebra dataset 6 .", "ner": [["horse\u2192zebra translation", "Task"], ["Horse 2 Zebra", "Dataset"]], "rel": [["Horse 2 Zebra", "Benchmark-For", "horse\u2192zebra translation"]], "rel_plus": [["Horse 2 Zebra:Dataset", "Benchmark-For", "horse\u2192zebra translation:Task"]]}
{"doc_id": "199543700", "sentence": "Experiments on several benchmark datasets show the effectiveness of the proposed SGGAN on image recognition and facial attribute recognition tasks .", "ner": [["SGGAN", "Method"], ["image recognition", "Task"], ["facial attribute recognition", "Task"]], "rel": [["SGGAN", "Used-For", "image recognition"], ["SGGAN", "Used-For", "facial attribute recognition"]], "rel_plus": [["SGGAN:Method", "Used-For", "image recognition:Task"], ["SGGAN:Method", "Used-For", "facial attribute recognition:Task"]]}
{"doc_id": "210861282", "sentence": "The WASP module is utilized in the UniPose architecture of Figure 2 for pose estimation .", "ner": [["WASP", "Method"], ["UniPose", "Method"], ["pose estimation", "Task"]], "rel": [["WASP", "Part-Of", "UniPose"], ["UniPose", "Used-For", "pose estimation"]], "rel_plus": [["WASP:Method", "Part-Of", "UniPose:Method"], ["UniPose:Method", "Used-For", "pose estimation:Task"]]}
{"doc_id": "210920315", "sentence": "Then , a multi - level RoI pooling ( mRoI ) is performed by concatenating the RoI Pooled features from multiple layers ( shallow and deep ) of the VGG backbone .", "ner": [["multi - level RoI pooling", "Method"], ["mRoI", "Method"], ["VGG", "Method"]], "rel": [["mRoI", "Synonym-Of", "multi - level RoI pooling"], ["multi - level RoI pooling", "Part-Of", "VGG"]], "rel_plus": [["mRoI:Method", "Synonym-Of", "multi - level RoI pooling:Method"], ["multi - level RoI pooling:Method", "Part-Of", "VGG:Method"]]}
{"doc_id": "209532167", "sentence": "For example , for both Variable Misuse and Function Docstring , CuBERT at 2 epochs and 1/ 3 rd training data outperforms the BiLSTM with Word 2 Vec and the Transformer baselines .", "ner": [["Variable Misuse", "Task"], ["Function Docstring", "Task"], ["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["Transformer", "Method"]], "rel": [["CuBERT", "Used-For", "Variable Misuse"], ["CuBERT", "Used-For", "Function Docstring"], ["CuBERT", "Compare-With", "BiLSTM"], ["CuBERT", "Compare-With", "Word 2 Vec"], ["CuBERT", "Compare-With", "Transformer"]], "rel_plus": [["CuBERT:Method", "Used-For", "Variable Misuse:Task"], ["CuBERT:Method", "Used-For", "Function Docstring:Task"], ["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "Word 2 Vec:Method"], ["CuBERT:Method", "Compare-With", "Transformer:Method"]]}
{"doc_id": "199543700", "sentence": "It is difficult to directly train a deep network in our case , so we propose a simple yet effective convolution block transformation ( CBT ) technique to transfer weights from a shallower network to a deep one by shortcut and an adaptive scaling layer following the shallower convolution block .", "ner": [["convolution block transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolution block transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolution block transformation:Method"]]}
{"doc_id": "202888751", "sentence": "Second , we evaluate the performance of a baseline convolutional neural network classifier for improved recognition using the resulting synthetic samples to augment our training set and compare it with the classical data augmentation scheme .", "ner": [["convolutional neural network", "Method"], ["recognition", "Task"], ["classical data augmentation", "Method"]], "rel": [["convolutional neural network", "Used-For", "recognition"]], "rel_plus": [["convolutional neural network:Method", "Used-For", "recognition:Task"]]}
{"doc_id": "202676714", "sentence": "Parseval networks are more robust against FGSM than naive models and can enhance adversarial training .", "ner": [["Parseval networks", "Method"], ["robust against FGSM", "Task"], ["adversarial training", "Method"]], "rel": [["Parseval networks", "Used-For", "robust against FGSM"], ["Parseval networks", "Part-Of", "adversarial training"]], "rel_plus": [["Parseval networks:Method", "Used-For", "robust against FGSM:Task"], ["Parseval networks:Method", "Part-Of", "adversarial training:Method"]]}
{"doc_id": "23569888", "sentence": "Secondary observations : \u2022 Fine - tuning the networks seems to lead to worse performance than RLSC on feature extraction both for CaffeNet and GoogLeNet . \u2022 Also in this setting we confirm ILSVRC results , with more recent networks outperforming previous ones but with VGG - 1 6 outperforming GoogLeNet when using feature extraction with RLSC . \u2022 To further support our findings , in the supplementary material we report results for a similar experiment but discriminating only between 1 0 or 5 categories .", "ner": [["RLSC", "Method"], ["feature extraction", "Method"], ["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["ILSVRC", "Dataset"], ["VGG - 1 6", "Method"], ["GoogLeNet", "Method"], ["feature extraction", "Method"], ["RLSC", "Method"]], "rel": [["RLSC", "Part-Of", "feature extraction"], ["RLSC", "Part-Of", "CaffeNet"], ["RLSC", "Part-Of", "GoogLeNet"], ["VGG - 1 6", "Evaluated-With", "ILSVRC"], ["GoogLeNet", "Evaluated-With", "ILSVRC"], ["RLSC", "Part-Of", "VGG - 1 6"], ["VGG - 1 6", "Compare-With", "GoogLeNet"], ["feature extraction", "Part-Of", "GoogLeNet"], ["RLSC", "Part-Of", "feature extraction"]], "rel_plus": [["RLSC:Method", "Part-Of", "feature extraction:Method"], ["RLSC:Method", "Part-Of", "CaffeNet:Method"], ["RLSC:Method", "Part-Of", "GoogLeNet:Method"], ["VGG - 1 6:Method", "Evaluated-With", "ILSVRC:Dataset"], ["GoogLeNet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["RLSC:Method", "Part-Of", "VGG - 1 6:Method"], ["VGG - 1 6:Method", "Compare-With", "GoogLeNet:Method"], ["feature extraction:Method", "Part-Of", "GoogLeNet:Method"], ["RLSC:Method", "Part-Of", "feature extraction:Method"]]}
{"doc_id": "198897554", "sentence": "In domain adaptation [ 1 7 , 1 8 , 1 9 , 2 0 ] , a model is trained to perform a visual task in a specific domain ( e.g. semantic segmentation in synthetic images ) , however , we need to apply it to perform the same task in a correlated , but significantly different , domain ( e.g. semantic segmentation in real - world images ) ; Finally , self - supervised learning [ 2 1 , 2 2 , 2 3 ] focuses on learning visual models without manual labeling ; more specifically , auxiliary relatively simple tasks , known as pretext tasks in this context , are created for training a generic visual model in the form of CNN .", "ner": [["domain adaptation", "Method"], ["semantic segmentation", "Task"], ["semantic segmentation", "Task"], ["self - supervised learning", "Method"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "153312532", "sentence": "Similar to Liu et al. ( 2 0 1 9 ) , we also use fine - tune BERT in multi - task learning framework for text classification .", "ner": [["BERT", "Method"], ["multi - task learning", "Method"], ["text classification", "Task"]], "rel": [["multi - task learning", "Used-For", "BERT"], ["BERT", "Used-For", "text classification"]], "rel_plus": [["multi - task learning:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "199543700", "sentence": "Image recognition is an important topic in computer vision and image processing , and has been mainly addressed by supervised deep learning methods , which need a large set of labeled images to achieve promising performance .", "ner": [["Image recognition", "Task"], ["computer vision", "Task"], ["image processing", "Task"], ["supervised deep learning", "Method"]], "rel": [["supervised deep learning", "Used-For", "Image recognition"], ["Image recognition", "SubTask-Of", "computer vision"], ["supervised deep learning", "Used-For", "computer vision"], ["supervised deep learning", "Used-For", "image processing"]], "rel_plus": [["supervised deep learning:Method", "Used-For", "Image recognition:Task"], ["Image recognition:Task", "SubTask-Of", "computer vision:Task"], ["supervised deep learning:Method", "Used-For", "computer vision:Task"], ["supervised deep learning:Method", "Used-For", "image processing:Task"]]}
{"doc_id": "24972096", "sentence": "Most previous methods focus on geometric 3D reconstruction and scene understanding independently notwithstanding the fact that joint estimation can boost the accuracy of the semantic mapping .", "ner": [["geometric 3D reconstruction", "Task"], ["scene understanding", "Task"], ["semantic mapping", "Task"]], "rel": [["geometric 3D reconstruction", "Used-For", "semantic mapping"], ["scene understanding", "Used-For", "semantic mapping"]], "rel_plus": [["geometric 3D reconstruction:Task", "Used-For", "semantic mapping:Task"], ["scene understanding:Task", "Used-For", "semantic mapping:Task"]]}
{"doc_id": "201646309", "sentence": "We evaluate SBERT on the Argument Facet Similarity ( AFS ) corpus by Misra et al. ( 2 0 1 6 ) .", "ner": [["SBERT", "Method"], ["Argument Facet Similarity", "Dataset"], ["AFS", "Dataset"]], "rel": [["AFS", "Synonym-Of", "Argument Facet Similarity"], ["SBERT", "Evaluated-With", "Argument Facet Similarity"]], "rel_plus": [["AFS:Dataset", "Synonym-Of", "Argument Facet Similarity:Dataset"], ["SBERT:Method", "Evaluated-With", "Argument Facet Similarity:Dataset"]]}
{"doc_id": "51559", "sentence": "We introduce quasi - recurrent neural networks ( QRNNs ) , an approach to neural sequence modeling that alternates convolutional layers , which apply in parallel across timesteps , and a minimalist recurrent pooling function that applies in parallel across channels .", "ner": [["quasi - recurrent neural networks", "Method"], ["QRNNs", "Method"], ["neural sequence modeling", "Task"], ["convolutional layers", "Method"], ["minimalist recurrent pooling function", "Method"]], "rel": [["QRNNs", "Synonym-Of", "quasi - recurrent neural networks"], ["convolutional layers", "Part-Of", "quasi - recurrent neural networks"], ["minimalist recurrent pooling function", "Part-Of", "quasi - recurrent neural networks"], ["quasi - recurrent neural networks", "Used-For", "neural sequence modeling"]], "rel_plus": [["QRNNs:Method", "Synonym-Of", "quasi - recurrent neural networks:Method"], ["convolutional layers:Method", "Part-Of", "quasi - recurrent neural networks:Method"], ["minimalist recurrent pooling function:Method", "Part-Of", "quasi - recurrent neural networks:Method"], ["quasi - recurrent neural networks:Method", "Used-For", "neural sequence modeling:Task"]]}
{"doc_id": "210839545", "sentence": "On Cityscapes , we compare GPSNet with several competitive baselines including the dilation - based methods , i.e. , DeepLabv 3 [ 4 ] , DUC - HDC [ 2 5 ] , DenseA - SPP [ 2 8 ] , region - based method i.e. , PSPNet [ 3 3 ] , and attention - based method i.e. , PSANet [ 3 4 ] , OCNet [ 3 0 ] .", "ner": [["Cityscapes", "Dataset"], ["GPSNet", "Method"], ["dilation - based methods", "Method"], ["DeepLabv 3", "Method"], ["DUC - HDC", "Method"], ["DenseA - SPP", "Method"], ["region - based method", "Method"], ["PSPNet", "Method"], ["attention - based method", "Method"], ["PSANet", "Method"], ["OCNet", "Method"]], "rel": [["GPSNet", "Evaluated-With", "Cityscapes"], ["GPSNet", "Compare-With", "dilation - based methods"], ["DeepLabv 3", "SubClass-Of", "dilation - based methods"], ["DUC - HDC", "SubClass-Of", "dilation - based methods"], ["DenseA - SPP", "SubClass-Of", "dilation - based methods"], ["GPSNet", "Compare-With", "DeepLabv 3"], ["GPSNet", "Compare-With", "DUC - HDC"], ["GPSNet", "Compare-With", "DenseA - SPP"], ["PSPNet", "SubClass-Of", "region - based method"], ["GPSNet", "Compare-With", "region - based method"], ["GPSNet", "Compare-With", "PSPNet"], ["PSANet", "SubClass-Of", "attention - based method"], ["OCNet", "SubClass-Of", "attention - based method"], ["GPSNet", "Compare-With", "attention - based method"], ["GPSNet", "Compare-With", "PSANet"], ["GPSNet", "Compare-With", "OCNet"]], "rel_plus": [["GPSNet:Method", "Evaluated-With", "Cityscapes:Dataset"], ["GPSNet:Method", "Compare-With", "dilation - based methods:Method"], ["DeepLabv 3:Method", "SubClass-Of", "dilation - based methods:Method"], ["DUC - HDC:Method", "SubClass-Of", "dilation - based methods:Method"], ["DenseA - SPP:Method", "SubClass-Of", "dilation - based methods:Method"], ["GPSNet:Method", "Compare-With", "DeepLabv 3:Method"], ["GPSNet:Method", "Compare-With", "DUC - HDC:Method"], ["GPSNet:Method", "Compare-With", "DenseA - SPP:Method"], ["PSPNet:Method", "SubClass-Of", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "PSPNet:Method"], ["PSANet:Method", "SubClass-Of", "attention - based method:Method"], ["OCNet:Method", "SubClass-Of", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "PSANet:Method"], ["GPSNet:Method", "Compare-With", "OCNet:Method"]]}
{"doc_id": "202676714", "sentence": "In fact , we observed that the effectiveness of maxnorm regularization is similar to weight decay .", "ner": [["maxnorm regularization", "Method"], ["weight decay", "Method"]], "rel": [["maxnorm regularization", "Compare-With", "weight decay"]], "rel_plus": [["maxnorm regularization:Method", "Compare-With", "weight decay:Method"]]}
{"doc_id": "204402755", "sentence": "For CNNs , we used the image classification datasets CI - FAR 1 0 , CIFAR 1 0 0 and STL 1 0 datasets .", "ner": [["CNNs", "Method"], ["image classification", "Task"], ["CI - FAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["STL 1 0", "Dataset"]], "rel": [["CI - FAR 1 0", "Used-For", "CNNs"], ["CIFAR 1 0 0", "Used-For", "CNNs"], ["STL 1 0", "Used-For", "CNNs"], ["CI - FAR 1 0", "Benchmark-For", "image classification"], ["CIFAR 1 0 0", "Benchmark-For", "image classification"], ["STL 1 0", "Benchmark-For", "image classification"]], "rel_plus": [["CI - FAR 1 0:Dataset", "Used-For", "CNNs:Method"], ["CIFAR 1 0 0:Dataset", "Used-For", "CNNs:Method"], ["STL 1 0:Dataset", "Used-For", "CNNs:Method"], ["CI - FAR 1 0:Dataset", "Benchmark-For", "image classification:Task"], ["CIFAR 1 0 0:Dataset", "Benchmark-For", "image classification:Task"], ["STL 1 0:Dataset", "Benchmark-For", "image classification:Task"]]}
{"doc_id": "3920676", "sentence": "P ERSON re - identification , or re - id , is a critical task in most surveillance and security applications [ 1 ] , [ 2 ] , [ 3 ] and has increasingly attracted attention from the computer vision community [ 4 ] , [ 5 ] , [ 6 ] , [ 7 ] , [ 8 ] , [ 9 ] , [ 1 0 ] , [ 1 1 ] , [ 1 2 ] , [ 1 3 ] , [ 1 4 ] , [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 8 ] , [ 1 9 ] , [ 2 0 ] , [ 2 1 ] , [ 2 2 ] , [ 2 3 ] .", "ner": [["P ERSON re - identification", "Task"], ["re - id", "Task"], ["computer vision", "Task"]], "rel": [["re - id", "Synonym-Of", "P ERSON re - identification"], ["P ERSON re - identification", "SubTask-Of", "computer vision"]], "rel_plus": [["re - id:Task", "Synonym-Of", "P ERSON re - identification:Task"], ["P ERSON re - identification:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "202750230", "sentence": "We apply our method to a variety of sequence modeling tasks : neural machine translation , language modeling , summarization , long form question answering , and various natural language understanding tasks .", "ner": [["sequence modeling", "Task"], ["neural machine translation", "Task"], ["language modeling", "Task"], ["summarization", "Task"], ["long form question answering", "Task"], ["natural language understanding", "Task"]], "rel": [["neural machine translation", "SubTask-Of", "sequence modeling"], ["language modeling", "SubTask-Of", "sequence modeling"], ["summarization", "SubTask-Of", "sequence modeling"], ["long form question answering", "SubTask-Of", "sequence modeling"], ["natural language understanding", "SubTask-Of", "sequence modeling"]], "rel_plus": [["neural machine translation:Task", "SubTask-Of", "sequence modeling:Task"], ["language modeling:Task", "SubTask-Of", "sequence modeling:Task"], ["summarization:Task", "SubTask-Of", "sequence modeling:Task"], ["long form question answering:Task", "SubTask-Of", "sequence modeling:Task"], ["natural language understanding:Task", "SubTask-Of", "sequence modeling:Task"]]}
{"doc_id": "202719032", "sentence": "Faster R - CNN , Mask R - CNN and R - FCN used a batch size of 1 and the Stochastic Gradient Descent ( SGD ) with momentum value of 0. 9 as optimizer .", "ner": [["Faster R - CNN", "Method"], ["Mask R - CNN", "Method"], ["R - FCN", "Method"], ["Stochastic Gradient Descent", "Method"], ["SGD", "Method"], ["momentum", "Method"]], "rel": [["Stochastic Gradient Descent", "Part-Of", "Faster R - CNN"], ["Stochastic Gradient Descent", "Part-Of", "Mask R - CNN"], ["Stochastic Gradient Descent", "Part-Of", "R - FCN"], ["SGD", "Synonym-Of", "Stochastic Gradient Descent"], ["momentum", "Part-Of", "Stochastic Gradient Descent"]], "rel_plus": [["Stochastic Gradient Descent:Method", "Part-Of", "Faster R - CNN:Method"], ["Stochastic Gradient Descent:Method", "Part-Of", "Mask R - CNN:Method"], ["Stochastic Gradient Descent:Method", "Part-Of", "R - FCN:Method"], ["SGD:Method", "Synonym-Of", "Stochastic Gradient Descent:Method"], ["momentum:Method", "Part-Of", "Stochastic Gradient Descent:Method"]]}
{"doc_id": "53719742", "sentence": "State - of - the - art CNN based object detection and segmentation frameworks have been widely used to solve the text detection problem recently .", "ner": [["CNN", "Method"], ["object detection", "Task"], ["segmentation", "Task"], ["text detection", "Task"]], "rel": [["CNN", "Used-For", "object detection"], ["CNN", "Used-For", "segmentation"], ["CNN", "Used-For", "text detection"]], "rel_plus": [["CNN:Method", "Used-For", "object detection:Task"], ["CNN:Method", "Used-For", "segmentation:Task"], ["CNN:Method", "Used-For", "text detection:Task"]]}
{"doc_id": "210164920", "sentence": "Yu and Koltun have introduced a modified version of traditional CNN , called dialated convolution or Dialated - Net [ 1 1 0 ] , to accumulate multi - scale contextual information systematically for better segmentation without suffering the loss of resolution .", "ner": [["CNN", "Method"], ["dialated convolution", "Method"], ["Dialated - Net", "Method"], ["segmentation", "Task"]], "rel": [["dialated convolution", "SubClass-Of", "CNN"], ["Dialated - Net", "SubClass-Of", "CNN"], ["Dialated - Net", "Used-For", "segmentation"], ["dialated convolution", "Used-For", "segmentation"], ["CNN", "Used-For", "segmentation"]], "rel_plus": [["dialated convolution:Method", "SubClass-Of", "CNN:Method"], ["Dialated - Net:Method", "SubClass-Of", "CNN:Method"], ["Dialated - Net:Method", "Used-For", "segmentation:Task"], ["dialated convolution:Method", "Used-For", "segmentation:Task"], ["CNN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "202750230", "sentence": "Further , our method does not need any post - processing : we simply prune every other layer of our RoBERTa model that has been pre - trained with LayerDrop and finetune the small models on each of the downstream tasks , following standard procedure .", "ner": [["RoBERTa", "Method"], ["LayerDrop", "Method"]], "rel": [["LayerDrop", "Part-Of", "RoBERTa"]], "rel_plus": [["LayerDrop:Method", "Part-Of", "RoBERTa:Method"]]}
{"doc_id": "3920676", "sentence": "The results of this experiment are shown in Fig. 4c , from which we can note that NFST exp gives the best performance on Market 1 5 0 1 , DukeMTMC , 3DPeS , and Airport , with XQDA and kLFDA not being too far behind .", "ner": [["NFST", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC", "Dataset"], ["3DPeS", "Dataset"], ["Airport", "Dataset"], ["XQDA", "Method"], ["kLFDA", "Method"]], "rel": [["NFST", "Evaluated-With", "Market 1 5 0 1"], ["XQDA", "Evaluated-With", "Market 1 5 0 1"], ["kLFDA", "Evaluated-With", "Market 1 5 0 1"], ["NFST", "Evaluated-With", "DukeMTMC"], ["XQDA", "Evaluated-With", "DukeMTMC"], ["kLFDA", "Evaluated-With", "DukeMTMC"], ["NFST", "Evaluated-With", "3DPeS"], ["XQDA", "Evaluated-With", "3DPeS"], ["kLFDA", "Evaluated-With", "3DPeS"], ["NFST", "Evaluated-With", "Airport"], ["XQDA", "Evaluated-With", "Airport"], ["kLFDA", "Evaluated-With", "Airport"], ["NFST", "Compare-With", "XQDA"], ["NFST", "Compare-With", "kLFDA"], ["XQDA", "Compare-With", "kLFDA"]], "rel_plus": [["NFST:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["XQDA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["kLFDA:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["NFST:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["XQDA:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["kLFDA:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["NFST:Method", "Evaluated-With", "3DPeS:Dataset"], ["XQDA:Method", "Evaluated-With", "3DPeS:Dataset"], ["kLFDA:Method", "Evaluated-With", "3DPeS:Dataset"], ["NFST:Method", "Evaluated-With", "Airport:Dataset"], ["XQDA:Method", "Evaluated-With", "Airport:Dataset"], ["kLFDA:Method", "Evaluated-With", "Airport:Dataset"], ["NFST:Method", "Compare-With", "XQDA:Method"], ["NFST:Method", "Compare-With", "kLFDA:Method"], ["XQDA:Method", "Compare-With", "kLFDA:Method"]]}
{"doc_id": "210860760", "sentence": "Goyal et al. [ 7 ] and Cui [ 1 2 ] presented the graph embedding techniques in three categories : factorization based , random walk based and deep learning based .", "ner": [["graph embedding", "Method"], ["factorization based", "Method"], ["random walk based", "Method"], ["deep learning based", "Method"]], "rel": [["random walk based", "SubClass-Of", "graph embedding"], ["factorization based", "SubClass-Of", "graph embedding"], ["deep learning based", "SubClass-Of", "graph embedding"]], "rel_plus": [["random walk based:Method", "SubClass-Of", "graph embedding:Method"], ["factorization based:Method", "SubClass-Of", "graph embedding:Method"], ["deep learning based:Method", "SubClass-Of", "graph embedding:Method"]]}
{"doc_id": "210839714", "sentence": "These two experiments indicate that not all prediction tasks require the true encounter structure , and it is our future work to apply GCT to various prediction tasks to evaluate its effectiveness .   In this section , we analyze the learned structure of both Transformer and GCT .", "ner": [["GCT", "Method"], ["Transformer", "Method"], ["GCT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "54447105", "sentence": "A SGD optimizer with momentum 0. 9 is used with a batch of 2 images , one from the first domain and the other from the second domain , without data augmentation .", "ner": [["SGD", "Method"], ["momentum", "Method"], ["data augmentation", "Method"]], "rel": [["momentum", "Part-Of", "SGD"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD:Method"]]}
{"doc_id": "21683040", "sentence": "The FSSD 5 1 2 takes the first place in VOC 2 0 1 2 leaderboard among all of the one - stage object detectors as of the time of submission .", "ner": [["FSSD 5 1 2", "Method"], ["VOC 2 0 1 2", "Dataset"]], "rel": [["FSSD 5 1 2", "Evaluated-With", "VOC 2 0 1 2"]], "rel_plus": [["FSSD 5 1 2:Method", "Evaluated-With", "VOC 2 0 1 2:Dataset"]]}
{"doc_id": "150374036", "sentence": "Liu et al. [ 4 0 ] propose a Facial Action Units based CNN architecture for expression recognition .", "ner": [["Facial Action Units based CNN", "Method"], ["expression recognition", "Task"]], "rel": [["Facial Action Units based CNN", "Used-For", "expression recognition"]], "rel_plus": [["Facial Action Units based CNN:Method", "Used-For", "expression recognition:Task"]]}
{"doc_id": "202565512", "sentence": "Graph Neural Networks for NLP Recently , Graph Neural Networks ( GNN ) has been utilized widely in NLP .", "ner": [["Graph Neural Networks", "Method"], ["NLP", "Task"], ["Graph Neural Networks", "Method"], ["GNN", "Method"], ["NLP", "Task"]], "rel": [["Graph Neural Networks", "Used-For", "NLP"], ["GNN", "Synonym-Of", "Graph Neural Networks"], ["Graph Neural Networks", "Used-For", "NLP"]], "rel_plus": [["Graph Neural Networks:Method", "Used-For", "NLP:Task"], ["GNN:Method", "Synonym-Of", "Graph Neural Networks:Method"], ["Graph Neural Networks:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "4246700", "sentence": "This is perhaps caused by the imbalance of Sydney - captions and the further analysis is presented in Section V - D 1 . 2 ) Results based on different CNNs : In order to evaluate the generated sentences based on CNNs features , the experiments based on CNNs features are conducted in this subsection .", "ner": [["Sydney - captions", "Dataset"], ["CNNs", "Method"], ["CNNs", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "Q -PPO is higher than Q -RF , which may because PPO method can improve the high variance problem using REINFORCE method on question re nement task .", "ner": [["Q -PPO", "Method"], ["Q -RF", "Method"], ["PPO", "Method"], ["REINFORCE", "Method"], ["question re nement task", "Task"]], "rel": [["Q -PPO", "Compare-With", "Q -RF"], ["REINFORCE", "Part-Of", "PPO"], ["REINFORCE", "Used-For", "question re nement task"]], "rel_plus": [["Q -PPO:Method", "Compare-With", "Q -RF:Method"], ["REINFORCE:Method", "Part-Of", "PPO:Method"], ["REINFORCE:Method", "Used-For", "question re nement task:Task"]]}
{"doc_id": "52180375", "sentence": "Furthermore , when we adopt a deeper pre - trained network ( ResNet - 1 0 1 ) , the network with two attention modules significantly improves the segmentation performance over the baseline model by 5. 0 3 % .", "ner": [["deeper pre - trained network", "Method"], ["ResNet - 1 0 1", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "deeper pre - trained network"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "deeper pre - trained network:Method"]]}
{"doc_id": "210164920", "sentence": "The authors have decomposed the instance seg - mentation task into three sub tasks : instance differentiation ( class agnostic bounding box generation for each instance ) , mask estimation(estimated a pixel - level mask/instance ) and object categorization ( instances are labeled categorically ) .", "ner": [["instance seg - mentation", "Task"], ["instance differentiation", "Task"], ["mask estimation(estimated", "Task"], ["object categorization", "Task"]], "rel": [["mask estimation(estimated", "SubTask-Of", "instance seg - mentation"], ["instance differentiation", "SubTask-Of", "instance seg - mentation"], ["object categorization", "SubTask-Of", "instance seg - mentation"]], "rel_plus": [["mask estimation(estimated:Task", "SubTask-Of", "instance seg - mentation:Task"], ["instance differentiation:Task", "SubTask-Of", "instance seg - mentation:Task"], ["object categorization:Task", "SubTask-Of", "instance seg - mentation:Task"]]}
{"doc_id": "4246700", "sentence": "Features extracted from convolutional layer of AlexNet can represent exhaustive content of remote sensing images .", "ner": [["convolutional layer", "Method"], ["AlexNet", "Method"]], "rel": [["convolutional layer", "Part-Of", "AlexNet"]], "rel_plus": [["convolutional layer:Method", "Part-Of", "AlexNet:Method"]]}
{"doc_id": "153312532", "sentence": "BERT is based on a multi - layer bidirectional Transformer ( Vaswani et al. , 2 0 1 7 ) and is trained on plain text for masked word prediction and next sentence prediction tasks .", "ner": [["BERT", "Method"], ["multi - layer bidirectional Transformer", "Method"], ["masked word prediction", "Task"], ["next sentence prediction", "Task"]], "rel": [["BERT", "SubClass-Of", "multi - layer bidirectional Transformer"], ["BERT", "Trained-With", "masked word prediction"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "SubClass-Of", "multi - layer bidirectional Transformer:Method"], ["BERT:Method", "Trained-With", "masked word prediction:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "67855714", "sentence": "Note also that even if SRGAN [ 1 ] is optimized to minimize a VGG loss , it does not give the lowest perceptual errors in terms of the perceptual metrics VGG and VGG - l , this is due to the fact that the VGG features are not relevant for the satellite images domain .", "ner": [["SRGAN", "Method"], ["VGG loss", "Method"], ["VGG", "Method"], ["VGG - l", "Method"], ["VGG", "Method"]], "rel": [["VGG loss", "Part-Of", "SRGAN"]], "rel_plus": [["VGG loss:Method", "Part-Of", "SRGAN:Method"]]}
{"doc_id": "202888751", "sentence": "CNNs are extensively used for solving image recognition tasks in computer vision .", "ner": [["CNNs", "Method"], ["image recognition", "Task"], ["computer vision", "Task"]], "rel": [["CNNs", "Used-For", "image recognition"], ["image recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["CNNs:Method", "Used-For", "image recognition:Task"], ["image recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "44148233", "sentence": "Currently , automatic evaluations are typically performed using machine translation and image captioning metrics , including Bilingual Evaluation Understudy ( BLEU ) [ 1 1 8 ] , Recall Oriented Understudy for Gisting Evaluation ( ROUGE ) [ 9 8 ] , Metric for Evaluation of Translation with Explicit Ordering ( METEOR ) [ 2 2 ] , Consensus based Image Description Evaluation ( CIDEr ) [ 1 5 8 ] , and the recently proposed Semantic Propositional Image Captioning Evaluation ( SPICE ) [ 1 4 ] and Word Mover 's Distance ( WMD ) [ 9 2 ] metrics .", "ner": [["machine translation and image captioning metrics", "Method"], ["Bilingual Evaluation Understudy", "Method"], ["BLEU", "Method"], ["Recall Oriented Understudy for Gisting Evaluation", "Method"], ["ROUGE", "Method"], ["Metric for Evaluation of Translation with Explicit Ordering", "Method"], ["METEOR", "Method"], ["Consensus based Image Description Evaluation", "Method"], ["CIDEr", "Method"], ["Semantic Propositional Image Captioning Evaluation", "Method"], ["SPICE", "Method"], ["Word Mover 's Distance", "Method"], ["WMD", "Method"]], "rel": [["Bilingual Evaluation Understudy", "SubClass-Of", "machine translation and image captioning metrics"], ["Recall Oriented Understudy for Gisting Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Metric for Evaluation of Translation with Explicit Ordering", "SubClass-Of", "machine translation and image captioning metrics"], ["Consensus based Image Description Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Semantic Propositional Image Captioning Evaluation", "SubClass-Of", "machine translation and image captioning metrics"], ["Word Mover 's Distance", "SubClass-Of", "machine translation and image captioning metrics"], ["BLEU", "Synonym-Of", "Bilingual Evaluation Understudy"], ["ROUGE", "Synonym-Of", "Recall Oriented Understudy for Gisting Evaluation"], ["METEOR", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering"], ["CIDEr", "Synonym-Of", "Consensus based Image Description Evaluation"], ["SPICE", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation"], ["WMD", "Synonym-Of", "Word Mover 's Distance"]], "rel_plus": [["Bilingual Evaluation Understudy:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Recall Oriented Understudy for Gisting Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Metric for Evaluation of Translation with Explicit Ordering:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Consensus based Image Description Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Semantic Propositional Image Captioning Evaluation:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["Word Mover 's Distance:Method", "SubClass-Of", "machine translation and image captioning metrics:Method"], ["BLEU:Method", "Synonym-Of", "Bilingual Evaluation Understudy:Method"], ["ROUGE:Method", "Synonym-Of", "Recall Oriented Understudy for Gisting Evaluation:Method"], ["METEOR:Method", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering:Method"], ["CIDEr:Method", "Synonym-Of", "Consensus based Image Description Evaluation:Method"], ["SPICE:Method", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation:Method"], ["WMD:Method", "Synonym-Of", "Word Mover 's Distance:Method"]]}
{"doc_id": "208202241", "sentence": "Our work mostly relates to the one proposed in ( Chen et al. 2 0 1 9 ) , which used GCN to propagate information among labels and merges label information with CNN features at the final classification stage .", "ner": [["GCN", "Method"], ["CNN", "Method"], ["classification", "Task"]], "rel": [["GCN", "Used-For", "classification"], ["CNN", "Used-For", "classification"]], "rel_plus": [["GCN:Method", "Used-For", "classification:Task"], ["CNN:Method", "Used-For", "classification:Task"]]}
{"doc_id": "23569888", "sentence": "ImageNet synsets corresponding to the 5 object categories on which the CNNs will be later trained and tested for invariance , namely book , flower , glass , hairbrush and hairclip .", "ner": [["ImageNet", "Dataset"], ["CNNs", "Method"]], "rel": [["CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "51876625", "sentence": "Finally , Figure 7 illustrates examples for which the false alarms of the Base - Model are removed by ACRN ( top row ) and the missing detections are captured by ACRN ( bottom row ) .", "ner": [["Base - Model", "Method"], ["ACRN", "Method"], ["ACRN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "In particular , BERT ( Devlin et al. , 2 0 1 9 ) produces a bidirectional Transformer encoder ( Vaswani et al. , 2 0 1 7 ) by training it to predict values of masked tokens and whether two sentences follow each other in a natural discourse .", "ner": [["BERT", "Method"], ["bidirectional Transformer encoder", "Method"]], "rel": [["BERT", "SubClass-Of", "bidirectional Transformer encoder"]], "rel_plus": [["BERT:Method", "SubClass-Of", "bidirectional Transformer encoder:Method"]]}
{"doc_id": "44148233", "sentence": "Below we summarize the recognition techniques used in the Stage I of the SVO tuples based approaches . \u2022 Object Recognition : Object recognition in SVO approaches was performed typically using conventional methods , including model - based shape matching through edge detection or color matching [ 8 4 ] , HAAR features matching [ 1 6 4 ] , context - based object recognition [ 1 5 6 ] , Scale Invariant Feature Transform ( SIFT ) [ 1 0 1 ] , discriminatively trained partbased models [ 5 4 ] and Deformable Parts Model ( DPM ) [ 5 2 ] , [ 5 3 ] . \u2022 Human and Activity Detection : Human detection methods employed features such as Histograms of Oriented Gradient ( HOG ) [ 3 9 ] followed by SVM .", "ner": [["SVO", "Method"], ["Object Recognition", "Task"], ["Object recognition", "Task"], ["SVO", "Method"], ["model - based shape matching", "Method"], ["edge detection", "Method"], ["color matching", "Method"], ["HAAR features matching", "Method"], ["context - based object recognition", "Method"], ["Scale Invariant Feature Transform", "Method"], ["SIFT", "Method"], ["discriminatively trained partbased models", "Method"], ["Deformable Parts Model", "Method"], ["DPM", "Method"], ["Human and Activity Detection", "Task"], ["Human detection", "Task"], ["Histograms of Oriented Gradient", "Method"], ["HOG", "Method"], ["SVM", "Method"]], "rel": [["model - based shape matching", "Used-For", "Object recognition"], ["HAAR features matching", "Used-For", "Object recognition"], ["context - based object recognition", "Used-For", "Object recognition"], ["Scale Invariant Feature Transform", "Used-For", "Object recognition"], ["discriminatively trained partbased models", "Used-For", "Object recognition"], ["Deformable Parts Model", "Used-For", "Object recognition"], ["SVO", "Used-For", "Object recognition"], ["color matching", "Part-Of", "model - based shape matching"], ["edge detection", "Part-Of", "model - based shape matching"], ["SIFT", "Synonym-Of", "Scale Invariant Feature Transform"], ["DPM", "Synonym-Of", "Deformable Parts Model"], ["Histograms of Oriented Gradient", "Used-For", "Human detection"], ["SVM", "Used-For", "Human detection"], ["HOG", "Synonym-Of", "Histograms of Oriented Gradient"]], "rel_plus": [["model - based shape matching:Method", "Used-For", "Object recognition:Task"], ["HAAR features matching:Method", "Used-For", "Object recognition:Task"], ["context - based object recognition:Method", "Used-For", "Object recognition:Task"], ["Scale Invariant Feature Transform:Method", "Used-For", "Object recognition:Task"], ["discriminatively trained partbased models:Method", "Used-For", "Object recognition:Task"], ["Deformable Parts Model:Method", "Used-For", "Object recognition:Task"], ["SVO:Method", "Used-For", "Object recognition:Task"], ["color matching:Method", "Part-Of", "model - based shape matching:Method"], ["edge detection:Method", "Part-Of", "model - based shape matching:Method"], ["SIFT:Method", "Synonym-Of", "Scale Invariant Feature Transform:Method"], ["DPM:Method", "Synonym-Of", "Deformable Parts Model:Method"], ["Histograms of Oriented Gradient:Method", "Used-For", "Human detection:Task"], ["SVM:Method", "Used-For", "Human detection:Task"], ["HOG:Method", "Synonym-Of", "Histograms of Oriented Gradient:Method"]]}
{"doc_id": "198147921", "sentence": "The ORN then refines the detection by discarding regions that do not correspond to the objects of interest .", "ner": [["ORN", "Method"], ["detection", "Task"]], "rel": [["ORN", "Used-For", "detection"]], "rel_plus": [["ORN:Method", "Used-For", "detection:Task"]]}
{"doc_id": "209386851", "sentence": "Fully convolutional networks ( FCNs ) [ 3 5 ] are the foundation of modern semantic segmentation approaches .", "ner": [["Fully convolutional networks", "Method"], ["FCNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCNs", "Synonym-Of", "Fully convolutional networks"], ["Fully convolutional networks", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully convolutional networks:Method"], ["Fully convolutional networks:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "153312532", "sentence": "Therefore , we can further pre - train BERT with masked language model and next sentence prediction tasks on the domain - specific data .", "ner": [["BERT", "Method"], ["masked language model", "Task"], ["next sentence prediction", "Task"]], "rel": [["BERT", "Trained-With", "masked language model"], ["BERT", "Trained-With", "next sentence prediction"]], "rel_plus": [["BERT:Method", "Trained-With", "masked language model:Task"], ["BERT:Method", "Trained-With", "next sentence prediction:Task"]]}
{"doc_id": "150374036", "sentence": "Due to its potential applications in various fields , such as intelligent tutoring systems , service robots , driver fatigue monitoring , Facial Expression Recognition ( FER ) has attracted increasing attention in the computer vision community recently [ 5 ] , [ 1 7 ] , [ 2 1 ] , [ 2 8 ] , [ 3 6 ] , [ 4 8 ] .", "ner": [["Facial Expression Recognition", "Task"], ["FER", "Task"], ["computer vision", "Task"]], "rel": [["FER", "Synonym-Of", "Facial Expression Recognition"], ["Facial Expression Recognition", "SubTask-Of", "computer vision"]], "rel_plus": [["FER:Task", "Synonym-Of", "Facial Expression Recognition:Task"], ["Facial Expression Recognition:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "102351044", "sentence": "The learning curves corroborate the aforementioned findings and further demonstrate that drop - operations of drop - neuron and dropchannel are effective in regularizing CNNs .", "ner": [["drop - neuron", "Method"], ["dropchannel", "Method"], ["CNNs", "Method"]], "rel": [["dropchannel", "Part-Of", "CNNs"], ["drop - neuron", "Part-Of", "CNNs"]], "rel_plus": [["dropchannel:Method", "Part-Of", "CNNs:Method"], ["drop - neuron:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "23569888", "sentence": "For our tests we have devised a prototypical vision task for a humanoid robot in which human - robot interaction is exploited to obtain realistic supervision and train an object recognition system .", "ner": [["human - robot interaction", "Task"], ["object recognition system", "Method"]], "rel": [["human - robot interaction", "Used-For", "object recognition system"]], "rel_plus": [["human - robot interaction:Task", "Used-For", "object recognition system:Method"]]}
{"doc_id": "210839545", "sentence": "Recent stateof - the - art semantic segmentation approaches [ 1 7 , 2 , 3 , 4 , 6 , 3 3 , 2 8 ] are typically based on the Fully Convolutional Networks ( FCNs ) [ 1 7 ] .", "ner": [["semantic segmentation", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"]], "rel": [["Fully Convolutional Networks", "Used-For", "semantic segmentation"], ["FCNs", "Synonym-Of", "Fully Convolutional Networks"]], "rel_plus": [["Fully Convolutional Networks:Method", "Used-For", "semantic segmentation:Task"], ["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"]]}
{"doc_id": "210839545", "sentence": "Weighted Sum .   In this section , we compare our GPSNet with the most relevant approaches including ASPP , Dense Atrous Spatial Pyramid Pooling ( DenseASPP ) , Deformable Convolution Network ( DCN ) .", "ner": [["GPSNet", "Method"], ["ASPP", "Method"], ["Dense Atrous Spatial Pyramid Pooling", "Method"], ["DenseASPP", "Method"], ["Deformable Convolution Network", "Method"], ["DCN", "Method"]], "rel": [["ASPP", "Part-Of", "GPSNet"], ["Dense Atrous Spatial Pyramid Pooling", "Part-Of", "GPSNet"], ["Deformable Convolution Network", "Part-Of", "GPSNet"], ["DenseASPP", "Synonym-Of", "Dense Atrous Spatial Pyramid Pooling"], ["DCN", "Synonym-Of", "Deformable Convolution Network"]], "rel_plus": [["ASPP:Method", "Part-Of", "GPSNet:Method"], ["Dense Atrous Spatial Pyramid Pooling:Method", "Part-Of", "GPSNet:Method"], ["Deformable Convolution Network:Method", "Part-Of", "GPSNet:Method"], ["DenseASPP:Method", "Synonym-Of", "Dense Atrous Spatial Pyramid Pooling:Method"], ["DCN:Method", "Synonym-Of", "Deformable Convolution Network:Method"]]}
{"doc_id": "208202241", "sentence": "Recently , graph convolutional network ( Kipf and Welling 2 0 1 6 ) , aka GCN , has witnessed prevailing success in modeling relationship among vertices of a graph .", "ner": [["graph convolutional network", "Method"], ["GCN", "Method"]], "rel": [["GCN", "Synonym-Of", "graph convolutional network"]], "rel_plus": [["GCN:Method", "Synonym-Of", "graph convolutional network:Method"]]}
{"doc_id": "210860962", "sentence": "The network is built with several convolutional blocks composed of a convolution ( or transposed convolution when upsampling ) , a dropout layer [ 4 9 ] , an instance normalization layer ( without computation of running statistics ) [ 5 5 ] and a rectified linear unit ( ReLU ) activation .", "ner": [["convolutional blocks", "Method"], ["convolution", "Method"], ["transposed convolution", "Method"], ["dropout", "Method"], ["instance normalization", "Method"], ["rectified linear unit", "Method"], ["ReLU", "Method"]], "rel": [["convolution", "Part-Of", "convolutional blocks"], ["transposed convolution", "Part-Of", "convolutional blocks"], ["dropout", "Part-Of", "convolutional blocks"], ["instance normalization", "Part-Of", "convolutional blocks"], ["rectified linear unit", "Part-Of", "convolutional blocks"], ["ReLU", "Synonym-Of", "rectified linear unit"]], "rel_plus": [["convolution:Method", "Part-Of", "convolutional blocks:Method"], ["transposed convolution:Method", "Part-Of", "convolutional blocks:Method"], ["dropout:Method", "Part-Of", "convolutional blocks:Method"], ["instance normalization:Method", "Part-Of", "convolutional blocks:Method"], ["rectified linear unit:Method", "Part-Of", "convolutional blocks:Method"], ["ReLU:Method", "Synonym-Of", "rectified linear unit:Method"]]}
{"doc_id": "203593581", "sentence": "Based on the truth that RCF has better final accuracy , we can infer that projection error has a higher priority in RCF .", "ner": [["RCF", "Method"], ["RCF", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "For each patch , a SIFT feature is obtained by Principal Component Analysis ( PCA ) of the origin SIFT features .", "ner": [["SIFT", "Method"], ["Principal Component Analysis", "Method"], ["PCA", "Method"], ["SIFT", "Method"]], "rel": [["Principal Component Analysis", "Used-For", "SIFT"], ["PCA", "Synonym-Of", "Principal Component Analysis"]], "rel_plus": [["Principal Component Analysis:Method", "Used-For", "SIFT:Method"], ["PCA:Method", "Synonym-Of", "Principal Component Analysis:Method"]]}
{"doc_id": "201124533", "sentence": "The main technical novelties compared with [ 1 0 5 ] lie in threefold . ( 1 ) We extend the network ( named as HRNetV 1 ) proposed in [ 1 0 5 ] , to two versions : HRNetV 2 and HRNetV 2 p , which explore all the four - resolution representations . ( 2 ) We build the connection between multi - resolution fusion and regular convolution , which provides an evidence for the necessity of exploring all the four - resolution representations in HRNetV 2 and HRNetV 2 p. ( 3 ) We show the superiority of HRNetV 2 and HRNetV 2 p over HRNetV 1 and present the applications of HRNetV 2 and HRNetV 2 p in a broad range of vision problems , including semantic segmentation and object detection .", "ner": [["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["multi - resolution fusion", "Method"], ["convolution", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p.", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["HRNetV 1", "Method"], ["HRNetV 2", "Method"], ["HRNetV 2 p", "Method"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["HRNetV 2", "SubClass-Of", "HRNetV 1"], ["HRNetV 2 p", "SubClass-Of", "HRNetV 1"], ["multi - resolution fusion", "Part-Of", "HRNetV 2"], ["convolution", "Part-Of", "HRNetV 2"], ["multi - resolution fusion", "Part-Of", "HRNetV 2 p."], ["convolution", "Part-Of", "HRNetV 2 p."], ["HRNetV 2 p", "Used-For", "semantic segmentation"], ["HRNetV 2", "Used-For", "object detection"]], "rel_plus": [["HRNetV 2:Method", "SubClass-Of", "HRNetV 1:Method"], ["HRNetV 2 p:Method", "SubClass-Of", "HRNetV 1:Method"], ["multi - resolution fusion:Method", "Part-Of", "HRNetV 2:Method"], ["convolution:Method", "Part-Of", "HRNetV 2:Method"], ["multi - resolution fusion:Method", "Part-Of", "HRNetV 2 p.:Method"], ["convolution:Method", "Part-Of", "HRNetV 2 p.:Method"], ["HRNetV 2 p:Method", "Used-For", "semantic segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "AUG126", "sentence": "Cityscapes dataset, abbreviated as Cityscapes, is enhanced by SegNet for semantic segmentation.", "ner": [["Cityscapes dataset", "Dataset"], ["Cityscapes", "Dataset"], ["SegNet", "Method"], ["semantic segmentation", "Task"]], "rel": [["Cityscapes", "Synonym-Of", "Cityscapes dataset"], ["SegNet", "Used-For", "semantic segmentation"], ["SegNet", "Evaluated-With", "Cityscapes"]], "rel_plus": [["Cityscapes:Dataset", "Synonym-Of", "Cityscapes dataset:Dataset"], ["SegNet:Method", "Used-For", "semantic segmentation:Task"], ["SegNet:Method", "Evaluated-With", "Cityscapes:Dataset"]]}
{"doc_id": "201646309", "sentence": "BERT ( Devlin et al. , 2 0 1 8 ) is a pre - trained transformer network ( Vaswani et al. , 2 0 1 7 ) , which set for various NLP tasks new state - of - the - art results , including question answering , sentence classification , and sentence - pair regression .", "ner": [["BERT", "Method"], ["transformer", "Method"], ["NLP", "Task"], ["question answering", "Task"], ["sentence classification", "Task"], ["sentence - pair regression", "Task"]], "rel": [["BERT", "SubClass-Of", "transformer"], ["question answering", "SubTask-Of", "NLP"], ["sentence classification", "SubTask-Of", "NLP"], ["sentence - pair regression", "SubTask-Of", "NLP"], ["BERT", "Used-For", "NLP"], ["BERT", "Used-For", "question answering"], ["BERT", "Used-For", "sentence classification"], ["BERT", "Used-For", "sentence - pair regression"]], "rel_plus": [["BERT:Method", "SubClass-Of", "transformer:Method"], ["question answering:Task", "SubTask-Of", "NLP:Task"], ["sentence classification:Task", "SubTask-Of", "NLP:Task"], ["sentence - pair regression:Task", "SubTask-Of", "NLP:Task"], ["BERT:Method", "Used-For", "NLP:Task"], ["BERT:Method", "Used-For", "question answering:Task"], ["BERT:Method", "Used-For", "sentence classification:Task"], ["BERT:Method", "Used-For", "sentence - pair regression:Task"]]}
{"doc_id": "AUG129", "sentence": "Librispeech Automatic Speech Recognition Corpus, or LibriSpeech, is enhanced by WaveNet for speech recognition.", "ner": [["Librispeech Automatic Speech Recognition Corpus", "Dataset"], ["LibriSpeech", "Dataset"], ["WaveNet", "Method"], ["speech recognition", "Task"]], "rel": [["LibriSpeech", "Synonym-Of", "Librispeech Automatic Speech Recognition Corpus"], ["WaveNet", "Used-For", "speech recognition"], ["WaveNet", "Trained-With", "LibriSpeech"]], "rel_plus": [["LibriSpeech:Dataset", "Synonym-Of", "Librispeech Automatic Speech Recognition Corpus:Dataset"], ["WaveNet:Method", "Used-For", "speech recognition:Task"], ["WaveNet:Method", "Trained-With", "LibriSpeech:Dataset"]]}
{"doc_id": "202676714", "sentence": "On the other hand , we observed overfitting in the adversarial training on CI - FAR 1 0 , CIFAR 1 0 0 , and SVHN .", "ner": [["adversarial training", "Method"], ["CI - FAR 1 0", "Dataset"], ["CIFAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [["adversarial training", "Trained-With", "CI - FAR 1 0"], ["adversarial training", "Trained-With", "CIFAR 1 0 0"], ["adversarial training", "Trained-With", "SVHN"]], "rel_plus": [["adversarial training:Method", "Trained-With", "CI - FAR 1 0:Dataset"], ["adversarial training:Method", "Trained-With", "CIFAR 1 0 0:Dataset"], ["adversarial training:Method", "Trained-With", "SVHN:Dataset"]]}
{"doc_id": "53719742", "sentence": "To solve this problem , some recent approaches like PixelLink [ 6 ] , FTSN [ 5 ] , and IncepText [ 3 7 ] , propose to formulate text detection as an instance segmentation problem so that both straight text and curved text can be detected in a unified manner .", "ner": [["PixelLink", "Method"], ["FTSN", "Method"], ["IncepText", "Method"], ["text detection", "Task"], ["instance segmentation", "Task"]], "rel": [["IncepText", "Used-For", "text detection"], ["FTSN", "Used-For", "text detection"], ["PixelLink", "Used-For", "text detection"], ["PixelLink", "Used-For", "instance segmentation"], ["FTSN", "Used-For", "instance segmentation"], ["IncepText", "Used-For", "instance segmentation"]], "rel_plus": [["IncepText:Method", "Used-For", "text detection:Task"], ["FTSN:Method", "Used-For", "text detection:Task"], ["PixelLink:Method", "Used-For", "text detection:Task"], ["PixelLink:Method", "Used-For", "instance segmentation:Task"], ["FTSN:Method", "Used-For", "instance segmentation:Task"], ["IncepText:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "208513596", "sentence": "The recent creation of large - scale datasets for action recognition [ 4 , 2 6 , 2 7 ] has undoubtedly enabled a major leap forward in the accuracy of models for video understanding .", "ner": [["action recognition", "Task"], ["video understanding", "Task"]], "rel": [["action recognition", "Used-For", "video understanding"]], "rel_plus": [["action recognition:Task", "Used-For", "video understanding:Task"]]}
{"doc_id": "211020570", "sentence": "We independently trained three models : ST - GAN , stacked hourglass network and face shape dictionary .", "ner": [["ST - GAN", "Method"], ["stacked hourglass network", "Method"], ["face shape dictionary", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "6116678", "sentence": "In addition , we also train a fully convolutional neural network ( FCN ) ( the FCN - 8 s network proposed in [ 3 1 ] ) for comparison .", "ner": [["convolutional neural network", "Method"], ["FCN", "Method"], ["FCN - 8 s", "Method"]], "rel": [["FCN", "Synonym-Of", "convolutional neural network"], ["convolutional neural network", "Compare-With", "FCN - 8 s"]], "rel_plus": [["FCN:Method", "Synonym-Of", "convolutional neural network:Method"], ["convolutional neural network:Method", "Compare-With", "FCN - 8 s:Method"]]}
{"doc_id": "195347056", "sentence": "One of this case can be seen in the experiments when we compare ArtGAN ( baseline ) and ArtGAN - EB in Table I .", "ner": [["ArtGAN", "Method"], ["ArtGAN - EB", "Method"]], "rel": [["ArtGAN", "Compare-With", "ArtGAN - EB"]], "rel_plus": [["ArtGAN:Method", "Compare-With", "ArtGAN - EB:Method"]]}
{"doc_id": "102351044", "sentence": "In this paper , we examine the four structural levels of dropout training mechanisms in a unified convolutional transformation framework , including drop - neuron , drop - channel , drop - path and drop - layer .", "ner": [["dropout training mechanisms", "Method"], ["convolutional transformation framework", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["drop - path", "Method"], ["drop - layer", "Method"]], "rel": [["drop - neuron", "SubClass-Of", "dropout training mechanisms"], ["drop - channel", "SubClass-Of", "dropout training mechanisms"], ["drop - path", "SubClass-Of", "dropout training mechanisms"], ["drop - layer", "SubClass-Of", "dropout training mechanisms"], ["dropout training mechanisms", "Part-Of", "convolutional transformation framework"], ["drop - neuron", "Part-Of", "convolutional transformation framework"], ["drop - channel", "Part-Of", "convolutional transformation framework"], ["drop - path", "Part-Of", "convolutional transformation framework"], ["drop - layer", "Part-Of", "convolutional transformation framework"]], "rel_plus": [["drop - neuron:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - channel:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - path:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["drop - layer:Method", "SubClass-Of", "dropout training mechanisms:Method"], ["dropout training mechanisms:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - neuron:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - channel:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - path:Method", "Part-Of", "convolutional transformation framework:Method"], ["drop - layer:Method", "Part-Of", "convolutional transformation framework:Method"]]}
{"doc_id": "3920676", "sentence": "In this experiment , we fix NFST exp as our metric learning algorithm and rank gallery candidates using all the 1 1 evaluated feature extraction algorithms .", "ner": [["NFST", "Method"], ["metric learning", "Method"], ["feature extraction", "Method"]], "rel": [["NFST", "SubClass-Of", "metric learning"]], "rel_plus": [["NFST:Method", "SubClass-Of", "metric learning:Method"]]}
{"doc_id": "209862890", "sentence": "Next , we introduce the general formulation of entity linking problem with a focus on the well known DeepED model ( Ganea and Hofmann 2 0 1 7 ) .", "ner": [["entity linking", "Task"], ["DeepED", "Method"]], "rel": [["DeepED", "Used-For", "entity linking"]], "rel_plus": [["DeepED:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "209862890", "sentence": "Our contributions can be summarized as follows . \u2022 We show current state - of - the - art ( SOTA ) neural entity linking models based on attention - based bag - of - words context model often produce type errors and analyze the possible causes . \u2022 We propose a novel entity embedding method based on pre - trained BERT to better capture latent entity type information . \u2022 We integrate a BERT - based entity similarity into the local model of a SOTA model ( Ganea and Hofmann 2 0 1 7 ) . \u2022 We verify the effectiveness of our model on standard benchmark datasets and achieve significant improvement over the baseline .", "ner": [["entity embedding", "Task"], ["BERT", "Method"], ["BERT - based entity similarity", "Method"]], "rel": [["BERT", "Used-For", "entity embedding"]], "rel_plus": [["BERT:Method", "Used-For", "entity embedding:Task"]]}
{"doc_id": "24972096", "sentence": "The former is used to evaluate the semantic segmentation on a single frame , while the latter provides raw RGB - D sequences , which can be used for the semantic segmentation evaluation on multiple frames .", "ner": [["semantic segmentation", "Task"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "We conjecture that the main reason behind NSP 's ineffectiveness is its lack of difficulty as a task , as compared to MLM .", "ner": [["NSP", "Task"], ["MLM", "Task"]], "rel": [["NSP", "Compare-With", "MLM"]], "rel_plus": [["NSP:Task", "Compare-With", "MLM:Task"]]}
{"doc_id": "AUG107", "sentence": "Recommendation systems is a task, not a method, improved by GraphSAGE on MovieLens.", "ner": [["recommendation systems", "Task"], ["GraphSAGE", "Method"], ["MovieLens", "Dataset"]], "rel": [["GraphSAGE", "Used-For", "recommendation systems"], ["GraphSAGE", "Trained-With", "MovieLens"]], "rel_plus": [["GraphSAGE:Method", "Used-For", "recommendation systems:Task"], ["GraphSAGE:Method", "Trained-With", "MovieLens:Dataset"]]}
{"doc_id": "202540590", "sentence": "To establish baseline performances on Cosmos QA , we experiment with several state - of - the - art neural architectures for reading comprehension , and also propose a new architecture that improves over the competitive baselines .", "ner": [["Cosmos QA", "Dataset"], ["reading comprehension", "Task"]], "rel": [["Cosmos QA", "Benchmark-For", "reading comprehension"]], "rel_plus": [["Cosmos QA:Dataset", "Benchmark-For", "reading comprehension:Task"]]}
{"doc_id": "210839714", "sentence": "Given c i and A , we can use graph networks or MiME 2 to derive the visit representation v and use it for downstream tasks such as heart failure prediction .", "ner": [["graph networks", "Method"], ["MiME", "Method"], ["heart failure prediction", "Task"]], "rel": [["MiME", "Used-For", "heart failure prediction"], ["graph networks", "Used-For", "heart failure prediction"]], "rel_plus": [["MiME:Method", "Used-For", "heart failure prediction:Task"], ["graph networks:Method", "Used-For", "heart failure prediction:Task"]]}
{"doc_id": "24972096", "sentence": "Finally , we conclude the paper in Section 5 .   To the best of our knowledge , the online dense 3D semantic mapping methods can be further grouped into three main sub - categories : semantic mapping based on 3D template matching [ 2 0 , 2 4 ] , 2D/ 2 . 5 D semantic segmentation [ 2 1 , 2 2 , [ 2 5 ] [ 2 6 ] [ 2 7 ] and RGB - D data association from multiple viewpoints [ 2 3 , 2 8 , 2 9 ] .", "ner": [["online dense 3D semantic mapping methods", "Method"], ["semantic mapping based on 3D template matching", "Method"], ["2D/ 2 . 5 D semantic segmentation", "Method"], ["RGB - D data association from multiple viewpoints", "Method"]], "rel": [["semantic mapping based on 3D template matching", "SubClass-Of", "online dense 3D semantic mapping methods"], ["2D/ 2 . 5 D semantic segmentation", "SubClass-Of", "online dense 3D semantic mapping methods"], ["RGB - D data association from multiple viewpoints", "SubClass-Of", "online dense 3D semantic mapping methods"]], "rel_plus": [["semantic mapping based on 3D template matching:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"], ["2D/ 2 . 5 D semantic segmentation:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"], ["RGB - D data association from multiple viewpoints:Method", "SubClass-Of", "online dense 3D semantic mapping methods:Method"]]}
{"doc_id": "210860962", "sentence": "The goal of this experiment is to assess the image translation quality of StarGAN vs CycleGAN and argue the choice of the image translation backbone in the proposed unsupervised domain adaptation model .", "ner": [["image translation", "Task"], ["StarGAN", "Method"], ["CycleGAN", "Method"], ["image translation", "Task"], ["unsupervised domain adaptation", "Method"]], "rel": [["StarGAN", "Used-For", "image translation"], ["CycleGAN", "Used-For", "image translation"], ["StarGAN", "Compare-With", "CycleGAN"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"], ["CycleGAN:Method", "Used-For", "image translation:Task"], ["StarGAN:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "4539700", "sentence": "In the final experiment , we examined the fine - tuning of Kinetics pretrained 3D CNNs on UCF - 1 0 1 and HMDB - 5 1 .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["UCF - 1 0 1", "Dataset"], ["HMDB - 5 1", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["3D CNNs", "Evaluated-With", "UCF - 1 0 1"], ["3D CNNs", "Evaluated-With", "HMDB - 5 1"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Evaluated-With", "UCF - 1 0 1:Dataset"], ["3D CNNs:Method", "Evaluated-With", "HMDB - 5 1:Dataset"]]}
{"doc_id": "201646309", "sentence": "On seven Semantic Textual Similarity ( STS ) tasks , SBERT achieves an improvement of 1 1 . 7 points compared to InferSent and 5. 5 points compared to Universal Sentence Encoder .", "ner": [["Semantic Textual Similarity", "Task"], ["STS", "Task"], ["SBERT", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder .", "Method"]], "rel": [["STS", "Synonym-Of", "Semantic Textual Similarity"], ["SBERT", "Compare-With", "InferSent"], ["SBERT", "Compare-With", "Universal Sentence Encoder ."]], "rel_plus": [["STS:Task", "Synonym-Of", "Semantic Textual Similarity:Task"], ["SBERT:Method", "Compare-With", "InferSent:Method"], ["SBERT:Method", "Compare-With", "Universal Sentence Encoder .:Method"]]}
{"doc_id": "AUG141", "sentence": "Airline Travel Information System, or ATIS, is improved by BERT intent for intent detection.", "ner": [["Airline Travel Information System", "Dataset"], ["ATIS", "Dataset"], ["BERT intent", "Method"], ["intent detection", "Task"]], "rel": [["ATIS", "Synonym-Of", "Airline Travel Information System"], ["BERT intent", "Used-For", "intent detection"], ["BERT intent", "Trained-With", "ATIS"]], "rel_plus": [["ATIS:Dataset", "Synonym-Of", "Airline Travel Information System:Dataset"], ["BERT intent:Method", "Used-For", "intent detection:Task"], ["BERT intent:Method", "Trained-With", "ATIS:Dataset"]]}
{"doc_id": "199668978", "sentence": "Customer Service Userlog ( CSU ): This anonymized dataset contains online question - answering Userlog from a commercial customer service platform containing 1 million instances in chinese language .", "ner": [["Customer Service Userlog", "Dataset"], ["CSU", "Dataset"], ["online question - answering", "Task"]], "rel": [["CSU", "Synonym-Of", "Customer Service Userlog"], ["Customer Service Userlog", "Benchmark-For", "online question - answering"]], "rel_plus": [["CSU:Dataset", "Synonym-Of", "Customer Service Userlog:Dataset"], ["Customer Service Userlog:Dataset", "Benchmark-For", "online question - answering:Task"]]}
{"doc_id": "204901567", "sentence": "We describe the models that we compare ( \u00a7 3. 1 ) , the experimental setting ( \u00a7 3. 2 ) , and the results on three classification datasets : XNLI ( \u00a7 3. 3 ) , MLDoc ( \u00a7 3. 4 ) and PAWS - X ( \u00a7 3. 5 ) .", "ner": [["classification", "Task"], ["XNLI", "Dataset"], ["MLDoc", "Dataset"], ["PAWS - X", "Dataset"]], "rel": [["XNLI", "Benchmark-For", "classification"], ["MLDoc", "Benchmark-For", "classification"], ["PAWS - X", "Benchmark-For", "classification"]], "rel_plus": [["XNLI:Dataset", "Benchmark-For", "classification:Task"], ["MLDoc:Dataset", "Benchmark-For", "classification:Task"], ["PAWS - X:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "102351044", "sentence": "We attribute the failure of standard dropouts to the increase of variance from random deactivation of the basic components , e.g. neurons in drop - neuron and channels in drop - channel , which conflicts with the essence of the batch normalization layer following each convolutional layer .", "ner": [["dropouts", "Method"], ["drop - neuron", "Method"], ["drop - channel", "Method"], ["batch normalization", "Method"], ["convolutional layer", "Method"]], "rel": [["batch normalization", "Part-Of", "convolutional layer"]], "rel_plus": [["batch normalization:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "208548469", "sentence": "However , when we consider MU and MUA in Table 4 ( R score 2 ) , the non - attention - based model ( MU ) is more robust than the attention - based model ( MUA ) .", "ner": [["MU", "Method"], ["MUA", "Method"], ["MU", "Method"], ["MUA", "Method"]], "rel": [["MU", "Compare-With", "MUA"]], "rel_plus": [["MU:Method", "Compare-With", "MUA:Method"]]}
{"doc_id": "199668978", "sentence": "S 2 S+W&C&B is seq 2 seq model using word - level , char - level , and BERT embedding .", "ner": [["S 2 S+W&C&B", "Method"], ["seq 2 seq", "Method"], ["word - level , char - level , and BERT embedding", "Method"]], "rel": [["word - level , char - level , and BERT embedding", "Part-Of", "S 2 S+W&C&B"], ["S 2 S+W&C&B", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["word - level , char - level , and BERT embedding:Method", "Part-Of", "S 2 S+W&C&B:Method"], ["S 2 S+W&C&B:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "28984897", "sentence": "Table 3 shows the EER for networks without a maxout layer and with a maxout layer with k = 4 , as well as the results for pruning from one up to three neurons from each maxout unit .", "ner": [["maxout layer", "Method"], ["maxout layer", "Method"], ["maxout unit", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "While [ 1 4 9 , 1 5 0 , 1 5 1 , 1 5 2 ] have used semantic segmentation models , Mask R - CNN [ 2 0 ] extends the object detection model Faster R - CNN by adding a binary mask prediction branch for instance segmentation .", "ner": [["semantic segmentation", "Task"], ["Mask R - CNN", "Method"], ["object detection", "Task"], ["Faster R - CNN", "Method"], ["binary mask prediction", "Task"], ["instance segmentation", "Task"]], "rel": [["Mask R - CNN", "Used-For", "semantic segmentation"], ["Faster R - CNN", "Used-For", "object detection"], ["binary mask prediction", "Used-For", "instance segmentation"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "semantic segmentation:Task"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["binary mask prediction:Task", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211020570", "sentence": "It is obvious that affine transformation methods have the same limitations as the conventional face alignment algorithm , regarding sensitivity to occlusion and blur .", "ner": [["affine transformation", "Task"], ["face alignment", "Task"]], "rel": [["affine transformation", "Compare-With", "face alignment"]], "rel_plus": [["affine transformation:Task", "Compare-With", "face alignment:Task"]]}
{"doc_id": "210164920", "sentence": "InstanceFCN , FCIs , MaskLab calculate position - sensitive score maps for instance segmentation .", "ner": [["InstanceFCN", "Method"], ["FCIs", "Method"], ["MaskLab", "Method"], ["instance segmentation", "Task"]], "rel": [["MaskLab", "Used-For", "instance segmentation"], ["FCIs", "Used-For", "instance segmentation"], ["InstanceFCN", "Used-For", "instance segmentation"]], "rel_plus": [["MaskLab:Method", "Used-For", "instance segmentation:Task"], ["FCIs:Method", "Used-For", "instance segmentation:Task"], ["InstanceFCN:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "56657874", "sentence": "We set the dimension of the last fully connected layer ( fc 8) to the number of categories ( for instance , 6 5 for Office - Home dataset ) .", "ner": [["fully connected layer", "Method"], ["fc 8)", "Method"], ["Office - Home", "Dataset"]], "rel": [["fc 8)", "Synonym-Of", "fully connected layer"]], "rel_plus": [["fc 8):Method", "Synonym-Of", "fully connected layer:Method"]]}
{"doc_id": "147703932", "sentence": "We select four common tasks in many recent works : 2D/ 3 D pose estimation , body parts segmentation and body depth estimation .", "ner": [["2D/ 3 D pose estimation", "Task"], ["body parts segmentation", "Task"], ["body depth estimation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Zhong et al. [ 4 0 ] and Liao et al. [ 2 0 ] employed the Faster R - CNN and SSD frameworks to solve the word - level horizontal text detection problem , respectively .", "ner": [["Faster R - CNN", "Method"], ["SSD", "Method"], ["word - level horizontal text detection problem", "Task"]], "rel": [["Faster R - CNN", "Used-For", "word - level horizontal text detection problem"], ["SSD", "Used-For", "word - level horizontal text detection problem"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "word - level horizontal text detection problem:Task"], ["SSD:Method", "Used-For", "word - level horizontal text detection problem:Task"]]}
{"doc_id": "202676714", "sentence": "Even so , Absum is more effective than other standard regularizations since it can efficiently improve robustness against black - box attacks ( SFA , transferred attacks , and High - Frequency attacks ) and enhance adversarial training , as mentioned above .", "ner": [["Absum", "Method"], ["standard regularizations", "Method"], ["robustness against black - box attacks", "Task"], ["SFA", "Task"], ["transferred attacks", "Task"], ["High - Frequency attacks", "Task"], ["adversarial training", "Method"]], "rel": [["Absum", "Compare-With", "standard regularizations"], ["Absum", "Used-For", "robustness against black - box attacks"], ["SFA", "SubTask-Of", "robustness against black - box attacks"], ["transferred attacks", "SubTask-Of", "robustness against black - box attacks"], ["High - Frequency attacks", "SubTask-Of", "robustness against black - box attacks"]], "rel_plus": [["Absum:Method", "Compare-With", "standard regularizations:Method"], ["Absum:Method", "Used-For", "robustness against black - box attacks:Task"], ["SFA:Task", "SubTask-Of", "robustness against black - box attacks:Task"], ["transferred attacks:Task", "SubTask-Of", "robustness against black - box attacks:Task"], ["High - Frequency attacks:Task", "SubTask-Of", "robustness against black - box attacks:Task"]]}
{"doc_id": "209862890", "sentence": "To verify this , we measure the performance of the two typing systems on AIDA - CoNLL development set . 1 3 As shown in Table 5 , the ultra - fine entity typing system ( Choi et al. 2 0 1 8) only achieves 2 6 . 5 2 % F 1 mi score while the ZOE system ( Zhou et al. 2 0 1 8 ) achieves 6 6 . 1 2 % F 1 mi score 1 4 which are insufficient to improve state - of - the - art entity linking system with more than 9 2 % F 1 score .", "ner": [["AIDA - CoNLL", "Dataset"], ["ultra - fine entity typing system", "Method"], ["ZOE", "Method"], ["entity linking", "Task"]], "rel": [["ultra - fine entity typing system", "Compare-With", "ZOE"], ["ZOE", "Used-For", "entity linking"], ["ultra - fine entity typing system", "Used-For", "entity linking"]], "rel_plus": [["ultra - fine entity typing system:Method", "Compare-With", "ZOE:Method"], ["ZOE:Method", "Used-For", "entity linking:Task"], ["ultra - fine entity typing system:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "201646309", "sentence": "Using the described siamese network structure and fine - tuning mechanism substantially improves the correlation , outperforming both InferSent and Universal Sentence Encoder substantially .", "ner": [["siamese network", "Method"], ["InferSent", "Method"], ["Universal Sentence Encoder", "Method"]], "rel": [["siamese network", "Compare-With", "InferSent"], ["siamese network", "Compare-With", "Universal Sentence Encoder"]], "rel_plus": [["siamese network:Method", "Compare-With", "InferSent:Method"], ["siamese network:Method", "Compare-With", "Universal Sentence Encoder:Method"]]}
{"doc_id": "21683040", "sentence": "FSSD means that we train the FSSD model with a pre - trained VGG model .", "ner": [["FSSD", "Method"], ["FSSD", "Method"], ["VGG", "Method"]], "rel": [["VGG", "Part-Of", "FSSD"]], "rel_plus": [["VGG:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "21683040", "sentence": "Even though our FSSD 5 1 2 is slightly lower than DSSD 5 1 3 , it should be noted that FSSD 's mAP on small objects is still higher than DSSD 5 1 3 , which proves that our feature fusion module is more powerful than DSSD 's FPN module on small objects ' detection .", "ner": [["FSSD 5 1 2", "Method"], ["DSSD 5 1 3", "Method"], ["FSSD", "Method"], ["DSSD 5 1 3", "Method"], ["feature fusion module", "Method"], ["DSSD", "Method"], ["FPN", "Method"]], "rel": [["feature fusion module", "Part-Of", "FSSD 5 1 2"], ["FSSD 5 1 2", "Compare-With", "DSSD 5 1 3"], ["FSSD", "Compare-With", "DSSD 5 1 3"], ["FPN", "Part-Of", "DSSD"]], "rel_plus": [["feature fusion module:Method", "Part-Of", "FSSD 5 1 2:Method"], ["FSSD 5 1 2:Method", "Compare-With", "DSSD 5 1 3:Method"], ["FSSD:Method", "Compare-With", "DSSD 5 1 3:Method"], ["FPN:Method", "Part-Of", "DSSD:Method"]]}
{"doc_id": "210839714", "sentence": "Similar to graph reconstruction , Transformer starts with an evenly distributed attentions , and develops its own structure .", "ner": [["graph reconstruction", "Task"], ["Transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Yu et al. [ 1 8 3 ] proposed a hierarchical recurrent neural network ( h - RNN ) that applies the attention mechanisms on both the temporal and spatial aspects .", "ner": [["hierarchical recurrent neural network", "Method"], ["h - RNN", "Method"], ["attention mechanisms", "Method"]], "rel": [["h - RNN", "Synonym-Of", "hierarchical recurrent neural network"], ["attention mechanisms", "Part-Of", "hierarchical recurrent neural network"]], "rel_plus": [["h - RNN:Method", "Synonym-Of", "hierarchical recurrent neural network:Method"], ["attention mechanisms:Method", "Part-Of", "hierarchical recurrent neural network:Method"]]}
{"doc_id": "203593581", "sentence": "Most of the existing quantization approaches ( Cai et al. , 2 0 1 7 ; Gong et al. , 2 0 1 9 ) , use uniform quantization although non - uniform quantization can usually achieve better accuracy ( Zhu et al. , 2 0 1 6 ) .", "ner": [["quantization", "Method"], ["uniform quantization", "Method"], ["non - uniform quantization", "Method"]], "rel": [["uniform quantization", "SubClass-Of", "quantization"], ["non - uniform quantization", "SubClass-Of", "quantization"]], "rel_plus": [["uniform quantization:Method", "SubClass-Of", "quantization:Method"], ["non - uniform quantization:Method", "SubClass-Of", "quantization:Method"]]}
{"doc_id": "211010786", "sentence": "First , we generated X either by drawing the value of each time step from a normal distribution with zero mean and a constant variance ( X ( t ) \u223c N ) or by Auto - Regressive Moving Average model ( ARMA or A. ) with X ( t ) = 0. 9 \u00b5 + 0. 1 X ( t \u2212 1 ) . e rst set we generated was of explicitly related pairs of time series X and Y , where Y emulates X with some time lag \u2206 \u2208 [ 1 , 2 0 ] ( X \u227a Y ) .", "ner": [["Auto - Regressive Moving Average model", "Method"], ["ARMA", "Method"]], "rel": [["ARMA", "Synonym-Of", "Auto - Regressive Moving Average model"]], "rel_plus": [["ARMA:Method", "Synonym-Of", "Auto - Regressive Moving Average model:Method"]]}
{"doc_id": "198231883", "sentence": "This video saliency metric prefers tracks Ours PReMVOS [ 2 1 ] DyeNet [ 1 9 ] FEELVOS [ 3 3 ] OSVOS - S [ 2 3 ] CINM [ 1 ] RGMP [ 3 9 ] OnAVOS [ 3 5 ] VideoMatch [ 1 2 ] OSVOS [ 4 ] FAVOS [ 6 ] SiamMask [ 3 6 ] OSMN [ 4 4 ] Figure 6 .", "ner": [["PReMVOS", "Method"], ["DyeNet", "Method"], ["FEELVOS", "Method"], ["OSVOS - S", "Method"], ["CINM", "Method"], ["RGMP", "Method"], ["OnAVOS", "Method"], ["VideoMatch", "Method"], ["OSVOS", "Method"], ["FAVOS", "Method"], ["SiamMask", "Method"], ["OSMN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "In the predict setting , we use two state - of - the - art fine grained entity typing systems : 1 ) Ultrafine ( Choi et al. 2 0 1 8) which predicts types in ultra - fine type sets ; 2 ) ZOE ( Zhou et al. 2 0 1 8 ) which can predict types in FIGER type sets .", "ner": [["fine grained entity typing systems", "Method"], ["Ultrafine", "Method"], ["ZOE", "Method"], ["FIGER", "Dataset"]], "rel": [["Ultrafine", "SubClass-Of", "fine grained entity typing systems"], ["ZOE", "SubClass-Of", "fine grained entity typing systems"], ["Ultrafine", "Evaluated-With", "FIGER"], ["ZOE", "Evaluated-With", "FIGER"]], "rel_plus": [["Ultrafine:Method", "SubClass-Of", "fine grained entity typing systems:Method"], ["ZOE:Method", "SubClass-Of", "fine grained entity typing systems:Method"], ["Ultrafine:Method", "Evaluated-With", "FIGER:Dataset"], ["ZOE:Method", "Evaluated-With", "FIGER:Dataset"]]}
{"doc_id": "202577400", "sentence": "Fig. 4 shows the mask maps learned in LDGA and GALD , where mask maps learned by GALD are more focused on regions inside large objects then weight global features more in these regions , while mask maps from LDGA have no obvious focus on large objects since the LD module has not accessed to global feature yet .", "ner": [["LDGA", "Method"], ["GALD", "Method"], ["GALD", "Method"], ["LDGA", "Method"], ["LD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "To demonstrate the broad applicability of the proposed SGGAN approach , we also compare it with the leading supervised deep learning approaches on two commonly used datasets for face attribute recognition .", "ner": [["SGGAN", "Method"], ["supervised deep learning", "Method"], ["face attribute recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "supervised deep learning"], ["SGGAN", "Used-For", "face attribute recognition"], ["supervised deep learning", "Used-For", "face attribute recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "supervised deep learning:Method"], ["SGGAN:Method", "Used-For", "face attribute recognition:Task"], ["supervised deep learning:Method", "Used-For", "face attribute recognition:Task"]]}
{"doc_id": "198897554", "sentence": "Given a set of N s labeled training images from the source domain , the segmentation network takes as input the feature maps from E(x s i ) and outputs the segmentation predictions : H \u00d7 W \u00d7 C , where C is the number of semantic categories , H and W are the height and width of the output respectively , and \u03b8 e and \u03b8 s convey the parameters of E and S , respectively .", "ner": [["segmentation network", "Method"], ["segmentation", "Task"]], "rel": [["segmentation network", "Used-For", "segmentation"]], "rel_plus": [["segmentation network:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "4539700", "sentence": "Thus , I 3 D is 6 4 times larger than ResNeXt - 1 0 1 .", "ner": [["I 3 D", "Method"], ["ResNeXt - 1 0 1", "Method"]], "rel": [["I 3 D", "Compare-With", "ResNeXt - 1 0 1"]], "rel_plus": [["I 3 D:Method", "Compare-With", "ResNeXt - 1 0 1:Method"]]}
{"doc_id": "202565512", "sentence": "Because relational GCNs usually over - parameterize the model ( Marcheggiani and Titov 2 0 1 7 ; Zhang , Qi , and Manning 2 0 1 8) , we apply GCNs on the undirected graph .", "ner": [["GCNs", "Method"], ["GCNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Deeper CNNs undoubtedly provides larger representational capacity , while widening each layer can lead to better representation complementarily . [ 2 1 ] replaces the filter kernel of the convolutional transformation with multilayer perceptron .", "ner": [["CNNs", "Method"], ["filter kernel", "Method"], ["convolutional transformation", "Method"], ["multilayer perceptron", "Method"]], "rel": [["convolutional transformation", "Part-Of", "CNNs"], ["multilayer perceptron", "Part-Of", "convolutional transformation"]], "rel_plus": [["convolutional transformation:Method", "Part-Of", "CNNs:Method"], ["multilayer perceptron:Method", "Part-Of", "convolutional transformation:Method"]]}
{"doc_id": "4246700", "sentence": "We use fully connected layers of several CNNs , including AlexNet [ 4 7 ] , VGGNet [ 5 0 ] , GoogLeNet [ 5 1 ] , pre - trained on ImageNet dataset , to extract features of remote sensing images . where I is a remote sensing image , e 0 is the feature of the remote sensing image whose dimension is notated u , and f F R is the feature representations process which the feature can be handcrafted feature or deep feature . 2 ) Representing sentences : In the first method , every word in a sentence is represented by a one - hot K dimension word vector w i , where K is the size of the vocabulary .", "ner": [["fully connected layers", "Method"], ["CNNs", "Method"], ["AlexNet", "Method"], ["VGGNet", "Method"], ["GoogLeNet", "Method"], ["ImageNet", "Dataset"]], "rel": [["AlexNet", "SubClass-Of", "CNNs"], ["VGGNet", "SubClass-Of", "CNNs"], ["GoogLeNet", "SubClass-Of", "CNNs"], ["fully connected layers", "Part-Of", "CNNs"], ["AlexNet", "Trained-With", "ImageNet"], ["VGGNet", "Trained-With", "ImageNet"], ["GoogLeNet", "Trained-With", "ImageNet"], ["CNNs", "Trained-With", "ImageNet"]], "rel_plus": [["AlexNet:Method", "SubClass-Of", "CNNs:Method"], ["VGGNet:Method", "SubClass-Of", "CNNs:Method"], ["GoogLeNet:Method", "SubClass-Of", "CNNs:Method"], ["fully connected layers:Method", "Part-Of", "CNNs:Method"], ["AlexNet:Method", "Trained-With", "ImageNet:Dataset"], ["VGGNet:Method", "Trained-With", "ImageNet:Dataset"], ["GoogLeNet:Method", "Trained-With", "ImageNet:Dataset"], ["CNNs:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "202734316", "sentence": "Nevertheless , besides the inherent dataset bias [ 4 4 ] , such automatic data generation methods suffer from the synthetic - to - real ( S 2 R ) domain discrepancy [ 4 9 ] that deteriorates the performance of models trained using rendered content when applied to real world acquired inputs .", "ner": [["synthetic - to - real", "Task"], ["S 2 R", "Task"]], "rel": [["S 2 R", "Synonym-Of", "synthetic - to - real"]], "rel_plus": [["S 2 R:Task", "Synonym-Of", "synthetic - to - real:Task"]]}
{"doc_id": "202888751", "sentence": "WGAN proposes to use the Wasserstein distance to measure the similarity between true and the learned data distribution , instead of using Jensen - Shannon divergence as in the original GAN model ( Goodfellow et al. , 2 0 1 4 ) .", "ner": [["WGAN", "Method"], ["Wasserstein distance", "Method"], ["measure the similarity", "Task"], ["Jensen - Shannon divergence", "Method"], ["GAN", "Method"]], "rel": [["Wasserstein distance", "Part-Of", "WGAN"], ["Wasserstein distance", "Used-For", "measure the similarity"], ["Jensen - Shannon divergence", "Part-Of", "GAN"]], "rel_plus": [["Wasserstein distance:Method", "Part-Of", "WGAN:Method"], ["Wasserstein distance:Method", "Used-For", "measure the similarity:Task"], ["Jensen - Shannon divergence:Method", "Part-Of", "GAN:Method"]]}
{"doc_id": "209532167", "sentence": "Sequence modeling on source code has been shown to be successful in a variety of software - engineering tasks , such as code completion ( Hindle et al. , 2 0 1 2 ; Raychev et al. , 2 0 1 4 ) , source code to pseudocode mapping ( Oda et al. , 2 0 1 5 ) , API - sequence prediction ( Gu et al. , 2 0 1 6 ) , program repair ( Pu et al. , 2 0 1 6 ; Gupta et al. , 2 0 1 7 ) , and natural language to code mapping ( Iyer et al. , 2 0 1 8) , among others .", "ner": [["Sequence modeling", "Method"], ["code completion", "Task"], ["source code to pseudocode mapping", "Task"], ["API - sequence prediction", "Task"], ["program repair", "Task"], ["natural language to code mapping", "Task"]], "rel": [["Sequence modeling", "Used-For", "code completion"], ["Sequence modeling", "Used-For", "source code to pseudocode mapping"], ["Sequence modeling", "Used-For", "API - sequence prediction"], ["Sequence modeling", "Used-For", "program repair"], ["Sequence modeling", "Used-For", "natural language to code mapping"]], "rel_plus": [["Sequence modeling:Method", "Used-For", "code completion:Task"], ["Sequence modeling:Method", "Used-For", "source code to pseudocode mapping:Task"], ["Sequence modeling:Method", "Used-For", "API - sequence prediction:Task"], ["Sequence modeling:Method", "Used-For", "program repair:Task"], ["Sequence modeling:Method", "Used-For", "natural language to code mapping:Task"]]}
{"doc_id": "102351044", "sentence": "The building blocks adopted for WRN , ResNeXt and DenseNet are basic block ( Block ) with two consecutive 3 \u00d7 3 conv , bottleneck block ( B - Block ) proposed in 3. 2 . 4 and DenseNet bottleneck block ( D - Block ) [ 1 3 ] with dimensionality reduction of 1 \u00d7 1 conv and a following transformation of 3 \u00d7 3 conv .", "ner": [["WRN", "Method"], ["ResNeXt", "Method"], ["DenseNet", "Method"], ["basic block", "Method"], ["Block", "Method"], ["3 \u00d7 3 conv", "Method"], ["bottleneck block", "Method"], ["B - Block", "Method"], ["DenseNet bottleneck block", "Method"], ["D - Block", "Method"], ["dimensionality reduction of 1 \u00d7 1 conv", "Method"], ["transformation of 3 \u00d7 3 conv", "Method"]], "rel": [["basic block", "Part-Of", "WRN"], ["DenseNet bottleneck block", "Part-Of", "WRN"], ["basic block", "Part-Of", "ResNeXt"], ["DenseNet bottleneck block", "Part-Of", "ResNeXt"], ["basic block", "Part-Of", "DenseNet"], ["DenseNet bottleneck block", "Part-Of", "DenseNet"], ["Block", "Synonym-Of", "basic block"], ["3 \u00d7 3 conv", "Part-Of", "basic block"], ["bottleneck block", "Part-Of", "basic block"], ["B - Block", "Synonym-Of", "bottleneck block"], ["D - Block", "Synonym-Of", "DenseNet bottleneck block"], ["transformation of 3 \u00d7 3 conv", "Part-Of", "DenseNet bottleneck block"], ["dimensionality reduction of 1 \u00d7 1 conv", "Part-Of", "DenseNet bottleneck block"]], "rel_plus": [["basic block:Method", "Part-Of", "WRN:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "WRN:Method"], ["basic block:Method", "Part-Of", "ResNeXt:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "ResNeXt:Method"], ["basic block:Method", "Part-Of", "DenseNet:Method"], ["DenseNet bottleneck block:Method", "Part-Of", "DenseNet:Method"], ["Block:Method", "Synonym-Of", "basic block:Method"], ["3 \u00d7 3 conv:Method", "Part-Of", "basic block:Method"], ["bottleneck block:Method", "Part-Of", "basic block:Method"], ["B - Block:Method", "Synonym-Of", "bottleneck block:Method"], ["D - Block:Method", "Synonym-Of", "DenseNet bottleneck block:Method"], ["transformation of 3 \u00d7 3 conv:Method", "Part-Of", "DenseNet bottleneck block:Method"], ["dimensionality reduction of 1 \u00d7 1 conv:Method", "Part-Of", "DenseNet bottleneck block:Method"]]}
{"doc_id": "53719742", "sentence": "Gupta et al. [ 1 0 ] borrowed the YOLO framework and employed a fullyconvolutional regression network to perform text detection and bounding box regression at all locations and multiple scales of an image .", "ner": [["YOLO", "Method"], ["fullyconvolutional regression network", "Method"], ["text detection", "Task"], ["bounding box regression", "Task"]], "rel": [["fullyconvolutional regression network", "Used-For", "text detection"], ["fullyconvolutional regression network", "Used-For", "bounding box regression"]], "rel_plus": [["fullyconvolutional regression network:Method", "Used-For", "text detection:Task"], ["fullyconvolutional regression network:Method", "Used-For", "bounding box regression:Task"]]}
{"doc_id": "102351044", "sentence": "Based on our observation and empirical evaluation , we propose general convolutional building blocks for the training of deep Each convolutional layer of state - of - the - art CNN models is typically coupled with a batch normalization layer ( BN ) [ 1 5 ] to normalize inputs batch - wisely , which stabilizes mean and variance of the input channels X received by each output channel yi .", "ner": [["convolutional building blocks", "Method"], ["convolutional layer", "Method"], ["CNN", "Method"], ["batch normalization", "Method"], ["BN", "Method"]], "rel": [["convolutional building blocks", "Part-Of", "convolutional layer"], ["convolutional layer", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "CNN"], ["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["convolutional building blocks:Method", "Part-Of", "convolutional layer:Method"], ["convolutional layer:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "CNN:Method"], ["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "210164920", "sentence": "Major changes in FCN which helped the model to achieve state of the art result are the base model VGG 1 6 , bipolar interpolation technique for up - sampling the final feature map and skip connection for combining low layer and high layer features in the final layer for fine - grained semantic segmentation .", "ner": [["FCN", "Method"], ["VGG 1 6", "Method"], ["bipolar interpolation", "Method"], ["skip connection", "Method"], ["fine - grained semantic segmentation", "Task"]], "rel": [["VGG 1 6", "Part-Of", "FCN"], ["bipolar interpolation", "Part-Of", "FCN"], ["skip connection", "Part-Of", "FCN"], ["FCN", "Used-For", "fine - grained semantic segmentation"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FCN:Method"], ["bipolar interpolation:Method", "Part-Of", "FCN:Method"], ["skip connection:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "fine - grained semantic segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "We use the SGD optimizer with the base learning rate of 0.0 1 , the momentum of 0. 9 and the weight decay of 0.0 0 0 5 .", "ner": [["SGD optimizer", "Method"], ["momentum", "Method"], ["weight decay", "Method"]], "rel": [["momentum", "Part-Of", "SGD optimizer"], ["weight decay", "Part-Of", "SGD optimizer"]], "rel_plus": [["momentum:Method", "Part-Of", "SGD optimizer:Method"], ["weight decay:Method", "Part-Of", "SGD optimizer:Method"]]}
{"doc_id": "202577400", "sentence": "GALD is evaluated for object detection on Pascal VOC based on Faster R - CNN , and for both object detection and instance segmentation on MS COCO based on Mask R - CNN .", "ner": [["GALD", "Method"], ["object detection", "Task"], ["Pascal VOC", "Dataset"], ["Faster R - CNN", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"], ["MS COCO", "Dataset"], ["Mask R - CNN", "Method"]], "rel": [["Pascal VOC", "Benchmark-For", "object detection"], ["Faster R - CNN", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["Faster R - CNN", "Evaluated-With", "Pascal VOC"], ["GALD", "Evaluated-With", "Pascal VOC"], ["GALD", "Part-Of", "Faster R - CNN"], ["MS COCO", "Benchmark-For", "object detection"], ["Mask R - CNN", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["MS COCO", "Benchmark-For", "instance segmentation"], ["Mask R - CNN", "Used-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"], ["Mask R - CNN", "Evaluated-With", "MS COCO"], ["GALD", "Evaluated-With", "MS COCO"], ["GALD", "Part-Of", "Mask R - CNN"]], "rel_plus": [["Pascal VOC:Dataset", "Benchmark-For", "object detection:Task"], ["Faster R - CNN:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["Faster R - CNN:Method", "Evaluated-With", "Pascal VOC:Dataset"], ["GALD:Method", "Evaluated-With", "Pascal VOC:Dataset"], ["GALD:Method", "Part-Of", "Faster R - CNN:Method"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["Mask R - CNN:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"], ["Mask R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Evaluated-With", "MS COCO:Dataset"], ["GALD:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "150374036", "sentence": "From the test set of FERPlus [ 5 ] , the validation set of AffectNet [ 4 8 ] , and the test set of RAF - DB [ 3 4 ] , we collect the Occlusion - FERPlus , Pose - FERPlus , Occlusion - AffectNet , Pose - AffectNet , Occlusion - RAF - DB , and Pose - RAF - DB for testing .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["Occlusion - FERPlus", "Dataset"], ["Pose - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Pose - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"], ["Pose - RAF - DB", "Dataset"]], "rel": [["Occlusion - FERPlus", "SubClass-Of", "FERPlus"], ["Pose - FERPlus", "SubClass-Of", "FERPlus"], ["Occlusion - AffectNet", "SubClass-Of", "AffectNet"], ["Pose - AffectNet", "SubClass-Of", "AffectNet"], ["Occlusion - RAF - DB", "SubClass-Of", "RAF - DB"], ["Pose - RAF - DB", "SubClass-Of", "RAF - DB"]], "rel_plus": [["Occlusion - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Pose - FERPlus:Dataset", "SubClass-Of", "FERPlus:Dataset"], ["Occlusion - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"], ["Pose - AffectNet:Dataset", "SubClass-Of", "AffectNet:Dataset"], ["Occlusion - RAF - DB:Dataset", "SubClass-Of", "RAF - DB:Dataset"], ["Pose - RAF - DB:Dataset", "SubClass-Of", "RAF - DB:Dataset"]]}
{"doc_id": "53719742", "sentence": "The weights of ResNet 5 0 or ResNeXt 5 0 related layers in the PAN backbone network are initialized by using the corresponding pre - trained models from the ImageNet classification task [ 1 2 , 3 6 ] .", "ner": [["ResNet 5 0", "Method"], ["ResNeXt 5 0", "Method"], ["PAN", "Method"], ["ImageNet", "Dataset"], ["classification", "Task"]], "rel": [["ResNeXt 5 0", "Part-Of", "PAN"], ["ResNet 5 0", "Part-Of", "PAN"], ["ResNeXt 5 0", "Trained-With", "ImageNet"], ["ResNet 5 0", "Trained-With", "ImageNet"], ["ImageNet", "Benchmark-For", "classification"]], "rel_plus": [["ResNeXt 5 0:Method", "Part-Of", "PAN:Method"], ["ResNet 5 0:Method", "Part-Of", "PAN:Method"], ["ResNeXt 5 0:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet 5 0:Method", "Trained-With", "ImageNet:Dataset"], ["ImageNet:Dataset", "Benchmark-For", "classification:Task"]]}
{"doc_id": "153312532", "sentence": "There are some experimental findings : 1 ) The top layer of BERT is more useful for text classification ; 2 ) With an appropriate layer - wise decreasing learning rate , BERT can overcome the catastrophic forgetting problem ; 3 ) Within - task and in - domain further pre - training can significantly boost its performance ; 4 ) A preceding multi - task fine - tuning is also helpful to the single - task fine - tuning , but its benefit is smaller than further pre - training ; 5 ) BERT can improve the task with small - size data .", "ner": [["BERT", "Method"], ["text classification", "Task"], ["BERT", "Method"], ["multi - task fine - tuning", "Method"], ["single - task fine - tuning", "Method"], ["pre - training", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "207880647", "sentence": "In a similar work , ( Tang et al. , 2 0 1 7 a ) consider learning question generation and question answering as a dual task , but learning them jointly using the entire dataset .", "ner": [["question generation", "Task"], ["question answering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202540590", "sentence": "For AMT based human evaluation , we randomly sample 2 0 0 paragraphs and questions , and for each question we randomly sample 4 automatically generated answers from the outputs of GPT 2 without fine - tuning and GPT 2 - FT .", "ner": [["AMT based human evaluation", "Dataset"], ["GPT 2", "Method"], ["GPT 2 - FT", "Method"]], "rel": [["GPT 2", "Evaluated-With", "AMT based human evaluation"], ["GPT 2 - FT", "Evaluated-With", "AMT based human evaluation"]], "rel_plus": [["GPT 2:Method", "Evaluated-With", "AMT based human evaluation:Dataset"], ["GPT 2 - FT:Method", "Evaluated-With", "AMT based human evaluation:Dataset"]]}
{"doc_id": "199543700", "sentence": "The proposed SGGAN model , the pre - trained VGG - 1 6 and Resnet - 5 0 networks are all fine - tuned with different numbers ( i.e. , 8 0 0 , 1 6 0 0 , 3 2 0 0 , 4 8 0 0 , 6 4 0 0 , 7 2 0 0 ) of labeled images in the CelebA dataset [ 4 0 ] with \" gender \" attribute in the comparison experiments .", "ner": [["SGGAN", "Method"], ["VGG - 1 6", "Method"], ["Resnet - 5 0", "Method"], ["CelebA", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "CelebA"], ["Resnet - 5 0", "Trained-With", "CelebA"], ["SGGAN", "Trained-With", "CelebA"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "CelebA:Dataset"], ["Resnet - 5 0:Method", "Trained-With", "CelebA:Dataset"], ["SGGAN:Method", "Trained-With", "CelebA:Dataset"]]}
{"doc_id": "54447105", "sentence": "Convolutional Neural Networks ( CNNs ) [ 3 4 , 2 6 ] has become the default method for any computer vision task , and is widely used for problems such as image classification , semantic segmentation or visual relationship detection .", "ner": [["Convolutional Neural Networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["visual relationship detection", "Task"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural Networks"], ["Convolutional Neural Networks", "Used-For", "computer vision"], ["Convolutional Neural Networks", "Used-For", "image classification"], ["Convolutional Neural Networks", "Used-For", "semantic segmentation"], ["Convolutional Neural Networks", "Used-For", "visual relationship detection"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural Networks:Method"], ["Convolutional Neural Networks:Method", "Used-For", "computer vision:Task"], ["Convolutional Neural Networks:Method", "Used-For", "image classification:Task"], ["Convolutional Neural Networks:Method", "Used-For", "semantic segmentation:Task"], ["Convolutional Neural Networks:Method", "Used-For", "visual relationship detection:Task"]]}
{"doc_id": "202577400", "sentence": "LD is a simple and universal module , and can be combined with existing GA modules to form different GALD modules for various detection and segmentation tasks .", "ner": [["LD", "Method"], ["GA", "Method"], ["GALD", "Method"], ["detection", "Task"], ["segmentation", "Task"]], "rel": [["GA", "Part-Of", "GALD"], ["LD", "Part-Of", "GALD"], ["GALD", "Used-For", "detection"], ["GALD", "Used-For", "segmentation"]], "rel_plus": [["GA:Method", "Part-Of", "GALD:Method"], ["LD:Method", "Part-Of", "GALD:Method"], ["GALD:Method", "Used-For", "detection:Task"], ["GALD:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "198231883", "sentence": "This penalizes large temporal gaps between tracklets , making it more likely that objects undergoing short occlusion are correctly tracked , and ensures that the most salient objects are grouped consistently throughout the video , as objects to be Ours VSD [ 4 5 ] KIS [ 7 ] RVOS [ 3 2 ] U 1 7 tracked in UVOS are present in mostly all frames .", "ner": [["VSD", "Method"], ["KIS", "Method"], ["RVOS", "Method"], ["UVOS", "Task"]], "rel": [["VSD", "Used-For", "UVOS"], ["KIS", "Used-For", "UVOS"], ["RVOS", "Used-For", "UVOS"]], "rel_plus": [["VSD:Method", "Used-For", "UVOS:Task"], ["KIS:Method", "Used-For", "UVOS:Task"], ["RVOS:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "202676714", "sentence": "Therefore , SNC biases CNNs towards low - frequency components of image data .", "ner": [["SNC", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "Interest in closing the generalization difference between adaptive methods and CM led to AdamW ( Loshchilov & Hutter , 2 0 1 7 ) , by fixing the weight decay of Adam , and Padam ( Chen & Gu , 2 0 1 8) , by lowering the exponent of the second moment .", "ner": [["CM", "Method"], ["AdamW", "Method"], ["weight decay", "Method"], ["Adam", "Method"], ["Padam", "Method"]], "rel": [["weight decay", "Part-Of", "Adam"]], "rel_plus": [["weight decay:Method", "Part-Of", "Adam:Method"]]}
{"doc_id": "202676714", "sentence": "However , when we use adversarial training , Absum improves robustness against PGD , the highest among regularization methods on almost all datasets .", "ner": [["adversarial training", "Method"], ["Absum", "Method"], ["robustness against PGD", "Task"]], "rel": [["Absum", "Used-For", "robustness against PGD"]], "rel_plus": [["Absum:Method", "Used-For", "robustness against PGD:Task"]]}
{"doc_id": "51559", "sentence": "Indeed , there are diminishing returns from further optimization of the QRNN itself as the softmax and optimization overhead take equal or greater time .", "ner": [["QRNN", "Method"], ["softmax", "Method"]], "rel": [["softmax", "Part-Of", "QRNN"]], "rel_plus": [["softmax:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "211020570", "sentence": "For the Common Subset and Fullset of 3 0 0 - W , comparable inter - ocular NRMSE values are obtained by the 3DDE using a UNet - based network and MSM using two stacked hourglass modules in which MSM obtained slightly higher and slightly lower NRMSE values respectively in the [ 5 9 ] .", "ner": [["3 0 0 - W", "Dataset"], ["3DDE", "Method"], ["UNet - based", "Method"], ["MSM", "Method"], ["hourglass modules", "Method"], ["MSM", "Method"]], "rel": [["3DDE", "Evaluated-With", "3 0 0 - W"], ["MSM", "Evaluated-With", "3 0 0 - W"], ["UNet - based", "Part-Of", "3DDE"], ["hourglass modules", "Part-Of", "MSM"]], "rel_plus": [["3DDE:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["MSM:Method", "Evaluated-With", "3 0 0 - W:Dataset"], ["UNet - based:Method", "Part-Of", "3DDE:Method"], ["hourglass modules:Method", "Part-Of", "MSM:Method"]]}
{"doc_id": "198231883", "sentence": "In this section we detail the specifics of our novel Un - OVOST algorithm for tackling the UVOS task .", "ner": [["Un - OVOST", "Method"], ["UVOS", "Task"]], "rel": [["Un - OVOST", "Used-For", "UVOS"]], "rel_plus": [["Un - OVOST:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "24972096", "sentence": "The improved version PointNet++ [ 1 9 ] is a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set , which can learn local features with increasing contextual scales .", "ner": [["PointNet++", "Method"], ["hierarchical neural network", "Method"], ["PointNet", "Method"]], "rel": [["PointNet", "Part-Of", "PointNet++"], ["PointNet++", "SubClass-Of", "hierarchical neural network"]], "rel_plus": [["PointNet:Method", "Part-Of", "PointNet++:Method"], ["PointNet++:Method", "SubClass-Of", "hierarchical neural network:Method"]]}
{"doc_id": "210839714", "sentence": "To test GCT on real - world EHR records , we use Philips eICU Collaborative Research Dataset 5 [ 2 5 ] . eICU consists of Intensive Care Unit ( ICU ) records filtered for remote caregivers , collected from multiple sites in the United States between 2 0 1 4 and 2 0 1 5 .", "ner": [["GCT", "Method"], ["Philips eICU Collaborative Research", "Dataset"], ["eICU", "Dataset"]], "rel": [["GCT", "Evaluated-With", "Philips eICU Collaborative Research"]], "rel_plus": [["GCT:Method", "Evaluated-With", "Philips eICU Collaborative Research:Dataset"]]}
{"doc_id": "22825560", "sentence": "When we compare other state of the art domain adaptation methods , like RTN , CORAL and JAN , it is easy to observe that they have better results than networks that are trained on the ImageNet , e.g. AlexNet , ResNet and DenseNet .", "ner": [["domain adaptation", "Method"], ["RTN", "Method"], ["CORAL", "Method"], ["JAN", "Method"], ["ImageNet", "Dataset"], ["AlexNet", "Method"], ["ResNet", "Method"], ["DenseNet", "Method"]], "rel": [["RTN", "SubClass-Of", "domain adaptation"], ["CORAL", "SubClass-Of", "domain adaptation"], ["JAN", "SubClass-Of", "domain adaptation"], ["AlexNet", "Trained-With", "ImageNet"], ["ResNet", "Trained-With", "ImageNet"], ["DenseNet", "Trained-With", "ImageNet"], ["RTN", "Compare-With", "AlexNet"], ["CORAL", "Compare-With", "AlexNet"], ["JAN", "Compare-With", "AlexNet"], ["RTN", "Compare-With", "ResNet"], ["CORAL", "Compare-With", "ResNet"], ["JAN", "Compare-With", "ResNet"], ["RTN", "Compare-With", "DenseNet"], ["CORAL", "Compare-With", "DenseNet"], ["JAN", "Compare-With", "DenseNet"]], "rel_plus": [["RTN:Method", "SubClass-Of", "domain adaptation:Method"], ["CORAL:Method", "SubClass-Of", "domain adaptation:Method"], ["JAN:Method", "SubClass-Of", "domain adaptation:Method"], ["AlexNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNet:Method", "Trained-With", "ImageNet:Dataset"], ["RTN:Method", "Compare-With", "AlexNet:Method"], ["CORAL:Method", "Compare-With", "AlexNet:Method"], ["JAN:Method", "Compare-With", "AlexNet:Method"], ["RTN:Method", "Compare-With", "ResNet:Method"], ["CORAL:Method", "Compare-With", "ResNet:Method"], ["JAN:Method", "Compare-With", "ResNet:Method"], ["RTN:Method", "Compare-With", "DenseNet:Method"], ["CORAL:Method", "Compare-With", "DenseNet:Method"], ["JAN:Method", "Compare-With", "DenseNet:Method"]]}
{"doc_id": "202888751", "sentence": "We aim to learn more discriminative embeddings with the \" training data \" containing both \" real data \" and \" generated data \" . \u2022 We propose a simple pipeline for synthetic augmentation of plant disease datasets using AR - GAN to improve the plant disease recognition performance in a data deficient environment . \u2022 We introduce our limited dataset of tomato plant disease images to validate the effectiveness of our pipeline to prove or disprove the hypothesis : ( i ) Does synthetic augmentation improve the performance of a deep convolutional neural network for plant disease recognition ? ( ii ) How does synthetic augmentation compare to classic augmentation in terms of performance in a plant disease recognition system ?   The biggest limitation with machine learning algorithms is that they require huge amounts of training data before they become effective .", "ner": [["synthetic augmentation", "Method"], ["AR - GAN", "Method"], ["plant disease recognition", "Task"], ["synthetic augmentation", "Method"], ["convolutional neural network", "Method"], ["plant disease recognition", "Task"], ["synthetic augmentation", "Method"], ["classic augmentation", "Method"], ["plant disease recognition", "Task"], ["machine learning", "Method"]], "rel": [["AR - GAN", "Used-For", "synthetic augmentation"], ["AR - GAN", "Used-For", "plant disease recognition"], ["convolutional neural network", "Used-For", "plant disease recognition"], ["synthetic augmentation", "Compare-With", "classic augmentation"], ["synthetic augmentation", "Used-For", "plant disease recognition"], ["classic augmentation", "Used-For", "plant disease recognition"]], "rel_plus": [["AR - GAN:Method", "Used-For", "synthetic augmentation:Method"], ["AR - GAN:Method", "Used-For", "plant disease recognition:Task"], ["convolutional neural network:Method", "Used-For", "plant disease recognition:Task"], ["synthetic augmentation:Method", "Compare-With", "classic augmentation:Method"], ["synthetic augmentation:Method", "Used-For", "plant disease recognition:Task"], ["classic augmentation:Method", "Used-For", "plant disease recognition:Task"]]}
{"doc_id": "210920315", "sentence": "The proposed PSC - Net achieves state - of - the - art detection performance on both .", "ner": [["PSC - Net", "Method"], ["detection", "Task"]], "rel": [["PSC - Net", "Used-For", "detection"]], "rel_plus": [["PSC - Net:Method", "Used-For", "detection:Task"]]}
{"doc_id": "AUG034", "sentence": "FastText improves word embeddings for named entity recognition on CoNLL03, not just a feature.", "ner": [["FastText", "Method"], ["named entity recognition", "Task"], ["CoNLL03", "Dataset"]], "rel": [["FastText", "Used-For", "named entity recognition"], ["FastText", "Evaluated-With", "CoNLL03"]], "rel_plus": [["FastText:Method", "Used-For", "named entity recognition:Task"], ["FastText:Method", "Evaluated-With", "CoNLL03:Dataset"]]}
{"doc_id": "198231883", "sentence": "This mapping results in 3 1 0 of the 1 0 0 0 INet classes being mapped to our 4 0 YT - VIS classes , with 1 2 3 INet classes being mapped to dog and 2 0 to truck .", "ner": [["INet", "Dataset"], ["YT - VIS", "Dataset"], ["INet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "Before we can quantify this advantage , we need to introduce our experimental setup in more detail .   To keep the comparison as meaningful as possible , we follow the BERT setup in using the BOOKCORPUS and English Wikipedia for pretraining baseline models .", "ner": [["BERT", "Method"], ["BOOKCORPUS", "Dataset"], ["English Wikipedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "Information - theoretic metric learning ( ITML ) [ 5 2 ] , large - margin nearest neighbor ( LMNN ) [ 5 5 ] , relative distance comparison ( PRDC ) [ 6 ] , keep - it - simple - and - straightforward metric ( KISSME ) [ 7 ] , and pairwise constrained component analysis ( PCCA ) [ 8 ] all learn Mahalanobis - type distance functions using variants of the basic pairwise constraints principle . kPCCA [ 8 ] , kLFDA [ 1 3 ] , and kMFA [ 1 3 ] kernelize PCCA , LFDA , and MFA , [ 5 6 ] adopts canonical correlation analysis to map the kernelized features into a common subspace .", "ner": [["Information - theoretic metric learning", "Method"], ["ITML", "Method"], ["large - margin nearest neighbor", "Method"], ["LMNN", "Method"], ["relative distance comparison", "Method"], ["PRDC", "Method"], ["keep - it - simple - and - straightforward metric", "Method"], ["KISSME", "Method"], ["pairwise constrained component analysis", "Method"], ["PCCA", "Method"], ["Mahalanobis - type distance functions", "Method"], ["kPCCA", "Method"], ["kLFDA", "Method"], ["kMFA", "Method"], ["PCCA", "Method"], ["LFDA", "Method"], ["MFA", "Method"]], "rel": [["ITML", "Synonym-Of", "Information - theoretic metric learning"], ["LMNN", "Synonym-Of", "large - margin nearest neighbor"], ["PRDC", "Synonym-Of", "relative distance comparison"], ["KISSME", "Synonym-Of", "keep - it - simple - and - straightforward metric"], ["PCCA", "Synonym-Of", "pairwise constrained component analysis"]], "rel_plus": [["ITML:Method", "Synonym-Of", "Information - theoretic metric learning:Method"], ["LMNN:Method", "Synonym-Of", "large - margin nearest neighbor:Method"], ["PRDC:Method", "Synonym-Of", "relative distance comparison:Method"], ["KISSME:Method", "Synonym-Of", "keep - it - simple - and - straightforward metric:Method"], ["PCCA:Method", "Synonym-Of", "pairwise constrained component analysis:Method"]]}
{"doc_id": "202565512", "sentence": "The SWAG dataset has been well solved by pre - trained language models like BERT ( Devlin et al. 2 0 1 9 ) .", "ner": [["SWAG", "Dataset"], ["BERT", "Method"]], "rel": [["SWAG", "Used-For", "BERT"]], "rel_plus": [["SWAG:Dataset", "Used-For", "BERT:Method"]]}
{"doc_id": "211004033", "sentence": "Overall , the error rates can be sorted in the following order : DeepParts+ \u2248 SENet+ > Ellipse R - CNN*. The reason is that the DeepParts+ is limited by its fixed number of occlusion patterns to learn , while the SENet+ learns a continuous attention vector to adjust feature weights but lacks the whole ellipse information to generalize different occlusions .", "ner": [["DeepParts+", "Method"], ["SENet+", "Method"], ["Ellipse R - CNN*.", "Method"], ["DeepParts+", "Method"], ["SENet+", "Method"]], "rel": [["DeepParts+", "Compare-With", "SENet+"], ["DeepParts+", "Compare-With", "Ellipse R - CNN*."], ["DeepParts+", "Compare-With", "SENet+"]], "rel_plus": [["DeepParts+:Method", "Compare-With", "SENet+:Method"], ["DeepParts+:Method", "Compare-With", "Ellipse R - CNN*.:Method"], ["DeepParts+:Method", "Compare-With", "SENet+:Method"]]}
{"doc_id": "198231883", "sentence": "UnOVOST even performs competitively with many semi - supervised video object segmentation algorithms even though it is not given any input as to which objects should be tracked and segmented .", "ner": [["UnOVOST", "Method"], ["semi - supervised video object segmentation", "Task"]], "rel": [["UnOVOST", "Used-For", "semi - supervised video object segmentation"]], "rel_plus": [["UnOVOST:Method", "Used-For", "semi - supervised video object segmentation:Task"]]}
{"doc_id": "210920315", "sentence": "P EDESTRIAN detection is a challenging problem in computer vision with various real - application applications , e.g. , robotics , autonomous driving and visual surveillance .", "ner": [["P EDESTRIAN detection", "Task"], ["computer vision", "Task"], ["robotics", "Task"], ["autonomous driving", "Task"], ["visual surveillance", "Task"]], "rel": [["P EDESTRIAN detection", "SubTask-Of", "computer vision"], ["robotics", "SubTask-Of", "computer vision"], ["autonomous driving", "SubTask-Of", "computer vision"], ["visual surveillance", "SubTask-Of", "computer vision"]], "rel_plus": [["P EDESTRIAN detection:Task", "SubTask-Of", "computer vision:Task"], ["robotics:Task", "SubTask-Of", "computer vision:Task"], ["autonomous driving:Task", "SubTask-Of", "computer vision:Task"], ["visual surveillance:Task", "SubTask-Of", "computer vision:Task"]]}
{"doc_id": "52009210", "sentence": "These results are consistent with the findings of Mohammed et al. ( 2 0 1 8) : the BiLSTM achieves a higher F 1 score than the CRF , which translates into higher recall in entity linking ( both R@ 1 and R@ 5 ) .", "ner": [["BiLSTM", "Method"], ["CRF", "Method"], ["entity linking", "Task"]], "rel": [["BiLSTM", "Compare-With", "CRF"], ["CRF", "Used-For", "entity linking"], ["BiLSTM", "Used-For", "entity linking"]], "rel_plus": [["BiLSTM:Method", "Compare-With", "CRF:Method"], ["CRF:Method", "Used-For", "entity linking:Task"], ["BiLSTM:Method", "Used-For", "entity linking:Task"]]}
{"doc_id": "52910494", "sentence": "This leads to the recent resurgence of exploration in sophisticated CNNs architectures [ 9 ] with hugely increased classification accuracy on ImageNet [ 1 ] , e.g. from AlexNet [ 1 4 ] to GoogLeNets [ 2 7 ] , and ResNets [ 6 ] to DenseNets [ 9 ] .", "ner": [["CNNs", "Method"], ["classification", "Task"], ["ImageNet", "Dataset"], ["AlexNet", "Method"], ["GoogLeNets", "Method"], ["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [["ResNets", "SubClass-Of", "CNNs"], ["GoogLeNets", "SubClass-Of", "CNNs"], ["AlexNet", "SubClass-Of", "CNNs"], ["DenseNets", "SubClass-Of", "CNNs"], ["CNNs", "Used-For", "classification"], ["ImageNet", "Benchmark-For", "classification"], ["DenseNets", "Used-For", "classification"], ["ResNets", "Used-For", "classification"], ["GoogLeNets", "Used-For", "classification"], ["AlexNet", "Used-For", "classification"], ["CNNs", "Evaluated-With", "ImageNet"], ["AlexNet", "Evaluated-With", "ImageNet"], ["GoogLeNets", "Evaluated-With", "ImageNet"], ["ResNets", "Evaluated-With", "ImageNet"], ["DenseNets", "Evaluated-With", "ImageNet"]], "rel_plus": [["ResNets:Method", "SubClass-Of", "CNNs:Method"], ["GoogLeNets:Method", "SubClass-Of", "CNNs:Method"], ["AlexNet:Method", "SubClass-Of", "CNNs:Method"], ["DenseNets:Method", "SubClass-Of", "CNNs:Method"], ["CNNs:Method", "Used-For", "classification:Task"], ["ImageNet:Dataset", "Benchmark-For", "classification:Task"], ["DenseNets:Method", "Used-For", "classification:Task"], ["ResNets:Method", "Used-For", "classification:Task"], ["GoogLeNets:Method", "Used-For", "classification:Task"], ["AlexNet:Method", "Used-For", "classification:Task"], ["CNNs:Method", "Evaluated-With", "ImageNet:Dataset"], ["AlexNet:Method", "Evaluated-With", "ImageNet:Dataset"], ["GoogLeNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["ResNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["DenseNets:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "209386851", "sentence": "Se - manticFPN uses a standard ResNet - 1 0 1 [ 2 0 ] , whereas DeeplabV 3 uses the ResNet - 1 0 3 proposed in [ 5 ] . 3 We follow the original papers ' training schedules and data augmentation ( details are in the appendix ) .", "ner": [["Se - manticFPN", "Method"], ["ResNet - 1 0 1", "Method"], ["DeeplabV 3", "Method"], ["ResNet - 1 0 3", "Method"], ["data augmentation", "Method"]], "rel": [["ResNet - 1 0 1", "Part-Of", "Se - manticFPN"], ["ResNet - 1 0 3", "Part-Of", "DeeplabV 3"]], "rel_plus": [["ResNet - 1 0 1:Method", "Part-Of", "Se - manticFPN:Method"], ["ResNet - 1 0 3:Method", "Part-Of", "DeeplabV 3:Method"]]}
{"doc_id": "AUG054", "sentence": "DistilBERT enhances document classification on 20NG, not just a transformer.", "ner": [["DistilBERT", "Method"], ["document classification", "Task"], ["20NG", "Dataset"]], "rel": [["DistilBERT", "Used-For", "document classification"], ["DistilBERT", "Evaluated-With", "20NG"]], "rel_plus": [["DistilBERT:Method", "Used-For", "document classification:Task"], ["DistilBERT:Method", "Evaluated-With", "20NG:Dataset"]]}
{"doc_id": "11241677", "sentence": "A similar effort is in progress in the video understanding domain where the community has quickly progressed from small , well - labeled datasets such as KTH [ 2 2 ] , Hollywood 2 [ 2 3 ] , Weizmann [ 5 ] , with a few thousand video clips , to medium - scale datasets such as UCF 1 0 1 [ 3 3 ] , Thumos' 1 4 [ 1 6 ] and HMDB 5 1 [ 2 1 ] , with more than 5 0 action categories .", "ner": [["video understanding", "Task"], ["KTH", "Dataset"], ["Hollywood 2", "Dataset"], ["Weizmann", "Dataset"], ["UCF 1 0 1", "Dataset"], ["Thumos' 1 4", "Dataset"], ["HMDB 5 1", "Dataset"]], "rel": [["KTH", "Benchmark-For", "video understanding"], ["Hollywood 2", "Benchmark-For", "video understanding"], ["Weizmann", "Benchmark-For", "video understanding"], ["UCF 1 0 1", "Benchmark-For", "video understanding"], ["Thumos' 1 4", "Benchmark-For", "video understanding"], ["HMDB 5 1", "Benchmark-For", "video understanding"]], "rel_plus": [["KTH:Dataset", "Benchmark-For", "video understanding:Task"], ["Hollywood 2:Dataset", "Benchmark-For", "video understanding:Task"], ["Weizmann:Dataset", "Benchmark-For", "video understanding:Task"], ["UCF 1 0 1:Dataset", "Benchmark-For", "video understanding:Task"], ["Thumos' 1 4:Dataset", "Benchmark-For", "video understanding:Task"], ["HMDB 5 1:Dataset", "Benchmark-For", "video understanding:Task"]]}
{"doc_id": "23569888", "sentence": "Indeed , the resulting dataset offers a natural testbed for visual recognition in robotics , which is as close as possible to the real application .", "ner": [["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "6423078", "sentence": "HED model trained on PASCAL - Context works surprisingly well on BSDS 5 0 0 ( ODS score 0. 7 7 8) , which suggests that features learned on semantic segmentation datasets can generalize well to the Fig. 7 HED results on four benchmark datasets including BSDS 5 0 0 ( Arbelaez et al. 2 0 1 1 ) , NYUD ( Silberman et al. 2 0 1 2 ) , Multicueedge/boundary ( M\u00e9ly et al. 2 0 1 5 ) , and PASCAL - Context ( Everingham et al. 2 0 1 4 ) .", "ner": [["HED", "Method"], ["PASCAL - Context", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["semantic segmentation", "Task"], ["HED", "Method"], ["BSDS 5 0 0", "Dataset"], ["NYUD", "Dataset"], ["Multicueedge/boundary", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [["HED", "Trained-With", "PASCAL - Context"], ["HED", "Evaluated-With", "BSDS 5 0 0"], ["PASCAL - Context", "Benchmark-For", "semantic segmentation"], ["HED", "Used-For", "semantic segmentation"], ["HED", "Evaluated-With", "BSDS 5 0 0"], ["HED", "Evaluated-With", "NYUD"], ["HED", "Evaluated-With", "Multicueedge/boundary"], ["HED", "Evaluated-With", "PASCAL - Context"]], "rel_plus": [["HED:Method", "Trained-With", "PASCAL - Context:Dataset"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["PASCAL - Context:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["HED:Method", "Used-For", "semantic segmentation:Task"], ["HED:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"], ["HED:Method", "Evaluated-With", "NYUD:Dataset"], ["HED:Method", "Evaluated-With", "Multicueedge/boundary:Dataset"], ["HED:Method", "Evaluated-With", "PASCAL - Context:Dataset"]]}
{"doc_id": "199543700", "sentence": "The rest of this paper is organized as follows : In Section II , we briefly reviews related work on semi - supervised learning , generative adversarial networks , the optimization of GAN and face attribute recognition .", "ner": [["semi - supervised learning", "Method"], ["generative adversarial networks", "Method"], ["GAN", "Method"], ["face attribute recognition", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "For object detection and instance segmentation task , GALD is added at the end of stage 4 of a ResNet backbone , FPN [ 2 0 ] is used to build a strong baseline with a feature pyramid for multi - scale object detection .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["GALD", "Method"], ["ResNet", "Method"], ["FPN", "Method"], ["feature pyramid", "Method"], ["multi - scale object detection", "Task"]], "rel": [["ResNet", "Used-For", "object detection"], ["GALD", "Used-For", "object detection"], ["ResNet", "Used-For", "instance segmentation"], ["GALD", "Used-For", "instance segmentation"], ["GALD", "Part-Of", "ResNet"], ["feature pyramid", "Part-Of", "FPN"], ["FPN", "Used-For", "multi - scale object detection"], ["feature pyramid", "Used-For", "multi - scale object detection"]], "rel_plus": [["ResNet:Method", "Used-For", "object detection:Task"], ["GALD:Method", "Used-For", "object detection:Task"], ["ResNet:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Used-For", "instance segmentation:Task"], ["GALD:Method", "Part-Of", "ResNet:Method"], ["feature pyramid:Method", "Part-Of", "FPN:Method"], ["FPN:Method", "Used-For", "multi - scale object detection:Task"], ["feature pyramid:Method", "Used-For", "multi - scale object detection:Task"]]}
{"doc_id": "211020570", "sentence": "The regression - based approach mentioned above employs the handcrafted feature descriptors ( e.g. , SIFT [ 2 2 ] , HoG [ 2 4 ] , or random forest/fern descriptors [ 1 1 ] ) to extract facial texture information .", "ner": [["handcrafted feature descriptors", "Method"], ["SIFT", "Method"], ["HoG", "Method"], ["random", "Method"]], "rel": [["SIFT", "SubClass-Of", "handcrafted feature descriptors"], ["HoG", "SubClass-Of", "handcrafted feature descriptors"], ["random", "SubClass-Of", "handcrafted feature descriptors"]], "rel_plus": [["SIFT:Method", "SubClass-Of", "handcrafted feature descriptors:Method"], ["HoG:Method", "SubClass-Of", "handcrafted feature descriptors:Method"], ["random:Method", "SubClass-Of", "handcrafted feature descriptors:Method"]]}
{"doc_id": "4319457", "sentence": "The first layer is a standard convolutional layer with 1 2 8 convolutional filters with size 9 \u00d7 9 , a stride of 1 , and the ReLU activation function .", "ner": [["convolutional layer", "Method"], ["convolutional filters", "Method"], ["ReLU activation", "Method"]], "rel": [["convolutional filters", "Part-Of", "convolutional layer"], ["ReLU activation", "Part-Of", "convolutional layer"]], "rel_plus": [["convolutional filters:Method", "Part-Of", "convolutional layer:Method"], ["ReLU activation:Method", "Part-Of", "convolutional layer:Method"]]}
{"doc_id": "210860962", "sentence": "Similarly to the CycleGAN model explained in Section III - B , the generator G is a FCN , which in our implementation has an initial convolutional block , followed by 2 downsampling blocks , 2 upsampling blocks and a final convolution that maps back to the image domain .", "ner": [["CycleGAN", "Method"], ["generator G", "Method"], ["FCN", "Method"], ["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["convolution", "Method"]], "rel": [["generator G", "Part-Of", "CycleGAN"], ["FCN", "Part-Of", "generator G"], ["convolutional block", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["convolution", "Part-Of", "FCN"]], "rel_plus": [["generator G:Method", "Part-Of", "CycleGAN:Method"], ["FCN:Method", "Part-Of", "generator G:Method"], ["convolutional block:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["convolution:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "208548469", "sentence": "We compare the performance of LASSO - based ranking method with non - LASSObased ranking methods including seven popular sen - tence evaluation metrics Papineni et al ( 2 0 0 2 ) ; Vedantam et al ( 2 0 1 5 ) ; Lin ( 2 0 0 4 ) ; Banerjee and Lavie ( 2 0 0 5 ) , namely BLEU - 1 , BLEU - 2 , BLEU - 3 , BLEU - 4 , ROUGE , CIDEr and METEOR that are also used to measure the similarity score between MQ and BQs .", "ner": [["LASSO - based ranking", "Method"], ["LASSObased ranking methods", "Method"], ["sen - tence evaluation metrics", "Method"], ["BLEU - 1", "Method"], ["BLEU - 2", "Method"], ["BLEU - 3", "Method"], ["BLEU - 4", "Method"], ["ROUGE", "Method"], ["CIDEr", "Method"], ["METEOR", "Method"], ["MQ", "Dataset"], ["BQs", "Dataset"]], "rel": [["LASSO - based ranking", "Compare-With", "LASSObased ranking methods"], ["BLEU - 1", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 2", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 3", "SubClass-Of", "sen - tence evaluation metrics"], ["BLEU - 4", "SubClass-Of", "sen - tence evaluation metrics"], ["ROUGE", "SubClass-Of", "sen - tence evaluation metrics"], ["CIDEr", "SubClass-Of", "sen - tence evaluation metrics"], ["METEOR", "SubClass-Of", "sen - tence evaluation metrics"]], "rel_plus": [["LASSO - based ranking:Method", "Compare-With", "LASSObased ranking methods:Method"], ["BLEU - 1:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 2:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 3:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["BLEU - 4:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["ROUGE:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["CIDEr:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"], ["METEOR:Method", "SubClass-Of", "sen - tence evaluation metrics:Method"]]}
{"doc_id": "102351044", "sentence": "Theoretically , the dropout regularizer is first - order equivalent to an L 2 regularizer . [ 1 ] also shows that dropout provides immediately the magnitude of the regularization term which is adaptively scaled by the inputs and the variance of the dropout variables . [ 5 , 1 7 ] instead formulate neural networks trained with dropout in the Bayesian inference framework , providing tools to model uncertainty with dropout training .", "ner": [["dropout", "Method"], ["L 2 regularizer", "Method"], ["dropout", "Method"], ["dropout", "Method"], ["neural networks", "Method"], ["dropout", "Method"], ["Bayesian inference", "Method"], ["dropout training", "Method"]], "rel": [["dropout", "Compare-With", "L 2 regularizer"], ["dropout", "Part-Of", "neural networks"], ["dropout training", "Part-Of", "Bayesian inference"], ["neural networks", "Part-Of", "Bayesian inference"]], "rel_plus": [["dropout:Method", "Compare-With", "L 2 regularizer:Method"], ["dropout:Method", "Part-Of", "neural networks:Method"], ["dropout training:Method", "Part-Of", "Bayesian inference:Method"], ["neural networks:Method", "Part-Of", "Bayesian inference:Method"]]}
{"doc_id": "54447105", "sentence": "Domain adaptation has been studied only recently for problems such as detection [ 6 0 , 6 6 , 2 4 , 5 0 , 1 3 , 6 8 ] .", "ner": [["Domain adaptation", "Method"], ["detection", "Task"]], "rel": [["Domain adaptation", "Used-For", "detection"]], "rel_plus": [["Domain adaptation:Method", "Used-For", "detection:Task"]]}
{"doc_id": "147703932", "sentence": "On the other hand 2D parts segmentation can benefit from 2D pose but not from 3D pose .", "ner": [["2D parts segmentation", "Task"], ["2D pose", "Task"], ["3D pose", "Task"]], "rel": [["2D pose", "Used-For", "2D parts segmentation"]], "rel_plus": [["2D pose:Task", "Used-For", "2D parts segmentation:Task"]]}
{"doc_id": "210860760", "sentence": "ExEm(fastText ) , ExEm(Word 2 vec ) and ExEm(fastText+Word 2 vec ) respectively demonstrate the implementation of ExEm with \" fastText \" , \" Word 2 Vec \" , and combination of \" fastText \" and \" Word 2 Vec \" .", "ner": [["ExEm(fastText )", "Method"], ["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"], ["ExEm", "Method"], ["fastText", "Method"], ["Word 2 Vec", "Method"], ["fastText", "Method"], ["Word 2 Vec", "Method"]], "rel": [["fastText", "Part-Of", "ExEm"], ["Word 2 Vec", "Part-Of", "ExEm"]], "rel_plus": [["fastText:Method", "Part-Of", "ExEm:Method"], ["Word 2 Vec:Method", "Part-Of", "ExEm:Method"]]}
{"doc_id": "199668978", "sentence": "QR - word is seq 2 seq model considering three embeddings and using word reward to train the RL model .", "ner": [["QR - word", "Method"], ["seq 2 seq", "Method"], ["RL", "Method"]], "rel": [["QR - word", "SubClass-Of", "seq 2 seq"]], "rel_plus": [["QR - word:Method", "SubClass-Of", "seq 2 seq:Method"]]}
{"doc_id": "44148233", "sentence": "Consensus based Image Description Evaluation ( CIDEr ) [ 1 5 8 ] and Semantic Propositional Image Captioning Evaluation ( SPICE ) [ 1 4 ] are two other recently introduced metrics specifically designed for image captioning tasks , that are also being used for automatic evaluation of video description .", "ner": [["Consensus based Image Description Evaluation", "Method"], ["CIDEr", "Method"], ["Semantic Propositional Image Captioning Evaluation", "Method"], ["SPICE", "Method"], ["image captioning", "Task"], ["video description", "Task"]], "rel": [["CIDEr", "Synonym-Of", "Consensus based Image Description Evaluation"], ["SPICE", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation"], ["Semantic Propositional Image Captioning Evaluation", "Used-For", "image captioning"], ["Consensus based Image Description Evaluation", "Used-For", "image captioning"], ["Consensus based Image Description Evaluation", "Used-For", "video description"], ["Semantic Propositional Image Captioning Evaluation", "Used-For", "video description"]], "rel_plus": [["CIDEr:Method", "Synonym-Of", "Consensus based Image Description Evaluation:Method"], ["SPICE:Method", "Synonym-Of", "Semantic Propositional Image Captioning Evaluation:Method"], ["Semantic Propositional Image Captioning Evaluation:Method", "Used-For", "image captioning:Task"], ["Consensus based Image Description Evaluation:Method", "Used-For", "image captioning:Task"], ["Consensus based Image Description Evaluation:Method", "Used-For", "video description:Task"], ["Semantic Propositional Image Captioning Evaluation:Method", "Used-For", "video description:Task"]]}
{"doc_id": "198231883", "sentence": "For the second - stage , we propose a novel Forest Path Cutting ( FPC ) algorithm .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "51559", "sentence": "Recurrent neural networks ( RNNs ) , including gated variants such as the long short - term memory ( LSTM ) ( Hochreiter & Schmidhuber , 1 9 9 7 ) have become the standard model architecture for deep learning approaches to sequence modeling tasks .", "ner": [["Recurrent neural networks", "Method"], ["RNNs", "Method"], ["long short - term memory", "Method"], ["LSTM", "Method"], ["deep learning", "Method"], ["sequence modeling", "Task"]], "rel": [["RNNs", "Synonym-Of", "Recurrent neural networks"], ["long short - term memory", "SubClass-Of", "Recurrent neural networks"], ["LSTM", "Synonym-Of", "long short - term memory"], ["Recurrent neural networks", "SubClass-Of", "deep learning"], ["long short - term memory", "SubClass-Of", "deep learning"], ["Recurrent neural networks", "Used-For", "sequence modeling"], ["long short - term memory", "Used-For", "sequence modeling"]], "rel_plus": [["RNNs:Method", "Synonym-Of", "Recurrent neural networks:Method"], ["long short - term memory:Method", "SubClass-Of", "Recurrent neural networks:Method"], ["LSTM:Method", "Synonym-Of", "long short - term memory:Method"], ["Recurrent neural networks:Method", "SubClass-Of", "deep learning:Method"], ["long short - term memory:Method", "SubClass-Of", "deep learning:Method"], ["Recurrent neural networks:Method", "Used-For", "sequence modeling:Task"], ["long short - term memory:Method", "Used-For", "sequence modeling:Task"]]}
{"doc_id": "52910494", "sentence": "To deal with these problems , several creative architectures , such as Highway networks [ 2 4 ] , Deeply - Supervised Nets [ 1 6 ] and ResNets [ 6 ] , have been designed .", "ner": [["Highway networks", "Method"], ["Deeply - Supervised Nets", "Method"], ["ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "Our proposed pipeline containing two components : a Synthetic Data Augmentation module with AR - GAN for unsupervised learning and a Recognition System with a Convolutional Neural Network for supervised learning .", "ner": [["Synthetic Data Augmentation", "Method"], ["AR - GAN", "Method"], ["unsupervised learning", "Task"], ["Recognition System", "Method"], ["Convolutional Neural Network", "Method"], ["supervised learning", "Task"]], "rel": [["AR - GAN", "Part-Of", "Synthetic Data Augmentation"], ["Synthetic Data Augmentation", "Used-For", "unsupervised learning"], ["Convolutional Neural Network", "Part-Of", "Recognition System"], ["Recognition System", "Used-For", "supervised learning"]], "rel_plus": [["AR - GAN:Method", "Part-Of", "Synthetic Data Augmentation:Method"], ["Synthetic Data Augmentation:Method", "Used-For", "unsupervised learning:Task"], ["Convolutional Neural Network:Method", "Part-Of", "Recognition System:Method"], ["Recognition System:Method", "Used-For", "supervised learning:Task"]]}
{"doc_id": "210164920", "sentence": "In this survey , we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN .", "ner": [["semantic and instance segmentation", "Task"], ["CNN", "Method"]], "rel": [["CNN", "Used-For", "semantic and instance segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic and instance segmentation:Task"]]}
{"doc_id": "202734316", "sentence": "SG - GAN [ 2 5 ] complements cycle consistencies and adversarial discriminators with a soft - gradient objective that aims to align texture edges with semantic boundaries , and presents results for the GTA to CS semantic segmentation task .", "ner": [["SG - GAN", "Method"], ["adversarial discriminators", "Method"], ["soft - gradient objective", "Method"], ["GTA", "Dataset"], ["CS", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["soft - gradient objective", "Part-Of", "SG - GAN"], ["adversarial discriminators", "Part-Of", "SG - GAN"], ["SG - GAN", "Evaluated-With", "GTA"], ["SG - GAN", "Evaluated-With", "CS"], ["GTA", "Benchmark-For", "semantic segmentation"], ["CS", "Benchmark-For", "semantic segmentation"], ["SG - GAN", "Used-For", "semantic segmentation"]], "rel_plus": [["soft - gradient objective:Method", "Part-Of", "SG - GAN:Method"], ["adversarial discriminators:Method", "Part-Of", "SG - GAN:Method"], ["SG - GAN:Method", "Evaluated-With", "GTA:Dataset"], ["SG - GAN:Method", "Evaluated-With", "CS:Dataset"], ["GTA:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["CS:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["SG - GAN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "204402755", "sentence": "Minimizing the objective function L ( \u00b7 ) , the simplest and most common momentum method , classical momentum ( CM ) ( Polyak , 1 9 6 4 ) , is given by the following recursion for variable vector \u03b8 t \u2208 R p : The coefficient \u03b2 - traditionally , selected constant in [ 0 , 1] - controls how quickly the momentum decays , g t represents a stochastic gradient , usually E[g t ] = \u2207L(\u03b8 t ) , and \u03b7 > 0 is the step size .", "ner": [["momentum", "Method"], ["classical momentum", "Method"], ["CM", "Method"]], "rel": [["CM", "Synonym-Of", "classical momentum"]], "rel_plus": [["CM:Method", "Synonym-Of", "classical momentum:Method"]]}
{"doc_id": "102351044", "sentence": "To alleviate overfitting , many explicit and implicit regularization methods have been proposed , including early stopping , weight decay , data augmentation etc .", "ner": [["explicit and implicit regularization", "Method"], ["early stopping", "Method"], ["weight decay", "Method"], ["data augmentation", "Method"]], "rel": [["early stopping", "SubClass-Of", "explicit and implicit regularization"], ["weight decay", "SubClass-Of", "explicit and implicit regularization"], ["data augmentation", "SubClass-Of", "explicit and implicit regularization"]], "rel_plus": [["early stopping:Method", "SubClass-Of", "explicit and implicit regularization:Method"], ["weight decay:Method", "SubClass-Of", "explicit and implicit regularization:Method"], ["data augmentation:Method", "SubClass-Of", "explicit and implicit regularization:Method"]]}
{"doc_id": "202734254", "sentence": "It is motivated by the fact that the position of the answer to the previous question is widely used in many of previous conversational question answering models , such as BiDAF - with - ctx ( Seo et al. , 2 0 1 6 ) and FlowQA ( Huang et al. , 2 0 1 9 ) .", "ner": [["conversational question answering", "Task"], ["BiDAF - with - ctx", "Method"], ["FlowQA", "Method"]], "rel": [["BiDAF - with - ctx", "Used-For", "conversational question answering"], ["FlowQA", "Used-For", "conversational question answering"]], "rel_plus": [["BiDAF - with - ctx:Method", "Used-For", "conversational question answering:Task"], ["FlowQA:Method", "Used-For", "conversational question answering:Task"]]}
{"doc_id": "204402755", "sentence": "AggMo ( Aggregated Momentum ) ( Lucas et al. , 2 0 1 8) takes a linear combination of multiple momentum buffers .", "ner": [["AggMo", "Method"], ["Aggregated Momentum", "Method"]], "rel": [["AggMo", "Synonym-Of", "Aggregated Momentum"]], "rel_plus": [["AggMo:Method", "Synonym-Of", "Aggregated Momentum:Method"]]}
{"doc_id": "198897554", "sentence": "The semantic segmentation training objective that we need to solve for E and S is : where the segmentation loss is the cross - entropy loss , defined as : With Eq. ( 1 ) and Eq. ( 3 ) , the objective function that selfsupervised domain adaptation must solve is : where \u03bb p is the weight to balance the two losses .", "ner": [["semantic segmentation", "Task"], ["segmentation loss", "Method"], ["cross - entropy loss", "Method"]], "rel": [["segmentation loss", "SubClass-Of", "cross - entropy loss"]], "rel_plus": [["segmentation loss:Method", "SubClass-Of", "cross - entropy loss:Method"]]}
{"doc_id": "150374036", "sentence": "Second , we propose a novel Region Attention Network ( RAN ) , to adaptively capture the importance of facial regions for occlusion and pose variant FER .", "ner": [["Region Attention Network", "Method"], ["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Synonym-Of", "Region Attention Network"], ["Region Attention Network", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Synonym-Of", "Region Attention Network:Method"], ["Region Attention Network:Method", "Used-For", "FER:Task"]]}
{"doc_id": "28984897", "sentence": "The LFW dataset is a standard benchmark dataset for face verification .", "ner": [["LFW", "Dataset"], ["face verification", "Task"]], "rel": [["LFW", "Benchmark-For", "face verification"]], "rel_plus": [["LFW:Dataset", "Benchmark-For", "face verification:Task"]]}
{"doc_id": "210861282", "sentence": "Deepflow [ 5 0 ] used optical flow to better connect predictions between frames in a more continuous detection .", "ner": [["Deepflow", "Method"], ["optical flow", "Method"], ["detection", "Task"]], "rel": [["optical flow", "Part-Of", "Deepflow"], ["Deepflow", "Used-For", "detection"]], "rel_plus": [["optical flow:Method", "Part-Of", "Deepflow:Method"], ["Deepflow:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202676714", "sentence": "Robustness against PGD with Adversarial Training Table 4 lists the accuracies of models trained by adversarial training on data perturbed by PGD .", "ner": [["Robustness against PGD", "Task"], ["Adversarial Training", "Method"], ["adversarial training", "Method"], ["PGD", "Method"]], "rel": [["Adversarial Training", "Used-For", "Robustness against PGD"], ["PGD", "Used-For", "adversarial training"]], "rel_plus": [["Adversarial Training:Method", "Used-For", "Robustness against PGD:Task"], ["PGD:Method", "Used-For", "adversarial training:Method"]]}
{"doc_id": "210164920", "sentence": "Most of the successful works are based on handcrafted machine learning features such as HOG [ 6 0 , 6 1 , 6 2 , 6 3 ] , SIFT [ 6 4 , 6 5 ] etc .", "ner": [["handcrafted machine learning features", "Method"], ["HOG", "Method"], ["SIFT", "Method"]], "rel": [["HOG", "SubClass-Of", "handcrafted machine learning features"], ["SIFT", "SubClass-Of", "handcrafted machine learning features"]], "rel_plus": [["HOG:Method", "SubClass-Of", "handcrafted machine learning features:Method"], ["SIFT:Method", "SubClass-Of", "handcrafted machine learning features:Method"]]}
{"doc_id": "59599694", "sentence": "Fully connected Long - Short Term Memory neural network ( LSTM ) is capable of capturing long - term temporal patterns .", "ner": [["Long - Short Term Memory neural network", "Method"], ["LSTM", "Method"]], "rel": [["LSTM", "Synonym-Of", "Long - Short Term Memory neural network"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long - Short Term Memory neural network:Method"]]}
{"doc_id": "51923817", "sentence": "Second , CornerNet uses corner pooling to better localize the corners .", "ner": [["CornerNet", "Method"], ["corner pooling", "Method"]], "rel": [["corner pooling", "Part-Of", "CornerNet"]], "rel_plus": [["corner pooling:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "67855714", "sentence": "Qualitative results are provided P sisr /mse P sisr /adv , mse SRGAN [ 1 ] P sisr /rec P sisr /dis , rec P sisr /adv P sisr /adv , rec Figure 6 : Results of different P sisr methods on a patch of an image from the Sat dataset .", "ner": [["P sisr /mse P sisr /adv", "Method"], ["mse SRGAN", "Method"], ["sisr /rec P sisr /dis", "Method"], ["rec P sisr /adv P sisr /adv", "Method"], ["Sat", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "52910494", "sentence": "Inspired by these findings , we design a novel MCA module , which first broadens the width of the initial convolution layer of DenseNets through multi - scale convolutions , then fuses the filters using cross - scale aggregation parameterised by trainable weights .", "ner": [["MCA", "Method"], ["convolution layer", "Method"], ["DenseNets", "Method"], ["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"]], "rel": [["MCA", "Part-Of", "convolution layer"], ["convolution layer", "Part-Of", "DenseNets"], ["multi - scale convolutions", "Part-Of", "DenseNets"], ["cross - scale aggregation", "Part-Of", "DenseNets"]], "rel_plus": [["MCA:Method", "Part-Of", "convolution layer:Method"], ["convolution layer:Method", "Part-Of", "DenseNets:Method"], ["multi - scale convolutions:Method", "Part-Of", "DenseNets:Method"], ["cross - scale aggregation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "195347056", "sentence": "While the generator loss L Geb is formulated as , In order to formulate a conditional energy - based loss function , ArtGAN - EB propose a novel discriminator loss function L Debc as , While , the new generator loss L Gae is defined as , ArtGAN - AE : The discriminator loss is similar to L Debc , except that we do not use the generated images as adversarial samples to update the decoder .", "ner": [["generator loss", "Method"], ["L Geb", "Method"], ["energy - based loss function", "Method"], ["ArtGAN - EB", "Method"], ["discriminator loss function", "Method"], ["L Debc", "Method"], ["generator loss", "Method"], ["L Gae", "Method"], ["ArtGAN - AE", "Method"], ["discriminator loss", "Method"], ["L Debc", "Method"], ["decoder", "Method"]], "rel": [["L Geb", "Synonym-Of", "generator loss"], ["discriminator loss function", "Part-Of", "ArtGAN - EB"], ["generator loss", "Part-Of", "ArtGAN - EB"], ["L Debc", "Synonym-Of", "discriminator loss function"], ["L Gae", "Synonym-Of", "generator loss"]], "rel_plus": [["L Geb:Method", "Synonym-Of", "generator loss:Method"], ["discriminator loss function:Method", "Part-Of", "ArtGAN - EB:Method"], ["generator loss:Method", "Part-Of", "ArtGAN - EB:Method"], ["L Debc:Method", "Synonym-Of", "discriminator loss function:Method"], ["L Gae:Method", "Synonym-Of", "generator loss:Method"]]}
{"doc_id": "52180375", "sentence": "Fully Convolutional Networks ( FCNs ) based methods have made great progress in semantic segmentation .", "ner": [["Fully Convolutional Networks", "Method"], ["FCNs", "Method"], ["semantic segmentation", "Task"]], "rel": [["FCNs", "Synonym-Of", "Fully Convolutional Networks"], ["Fully Convolutional Networks", "Used-For", "semantic segmentation"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"], ["Fully Convolutional Networks:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "202577400", "sentence": "Mask - RCNN + GALD   In this paper , we propose GALD to adaptively distribute global information to each position for scene understanding tasks .", "ner": [["Mask - RCNN + GALD", "Method"], ["GALD", "Method"], ["scene understanding", "Task"]], "rel": [["GALD", "Used-For", "scene understanding"]], "rel_plus": [["GALD:Method", "Used-For", "scene understanding:Task"]]}
{"doc_id": "67855714", "sentence": "More generally , the GCN framework offers a large vision on the wide variety of existing loss functions used in the literature of learning mappingsbased problems ( e.g. , super - resolution , image completion , artistic style transfer , etc . ) .", "ner": [["GCN", "Method"], ["super - resolution", "Task"], ["image completion", "Task"], ["artistic style transfer", "Task"]], "rel": [["GCN", "Used-For", "super - resolution"], ["GCN", "Used-For", "image completion"], ["GCN", "Used-For", "artistic style transfer"]], "rel_plus": [["GCN:Method", "Used-For", "super - resolution:Task"], ["GCN:Method", "Used-For", "image completion:Task"], ["GCN:Method", "Used-For", "artistic style transfer:Task"]]}
{"doc_id": "209862890", "sentence": "The main challenges which the entity linking task has over other tasks e.g. sentence classification , named entity recognition , where BERT has been applied are : ( 1 ) a very large label space , i.e. every mention has many target entities and ( 2 ) the zero - shot nature of the entity linking task .", "ner": [["entity linking", "Task"], ["sentence classification", "Task"], ["named entity recognition", "Task"], ["BERT", "Method"], ["entity linking", "Task"]], "rel": [["sentence classification", "SubTask-Of", "entity linking"], ["named entity recognition", "SubTask-Of", "entity linking"], ["BERT", "Used-For", "sentence classification"], ["BERT", "Used-For", "named entity recognition"]], "rel_plus": [["sentence classification:Task", "SubTask-Of", "entity linking:Task"], ["named entity recognition:Task", "SubTask-Of", "entity linking:Task"], ["BERT:Method", "Used-For", "sentence classification:Task"], ["BERT:Method", "Used-For", "named entity recognition:Task"]]}
{"doc_id": "53731879", "sentence": "Despite the apparent simplify , our SSG outperforms the state - of - the - arts by more than 4. 6 % ( DukeMTMC to Market 1 5 0 1 ) and 4. 4 % ( Market 1 5 0 1 to DukeMTMC ) in mAP , respectively .", "ner": [["SSG", "Method"], ["DukeMTMC", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC", "Dataset"]], "rel": [["SSG", "Evaluated-With", "DukeMTMC"], ["SSG", "Evaluated-With", "Market 1 5 0 1"], ["SSG", "Evaluated-With", "Market 1 5 0 1"], ["SSG", "Evaluated-With", "DukeMTMC"]], "rel_plus": [["SSG:Method", "Evaluated-With", "DukeMTMC:Dataset"], ["SSG:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["SSG:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["SSG:Method", "Evaluated-With", "DukeMTMC:Dataset"]]}
{"doc_id": "AUG052", "sentence": "Isolation Forest enhances anomaly detection on KDD Cup 99, not just a forest.", "ner": [["Isolation Forest", "Method"], ["anomaly detection", "Task"], ["KDD Cup 99", "Dataset"]], "rel": [["Isolation Forest", "Used-For", "anomaly detection"], ["Isolation Forest", "Evaluated-With", "KDD Cup 99"]], "rel_plus": [["Isolation Forest:Method", "Used-For", "anomaly detection:Task"], ["Isolation Forest:Method", "Evaluated-With", "KDD Cup 99:Dataset"]]}
{"doc_id": "201646309", "sentence": "BERT uses a cross - encoder : Two sentences are passed to the transformer network and the target value is predicted .", "ner": [["BERT", "Method"], ["cross - encoder", "Method"], ["transformer", "Method"]], "rel": [["cross - encoder", "Part-Of", "BERT"]], "rel_plus": [["cross - encoder:Method", "Part-Of", "BERT:Method"]]}
{"doc_id": "203593581", "sentence": "APoT quantization has a reasonable distribution of quantization levels , with more levels in the peak area ( near 0 ) and relatively higher resolution than the vanilla PoT quantization at the tail ( near 1 ) .", "ner": [["APoT quantization", "Method"], ["PoT quantization", "Method"]], "rel": [["APoT quantization", "Compare-With", "PoT quantization"]], "rel_plus": [["APoT quantization:Method", "Compare-With", "PoT quantization:Method"]]}
{"doc_id": "210164920", "sentence": "Pyramid Scene Parsing Network(PSPNet ) [ 9 0 ] , proposed by Zhao et al. , has also used global contextual information for better segmentation .", "ner": [["Pyramid Scene Parsing Network(PSPNet )", "Method"], ["segmentation", "Task"]], "rel": [["Pyramid Scene Parsing Network(PSPNet )", "Used-For", "segmentation"]], "rel_plus": [["Pyramid Scene Parsing Network(PSPNet ):Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "We evaluate CARAFE on Faster RCNN and Mask RCNN with the ResNet - 5 0 w/ FPN backbone , and follow the 1x training schedule settings as Detectron [ 7 ] and MMDetection [ 2 ] .", "ner": [["CARAFE", "Method"], ["Faster RCNN", "Method"], ["Mask RCNN", "Method"], ["ResNet - 5 0", "Method"], ["FPN", "Method"], ["Detectron", "Method"], ["MMDetection", "Method"]], "rel": [["CARAFE", "Part-Of", "Faster RCNN"], ["ResNet - 5 0", "Part-Of", "Faster RCNN"], ["MMDetection", "Part-Of", "Faster RCNN"], ["Detectron", "Part-Of", "Faster RCNN"], ["CARAFE", "Part-Of", "Mask RCNN"], ["ResNet - 5 0", "Part-Of", "Mask RCNN"], ["Detectron", "Part-Of", "Mask RCNN"], ["MMDetection", "Part-Of", "Mask RCNN"], ["FPN", "Part-Of", "ResNet - 5 0"]], "rel_plus": [["CARAFE:Method", "Part-Of", "Faster RCNN:Method"], ["ResNet - 5 0:Method", "Part-Of", "Faster RCNN:Method"], ["MMDetection:Method", "Part-Of", "Faster RCNN:Method"], ["Detectron:Method", "Part-Of", "Faster RCNN:Method"], ["CARAFE:Method", "Part-Of", "Mask RCNN:Method"], ["ResNet - 5 0:Method", "Part-Of", "Mask RCNN:Method"], ["Detectron:Method", "Part-Of", "Mask RCNN:Method"], ["MMDetection:Method", "Part-Of", "Mask RCNN:Method"], ["FPN:Method", "Part-Of", "ResNet - 5 0:Method"]]}
{"doc_id": "202734316", "sentence": "Setting out to Table 3 , the same applies regarding metrics and results presentation , except from ours , where the second row shows results for the configuration GTA(RS - Map ) \u2192 Map , and the third for GTA(PH - Map ) \u2192 Map . validate and assess our proposed technique , we fix K to 5 for all conducted experiments , which is a good compromise between efficiency and effectiveness .    In this section , we present results for data restyling on the semantic segmentation task in a synthetic to - real adaptation setting .", "ner": [["GTA(RS - Map )", "Dataset"], ["Map", "Dataset"], ["GTA(PH - Map", "Dataset"], ["Map", "Dataset"], ["semantic segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "In the original paper that introduced GANs [ Goodfellow et al. , 2 0 1 4 ] the authors use a multilayer perceptron for both the generator and discriminator .", "ner": [["GANs", "Method"], ["multilayer perceptron", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["multilayer perceptron", "Part-Of", "GANs"], ["multilayer perceptron", "Used-For", "generator"], ["multilayer perceptron", "Used-For", "discriminator"]], "rel_plus": [["multilayer perceptron:Method", "Part-Of", "GANs:Method"], ["multilayer perceptron:Method", "Used-For", "generator:Method"], ["multilayer perceptron:Method", "Used-For", "discriminator:Method"]]}
{"doc_id": "AUG091", "sentence": "Optical flow estimation is a task solved by FlowNet, not a method, using KITTI.", "ner": [["optical flow estimation", "Task"], ["FlowNet", "Method"], ["KITTI", "Dataset"]], "rel": [["FlowNet", "Used-For", "optical flow estimation"], ["FlowNet", "Trained-With", "KITTI"]], "rel_plus": [["FlowNet:Method", "Used-For", "optical flow estimation:Task"], ["FlowNet:Method", "Trained-With", "KITTI:Dataset"]]}
{"doc_id": "150374036", "sentence": "Extensive experiments show that our RAN and region biased loss largely improve the performance of FER with occlusion and variant pose .", "ner": [["RAN", "Method"], ["FER", "Task"]], "rel": [["RAN", "Used-For", "FER"]], "rel_plus": [["RAN:Method", "Used-For", "FER:Task"]]}
{"doc_id": "209532167", "sentence": "Here we compare our results on the classification tasks to a Transformer - based model trained from scratch , i.e. , without the benefit of a pre - trained embedding .", "ner": [["classification", "Task"], ["Transformer", "Method"]], "rel": [["Transformer", "Used-For", "classification"]], "rel_plus": [["Transformer:Method", "Used-For", "classification:Task"]]}
{"doc_id": "209386851", "sentence": "We use the Cityscapes [ 9 ] semantic segmentation set with 1 9 categories , 2 9 7 5 training images , and 5 0 0 validation images .", "ner": [["Cityscapes", "Dataset"], ["semantic segmentation", "Task"]], "rel": [["Cityscapes", "Benchmark-For", "semantic segmentation"]], "rel_plus": [["Cityscapes:Dataset", "Benchmark-For", "semantic segmentation:Task"]]}
{"doc_id": "150374036", "sentence": "Besides , we also build occlusion and pose variant test datasets from FERPlus , AffectNet , and RAF - DB . 1 ) FERPlus : [ 5 ] .", "ner": [["FERPlus", "Dataset"], ["AffectNet", "Dataset"], ["RAF - DB", "Dataset"], ["FERPlus", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "Our feature fusion module can also work better than FPN [ 1 8 ] in the object detection task .", "ner": [["feature fusion module", "Method"], ["FPN", "Method"], ["object detection", "Task"]], "rel": [["feature fusion module", "Compare-With", "FPN"], ["feature fusion module", "Used-For", "object detection"], ["FPN", "Used-For", "object detection"]], "rel_plus": [["feature fusion module:Method", "Compare-With", "FPN:Method"], ["feature fusion module:Method", "Used-For", "object detection:Task"], ["FPN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "150374036", "sentence": "Since model ensemble is popular on SFEW , we also conduct a naive model fusion by averaging the scores of ResNet 1 8 and VGG 1 6 which obtains 5 6 . 4 % .", "ner": [["SFEW", "Dataset"], ["ResNet 1 8", "Method"], ["VGG 1 6", "Method"]], "rel": [["ResNet 1 8", "Evaluated-With", "SFEW"], ["VGG 1 6", "Evaluated-With", "SFEW"]], "rel_plus": [["ResNet 1 8:Method", "Evaluated-With", "SFEW:Dataset"], ["VGG 1 6:Method", "Evaluated-With", "SFEW:Dataset"]]}
{"doc_id": "202734316", "sentence": "As with GTA(Map ) \u2192 Map , a performance increase is observed for PH compared to RS .", "ner": [["GTA(Map )", "Dataset"], ["Map", "Dataset"], ["PH", "Method"], ["RS", "Method"]], "rel": [["PH", "Compare-With", "RS"]], "rel_plus": [["PH:Method", "Compare-With", "RS:Method"]]}
{"doc_id": "52910494", "sentence": "The comparison shows that placing the additional dense block with SFR at the end of the DenseNet generates lower error rates on all three datasets .", "ner": [["dense block", "Method"], ["SFR", "Method"], ["DenseNet", "Method"]], "rel": [["SFR", "Part-Of", "dense block"], ["dense block", "Part-Of", "DenseNet"]], "rel_plus": [["SFR:Method", "Part-Of", "dense block:Method"], ["dense block:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "6116678", "sentence": "Though early work primarily focused on predicting eye - fixations in images , research has shown that salient object detection , which emphasizes object - level integrity of saliency prediction results , is more useful and can serve as a pre - processing step for a variety of computer vision and image processing tasks including content - aware image editing [ 3 ] , object detection [ 3 7 ] , image classification [ 4 6 ] , person re - identification [ 4 ] and video summarization [ 3 3 ] .", "ner": [["salient object detection", "Task"], ["saliency prediction", "Task"], ["computer vision", "Task"], ["image processing", "Task"], ["content - aware image editing", "Task"], ["object detection", "Task"], ["image classification", "Task"], ["person re - identification", "Task"], ["video summarization", "Task"]], "rel": [["salient object detection", "SubTask-Of", "saliency prediction"], ["salient object detection", "SubTask-Of", "computer vision"], ["content - aware image editing", "SubTask-Of", "image processing"], ["object detection", "SubTask-Of", "image processing"], ["image classification", "SubTask-Of", "image processing"], ["person re - identification", "SubTask-Of", "image processing"], ["video summarization", "SubTask-Of", "image processing"]], "rel_plus": [["salient object detection:Task", "SubTask-Of", "saliency prediction:Task"], ["salient object detection:Task", "SubTask-Of", "computer vision:Task"], ["content - aware image editing:Task", "SubTask-Of", "image processing:Task"], ["object detection:Task", "SubTask-Of", "image processing:Task"], ["image classification:Task", "SubTask-Of", "image processing:Task"], ["person re - identification:Task", "SubTask-Of", "image processing:Task"], ["video summarization:Task", "SubTask-Of", "image processing:Task"]]}
{"doc_id": "67855714", "sentence": "In particular , we show on a dataset of satellite images ( different from the ImageNet domain ) that our method P sisr /adv , rec outperforms the SRGAN method [ 1 ] by a large margin on the considered domain .", "ner": [["satellite images", "Dataset"], ["ImageNet", "Dataset"], ["SRGAN", "Method"]], "rel": [["SRGAN", "Evaluated-With", "satellite images"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "satellite images:Dataset"]]}
{"doc_id": "210839545", "sentence": "On ADE 2 0 K , we compare our evaluated GPSNet with three attention - based method , i.e. , PSANet [ 3 4 ] , EncNet [ 3 1 ] , OCNet [ 3 0 ] , and region - based method , i.e. , PSPNet .", "ner": [["ADE 2 0 K", "Dataset"], ["GPSNet", "Method"], ["attention - based method", "Method"], ["PSANet", "Method"], ["EncNet", "Method"], ["OCNet", "Method"], ["region - based method", "Method"], ["PSPNet", "Method"]], "rel": [["GPSNet", "Evaluated-With", "ADE 2 0 K"], ["GPSNet", "Compare-With", "attention - based method"], ["PSANet", "SubClass-Of", "attention - based method"], ["EncNet", "SubClass-Of", "attention - based method"], ["OCNet", "SubClass-Of", "attention - based method"], ["GPSNet", "Compare-With", "PSANet"], ["GPSNet", "Compare-With", "EncNet"], ["GPSNet", "Compare-With", "OCNet"], ["PSPNet", "SubClass-Of", "region - based method"], ["GPSNet", "Compare-With", "region - based method"], ["GPSNet", "Compare-With", "PSPNet"]], "rel_plus": [["GPSNet:Method", "Evaluated-With", "ADE 2 0 K:Dataset"], ["GPSNet:Method", "Compare-With", "attention - based method:Method"], ["PSANet:Method", "SubClass-Of", "attention - based method:Method"], ["EncNet:Method", "SubClass-Of", "attention - based method:Method"], ["OCNet:Method", "SubClass-Of", "attention - based method:Method"], ["GPSNet:Method", "Compare-With", "PSANet:Method"], ["GPSNet:Method", "Compare-With", "EncNet:Method"], ["GPSNet:Method", "Compare-With", "OCNet:Method"], ["PSPNet:Method", "SubClass-Of", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "region - based method:Method"], ["GPSNet:Method", "Compare-With", "PSPNet:Method"]]}
{"doc_id": "204901567", "sentence": "Our classification experiments demonstrate that MONOTRANS is competitive with JOINTMULTI and JOINTPAIR , despite being multilingual at the embedding layer only ( i.e. the transformer body is trained exclusively on English ) .", "ner": [["classification", "Task"], ["MONOTRANS", "Method"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"], ["transformer", "Method"]], "rel": [["MONOTRANS", "Used-For", "classification"], ["JOINTMULTI", "Used-For", "classification"], ["JOINTPAIR", "Used-For", "classification"], ["MONOTRANS", "Compare-With", "JOINTMULTI"], ["MONOTRANS", "Compare-With", "JOINTPAIR"]], "rel_plus": [["MONOTRANS:Method", "Used-For", "classification:Task"], ["JOINTMULTI:Method", "Used-For", "classification:Task"], ["JOINTPAIR:Method", "Used-For", "classification:Task"], ["MONOTRANS:Method", "Compare-With", "JOINTMULTI:Method"], ["MONOTRANS:Method", "Compare-With", "JOINTPAIR:Method"]]}
{"doc_id": "11241677", "sentence": "Since training batch SVMs on such a large dataset is impossible , we use the online SVM approach .", "ner": [["SVMs", "Method"], ["SVM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860962", "sentence": "The great majority of existing domain adaptation models rely on image translation networks , which often contain a huge amount of domain - specific parameters .", "ner": [["domain adaptation models", "Method"], ["image translation networks", "Method"]], "rel": [["image translation networks", "Used-For", "domain adaptation models"]], "rel_plus": [["image translation networks:Method", "Used-For", "domain adaptation models:Method"]]}
{"doc_id": "52180375", "sentence": "We make the code and trained model publicly available at https://github.com/junfu 1 1 1 5 /DANet Scene segmentation is a fundamental and challenging problem , whose goal is to segment and parse a scene image into different image regions associated with semantic categories including stuff ( e.g. sky , road , grass ) and discrete objects ( e.g. person , car , bicycle ) .", "ner": [["Scene segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "202888751", "sentence": "We use stochastic gradient descent optimization with Nesterov momentum updates ( Nesterov , 1 9 8 3 ) .", "ner": [["stochastic gradient descent", "Method"], ["Nesterov momentum", "Method"]], "rel": [["Nesterov momentum", "Part-Of", "stochastic gradient descent"]], "rel_plus": [["Nesterov momentum:Method", "Part-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "4539700", "sentence": "Following the significant advances in image recognition made by 2D CNNs and ImageNet , pretrained 2D CNNs on Ima - geNet experienced significant progress in various tasks such as object detection , semantic segmentation , and image captioning .", "ner": [["image recognition", "Task"], ["2D CNNs", "Method"], ["ImageNet", "Dataset"], ["2D CNNs", "Method"], ["Ima - geNet", "Dataset"], ["object detection", "Task"], ["semantic segmentation", "Task"], ["image captioning", "Task"]], "rel": [["2D CNNs", "Used-For", "image recognition"], ["ImageNet", "Benchmark-For", "image recognition"], ["2D CNNs", "Trained-With", "Ima - geNet"], ["2D CNNs", "Used-For", "object detection"], ["2D CNNs", "Used-For", "semantic segmentation"], ["2D CNNs", "Used-For", "image captioning"]], "rel_plus": [["2D CNNs:Method", "Used-For", "image recognition:Task"], ["ImageNet:Dataset", "Benchmark-For", "image recognition:Task"], ["2D CNNs:Method", "Trained-With", "Ima - geNet:Dataset"], ["2D CNNs:Method", "Used-For", "object detection:Task"], ["2D CNNs:Method", "Used-For", "semantic segmentation:Task"], ["2D CNNs:Method", "Used-For", "image captioning:Task"]]}
{"doc_id": "146808333", "sentence": "MobileNetV 3 - Large is 3. 2 \\% more accurate on ImageNet classification while reducing latency by 1 5 \\% compared to MobileNetV 2 .", "ner": [["MobileNetV 3 - Large", "Method"], ["ImageNet", "Dataset"], ["MobileNetV 2 .", "Method"]], "rel": [["MobileNetV 3 - Large", "Evaluated-With", "ImageNet"], ["MobileNetV 3 - Large", "Compare-With", "MobileNetV 2 ."]], "rel_plus": [["MobileNetV 3 - Large:Method", "Evaluated-With", "ImageNet:Dataset"], ["MobileNetV 3 - Large:Method", "Compare-With", "MobileNetV 2 .:Method"]]}
{"doc_id": "199543700", "sentence": "All these results demonstrate the competing ability of the proposed SGGAN approach as a whole system over the leading VGG and ResNet networks on image recognition tasks such as face attribute recognition .", "ner": [["SGGAN", "Method"], ["VGG", "Method"], ["ResNet", "Method"], ["image recognition", "Task"], ["face attribute recognition", "Task"]], "rel": [["SGGAN", "Compare-With", "VGG"], ["SGGAN", "Compare-With", "ResNet"], ["SGGAN", "Used-For", "image recognition"], ["VGG", "Used-For", "image recognition"], ["ResNet", "Used-For", "image recognition"], ["SGGAN", "Used-For", "face attribute recognition"], ["VGG", "Used-For", "face attribute recognition"], ["ResNet", "Used-For", "face attribute recognition"]], "rel_plus": [["SGGAN:Method", "Compare-With", "VGG:Method"], ["SGGAN:Method", "Compare-With", "ResNet:Method"], ["SGGAN:Method", "Used-For", "image recognition:Task"], ["VGG:Method", "Used-For", "image recognition:Task"], ["ResNet:Method", "Used-For", "image recognition:Task"], ["SGGAN:Method", "Used-For", "face attribute recognition:Task"], ["VGG:Method", "Used-For", "face attribute recognition:Task"], ["ResNet:Method", "Used-For", "face attribute recognition:Task"]]}
{"doc_id": "211020570", "sentence": "Recently , CNNs have made a series of breakthroughs in many visual analysis tasks such as image classification [ 2 6 ] , semantic segmentation [ 5 ] , and human pose estimation [ 6 ] .", "ner": [["CNNs", "Method"], ["visual analysis tasks", "Task"], ["image classification", "Task"], ["semantic segmentation", "Task"], ["human pose estimation", "Task"]], "rel": [["image classification", "SubTask-Of", "visual analysis tasks"], ["semantic segmentation", "SubTask-Of", "visual analysis tasks"], ["human pose estimation", "SubTask-Of", "visual analysis tasks"], ["CNNs", "Used-For", "visual analysis tasks"], ["CNNs", "Used-For", "image classification"], ["CNNs", "Used-For", "semantic segmentation"], ["CNNs", "Used-For", "human pose estimation"]], "rel_plus": [["image classification:Task", "SubTask-Of", "visual analysis tasks:Task"], ["semantic segmentation:Task", "SubTask-Of", "visual analysis tasks:Task"], ["human pose estimation:Task", "SubTask-Of", "visual analysis tasks:Task"], ["CNNs:Method", "Used-For", "visual analysis tasks:Task"], ["CNNs:Method", "Used-For", "image classification:Task"], ["CNNs:Method", "Used-For", "semantic segmentation:Task"], ["CNNs:Method", "Used-For", "human pose estimation:Task"]]}
{"doc_id": "AUG078", "sentence": "Graph-based recommendation systems are enhanced by GraphSAGE on MovieLens.", "ner": [["graph-based recommendation systems", "Task"], ["GraphSAGE", "Method"], ["MovieLens", "Dataset"]], "rel": [["GraphSAGE", "Used-For", "graph-based recommendation systems"], ["GraphSAGE", "Trained-With", "MovieLens"]], "rel_plus": [["GraphSAGE:Method", "Used-For", "graph-based recommendation systems:Task"], ["GraphSAGE:Method", "Trained-With", "MovieLens:Dataset"]]}
{"doc_id": "AUG023", "sentence": "The context-augmented feature learning approach enhances anomaly detection on KDD Cup 99.", "ner": [["context-augmented feature learning", "Method"], ["anomaly detection", "Task"], ["KDD Cup 99", "Dataset"]], "rel": [["context-augmented feature learning", "Used-For", "anomaly detection"], ["context-augmented feature learning", "Evaluated-With", "KDD Cup 99"]], "rel_plus": [["context-augmented feature learning:Method", "Used-For", "anomaly detection:Task"], ["context-augmented feature learning:Method", "Evaluated-With", "KDD Cup 99:Dataset"]]}
{"doc_id": "204402755", "sentence": "At the same time , much of the state - of - the - art performance on highly contested benchmarks - such as the image classification dataset ImageNet - have been produced with accelerated SGD ( Krizhevsky et al. , 2 0 1 2 ; He et al. , 2 0 1 6 ; Xie et al. , 2 0 1 7 ; Zagoruyko & Komodakis , 2 0 1 6 ; Huang et al. , 2 0 1 7 ; Ren et al. , 2 0 1 5 ; Howard et al. , 2 0 1 7 ) .", "ner": [["image classification", "Task"], ["ImageNet", "Dataset"], ["SGD", "Method"]], "rel": [["ImageNet", "Benchmark-For", "image classification"], ["SGD", "Used-For", "image classification"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image classification:Task"], ["SGD:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "195347056", "sentence": "The source code and models are available at : https://github.com/cs - chan/ArtGAN Recently , Goodfellow et al. [ 1 ] introduced an interesting features learning method via adversarial training a deep generative model , called the Generative Adversarial Networks ( GAN ) .", "ner": [["chan/ArtGAN", "Method"], ["Generative Adversarial Networks", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "150374036", "sentence": "To evaluate our method , we use four popular in - thewild facial expression datasets , namely FERPlus [ 5 ] , Affect - Net [ 4 8 ] , RAF - DB [ 3 4 ] , and SFEW [ 1 5 ] .", "ner": [["in - thewild facial expression", "Task"], ["FERPlus", "Dataset"], ["Affect - Net", "Dataset"], ["RAF - DB", "Dataset"], ["SFEW", "Dataset"]], "rel": [["FERPlus", "Benchmark-For", "in - thewild facial expression"], ["Affect - Net", "Benchmark-For", "in - thewild facial expression"], ["RAF - DB", "Benchmark-For", "in - thewild facial expression"], ["SFEW", "Benchmark-For", "in - thewild facial expression"]], "rel_plus": [["FERPlus:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["Affect - Net:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["RAF - DB:Dataset", "Benchmark-For", "in - thewild facial expression:Task"], ["SFEW:Dataset", "Benchmark-For", "in - thewild facial expression:Task"]]}
{"doc_id": "52180375", "sentence": "Among the compared methods , DAG - RNN [ 1 8 ] utilizes chain - RNNs for 2D images to model rich spatial dependencies , and Ding et al. [ 6 ] adopts a gating mechanism in the decoder stage for improving inconspicuous objects Mean IoU% FCN - 8 s [ 1 3 ] 2 2 . 7 DeepLab - v 2 (Res 1 0 1 ) [ 3 ] 2 6 . 9 DAG - RNN [ 1 8 ] 3 1 . 2 RefineNet ( Res 1 0 1 ) [ 1 0 ] 3 3 . 6 Ding et al. ( Res 1 0 1 ) [ 6 ] 3 5 . 7 Dilated FCN ( Res 5 0 ) 3 1 . 9 DANet ( Res 5 0 ) 3 7 . 2 DANet ( Res 1 0 1 ) 3 9 . 7 and background stuff segmentation . our method could capture long - range contextual information more effectively and learn better feature representation in scene segmentation .", "ner": [["DAG - RNN", "Method"], ["RNNs", "Method"], ["FCN - 8 s", "Method"], ["DeepLab - v 2", "Method"], ["(Res 1 0 1", "Method"], ["DAG - RNN", "Method"], ["RefineNet", "Method"], ["Res 1 0 1", "Method"], ["Ding et al.", "Method"], ["Res 1 0 1", "Method"], ["Dilated FCN", "Method"], ["Res 5 0", "Method"], ["DANet", "Method"], ["Res 5 0", "Method"], ["DANet", "Method"], ["Res 1 0 1", "Method"], ["segmentation", "Task"], ["scene segmentation", "Task"]], "rel": [["(Res 1 0 1", "Part-Of", "DeepLab - v 2"], ["Res 1 0 1", "Part-Of", "RefineNet"], ["Res 1 0 1", "Part-Of", "Ding et al."], ["Res 5 0", "Part-Of", "Dilated FCN"], ["Res 5 0", "Part-Of", "DANet"], ["Res 1 0 1", "Part-Of", "DANet"]], "rel_plus": [["(Res 1 0 1:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 0 1:Method", "Part-Of", "RefineNet:Method"], ["Res 1 0 1:Method", "Part-Of", "Ding et al.:Method"], ["Res 5 0:Method", "Part-Of", "Dilated FCN:Method"], ["Res 5 0:Method", "Part-Of", "DANet:Method"], ["Res 1 0 1:Method", "Part-Of", "DANet:Method"]]}
{"doc_id": "51923817", "sentence": "Point Linking Network ( PLN ) is an one - stage detector without anchor boxes .", "ner": [["Point Linking Network", "Method"], ["PLN", "Method"]], "rel": [["PLN", "Synonym-Of", "Point Linking Network"]], "rel_plus": [["PLN:Method", "Synonym-Of", "Point Linking Network:Method"]]}
{"doc_id": "44148233", "sentence": "In 2 0 1 6 , to further motivate and challenge the academic and the tech industry research community , Microsoft started the Microsoft Research -Video to Text ( MSR - VTT ) [ 4 ] competition aiming at bringing together computer vision and language researchers .", "ner": [["Microsoft Research -Video to Text", "Dataset"], ["MSR - VTT", "Dataset"], ["computer vision", "Task"]], "rel": [["MSR - VTT", "Synonym-Of", "Microsoft Research -Video to Text"], ["Microsoft Research -Video to Text", "Benchmark-For", "computer vision"]], "rel_plus": [["MSR - VTT:Dataset", "Synonym-Of", "Microsoft Research -Video to Text:Dataset"], ["Microsoft Research -Video to Text:Dataset", "Benchmark-For", "computer vision:Task"]]}
{"doc_id": "201124533", "sentence": "In particular , under the Hybrid Task Cascade framework , the HRNet performs slightly worse than ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN for 2 0 e , but better for 2 8 e. This implies that our HRNet benefits more from longer training .", "ner": [["Task Cascade", "Method"], ["HRNet", "Method"], ["ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN", "Method"], ["HRNet", "Method"]], "rel": [["HRNet", "Compare-With", "ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN"]], "rel_plus": [["HRNet:Method", "Compare-With", "ResNeXt - 1 0 1 - 6 4 \u00d7 4 d - FPN:Method"]]}
{"doc_id": "24972096", "sentence": "Our method is faster than most state - of - the - art methods ( up to around 1 3 Hz using an i 7 eight - core PC with Titan X GPU ) and can be integrated into a SLAM system for near - real - time application in robotics .", "ner": [["SLAM", "Method"], ["robotics", "Task"]], "rel": [["SLAM", "Used-For", "robotics"]], "rel_plus": [["SLAM:Method", "Used-For", "robotics:Task"]]}
{"doc_id": "102351044", "sentence": "While for CNNs with drop - path , we again replace each convolutional layer with the proposed general convolutional building block in Section 3. 2 . 4 supporting drop - path with bottleneck and group convolution .", "ner": [["CNNs", "Method"], ["drop - path", "Method"], ["convolutional layer", "Method"], ["general convolutional building block", "Method"], ["drop - path", "Method"], ["bottleneck", "Method"], ["group convolution", "Method"]], "rel": [["drop - path", "Part-Of", "CNNs"], ["general convolutional building block", "Part-Of", "CNNs"], ["drop - path", "Part-Of", "general convolutional building block"], ["group convolution", "Part-Of", "general convolutional building block"], ["bottleneck", "Part-Of", "drop - path"]], "rel_plus": [["drop - path:Method", "Part-Of", "CNNs:Method"], ["general convolutional building block:Method", "Part-Of", "CNNs:Method"], ["drop - path:Method", "Part-Of", "general convolutional building block:Method"], ["group convolution:Method", "Part-Of", "general convolutional building block:Method"], ["bottleneck:Method", "Part-Of", "drop - path:Method"]]}
{"doc_id": "201070697", "sentence": "Inspired from human attention mechanism [ 1 8 ] , attentionbased models have gained popularity in a variety of computer vision and machine learning tasks including neural machine translation [ 1 9 ] , image classification [ 2 0 ] , [ 2 1 ] , image segmentation [ 2 2 ] , image and video captioning [ 2 3 ] , [ 2 4 ] and visual question answering [ 2 5 ] .", "ner": [["attention mechanism", "Method"], ["attentionbased models", "Method"], ["computer vision", "Task"], ["neural machine translation", "Task"], ["image classification", "Task"], ["image segmentation", "Task"], ["image and video captioning", "Task"], ["visual question answering", "Task"]], "rel": [["attentionbased models", "Used-For", "computer vision"], ["attentionbased models", "Used-For", "neural machine translation"], ["attentionbased models", "Used-For", "image classification"], ["attentionbased models", "Used-For", "image segmentation"], ["attentionbased models", "Used-For", "image and video captioning"], ["attentionbased models", "Used-For", "visual question answering"]], "rel_plus": [["attentionbased models:Method", "Used-For", "computer vision:Task"], ["attentionbased models:Method", "Used-For", "neural machine translation:Task"], ["attentionbased models:Method", "Used-For", "image classification:Task"], ["attentionbased models:Method", "Used-For", "image segmentation:Task"], ["attentionbased models:Method", "Used-For", "image and video captioning:Task"], ["attentionbased models:Method", "Used-For", "visual question answering:Task"]]}
{"doc_id": "102351044", "sentence": "Path level ( drop - path ) and layer level ( drop - layer ) dropout are proposed in FractalNet [ 2 0 ] and ResNet with Stochastic Depth [ 1 4 ] respectively .", "ner": [["drop - path", "Method"], ["drop - layer", "Method"], ["dropout", "Method"], ["FractalNet", "Method"], ["ResNet", "Method"], ["Stochastic Depth", "Method"]], "rel": [["drop - layer", "SubClass-Of", "dropout"], ["drop - path", "SubClass-Of", "dropout"], ["dropout", "Part-Of", "FractalNet"], ["dropout", "Part-Of", "ResNet"], ["dropout", "Part-Of", "Stochastic Depth"]], "rel_plus": [["drop - layer:Method", "SubClass-Of", "dropout:Method"], ["drop - path:Method", "SubClass-Of", "dropout:Method"], ["dropout:Method", "Part-Of", "FractalNet:Method"], ["dropout:Method", "Part-Of", "ResNet:Method"], ["dropout:Method", "Part-Of", "Stochastic Depth:Method"]]}
{"doc_id": "153312532", "sentence": "The contributions of our paper are as follows : \u2022 We propose a general solution to fine - tune the pre - trained BERT model , which includes three steps : ( 1 ) further pre - train BERT on within - task training data or in - domain data ; ( 2 ) optional fine - tuning BERT with multitask learning if several related tasks are available ; ( 3 ) fine - tune BERT for the target task . \u2022 We also investigate the fine - tuning methods for BERT on target task , including preprocess of long text , layer selection , layerwise learning rate , catastrophic forgetting , and low - shot learning problems . \u2022 We achieve the new state - of - the - art results on seven widely - studied English text classifica - tion datasets and one Chinese news classification dataset .", "ner": [["BERT", "Method"], ["BERT", "Method"], ["BERT", "Method"], ["multitask learning", "Method"], ["BERT", "Method"], ["BERT", "Method"], ["preprocess of long text", "Task"], ["layer selection", "Task"], ["layerwise learning rate", "Task"], ["catastrophic forgetting", "Task"], ["low - shot learning problems", "Task"], ["Chinese news classification", "Task"]], "rel": [["multitask learning", "Used-For", "BERT"], ["BERT", "Used-For", "preprocess of long text"], ["BERT", "Used-For", "layer selection"], ["BERT", "Used-For", "layerwise learning rate"], ["BERT", "Used-For", "catastrophic forgetting"], ["BERT", "Used-For", "low - shot learning problems"]], "rel_plus": [["multitask learning:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "preprocess of long text:Task"], ["BERT:Method", "Used-For", "layer selection:Task"], ["BERT:Method", "Used-For", "layerwise learning rate:Task"], ["BERT:Method", "Used-For", "catastrophic forgetting:Task"], ["BERT:Method", "Used-For", "low - shot learning problems:Task"]]}
{"doc_id": "211010520", "sentence": "The mean HIT completion times for BiDAF , BERT and RoBERTa are 5 5 1 . 8 s , 7 2 2 . 4 s and 6 8 6 . 4 s respectively .", "ner": [["HIT completion", "Task"], ["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BiDAF", "Used-For", "HIT completion"], ["BERT", "Used-For", "HIT completion"], ["RoBERTa", "Used-For", "HIT completion"]], "rel_plus": [["BiDAF:Method", "Used-For", "HIT completion:Task"], ["BERT:Method", "Used-For", "HIT completion:Task"], ["RoBERTa:Method", "Used-For", "HIT completion:Task"]]}
{"doc_id": "102351044", "sentence": "In this paper , we revisit this issue and examine various dropout variants in an attempt to improve existing dropout - based regularization techniques for CNNs .", "ner": [["dropout", "Method"], ["dropout - based regularization", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "SubClass-Of", "dropout - based regularization"], ["dropout - based regularization", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "SubClass-Of", "dropout - based regularization:Method"], ["dropout - based regularization:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "53719742", "sentence": "Instance segmentation is a challenging task because it requires the correct detection of all objects in an image while also precisely segmenting each instance .", "ner": [["Instance segmentation", "Task"], ["detection", "Task"]], "rel": [["detection", "SubTask-Of", "Instance segmentation"]], "rel_plus": [["detection:Task", "SubTask-Of", "Instance segmentation:Task"]]}
{"doc_id": "201124533", "sentence": "GFLOPs and # parameters of the models are given in R - CNN models for both our HRNetV 2 p and the ResNet on the public MMDetection platform [ 1 3 ] with the provided training setup , except that we use the learning rate schedule suggested in [ 3 7 ] for 2 \u00d7 , and FCOS [ 1 1 1 ] and CenterNet [ 2 7 ] from the implementations provided by the authors .", "ner": [["R - CNN", "Method"], ["HRNetV 2 p", "Method"], ["ResNet", "Method"], ["FCOS", "Method"], ["CenterNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "The word embedding dimension and hidden state dimension of LSTM are respectively set to 5 1 2 and 5 1 2 for attention based method , and the learning rate of attention based method is 0.0 0 0 1 .", "ner": [["word embedding", "Method"], ["LSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "24972096", "sentence": "DA - RNN employs a recurrent neural network to tightly combine the information contained in multiple viewpoints of an RGB - D video stream to improve the semantic segmentation performance .", "ner": [["DA - RNN", "Method"], ["recurrent neural network", "Method"], ["semantic segmentation", "Task"]], "rel": [["recurrent neural network", "Part-Of", "DA - RNN"], ["recurrent neural network", "Used-For", "semantic segmentation"], ["DA - RNN", "Used-For", "semantic segmentation"]], "rel_plus": [["recurrent neural network:Method", "Part-Of", "DA - RNN:Method"], ["recurrent neural network:Method", "Used-For", "semantic segmentation:Task"], ["DA - RNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "209862890", "sentence": "This can leverage both the pre - trained entity embeddings from BERT and the domain adaption capability of BERT via fine - tuning .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"], ["domain adaption", "Method"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"], ["BERT", "Used-For", "domain adaption"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"], ["BERT:Method", "Used-For", "domain adaption:Method"]]}
{"doc_id": "202734316", "sentence": "In [ 5 3 ] the novel concept of self - training is introduced , where target domain pseudo - labels are estimated under an iterative self - training framework with spatial priors , achieving state - of - the - art results on the S 2 R adaptation task on SYNTHIA and GTA to CS experiments .", "ner": [["self - training", "Method"], ["self - training", "Method"], ["S 2 R", "Task"], ["SYNTHIA", "Dataset"], ["GTA", "Dataset"], ["CS", "Dataset"]], "rel": [["SYNTHIA", "Benchmark-For", "S 2 R"], ["GTA", "Benchmark-For", "S 2 R"], ["CS", "Benchmark-For", "S 2 R"], ["self - training", "Used-For", "S 2 R"], ["self - training", "Evaluated-With", "SYNTHIA"], ["self - training", "Evaluated-With", "GTA"], ["self - training", "Evaluated-With", "CS"]], "rel_plus": [["SYNTHIA:Dataset", "Benchmark-For", "S 2 R:Task"], ["GTA:Dataset", "Benchmark-For", "S 2 R:Task"], ["CS:Dataset", "Benchmark-For", "S 2 R:Task"], ["self - training:Method", "Used-For", "S 2 R:Task"], ["self - training:Method", "Evaluated-With", "SYNTHIA:Dataset"], ["self - training:Method", "Evaluated-With", "GTA:Dataset"], ["self - training:Method", "Evaluated-With", "CS:Dataset"]]}
{"doc_id": "211020570", "sentence": "The work of [ 1 2 ] proposed a deep regression framework with two - stage reinitialization to address the problems of face image initialization and landmark detection .", "ner": [["deep regression framework", "Method"], ["two - stage reinitialization", "Method"], ["face image initialization", "Task"], ["landmark detection", "Task"]], "rel": [["two - stage reinitialization", "Part-Of", "deep regression framework"], ["deep regression framework", "Used-For", "face image initialization"], ["deep regression framework", "Used-For", "landmark detection"]], "rel_plus": [["two - stage reinitialization:Method", "Part-Of", "deep regression framework:Method"], ["deep regression framework:Method", "Used-For", "face image initialization:Task"], ["deep regression framework:Method", "Used-For", "landmark detection:Task"]]}
{"doc_id": "6116678", "sentence": "In this paper , we use a slightly modified version of the SLIC algorithm [ 2 ] , which uses geodesic image distance [ 9 ] during K - means clustering in the CIELab color space .", "ner": [["SLIC", "Method"], ["geodesic image distance", "Method"], ["K - means clustering", "Method"]], "rel": [["geodesic image distance", "Part-Of", "K - means clustering"]], "rel_plus": [["geodesic image distance:Method", "Part-Of", "K - means clustering:Method"]]}
{"doc_id": "59599694", "sentence": "However , the performance of C - CNN - LSTM - DA in peak hours is highly better than LSTM model .", "ner": [["C - CNN - LSTM - DA", "Method"], ["LSTM", "Method"]], "rel": [["C - CNN - LSTM - DA", "Compare-With", "LSTM"]], "rel_plus": [["C - CNN - LSTM - DA:Method", "Compare-With", "LSTM:Method"]]}
{"doc_id": "53731879", "sentence": "Although unsupervised domain adaptation for person re - ID has been studied extensively [ 8 , 4 0 , 3 9 , 4 9 ] recently , there is still more than 2 5 % and 1 5 % performance drop in mAP and Rank - 1 accuracy comparing to fully supervised baseline , as shown in Fig 1 .", "ner": [["unsupervised domain adaptation", "Method"], ["person re - ID", "Task"]], "rel": [["unsupervised domain adaptation", "Used-For", "person re - ID"]], "rel_plus": [["unsupervised domain adaptation:Method", "Used-For", "person re - ID:Task"]]}
{"doc_id": "147703932", "sentence": "In contrast , at the 3D level , depth estimation and human pose recovery benefit from segmentation , similarly to the two 2D tasks .", "ner": [["depth estimation", "Task"], ["human pose recovery", "Task"], ["segmentation", "Task"]], "rel": [["segmentation", "Used-For", "depth estimation"], ["segmentation", "Used-For", "human pose recovery"]], "rel_plus": [["segmentation:Task", "Used-For", "depth estimation:Task"], ["segmentation:Task", "Used-For", "human pose recovery:Task"]]}
{"doc_id": "202577400", "sentence": "It has been widely proven that modelling long - range dependencies in fully convolutional networks ( FCNs ) via global aggregation modules is critical for complex scene understanding tasks such as semantic segmentation and object detection .", "ner": [["fully convolutional networks", "Method"], ["FCNs", "Method"], ["scene understanding", "Task"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["FCNs", "Synonym-Of", "fully convolutional networks"], ["semantic segmentation", "SubTask-Of", "scene understanding"], ["object detection", "SubTask-Of", "scene understanding"], ["fully convolutional networks", "Used-For", "scene understanding"], ["fully convolutional networks", "Used-For", "semantic segmentation"], ["fully convolutional networks", "Used-For", "object detection"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "fully convolutional networks:Method"], ["semantic segmentation:Task", "SubTask-Of", "scene understanding:Task"], ["object detection:Task", "SubTask-Of", "scene understanding:Task"], ["fully convolutional networks:Method", "Used-For", "scene understanding:Task"], ["fully convolutional networks:Method", "Used-For", "semantic segmentation:Task"], ["fully convolutional networks:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "202577400", "sentence": "All previous work focus on global context modeling , our work also utilizes global information modeling but takes a further step to better distribute the global information to each position , and further improves GA modules on both detection and segmentation tasks .   Our method , Global Aggregation ( GA ) then Local Distribution ( LD ) , dubbed GALD , exploits the long - range contextual information of the feature F \u2208 R H \u00d7 W \u00d7C from a fully - convolution network ( FCN ) , and then adaptively distributed the global context to each spatial and channel position of the output feature , F GALD \u2208 R H \u00d7 W \u00d7C .", "ner": [["GA", "Method"], ["detection", "Task"], ["segmentation", "Task"], ["Global Aggregation", "Method"], ["GA", "Method"], ["Local Distribution", "Method"], ["LD", "Method"], ["GALD", "Method"], ["fully - convolution network", "Method"], ["FCN", "Method"], ["F GALD", "Method"]], "rel": [["GA", "Used-For", "detection"], ["GA", "Used-For", "segmentation"], ["GA", "Synonym-Of", "Global Aggregation"], ["LD", "Synonym-Of", "Local Distribution"], ["FCN", "Synonym-Of", "fully - convolution network"]], "rel_plus": [["GA:Method", "Used-For", "detection:Task"], ["GA:Method", "Used-For", "segmentation:Task"], ["GA:Method", "Synonym-Of", "Global Aggregation:Method"], ["LD:Method", "Synonym-Of", "Local Distribution:Method"], ["FCN:Method", "Synonym-Of", "fully - convolution network:Method"]]}
{"doc_id": "AUG032", "sentence": "Transformer improves machine translation on WMT 2018, not just a simple encoder.", "ner": [["Transformer", "Method"], ["machine translation", "Task"], ["WMT 2018", "Dataset"]], "rel": [["Transformer", "Used-For", "machine translation"], ["Transformer", "Evaluated-With", "WMT 2018"]], "rel_plus": [["Transformer:Method", "Used-For", "machine translation:Task"], ["Transformer:Method", "Evaluated-With", "WMT 2018:Dataset"]]}
{"doc_id": "202734316", "sentence": "The proven capability of deep convolutional neural networks ( CNNs ) to represent complex functions has been steadily pushing their adoption in many computer vision tasks .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "146120936", "sentence": "Other than the softmax function , we also test other alternatives in the kernel normalizer , such as sigmoid or sigmoid with normalization .", "ner": [["softmax", "Method"], ["kernel normalizer", "Method"], ["sigmoid", "Method"], ["sigmoid with normalization", "Method"]], "rel": [["sigmoid", "SubClass-Of", "kernel normalizer"], ["sigmoid with normalization", "SubClass-Of", "kernel normalizer"]], "rel_plus": [["sigmoid:Method", "SubClass-Of", "kernel normalizer:Method"], ["sigmoid with normalization:Method", "SubClass-Of", "kernel normalizer:Method"]]}
{"doc_id": "207880647", "sentence": "This is followed by elaborating on how we adapt GPT - 2 's language model , for question generation .", "ner": [["GPT - 2", "Method"], ["question generation", "Task"]], "rel": [["GPT - 2", "Used-For", "question generation"]], "rel_plus": [["GPT - 2:Method", "Used-For", "question generation:Task"]]}
{"doc_id": "209862890", "sentence": "To capture latent entity type information , we design a BERT - based entity similarity score \u03a8 BERT ( e , c ) .", "ner": [["BERT - based entity similarity score", "Method"], ["\u03a8 BERT ( e , c )", "Method"]], "rel": [["\u03a8 BERT ( e , c )", "Synonym-Of", "BERT - based entity similarity score"]], "rel_plus": [["\u03a8 BERT ( e , c ):Method", "Synonym-Of", "BERT - based entity similarity score:Method"]]}
{"doc_id": "202676714", "sentence": "The L \u221e norm of the perturbation \u03b5 was set to \u03b5 = 0. 3 for MNIST and FMNIST and \u03b5 = 8/ 2 5 5 for CIFAR 1 0 , CI - FAR 1 0 0 , and SVHN at training time .", "ner": [["MNIST", "Dataset"], ["FMNIST", "Dataset"], ["CIFAR 1 0", "Dataset"], ["CI - FAR 1 0 0", "Dataset"], ["SVHN", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "28984897", "sentence": "The evaluation is performed on two tasks , the MNIST handwritten digit recognition and the LFW face verification , using a LeNet - 5 and a VGG 1 6 network architecture .", "ner": [["MNIST", "Dataset"], ["handwritten digit recognition", "Task"], ["LFW", "Dataset"], ["face verification", "Task"], ["LeNet - 5", "Method"], ["VGG 1 6", "Method"]], "rel": [["VGG 1 6", "Evaluated-With", "MNIST"], ["MNIST", "Benchmark-For", "handwritten digit recognition"], ["VGG 1 6", "Used-For", "handwritten digit recognition"], ["LeNet - 5", "Evaluated-With", "LFW"], ["LFW", "Benchmark-For", "face verification"], ["LeNet - 5", "Used-For", "face verification"]], "rel_plus": [["VGG 1 6:Method", "Evaluated-With", "MNIST:Dataset"], ["MNIST:Dataset", "Benchmark-For", "handwritten digit recognition:Task"], ["VGG 1 6:Method", "Used-For", "handwritten digit recognition:Task"], ["LeNet - 5:Method", "Evaluated-With", "LFW:Dataset"], ["LFW:Dataset", "Benchmark-For", "face verification:Task"], ["LeNet - 5:Method", "Used-For", "face verification:Task"]]}
{"doc_id": "198147921", "sentence": "PASCAL 3 D+ [ 8 9 ] Vehicular and indoor objects ( augments the PASCAL VOC dataset [ 1 9 ] ) . 1 2 object categories with 3 , 0 0 0 instances per category .", "ner": [["PASCAL 3 D+", "Dataset"], ["PASCAL VOC", "Dataset"]], "rel": [["PASCAL 3 D+", "Compare-With", "PASCAL VOC"]], "rel_plus": [["PASCAL 3 D+:Dataset", "Compare-With", "PASCAL VOC:Dataset"]]}
{"doc_id": "208548469", "sentence": "Regarding the difference between METEOR and BLEU , METEOR evaluates the correlation at the sentence and segment level whereas BLEU looks for correlations at the corpus level .", "ner": [["METEOR", "Method"], ["BLEU", "Method"], ["METEOR", "Method"], ["BLEU", "Method"]], "rel": [["METEOR", "Compare-With", "BLEU"]], "rel_plus": [["METEOR:Method", "Compare-With", "BLEU:Method"]]}
{"doc_id": "102351044", "sentence": "We adopt widely benchmarked datasets CIFAR - 1 0 , CIFAR - 1 0 0 , SVHN and ImageNet where significant improvement in terms of accuracy is observed even with the presence of extensive data augmentation and batch normalization .", "ner": [["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0 0", "Dataset"], ["SVHN", "Dataset"], ["ImageNet", "Dataset"], ["data augmentation", "Method"], ["batch normalization", "Method"]], "rel": [["data augmentation", "Evaluated-With", "CIFAR - 1 0"], ["batch normalization", "Evaluated-With", "CIFAR - 1 0"], ["data augmentation", "Evaluated-With", "CIFAR - 1 0 0"], ["batch normalization", "Evaluated-With", "CIFAR - 1 0 0"], ["data augmentation", "Evaluated-With", "SVHN"], ["batch normalization", "Evaluated-With", "SVHN"], ["data augmentation", "Evaluated-With", "ImageNet"], ["batch normalization", "Evaluated-With", "ImageNet"]], "rel_plus": [["data augmentation:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["batch normalization:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"], ["data augmentation:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["batch normalization:Method", "Evaluated-With", "CIFAR - 1 0 0:Dataset"], ["data augmentation:Method", "Evaluated-With", "SVHN:Dataset"], ["batch normalization:Method", "Evaluated-With", "SVHN:Dataset"], ["data augmentation:Method", "Evaluated-With", "ImageNet:Dataset"], ["batch normalization:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "211010520", "sentence": "Remarkably , neither BERT or RoBERTa show a substantial drop when trained on D BiDAF compared to training on SQuAD data ( \u2212 2 . 0 F 1 , and \u2212 3 . 3 F 1 ): training these models on a dataset with a weaker model in the loop still leads to strong generalisation even to data from the original SQuAD distribution , which all models in the loop are trained on .", "ner": [["BERT", "Method"], ["RoBERTa", "Method"], ["D BiDAF", "Dataset"], ["SQuAD", "Dataset"], ["SQuAD", "Dataset"]], "rel": [["BERT", "Trained-With", "D BiDAF"], ["RoBERTa", "Trained-With", "D BiDAF"], ["BERT", "Trained-With", "SQuAD"], ["RoBERTa", "Trained-With", "SQuAD"], ["D BiDAF", "Compare-With", "SQuAD"]], "rel_plus": [["BERT:Method", "Trained-With", "D BiDAF:Dataset"], ["RoBERTa:Method", "Trained-With", "D BiDAF:Dataset"], ["BERT:Method", "Trained-With", "SQuAD:Dataset"], ["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"], ["D BiDAF:Dataset", "Compare-With", "SQuAD:Dataset"]]}
{"doc_id": "11241677", "sentence": "Starting from a number of well labeled small - scale datasets such as Caltech 1 0 1 / 2 5 6 [ 8 , 1 0 ] , MSRC [ 3 2 ] , PASCAL [ 7 ] , image understanding research has rapidly advanced to utilizing larger datasets such as ImageNet [ 6 ] and SUN [ 3 8 ] for the next generation of vision algorithms .", "ner": [["Caltech 1 0 1 / 2 5 6", "Dataset"], ["MSRC", "Dataset"], ["PASCAL", "Dataset"], ["image understanding", "Task"], ["ImageNet", "Dataset"], ["SUN", "Dataset"]], "rel": [["ImageNet", "Benchmark-For", "image understanding"], ["SUN", "Benchmark-For", "image understanding"]], "rel_plus": [["ImageNet:Dataset", "Benchmark-For", "image understanding:Task"], ["SUN:Dataset", "Benchmark-For", "image understanding:Task"]]}
{"doc_id": "52910494", "sentence": "That is , we have the final output of MCA module MC(x , W ): With maxout layer introduced , the whole MCA module can be viewed as a highly non - linear transformation between original input and the first dense block of DenseNets .", "ner": [["MCA", "Method"], ["MC(x , W ):", "Method"], ["maxout", "Method"], ["MCA", "Method"], ["dense block", "Method"], ["DenseNets", "Method"]], "rel": [["MC(x , W ):", "Synonym-Of", "MCA"], ["maxout", "Part-Of", "MCA"], ["dense block", "Part-Of", "DenseNets"], ["MCA", "Part-Of", "DenseNets"]], "rel_plus": [["MC(x , W )::Method", "Synonym-Of", "MCA:Method"], ["maxout:Method", "Part-Of", "MCA:Method"], ["dense block:Method", "Part-Of", "DenseNets:Method"], ["MCA:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "202577400", "sentence": "In our experiment , LD is verified on GA modules such as PSP [ 4 2 ] , ASPP [ 5 ] , Non - Local [ 2 8 ] and CGNL [ 3 6 ] , and achieves consistent performance improvement .", "ner": [["LD", "Method"], ["GA", "Method"], ["PSP", "Method"], ["ASPP", "Method"], ["Non - Local", "Method"], ["CGNL", "Method"]], "rel": [["PSP", "SubClass-Of", "GA"], ["ASPP", "SubClass-Of", "GA"], ["Non - Local", "SubClass-Of", "GA"], ["CGNL", "SubClass-Of", "GA"]], "rel_plus": [["PSP:Method", "SubClass-Of", "GA:Method"], ["ASPP:Method", "SubClass-Of", "GA:Method"], ["Non - Local:Method", "SubClass-Of", "GA:Method"], ["CGNL:Method", "SubClass-Of", "GA:Method"]]}
{"doc_id": "6116678", "sentence": "The fused saliency map from these two streams is further refined with a fully connected CRF for better spatial coherence and contour localization . \u2022 We propose a multi - scale fully convolutional network as the first stream in our deep contrast network to infer a pixel - level saliency map directly from the raw input image .", "ner": [["fused saliency map", "Method"], ["fully connected CRF", "Method"], ["spatial coherence", "Task"], ["contour localization", "Task"], ["multi - scale fully convolutional network", "Method"], ["deep contrast network", "Method"], ["pixel - level saliency map", "Task"]], "rel": [["fully connected CRF", "Part-Of", "fused saliency map"], ["fused saliency map", "Used-For", "spatial coherence"], ["fused saliency map", "Used-For", "contour localization"], ["multi - scale fully convolutional network", "Part-Of", "deep contrast network"], ["multi - scale fully convolutional network", "Used-For", "pixel - level saliency map"]], "rel_plus": [["fully connected CRF:Method", "Part-Of", "fused saliency map:Method"], ["fused saliency map:Method", "Used-For", "spatial coherence:Task"], ["fused saliency map:Method", "Used-For", "contour localization:Task"], ["multi - scale fully convolutional network:Method", "Part-Of", "deep contrast network:Method"], ["multi - scale fully convolutional network:Method", "Used-For", "pixel - level saliency map:Task"]]}
{"doc_id": "208202241", "sentence": "Experiments on MS - COCO and Charades have demonstrated the effectiveness of our proposed KS graph and KSSNet for both multi - label image and video recognition tasks .", "ner": [["MS - COCO", "Dataset"], ["Charades", "Dataset"], ["KS graph", "Method"], ["KSSNet", "Method"], ["multi - label image", "Task"], ["video recognition", "Task"]], "rel": [["KS graph", "Evaluated-With", "MS - COCO"], ["KSSNet", "Evaluated-With", "MS - COCO"], ["KS graph", "Evaluated-With", "Charades"], ["KSSNet", "Evaluated-With", "Charades"], ["KS graph", "Used-For", "multi - label image"], ["KSSNet", "Used-For", "multi - label image"], ["MS - COCO", "Benchmark-For", "multi - label image"], ["KSSNet", "Used-For", "video recognition"], ["KS graph", "Used-For", "video recognition"], ["Charades", "Benchmark-For", "video recognition"]], "rel_plus": [["KS graph:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KSSNet:Method", "Evaluated-With", "MS - COCO:Dataset"], ["KS graph:Method", "Evaluated-With", "Charades:Dataset"], ["KSSNet:Method", "Evaluated-With", "Charades:Dataset"], ["KS graph:Method", "Used-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "multi - label image:Task"], ["MS - COCO:Dataset", "Benchmark-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "video recognition:Task"], ["KS graph:Method", "Used-For", "video recognition:Task"], ["Charades:Dataset", "Benchmark-For", "video recognition:Task"]]}
{"doc_id": "204901567", "sentence": "More concretely , we first train a transformer - based masked language model on one language , and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective , freezing parameters of all other layers .", "ner": [["transformer - based masked language model", "Method"], ["masked language modeling objective", "Method"]], "rel": [["masked language modeling objective", "Part-Of", "transformer - based masked language model"]], "rel_plus": [["masked language modeling objective:Method", "Part-Of", "transformer - based masked language model:Method"]]}
{"doc_id": "211004033", "sentence": "Adapting convolutional neural networks ( CNNs ) for object detection [ 6 ] and instance segmentation ( e.g. , Mask R - CNN [ 7 ] ) to this canonical task is a promising way to extract object information .", "ner": [["convolutional neural networks", "Method"], ["CNNs", "Method"], ["object detection", "Task"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["Mask R - CNN", "SubClass-Of", "convolutional neural networks"], ["convolutional neural networks", "Used-For", "object detection"], ["convolutional neural networks", "Used-For", "instance segmentation"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["Mask R - CNN:Method", "SubClass-Of", "convolutional neural networks:Method"], ["convolutional neural networks:Method", "Used-For", "object detection:Task"], ["convolutional neural networks:Method", "Used-For", "instance segmentation:Task"]]}
{"doc_id": "211020570", "sentence": "In this model , the spatial transformer networks ( STNs ) is embedded as subnets at each stage .", "ner": [["spatial transformer networks", "Method"], ["STNs", "Method"]], "rel": [["STNs", "Synonym-Of", "spatial transformer networks"]], "rel_plus": [["STNs:Method", "Synonym-Of", "spatial transformer networks:Method"]]}
{"doc_id": "23569888", "sentence": "Interestingly however , when using less aggressive strategies such as the conservative fine - tuning or the feature extraction based classifier , modern networks such as GoogleNet and ResNet - 5 0 seem in general quite robust .", "ner": [["feature extraction based classifier", "Method"], ["GoogleNet", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["ResNet - 5 0", "SubClass-Of", "feature extraction based classifier"], ["GoogleNet", "SubClass-Of", "feature extraction based classifier"]], "rel_plus": [["ResNet - 5 0:Method", "SubClass-Of", "feature extraction based classifier:Method"], ["GoogleNet:Method", "SubClass-Of", "feature extraction based classifier:Method"]]}
{"doc_id": "202734254", "sentence": "Answers in QuAC is generally longer than answers in CoQA shown in Figure 2 , where the distribution implies that QuAC is more realistic than CoQA .", "ner": [["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"], ["CoQA", "Dataset"]], "rel": [["QuAC", "Compare-With", "CoQA"], ["QuAC", "Compare-With", "CoQA"]], "rel_plus": [["QuAC:Dataset", "Compare-With", "CoQA:Dataset"], ["QuAC:Dataset", "Compare-With", "CoQA:Dataset"]]}
{"doc_id": "3920676", "sentence": "We fix GOG features with the highest dimensionality in our evaluation framework as the feature extraction scheme and perform experiments with and without PCA .", "ner": [["GOG", "Method"], ["feature extraction", "Method"], ["PCA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208513596", "sentence": "Previous cross - modal self - supervised representation learning methods most relevant to our work include audio - visual correspondence [ 1 ] , audio - visual temporal synchronization [ 2 9 , 4 4 ] , and learning image representations using ambient sound [ 4 5 ] .", "ner": [["audio - visual correspondence", "Method"], ["audio - visual temporal synchronization", "Method"], ["learning image representations using ambient sound", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210713911", "sentence": "It uses reinforcement learning to choose a sequence of image augmentation operations with the best accuracy by searching in a discrete search space of their probability of application and magnitude .", "ner": [["reinforcement learning", "Method"], ["image augmentation", "Task"]], "rel": [["reinforcement learning", "Used-For", "image augmentation"]], "rel_plus": [["reinforcement learning:Method", "Used-For", "image augmentation:Task"]]}
{"doc_id": "210839545", "sentence": "The pioneering work FCN [ 1 7 ] proposed to remove the fully connected layers in classification CNN networks [ 1 5 , 2 3 ] , leading to a fully convolutional architecture for dense semantic segmentation .", "ner": [["FCN", "Method"], ["fully connected layers", "Method"], ["classification CNN", "Method"], ["fully convolutional architecture", "Method"], ["dense semantic segmentation", "Task"]], "rel": [["classification CNN", "Part-Of", "FCN"], ["fully connected layers", "Part-Of", "classification CNN"], ["fully convolutional architecture", "Used-For", "dense semantic segmentation"]], "rel_plus": [["classification CNN:Method", "Part-Of", "FCN:Method"], ["fully connected layers:Method", "Part-Of", "classification CNN:Method"], ["fully convolutional architecture:Method", "Used-For", "dense semantic segmentation:Task"]]}
{"doc_id": "146120936", "sentence": "We first evaluate our method by substituting the nearest neighbor interpolation in FPN with CARAFE for both Faster RCNN and Mask RCNN , and the deconvolution layer in the mask head for Mask RCNN .", "ner": [["nearest neighbor interpolation", "Method"], ["FPN", "Method"], ["CARAFE", "Method"], ["Faster RCNN", "Method"], ["Mask RCNN", "Method"], ["deconvolution", "Method"], ["Mask RCNN", "Method"]], "rel": [["CARAFE", "Part-Of", "FPN"], ["nearest neighbor interpolation", "Part-Of", "FPN"], ["FPN", "Part-Of", "Faster RCNN"], ["FPN", "Part-Of", "Mask RCNN"], ["deconvolution", "Part-Of", "Mask RCNN"], ["CARAFE", "Part-Of", "Mask RCNN"]], "rel_plus": [["CARAFE:Method", "Part-Of", "FPN:Method"], ["nearest neighbor interpolation:Method", "Part-Of", "FPN:Method"], ["FPN:Method", "Part-Of", "Faster RCNN:Method"], ["FPN:Method", "Part-Of", "Mask RCNN:Method"], ["deconvolution:Method", "Part-Of", "Mask RCNN:Method"], ["CARAFE:Method", "Part-Of", "Mask RCNN:Method"]]}
{"doc_id": "52180375", "sentence": "In short , these visualizations further demonstrate the necessity of capturing long - range dependencies for improving feature representation in scene segmentation . [ 1 5 ] 7 6 . 9 - [ 2 9 ] 7 8   We further compare our method with existing methods on the Cityscapes testing set .", "ner": [["scene segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209862890", "sentence": "Our model consists of two phrases : ( 1 ) Build entity embeddings from BERT ( 2 ) Add a BERT - based entity similarity component to the local model .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"], ["BERT - based entity similarity component", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "6116678", "sentence": "To obtain more robust features than handcrafted ones for salient object detection , deep convolutional neural networks ( CNNs ) have recently been employed , achieving substantially better results than previous state of the art [ 2 6 , 5 0 , 4 4 ] .", "ner": [["salient object detection", "Task"], ["deep convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["deep convolutional neural networks", "Used-For", "salient object detection"], ["CNNs", "Synonym-Of", "deep convolutional neural networks"]], "rel_plus": [["deep convolutional neural networks:Method", "Used-For", "salient object detection:Task"], ["CNNs:Method", "Synonym-Of", "deep convolutional neural networks:Method"]]}
{"doc_id": "210920315", "sentence": "Instead of performing an RoI pooling operation only on the high - level features ( e. g. , conv 5 of VGG ) , as in [ 5 1 ] , [ 5 2 ] , [ 3 3 ] , the PD branch in our PSC - Net utilizes a multi - level RoI pooling ( mRoI ) that concatenates features ( shallow and deep ) from all conv layers of the backbone .", "ner": [["RoI pooling operation", "Method"], ["conv 5", "Method"], ["VGG", "Method"], ["PD branch", "Method"], ["PSC - Net", "Method"], ["multi - level RoI pooling", "Method"], ["mRoI", "Method"], ["conv layers", "Method"]], "rel": [["PD branch", "Part-Of", "PSC - Net"], ["conv layers", "Part-Of", "PSC - Net"], ["multi - level RoI pooling", "Part-Of", "PSC - Net"], ["mRoI", "Synonym-Of", "multi - level RoI pooling"]], "rel_plus": [["PD branch:Method", "Part-Of", "PSC - Net:Method"], ["conv layers:Method", "Part-Of", "PSC - Net:Method"], ["multi - level RoI pooling:Method", "Part-Of", "PSC - Net:Method"], ["mRoI:Method", "Synonym-Of", "multi - level RoI pooling:Method"]]}
{"doc_id": "AUG059", "sentence": "KG-BERT enhances knowledge graph completion on Freebase, not a simple embedding.", "ner": [["KG-BERT", "Method"], ["knowledge graph completion", "Task"], ["Freebase", "Dataset"]], "rel": [["KG-BERT", "Used-For", "knowledge graph completion"], ["KG-BERT", "Trained-With", "Freebase"]], "rel_plus": [["KG-BERT:Method", "Used-For", "knowledge graph completion:Task"], ["KG-BERT:Method", "Trained-With", "Freebase:Dataset"]]}
{"doc_id": "21683040", "sentence": "Object detector with deep ConvNet Benefited from the power of Deep ConvNes , object detector such as Over - Feat [ 2 9 ] and R - CNN [ 9 ] have began to show the dramatic improvements in accuracy .", "ner": [["Object detector", "Method"], ["ConvNet", "Method"], ["Deep ConvNes", "Method"], ["object detector", "Method"], ["Over - Feat", "Method"], ["R - CNN", "Method"]], "rel": [["ConvNet", "Part-Of", "Object detector"], ["Over - Feat", "SubClass-Of", "object detector"], ["R - CNN", "SubClass-Of", "object detector"], ["Deep ConvNes", "Part-Of", "object detector"], ["Deep ConvNes", "Part-Of", "Over - Feat"], ["Deep ConvNes", "Part-Of", "R - CNN"]], "rel_plus": [["ConvNet:Method", "Part-Of", "Object detector:Method"], ["Over - Feat:Method", "SubClass-Of", "object detector:Method"], ["R - CNN:Method", "SubClass-Of", "object detector:Method"], ["Deep ConvNes:Method", "Part-Of", "object detector:Method"], ["Deep ConvNes:Method", "Part-Of", "Over - Feat:Method"], ["Deep ConvNes:Method", "Part-Of", "R - CNN:Method"]]}
{"doc_id": "210861282", "sentence": "UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage , with high accuracy , without relying on statistical postprocessing methods .", "ner": [["UniPose", "Method"], ["contextual segmentation", "Task"], ["joint localization", "Task"]], "rel": [["contextual segmentation", "Used-For", "UniPose"], ["joint localization", "Used-For", "UniPose"]], "rel_plus": [["contextual segmentation:Task", "Used-For", "UniPose:Method"], ["joint localization:Task", "Used-For", "UniPose:Method"]]}
{"doc_id": "52910494", "sentence": "Feeding the concatenation of four groups of convolutions , M 1 ( x , W ) , into DenseNets directly helps to improve the performance of the network since the network bandwidth is increased .", "ner": [["concatenation", "Method"], ["convolutions", "Method"], ["DenseNets", "Method"]], "rel": [["concatenation", "Used-For", "convolutions"], ["convolutions", "Part-Of", "DenseNets"], ["concatenation", "Part-Of", "DenseNets"]], "rel_plus": [["concatenation:Method", "Used-For", "convolutions:Method"], ["convolutions:Method", "Part-Of", "DenseNets:Method"], ["concatenation:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "210164920", "sentence": "Though sliding window approach is popular in object detection but it was missing in case of instance segmentation task .", "ner": [["object detection", "Task"], ["instance segmentation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "Some recent works [ 9 8 , 9 9 , 1 0 0 ] have used combination of encoder - decoder architecture and dilated convolution for better segmentation .", "ner": [["encoder - decoder", "Method"], ["dilated convolution", "Method"], ["segmentation", "Task"]], "rel": [["dilated convolution", "Used-For", "segmentation"], ["encoder - decoder", "Used-For", "segmentation"]], "rel_plus": [["dilated convolution:Method", "Used-For", "segmentation:Task"], ["encoder - decoder:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "The output of a panoptic segmentation model will contain two channels : one for pixel 's label ( semantic segmentation ) and another for predicting each pixel instance ( instance segmentation ) .   Convolutional neural network based image segmentation is a challenging work as it needs spatially variant features to preserve the context of a pixel for semantic labeling .", "ner": [["panoptic segmentation", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"], ["Convolutional neural network", "Method"], ["image segmentation", "Task"], ["semantic labeling", "Task"]], "rel": [["Convolutional neural network", "Used-For", "image segmentation"]], "rel_plus": [["Convolutional neural network:Method", "Used-For", "image segmentation:Task"]]}
{"doc_id": "207880647", "sentence": "For this purpose , we train a BERT QA model on generated questions , as well as combination of generated questions and groundtruth questions .", "ner": [["BERT QA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "Faster R - CNN [ 2 4 ] is an improved version of R - CNN [ 9 ] and Fast R - CNN [ 8 ] .", "ner": [["Faster R - CNN", "Method"], ["R - CNN", "Method"], ["Fast R - CNN", "Method"]], "rel": [["Faster R - CNN", "SubClass-Of", "R - CNN"], ["Faster R - CNN", "SubClass-Of", "Fast R - CNN"]], "rel_plus": [["Faster R - CNN:Method", "SubClass-Of", "R - CNN:Method"], ["Faster R - CNN:Method", "SubClass-Of", "Fast R - CNN:Method"]]}
{"doc_id": "202888986", "sentence": "Different from our observations , Dehghani et al. ( 2 0 1 8) show that networks with cross - layer parameter sharing ( Universal Transformer , UT ) get better performance on language modeling and subject - verb agreement than the standard transformer .", "ner": [["Universal Transformer", "Method"], ["UT", "Method"], ["language modeling", "Task"], ["subject - verb agreement", "Task"], ["transformer", "Method"]], "rel": [["UT", "Synonym-Of", "Universal Transformer"], ["Universal Transformer", "Used-For", "language modeling"], ["transformer", "Used-For", "language modeling"], ["Universal Transformer", "Used-For", "subject - verb agreement"], ["transformer", "Used-For", "subject - verb agreement"], ["Universal Transformer", "Compare-With", "transformer"]], "rel_plus": [["UT:Method", "Synonym-Of", "Universal Transformer:Method"], ["Universal Transformer:Method", "Used-For", "language modeling:Task"], ["transformer:Method", "Used-For", "language modeling:Task"], ["Universal Transformer:Method", "Used-For", "subject - verb agreement:Task"], ["transformer:Method", "Used-For", "subject - verb agreement:Task"], ["Universal Transformer:Method", "Compare-With", "transformer:Method"]]}
{"doc_id": "44148233", "sentence": "Thanks to the release of benchmark datasets MS COCO [ 9 9 ] and Flicker 3 0 k [ 1 8 0 ] , research in image captioning and retrieval [ 4 5 ] , [ 4 9 ] , [ 8 2 ] , [ 1 0 4 ] , and image question answering [ 1 8 ] , [ 1 0 3 ] , [ 1 2 7 ] , [ 1 8 4 ] has also become very active .", "ner": [["MS COCO", "Dataset"], ["Flicker 3 0 k", "Dataset"], ["image captioning", "Task"], ["retrieval", "Task"], ["image question answering", "Task"]], "rel": [["MS COCO", "Benchmark-For", "image captioning"], ["Flicker 3 0 k", "Benchmark-For", "image captioning"], ["MS COCO", "Benchmark-For", "retrieval"], ["Flicker 3 0 k", "Benchmark-For", "retrieval"], ["MS COCO", "Benchmark-For", "image question answering"], ["Flicker 3 0 k", "Benchmark-For", "image question answering"]], "rel_plus": [["MS COCO:Dataset", "Benchmark-For", "image captioning:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "image captioning:Task"], ["MS COCO:Dataset", "Benchmark-For", "retrieval:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "retrieval:Task"], ["MS COCO:Dataset", "Benchmark-For", "image question answering:Task"], ["Flicker 3 0 k:Dataset", "Benchmark-For", "image question answering:Task"]]}
{"doc_id": "56657874", "sentence": "We propose a deep domain generalization framework using synthetic data generated by a GAN where the styles are transferred from one domain into another domain .", "ner": [["deep domain generalization", "Method"], ["GAN", "Method"]], "rel": [["GAN", "Part-Of", "deep domain generalization"]], "rel_plus": [["GAN:Method", "Part-Of", "deep domain generalization:Method"]]}
{"doc_id": "199668978", "sentence": "The widely - used context - free models such as Skip - gram [ 1 9 ] or GloVe [ 2 6 ] can not consider the correlation between words .", "ner": [["context - free models", "Method"], ["Skip - gram", "Method"], ["GloVe", "Method"]], "rel": [["Skip - gram", "SubClass-Of", "context - free models"], ["GloVe", "SubClass-Of", "context - free models"]], "rel_plus": [["Skip - gram:Method", "SubClass-Of", "context - free models:Method"], ["GloVe:Method", "SubClass-Of", "context - free models:Method"]]}
{"doc_id": "211020570", "sentence": "Valle et al. [ 3 5 ] combined a CNN and ensemble of regression trees ( ERT ) to enhance computational efficiency .", "ner": [["CNN", "Method"], ["ensemble of regression trees", "Method"], ["ERT", "Method"]], "rel": [["ERT", "Synonym-Of", "ensemble of regression trees"]], "rel_plus": [["ERT:Method", "Synonym-Of", "ensemble of regression trees:Method"]]}
{"doc_id": "209532167", "sentence": "These pre - training objectives , along with the use of a Transformer - based architecture , gave BERT an accuracy boost in a number of NLP tasks over the state - of - the - art .", "ner": [["Transformer - based architecture", "Method"], ["BERT", "Method"], ["NLP", "Task"]], "rel": [["Transformer - based architecture", "Used-For", "BERT"], ["BERT", "Used-For", "NLP"]], "rel_plus": [["Transformer - based architecture:Method", "Used-For", "BERT:Method"], ["BERT:Method", "Used-For", "NLP:Task"]]}
{"doc_id": "210860962", "sentence": "The discriminators D follow the PatchGAN architecture [ 2 5 ] with 4 downsampling blocks and a fully connected layer with a single output .", "ner": [["discriminators D", "Method"], ["PatchGAN", "Method"], ["downsampling blocks", "Method"], ["fully connected layer", "Method"]], "rel": [["downsampling blocks", "Part-Of", "discriminators D"], ["fully connected layer", "Part-Of", "discriminators D"], ["discriminators D", "Part-Of", "PatchGAN"]], "rel_plus": [["downsampling blocks:Method", "Part-Of", "discriminators D:Method"], ["fully connected layer:Method", "Part-Of", "discriminators D:Method"], ["discriminators D:Method", "Part-Of", "PatchGAN:Method"]]}
{"doc_id": "210860962", "sentence": "For this particular reason and to highlight the benefit of having domain specific encoder parameters in a single image translation network , we define this experiment to compare and contrast the results achieved by CycleGAN ( Section III - B ) w.r.t . the ones achieved by StarGAN ( Section III - C ) .", "ner": [["image translation network", "Method"], ["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [["image translation network", "Compare-With", "CycleGAN"]], "rel_plus": [["image translation network:Method", "Compare-With", "CycleGAN:Method"]]}
{"doc_id": "210860760", "sentence": "Moreover , the results of experiments in Figure 2 2 : The value of F 1 - score which is achieved by ExEm along with the other two related approaches on Scopus dataset for recommendation task for three topics IE , NLP and M. section of 7 is an important evidence for usefulness of the prepared data for multi - label classification , link prediction and expert recommendation tasks . \u2022 A 2 .", "ner": [["ExEm", "Method"], ["Scopus dataset", "Dataset"], ["recommendation", "Task"], ["IE", "Task"], ["NLP", "Task"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"]], "rel": [["ExEm", "Evaluated-With", "Scopus dataset"], ["Scopus dataset", "Benchmark-For", "recommendation"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Evaluated-With", "Scopus dataset:Dataset"], ["Scopus dataset:Dataset", "Benchmark-For", "recommendation:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "203593581", "sentence": "To provide a refined gradient for the clipping threshold , we design a Reparameterized Clipping Function ( RCF ) a\u015d Instead of directly clipping them to [ \u2212\u03b1 , \u03b1 ] , RCF outputs a constant clipping range and rescales weights back after the projection , which is mathematically equivalent to Equation ( 1 ) during forward .", "ner": [["Reparameterized Clipping Function", "Method"], ["RCF", "Method"], ["RCF", "Method"]], "rel": [["RCF", "Synonym-Of", "Reparameterized Clipping Function"]], "rel_plus": [["RCF:Method", "Synonym-Of", "Reparameterized Clipping Function:Method"]]}
{"doc_id": "201070522", "sentence": "InferSent and BERT reach comparable values to the best Word 2 Vec models for image - sentence retrieval on Flickr 3 0 K , performing more poorly for the MSCOCO dataset .", "ner": [["InferSent", "Method"], ["BERT", "Method"], ["Word 2 Vec", "Method"], ["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"], ["MSCOCO", "Dataset"]], "rel": [["BERT", "Used-For", "image - sentence retrieval"], ["InferSent", "Used-For", "image - sentence retrieval"], ["Word 2 Vec", "Used-For", "image - sentence retrieval"], ["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"], ["BERT", "Evaluated-With", "Flickr 3 0 K"], ["InferSent", "Evaluated-With", "Flickr 3 0 K"], ["Word 2 Vec", "Evaluated-With", "Flickr 3 0 K"], ["InferSent", "Evaluated-With", "MSCOCO"], ["BERT", "Evaluated-With", "MSCOCO"], ["Word 2 Vec", "Evaluated-With", "MSCOCO"]], "rel_plus": [["BERT:Method", "Used-For", "image - sentence retrieval:Task"], ["InferSent:Method", "Used-For", "image - sentence retrieval:Task"], ["Word 2 Vec:Method", "Used-For", "image - sentence retrieval:Task"], ["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"], ["BERT:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["InferSent:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["Word 2 Vec:Method", "Evaluated-With", "Flickr 3 0 K:Dataset"], ["InferSent:Method", "Evaluated-With", "MSCOCO:Dataset"], ["BERT:Method", "Evaluated-With", "MSCOCO:Dataset"], ["Word 2 Vec:Method", "Evaluated-With", "MSCOCO:Dataset"]]}
{"doc_id": "198231883", "sentence": "The task of Video Instance Segmentation ( VIS ) is very similar to UVOS , however in VIS the objects to be tracked must be classified into a set of predefined classes rather than just being salient throughout a video .", "ner": [["Video Instance Segmentation", "Task"], ["VIS", "Task"], ["UVOS", "Task"], ["VIS", "Task"]], "rel": [["VIS", "Synonym-Of", "Video Instance Segmentation"], ["Video Instance Segmentation", "Compare-With", "UVOS"]], "rel_plus": [["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"], ["Video Instance Segmentation:Task", "Compare-With", "UVOS:Task"]]}
{"doc_id": "6423078", "sentence": "Prior to this explosive development in deep learning , the Structured Edges method ( typically abbreviated SE ) ( Doll\u00e1r and Zitnick 2 0 1 5 ) emerged as one of the most celebrated systems for edge detection , thanks to its state - of - the - art performance on the BSDS 5 0 0 dataset ( Martin et al. 2 0 0 4 ; Arbelaez et al. 2 0 1 1 ) ( with , e.g. , F - score of 0. 7 4 6 ) and its practically significant speed of 2. 5 frames per second .", "ner": [["deep learning", "Method"], ["Structured Edges", "Method"], ["SE", "Method"], ["edge detection", "Task"], ["BSDS 5 0 0", "Dataset"]], "rel": [["SE", "Synonym-Of", "Structured Edges"], ["deep learning", "Used-For", "edge detection"], ["Structured Edges", "Used-For", "edge detection"], ["BSDS 5 0 0", "Benchmark-For", "edge detection"], ["Structured Edges", "Evaluated-With", "BSDS 5 0 0"]], "rel_plus": [["SE:Method", "Synonym-Of", "Structured Edges:Method"], ["deep learning:Method", "Used-For", "edge detection:Task"], ["Structured Edges:Method", "Used-For", "edge detection:Task"], ["BSDS 5 0 0:Dataset", "Benchmark-For", "edge detection:Task"], ["Structured Edges:Method", "Evaluated-With", "BSDS 5 0 0:Dataset"]]}
{"doc_id": "AUG071", "sentence": "The 4-layer deep clustering network optimizes clustering on MNIST.", "ner": [["4-layer deep clustering network", "Method"], ["clustering", "Task"], ["MNIST", "Dataset"]], "rel": [["4-layer deep clustering network", "Used-For", "clustering"], ["4-layer deep clustering network", "Trained-With", "MNIST"]], "rel_plus": [["4-layer deep clustering network:Method", "Used-For", "clustering:Task"], ["4-layer deep clustering network:Method", "Trained-With", "MNIST:Dataset"]]}
{"doc_id": "203593581", "sentence": "We compare the Quantization Error Minimization ( QEM ) method with our RCF on the quantized ResNet - 1 8 model .", "ner": [["Quantization Error Minimization", "Method"], ["QEM", "Method"], ["RCF", "Method"], ["ResNet - 1 8", "Method"]], "rel": [["QEM", "Synonym-Of", "Quantization Error Minimization"], ["Quantization Error Minimization", "Compare-With", "RCF"], ["Quantization Error Minimization", "Used-For", "ResNet - 1 8"], ["RCF", "Used-For", "ResNet - 1 8"]], "rel_plus": [["QEM:Method", "Synonym-Of", "Quantization Error Minimization:Method"], ["Quantization Error Minimization:Method", "Compare-With", "RCF:Method"], ["Quantization Error Minimization:Method", "Used-For", "ResNet - 1 8:Method"], ["RCF:Method", "Used-For", "ResNet - 1 8:Method"]]}
{"doc_id": "210164920", "sentence": "In papers [ 1 3 6 ] , [ 1 3 7 ] , [ 8 0 ] , [ 1 7 ] , [ 1 3 8 ] , [ 1 2 8 ] , [ 7 8 ] , [ 1 3 9 ] researchers used contextual information and low level features into CNN in various ways for better segmentation .", "ner": [["CNN", "Method"], ["segmentation", "Task"]], "rel": [["CNN", "Used-For", "segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "195347056", "sentence": "This paper proposes a series of new approaches to improve Generative Adversarial Network ( GAN ) for conditional image synthesis and we name the proposed model as ArtGAN .", "ner": [["Generative Adversarial Network", "Method"], ["GAN", "Method"], ["conditional image synthesis", "Task"], ["ArtGAN", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Network"], ["ArtGAN", "SubClass-Of", "Generative Adversarial Network"], ["Generative Adversarial Network", "Used-For", "conditional image synthesis"], ["ArtGAN", "Used-For", "conditional image synthesis"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Network:Method"], ["ArtGAN:Method", "SubClass-Of", "Generative Adversarial Network:Method"], ["Generative Adversarial Network:Method", "Used-For", "conditional image synthesis:Task"], ["ArtGAN:Method", "Used-For", "conditional image synthesis:Task"]]}
{"doc_id": "209532167", "sentence": "Additionally , to further evaluate whether LSTMs alone are sufficient without pre - training , we try initializing the BiLSTM with an embedding matrix that is trained from scratch .", "ner": [["LSTMs", "Method"], ["BiLSTM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Hierarchical softmax with a binary - tree structure operates over the full vertex set to approximate the normalizing factor [ 5 7 ] . \u2022 Node 2 vec [ 5 8 ] : Node 2 vec is the extended version of DeepWalk with a more elaborate random walk .", "ner": [["Hierarchical softmax", "Method"], ["Node 2 vec", "Method"], ["Node 2 vec", "Method"], ["DeepWalk", "Method"], ["random walk", "Method"]], "rel": [["random walk", "Part-Of", "Node 2 vec"], ["Node 2 vec", "SubClass-Of", "DeepWalk"]], "rel_plus": [["random walk:Method", "Part-Of", "Node 2 vec:Method"], ["Node 2 vec:Method", "SubClass-Of", "DeepWalk:Method"]]}
{"doc_id": "53719742", "sentence": "After the Fast R - CNN step , quadrilateral bounding boxes of detected text instances are predicted and suppressed by the Skewed NMS [ 2 8 ]   In this section , we conduct a series of ablation experiments to evaluate the effectiveness of the base convolutional network and PAN on ICDAR - 2 0 1 7 MLT , ICDAR - 2 0 1 5 and SCUT - CTW 1 5 0 0 text detection benchmark datasets .", "ner": [["Fast R - CNN", "Method"], ["Skewed NMS", "Method"], ["convolutional network", "Method"], ["PAN", "Method"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"], ["SCUT - CTW 1 5 0 0 text detection", "Dataset"]], "rel": [["PAN", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["convolutional network", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["PAN", "Evaluated-With", "ICDAR - 2 0 1 5"], ["convolutional network", "Evaluated-With", "ICDAR - 2 0 1 5"], ["PAN", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection"], ["convolutional network", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection"]], "rel_plus": [["PAN:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["convolutional network:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["PAN:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["convolutional network:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["PAN:Method", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection:Dataset"], ["convolutional network:Method", "Evaluated-With", "SCUT - CTW 1 5 0 0 text detection:Dataset"]]}
{"doc_id": "AUG040", "sentence": "Mask R-CNN improves image segmentation on PASCAL VOC, not just a mask.", "ner": [["Mask R-CNN", "Method"], ["image segmentation", "Task"], ["PASCAL VOC", "Dataset"]], "rel": [["Mask R-CNN", "Used-For", "image segmentation"], ["Mask R-CNN", "Evaluated-With", "PASCAL VOC"]], "rel_plus": [["Mask R-CNN:Method", "Used-For", "image segmentation:Task"], ["Mask R-CNN:Method", "Evaluated-With", "PASCAL VOC:Dataset"]]}
{"doc_id": "202888751", "sentence": "In image translation methods , the fidelity and resemblance between images generated by GANs and real images is a trade - off .", "ner": [["image translation", "Task"], ["GANs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210164517", "sentence": "As the fully connected layer in a CNN model has mostly 9 0 % of the parameters of the entire network , so using Global Average Pooling [ 2 6 ] has reduced the excess parameters .", "ner": [["fully connected layer", "Method"], ["CNN", "Method"], ["Global Average Pooling", "Method"]], "rel": [["fully connected layer", "Part-Of", "CNN"], ["Global Average Pooling", "Part-Of", "CNN"]], "rel_plus": [["fully connected layer:Method", "Part-Of", "CNN:Method"], ["Global Average Pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "4539700", "sentence": "If Kinetics can train very deep CNNs , such as ResNet - 1 5 2 , which achieved the best performance in ResNets on ImageNet [ 1 0 ] , we can be confident that they have sufficient data to train 3D CNNs .", "ner": [["Kinetics", "Dataset"], ["CNNs", "Method"], ["ResNet - 1 5 2", "Method"], ["ResNets", "Method"], ["ImageNet", "Dataset"], ["3D CNNs", "Method"]], "rel": [["CNNs", "Trained-With", "Kinetics"], ["ResNet - 1 5 2", "SubClass-Of", "CNNs"], ["ResNets", "Evaluated-With", "ImageNet"], ["ResNet - 1 5 2", "Evaluated-With", "ImageNet"]], "rel_plus": [["CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["ResNet - 1 5 2:Method", "SubClass-Of", "CNNs:Method"], ["ResNets:Method", "Evaluated-With", "ImageNet:Dataset"], ["ResNet - 1 5 2:Method", "Evaluated-With", "ImageNet:Dataset"]]}
{"doc_id": "198897554", "sentence": "Figure 5 depicts the accuracy of pretext task vs. domain adaptation ( in the semantic segmentation task For a fixed model architecture , the best performing domain adaptation model does have relatively higher accuracy on pretext task , but the best pretext task accuracy does not indicate best domain adaptation model .", "ner": [["domain adaptation", "Method"], ["semantic segmentation", "Task"], ["domain adaptation model", "Method"], ["domain adaptation model", "Method"]], "rel": [["domain adaptation", "Used-For", "semantic segmentation"]], "rel_plus": [["domain adaptation:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "204901567", "sentence": "Similar to our findings in the XNLI experiment , the vocabulary size has a large impact in JOINTMULTI , and JOINTPAIR models with disjoint vocabularies perform the best .", "ner": [["XNLI", "Dataset"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"]], "rel": [["JOINTMULTI", "Evaluated-With", "XNLI"], ["JOINTPAIR", "Evaluated-With", "XNLI"], ["JOINTMULTI", "Compare-With", "JOINTPAIR"]], "rel_plus": [["JOINTMULTI:Method", "Evaluated-With", "XNLI:Dataset"], ["JOINTPAIR:Method", "Evaluated-With", "XNLI:Dataset"], ["JOINTMULTI:Method", "Compare-With", "JOINTPAIR:Method"]]}
{"doc_id": "54447105", "sentence": "We leverage the currently biggest detection dataset with objects in context , COCO [ 3 9 ] , with 8 0 objects ( such as glasses , firehydrant ) labeled on 1 1 7 , 2 8 1 training images .", "ner": [["detection", "Task"], ["COCO", "Dataset"]], "rel": [["COCO", "Benchmark-For", "detection"]], "rel_plus": [["COCO:Dataset", "Benchmark-For", "detection:Task"]]}
{"doc_id": "102351044", "sentence": "Secondly , we evaluate the effectiveness of convolutional building blocks with proposed drop - operations of drop - neuron and dropchannel , which is the foundation for the new state - of - theart results on CIFAR and SVHN datasets .", "ner": [["convolutional building blocks", "Method"], ["drop - operations", "Method"], ["drop - neuron", "Method"], ["dropchannel", "Method"], ["CIFAR", "Dataset"], ["SVHN", "Dataset"]], "rel": [["drop - neuron", "Part-Of", "convolutional building blocks"], ["dropchannel", "Part-Of", "convolutional building blocks"], ["drop - operations", "Part-Of", "convolutional building blocks"], ["drop - neuron", "SubClass-Of", "drop - operations"], ["dropchannel", "SubClass-Of", "drop - operations"], ["convolutional building blocks", "Evaluated-With", "CIFAR"], ["convolutional building blocks", "Evaluated-With", "SVHN"]], "rel_plus": [["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["dropchannel:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - operations:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - neuron:Method", "SubClass-Of", "drop - operations:Method"], ["dropchannel:Method", "SubClass-Of", "drop - operations:Method"], ["convolutional building blocks:Method", "Evaluated-With", "CIFAR:Dataset"], ["convolutional building blocks:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "202565512", "sentence": "This paper utilizes XLNet ( Yang et al. 2 0 1 9 ) as the backend and propose our approach to study the commonsense question answering problem .", "ner": [["XLNet", "Method"], ["commonsense question answering", "Task"]], "rel": [["XLNet", "Used-For", "commonsense question answering"]], "rel_plus": [["XLNet:Method", "Used-For", "commonsense question answering:Task"]]}
{"doc_id": "211020570", "sentence": "Since Res - 5 0 requires input images size of 2 2 4 \u00d7 2 2 4 , the size of the average pooling kernel in Res - 5 0 is resized from 7 to 4 , and the size of the network input is 1 2 8 \u00d7 1 2 8 .", "ner": [["Res - 5 0", "Method"], ["average pooling", "Method"], ["Res - 5 0", "Method"]], "rel": [["average pooling", "Part-Of", "Res - 5 0"]], "rel_plus": [["average pooling:Method", "Part-Of", "Res - 5 0:Method"]]}
{"doc_id": "4539700", "sentence": "More specifically , can the use of 3D CNNs trained on Kinetics produces significant progress in action recognition and other various tasks ? ( See bottom row in Figure 1 . ) To achieve such progress , we consider that Kinetics for 3D CNNs should be as large - scale as Ima - geNet for 2D CNNs , though no previous work has examined enough about the scale of Kinetics .", "ner": [["3D CNNs", "Method"], ["Kinetics", "Dataset"], ["action recognition", "Task"], ["Kinetics", "Dataset"], ["3D CNNs", "Method"], ["Ima - geNet", "Dataset"], ["2D CNNs", "Method"], ["Kinetics", "Dataset"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"], ["3D CNNs", "Used-For", "action recognition"], ["Kinetics", "Benchmark-For", "action recognition"], ["Kinetics", "Used-For", "3D CNNs"], ["Ima - geNet", "Used-For", "2D CNNs"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"], ["3D CNNs:Method", "Used-For", "action recognition:Task"], ["Kinetics:Dataset", "Benchmark-For", "action recognition:Task"], ["Kinetics:Dataset", "Used-For", "3D CNNs:Method"], ["Ima - geNet:Dataset", "Used-For", "2D CNNs:Method"]]}
{"doc_id": "201124533", "sentence": "Each unit contains two 3 \u00d7 3 convolutions for each resolution , where each convolution is followed by batch normalization and the nonlinear activation ReLU .", "ner": [["3 \u00d7 3 convolutions", "Method"], ["convolution", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [["convolution", "Part-Of", "3 \u00d7 3 convolutions"], ["batch normalization", "Part-Of", "convolution"], ["ReLU", "Part-Of", "convolution"]], "rel_plus": [["convolution:Method", "Part-Of", "3 \u00d7 3 convolutions:Method"], ["batch normalization:Method", "Part-Of", "convolution:Method"], ["ReLU:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "150374036", "sentence": "With face alignment , our attention modules improve the baselines by 0. 8 3 % and 1. 8 5 % on FERPlus and AffectNet , respectively .", "ner": [["face alignment", "Task"], ["attention modules", "Method"], ["FERPlus", "Dataset"], ["AffectNet", "Dataset"]], "rel": [["face alignment", "Used-For", "attention modules"], ["attention modules", "Evaluated-With", "FERPlus"], ["attention modules", "Evaluated-With", "AffectNet"]], "rel_plus": [["face alignment:Task", "Used-For", "attention modules:Method"], ["attention modules:Method", "Evaluated-With", "FERPlus:Dataset"], ["attention modules:Method", "Evaluated-With", "AffectNet:Dataset"]]}
{"doc_id": "201646309", "sentence": "We evaluate the performance of SBERT for STS without using any STS specific training data .", "ner": [["SBERT", "Method"], ["STS", "Task"], ["STS", "Task"]], "rel": [["SBERT", "Used-For", "STS"]], "rel_plus": [["SBERT:Method", "Used-For", "STS:Task"]]}
{"doc_id": "202676714", "sentence": "In PGD , the L \u221e norm of the perturbation \u03b5 was set to \u03b5 = [ 0 . 0 5 , 0. 1 , 0. 1 5 , 0. 2 , 0. 2 5 , 0. 4 ] for and \u03b5 = [ 4/ 2 5 5 , 8/ 2 5 5 , 1 2 / 2 5 5 , 1 6 / 2 5 5 , 2 0 / 2 5 5 ] for CIFAR 1 0 at evaluation time .", "ner": [["PGD", "Method"], ["L \u221e norm", "Method"], ["CIFAR 1 0", "Dataset"]], "rel": [["L \u221e norm", "Part-Of", "PGD"], ["PGD", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["L \u221e norm:Method", "Part-Of", "PGD:Method"], ["PGD:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "199668978", "sentence": "Following the similar ranking loss in [ 1 0 , 3 3 ] , the answer correlation module de nes the training objective as a hinge loss :   LSTM a ( a ) ) + sim(LSTM q ( y ) , LSTM a ( a ) ) } , where we use two separate LSTMs , LSTM q and LSTM a , to encode the question and answer to the vector representation .", "ner": [["hinge loss", "Method"], ["LSTM a ( a ) )", "Method"], ["sim(LSTM q ( y )", "Method"], ["LSTM a ( a )", "Method"], ["LSTMs", "Method"], ["LSTM q", "Method"], ["LSTM a", "Method"]], "rel": [["LSTM a", "Part-Of", "LSTMs"], ["LSTM q", "Part-Of", "LSTMs"]], "rel_plus": [["LSTM a:Method", "Part-Of", "LSTMs:Method"], ["LSTM q:Method", "Part-Of", "LSTMs:Method"]]}
{"doc_id": "210839545", "sentence": "Deeplab [ 3 ] and Dilated Conv [ 2 9 ] proposed atrous convolutions to enlarge the network receptive field without sacrificing the resolution , which enables the network to harvest contextual information in a larger region for semantic segmentation .", "ner": [["Deeplab", "Method"], ["Dilated Conv", "Method"], ["atrous convolutions", "Method"], ["semantic segmentation", "Task"]], "rel": [["atrous convolutions", "Part-Of", "Deeplab"], ["atrous convolutions", "Part-Of", "Dilated Conv"], ["Dilated Conv", "Used-For", "semantic segmentation"], ["Deeplab", "Used-For", "semantic segmentation"]], "rel_plus": [["atrous convolutions:Method", "Part-Of", "Deeplab:Method"], ["atrous convolutions:Method", "Part-Of", "Dilated Conv:Method"], ["Dilated Conv:Method", "Used-For", "semantic segmentation:Task"], ["Deeplab:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "210164920", "sentence": "Deconvnet [ 8 5 ] used convolutional network followed by hierarchically opposite de - convolutional network for semantic segmentation as discussed in section 3. 2 . 3 .", "ner": [["Deconvnet", "Method"], ["convolutional network", "Method"], ["de - convolutional network", "Method"], ["semantic segmentation", "Task"]], "rel": [["convolutional network", "Part-Of", "Deconvnet"], ["de - convolutional network", "Part-Of", "Deconvnet"], ["Deconvnet", "Used-For", "semantic segmentation"]], "rel_plus": [["convolutional network:Method", "Part-Of", "Deconvnet:Method"], ["de - convolutional network:Method", "Part-Of", "Deconvnet:Method"], ["Deconvnet:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "102351044", "sentence": "For instance , the new convolutional layer NIN [ 2 1 ] and Inception modules [ 3 0 ] enable more complicated feature extractions with minimum additional parameters .", "ner": [["convolutional layer", "Method"], ["NIN", "Method"], ["Inception", "Method"]], "rel": [["NIN", "SubClass-Of", "convolutional layer"], ["Inception", "SubClass-Of", "convolutional layer"]], "rel_plus": [["NIN:Method", "SubClass-Of", "convolutional layer:Method"], ["Inception:Method", "SubClass-Of", "convolutional layer:Method"]]}
{"doc_id": "202565512", "sentence": "AristoBERTv 7 utilizes the information from machine reading comprehension data RACE ( Lai et al. 2 0 1 7 ) and extracts evidence from text sources such as Wikipedia , SimpleWikipedia , etc .", "ner": [["AristoBERTv 7", "Method"], ["machine reading comprehension", "Task"], ["RACE", "Dataset"], ["Wikipedia", "Dataset"], ["SimpleWikipedia", "Dataset"]], "rel": [["RACE", "Used-For", "AristoBERTv 7"], ["Wikipedia", "Used-For", "AristoBERTv 7"], ["SimpleWikipedia", "Used-For", "AristoBERTv 7"], ["RACE", "Benchmark-For", "machine reading comprehension"], ["AristoBERTv 7", "Used-For", "machine reading comprehension"]], "rel_plus": [["RACE:Dataset", "Used-For", "AristoBERTv 7:Method"], ["Wikipedia:Dataset", "Used-For", "AristoBERTv 7:Method"], ["SimpleWikipedia:Dataset", "Used-For", "AristoBERTv 7:Method"], ["RACE:Dataset", "Benchmark-For", "machine reading comprehension:Task"], ["AristoBERTv 7:Method", "Used-For", "machine reading comprehension:Task"]]}
{"doc_id": "102351044", "sentence": "These dropout variants in convolutional neural networks apply dropout to basic units of CNNs , harnessing both the regularization and model ensemble benefits .", "ner": [["dropout", "Method"], ["convolutional neural networks", "Method"], ["dropout", "Method"], ["CNNs", "Method"]], "rel": [["dropout", "Part-Of", "convolutional neural networks"], ["dropout", "Part-Of", "CNNs"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional neural networks:Method"], ["dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "207880647", "sentence": "Scores such as BLEU and Table 4 : Question answering performance on SQuAD 1. 1 ( all ) , with exact measure ( EM ) and F 1 .", "ner": [["Question answering", "Task"], ["SQuAD 1. 1", "Dataset"]], "rel": [["SQuAD 1. 1", "Benchmark-For", "Question answering"]], "rel_plus": [["SQuAD 1. 1:Dataset", "Benchmark-For", "Question answering:Task"]]}
{"doc_id": "202734316", "sentence": "Other approaches rely on image - to - image translation to create a mapping between the source and target domains [ 4 2 ] or employ GANs to transfer the style of the target domain to the labelled source images [ 3 ] .", "ner": [["image - to - image translation", "Task"], ["GANs", "Method"]], "rel": [["GANs", "Used-For", "image - to - image translation"]], "rel_plus": [["GANs:Method", "Used-For", "image - to - image translation:Task"]]}
{"doc_id": "210860760", "sentence": "In this study to train the SKIP - GRAM model , we use two state - of - the - art word embedding methods , \" Word 2 Vec \" [ 4 0 ] and \" fastText \" [ 4 1 ] .", "ner": [["SKIP - GRAM", "Method"], ["word embedding methods", "Method"], ["Word 2 Vec", "Method"], ["fastText", "Method"]], "rel": [["word embedding methods", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "Part-Of", "SKIP - GRAM"], ["fastText", "Part-Of", "SKIP - GRAM"], ["Word 2 Vec", "SubClass-Of", "word embedding methods"], ["fastText", "SubClass-Of", "word embedding methods"]], "rel_plus": [["word embedding methods:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "Part-Of", "SKIP - GRAM:Method"], ["fastText:Method", "Part-Of", "SKIP - GRAM:Method"], ["Word 2 Vec:Method", "SubClass-Of", "word embedding methods:Method"], ["fastText:Method", "SubClass-Of", "word embedding methods:Method"]]}
{"doc_id": "202540590", "sentence": "GPT - FT ( Radford et al. , 2 0 1 8 ) is based on a generative pre - trained transformer language model , following a fine - tuning step on COSMOS .   where W t and b t are learnable parameters .", "ner": [["GPT - FT", "Method"], ["generative pre - trained transformer language model", "Method"], ["COSMOS", "Dataset"]], "rel": [["GPT - FT", "SubClass-Of", "generative pre - trained transformer language model"], ["GPT - FT", "Trained-With", "COSMOS"]], "rel_plus": [["GPT - FT:Method", "SubClass-Of", "generative pre - trained transformer language model:Method"], ["GPT - FT:Method", "Trained-With", "COSMOS:Dataset"]]}
{"doc_id": "23569888", "sentence": "As we discuss in detail later in the paper , investigating this latter question highlighted some differences between iCWT and other datasets such as ImageNet ( Russakovsky et al 2 0 1 5 ) .", "ner": [["iCWT", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["iCWT", "Compare-With", "ImageNet"]], "rel_plus": [["iCWT:Dataset", "Compare-With", "ImageNet:Dataset"]]}
{"doc_id": "211010758", "sentence": "The reported mAP gain of DA - Faster [ 5 ] in its original report ( 7. 8 points ) is significantly different from its reproduced gain ( - 0 . 4 points ) in SWDA [ 4 6 ] .", "ner": [["DA - Faster", "Method"], ["SWDA", "Method"]], "rel": [["DA - Faster", "Compare-With", "SWDA"]], "rel_plus": [["DA - Faster:Method", "Compare-With", "SWDA:Method"]]}
{"doc_id": "208202241", "sentence": "Taking the KSSNet with backbone of Inception - I 3 D ( Carreira and Zisserman 2 0 1 7 ) designed for multi - label video classification as example , we show its block - diagram in Figure 2 .", "ner": [["KSSNet", "Method"], ["Inception - I 3 D", "Method"], ["multi - label video classification", "Task"]], "rel": [["Inception - I 3 D", "Synonym-Of", "KSSNet"], ["KSSNet", "Used-For", "multi - label video classification"]], "rel_plus": [["Inception - I 3 D:Method", "Synonym-Of", "KSSNet:Method"], ["KSSNet:Method", "Used-For", "multi - label video classification:Task"]]}
{"doc_id": "59599694", "sentence": "Among neural network models , the MLP model has the worst traffic flow prediction performance , while C - CNN - LSTM - DA is the best in Table 1 .", "ner": [["MLP", "Method"], ["traffic flow prediction", "Task"], ["C - CNN - LSTM - DA", "Method"]], "rel": [["MLP", "Used-For", "traffic flow prediction"], ["MLP", "Compare-With", "C - CNN - LSTM - DA"]], "rel_plus": [["MLP:Method", "Used-For", "traffic flow prediction:Task"], ["MLP:Method", "Compare-With", "C - CNN - LSTM - DA:Method"]]}
{"doc_id": "204901567", "sentence": "Our best JOINTMULTI model is substantially better than mBERT , and only one point worse ( on average ) than the unsupervised XLM model , which is larger in size . \u2022 Among the tested JOINTMULTI variants , we observe that using a larger vocabulary size has a notable positive impact . \u2022 JOINTPAIR models with a joint vocabulary perform comparably with JOINTMULTI .", "ner": [["JOINTMULTI", "Method"], ["mBERT", "Method"], ["XLM", "Method"], ["JOINTMULTI", "Method"], ["JOINTPAIR", "Method"], ["JOINTMULTI", "Method"]], "rel": [["JOINTMULTI", "Compare-With", "mBERT"], ["JOINTMULTI", "Compare-With", "XLM"], ["JOINTPAIR", "Compare-With", "JOINTMULTI"]], "rel_plus": [["JOINTMULTI:Method", "Compare-With", "mBERT:Method"], ["JOINTMULTI:Method", "Compare-With", "XLM:Method"], ["JOINTPAIR:Method", "Compare-With", "JOINTMULTI:Method"]]}
{"doc_id": "202750230", "sentence": "Reducing the memory footprint of Transformer architectures and BERT in particular is an active subject of research .", "ner": [["Transformer", "Method"], ["BERT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "Unlike FCN - based methods , another category of methods treats text as a specific object and leverages effective object detection frameworks like R - CNN [ 9 ] , Faster R - CNN [ 3 2 ] , SSD [ 2 3 ] , YOLO [ 3 1 ] and DenseBox [ 1 4 ] to detect words or text - lines from images directly .", "ner": [["FCN - based methods", "Method"], ["object detection frameworks", "Method"], ["R - CNN", "Method"], ["Faster R - CNN", "Method"], ["SSD", "Method"], ["YOLO", "Method"], ["DenseBox", "Method"]], "rel": [["R - CNN", "SubClass-Of", "object detection frameworks"], ["Faster R - CNN", "SubClass-Of", "object detection frameworks"], ["SSD", "SubClass-Of", "object detection frameworks"], ["YOLO", "SubClass-Of", "object detection frameworks"], ["DenseBox", "SubClass-Of", "object detection frameworks"], ["FCN - based methods", "Compare-With", "object detection frameworks"]], "rel_plus": [["R - CNN:Method", "SubClass-Of", "object detection frameworks:Method"], ["Faster R - CNN:Method", "SubClass-Of", "object detection frameworks:Method"], ["SSD:Method", "SubClass-Of", "object detection frameworks:Method"], ["YOLO:Method", "SubClass-Of", "object detection frameworks:Method"], ["DenseBox:Method", "SubClass-Of", "object detection frameworks:Method"], ["FCN - based methods:Method", "Compare-With", "object detection frameworks:Method"]]}
{"doc_id": "204901567", "sentence": "In natural language inference ( NLI ) , given two sentences ( a premise and a hypothesis ) , the goal is to decide whether there is an entailment , with the previous results from mBERT and XLM . 6 We summarize our main findings below : \u2022 Our JOINTMULTI results are comparable with similar models reported in the literature .", "ner": [["natural language inference", "Task"], ["NLI", "Task"], ["entailment", "Task"], ["mBERT", "Method"], ["XLM", "Method"], ["JOINTMULTI", "Method"]], "rel": [["NLI", "Synonym-Of", "natural language inference"], ["entailment", "SubTask-Of", "natural language inference"], ["mBERT", "Used-For", "natural language inference"], ["XLM", "Used-For", "natural language inference"], ["mBERT", "Used-For", "entailment"], ["XLM", "Used-For", "entailment"]], "rel_plus": [["NLI:Task", "Synonym-Of", "natural language inference:Task"], ["entailment:Task", "SubTask-Of", "natural language inference:Task"], ["mBERT:Method", "Used-For", "natural language inference:Task"], ["XLM:Method", "Used-For", "natural language inference:Task"], ["mBERT:Method", "Used-For", "entailment:Task"], ["XLM:Method", "Used-For", "entailment:Task"]]}
{"doc_id": "AUG028", "sentence": "Context-aware sequence learning improves dialogue generation on DailyDialog.", "ner": [["context-aware sequence learning", "Method"], ["dialogue generation", "Task"], ["DailyDialog", "Dataset"]], "rel": [["context-aware sequence learning", "Used-For", "dialogue generation"], ["context-aware sequence learning", "Trained-With", "DailyDialog"]], "rel_plus": [["context-aware sequence learning:Method", "Used-For", "dialogue generation:Task"], ["context-aware sequence learning:Method", "Trained-With", "DailyDialog:Dataset"]]}
{"doc_id": "21683040", "sentence": "FSSD can achieve state - of - the - art performance on both PASCAL VOC dataset and MS COCO dataset .", "ner": [["FSSD", "Method"], ["PASCAL VOC", "Dataset"], ["MS COCO", "Dataset"]], "rel": [["FSSD", "Evaluated-With", "PASCAL VOC"], ["FSSD", "Evaluated-With", "MS COCO"]], "rel_plus": [["FSSD:Method", "Evaluated-With", "PASCAL VOC:Dataset"], ["FSSD:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "210164517", "sentence": "In [ 1 4 ] , authors have added SVM for classification purposes of features extracted via AlexNet model .", "ner": [["SVM", "Method"], ["classification", "Task"], ["AlexNet", "Method"]], "rel": [["AlexNet", "Used-For", "SVM"], ["SVM", "Used-For", "classification"]], "rel_plus": [["AlexNet:Method", "Used-For", "SVM:Method"], ["SVM:Method", "Used-For", "classification:Task"]]}
{"doc_id": "44148233", "sentence": "For automatic evaluation , when comparing the generated sentences with ground truth descriptions , three evaluation metrics are borrowed from machine translation , namely , Bilingual Evaluation Understudy ( BLEU ) [ 1 1 8 ] , Recall Oriented Understudy of Gisting Evaluation ( ROUGE ) [ 9 8 ] and Metric for Evaluation of Translation with Explicit Ordering ( METEOR ) [ 2 2 ] .", "ner": [["machine translation", "Task"], ["Bilingual Evaluation Understudy", "Method"], ["BLEU", "Method"], ["Recall Oriented Understudy of Gisting Evaluation", "Method"], ["ROUGE", "Method"], ["Metric for Evaluation of Translation with Explicit Ordering", "Method"], ["METEOR", "Method"]], "rel": [["Bilingual Evaluation Understudy", "Used-For", "machine translation"], ["Recall Oriented Understudy of Gisting Evaluation", "Used-For", "machine translation"], ["Metric for Evaluation of Translation with Explicit Ordering", "Used-For", "machine translation"], ["BLEU", "Synonym-Of", "Bilingual Evaluation Understudy"], ["ROUGE", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation"], ["METEOR", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering"]], "rel_plus": [["Bilingual Evaluation Understudy:Method", "Used-For", "machine translation:Task"], ["Recall Oriented Understudy of Gisting Evaluation:Method", "Used-For", "machine translation:Task"], ["Metric for Evaluation of Translation with Explicit Ordering:Method", "Used-For", "machine translation:Task"], ["BLEU:Method", "Synonym-Of", "Bilingual Evaluation Understudy:Method"], ["ROUGE:Method", "Synonym-Of", "Recall Oriented Understudy of Gisting Evaluation:Method"], ["METEOR:Method", "Synonym-Of", "Metric for Evaluation of Translation with Explicit Ordering:Method"]]}
{"doc_id": "23569888", "sentence": "Indeed , CNNs are known to need massive amount of data to work and data - augmentation is often used to improve results .", "ner": [["CNNs", "Method"], ["data - augmentation", "Method"]], "rel": [["data - augmentation", "Used-For", "CNNs"]], "rel_plus": [["data - augmentation:Method", "Used-For", "CNNs:Method"]]}
{"doc_id": "198231883", "sentence": "To adapt this network to VIS , we created a training set by combining the YT - VIS [ 4 3 ] , COCO [ 2 0 ] and OpenImages [ 1 5 ] datasets .", "ner": [["VIS", "Task"], ["YT - VIS", "Dataset"], ["COCO", "Dataset"], ["OpenImages", "Dataset"]], "rel": [["OpenImages", "Benchmark-For", "VIS"], ["YT - VIS", "Benchmark-For", "VIS"], ["COCO", "Benchmark-For", "VIS"]], "rel_plus": [["OpenImages:Dataset", "Benchmark-For", "VIS:Task"], ["YT - VIS:Dataset", "Benchmark-For", "VIS:Task"], ["COCO:Dataset", "Benchmark-For", "VIS:Task"]]}
{"doc_id": "AUG074", "sentence": "Context-aware question answering is optimized by RoBERTa on SQuAD.", "ner": [["context-aware question answering", "Task"], ["RoBERTa", "Method"], ["SQuAD", "Dataset"]], "rel": [["RoBERTa", "Used-For", "context-aware question answering"], ["RoBERTa", "Trained-With", "SQuAD"]], "rel_plus": [["RoBERTa:Method", "Used-For", "context-aware question answering:Task"], ["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "201070522", "sentence": "While all language models perform closely on ReferIt phrase grounding , this still suggests that there is no need to use the more complex LSTM language model without additional modification .", "ner": [["ReferIt", "Dataset"], ["phrase grounding", "Task"], ["LSTM", "Method"]], "rel": [["ReferIt", "Benchmark-For", "phrase grounding"]], "rel_plus": [["ReferIt:Dataset", "Benchmark-For", "phrase grounding:Task"]]}
{"doc_id": "204402755", "sentence": "Smaller values of \u03b2 have gradually been employed for Generative Adversarial Networks ( GAN ) , and recent developments in game dynamics ( Gidel et al. , 2 0 1 8) show a negative momentum is helpful for GANs .", "ner": [["Generative Adversarial Networks", "Method"], ["GAN", "Method"], ["GANs", "Method"]], "rel": [["GAN", "Synonym-Of", "Generative Adversarial Networks"]], "rel_plus": [["GAN:Method", "Synonym-Of", "Generative Adversarial Networks:Method"]]}
{"doc_id": "199543700", "sentence": "We solve the second problem by proposing a novel technique named convolutionblock - transformation ( CBT ) proposed by us .", "ner": [["convolutionblock - transformation", "Method"], ["CBT", "Method"]], "rel": [["CBT", "Synonym-Of", "convolutionblock - transformation"]], "rel_plus": [["CBT:Method", "Synonym-Of", "convolutionblock - transformation:Method"]]}
{"doc_id": "21683040", "sentence": "According to the experiments in Table 2 ( rows 2 and 5 ) , these two ways have little difference on the final result but training FSSD from VGG 1 6 has a slightly better results than the one which is trained from SSD model .", "ner": [["FSSD", "Method"], ["VGG 1 6", "Method"], ["SSD", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["SSD", "Part-Of", "FSSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["SSD:Method", "Part-Of", "FSSD:Method"]]}
{"doc_id": "210861282", "sentence": "PCK for Penn Action UniPose - LSTM ( ours ) 9 9 . 3 % LSTM - PM [ 2 7 ] 9 7 . 7 % CPM [ 4 9 ] 9 7 . 1 % Thin - Slicing [ 4 0 ] 9 6 . 5 % N - best [ 3 2 ] 9 1 . 8 % Iqbal [ 2 0 ] 8 1 . 1 % Table 3 .", "ner": [["Penn Action UniPose - LSTM", "Method"], ["LSTM - PM", "Method"], ["CPM", "Method"], ["Thin - Slicing", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Many CNNs have tried to apply dropout to convolution layers .", "ner": [["CNNs", "Method"], ["dropout", "Method"], ["convolution", "Method"]], "rel": [["convolution", "Part-Of", "CNNs"], ["dropout", "Part-Of", "convolution"]], "rel_plus": [["convolution:Method", "Part-Of", "CNNs:Method"], ["dropout:Method", "Part-Of", "convolution:Method"]]}
{"doc_id": "199668978", "sentence": "And we set the word out of the vocabulary as < UNK>. We choose word embedding of 2 0 0 dimensions for CSU dataset and use the Glove model [ 2 6 ] to get the pre - trained word embedding .", "ner": [["word embedding", "Task"], ["CSU", "Dataset"], ["Glove", "Method"], ["word embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201646309", "sentence": "We also tested XLNet ( Yang et al. , 2 0 1 9 ) , but it led in general to worse results than BERT .", "ner": [["XLNet", "Method"], ["BERT", "Method"]], "rel": [["XLNet", "Compare-With", "BERT"]], "rel_plus": [["XLNet:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "35249701", "sentence": "After pruning the initial ImageNet - trained network , we fine - tune it on the ImageNet dataset for 1 0 epochs with a learning rate of 1e - 3 decayed by a factor of 1 0 after 5 epochs .", "ner": [["ImageNet", "Dataset"], ["ImageNet", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Given an input sequence X \u2208 R T \u00d7n of T n - dimensional vectors x 1 . . . x T , the convolutional subcomponent of a QRNN performs convolutions in the timestep dimension with a bank of m filters , producing a sequence Z \u2208 R T \u00d7m of m - dimensional candidate vectors z t .", "ner": [["convolutional subcomponent", "Method"], ["QRNN", "Method"], ["convolutions", "Method"]], "rel": [["convolutional subcomponent", "Part-Of", "QRNN"]], "rel_plus": [["convolutional subcomponent:Method", "Part-Of", "QRNN:Method"]]}
{"doc_id": "6116678", "sentence": "However , directly applying existing fully convolutional network architecture to salient object detection would not be most appropriate because a standard fully convolutional model is not particularly good at capturing subtle visual contrast in an image .", "ner": [["fully convolutional network", "Method"], ["salient object detection", "Task"]], "rel": [["fully convolutional network", "Used-For", "salient object detection"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "salient object detection:Task"]]}
{"doc_id": "202577400", "sentence": "Method road swalk build wall fence pole tlight sign veg . terrain sky person rider car truck bus train mbike bike mIoU ResNet 3 8 [ 3 1 ] Ctiyscapes provides about 2 0 0 0 0 coarse labeled images for training .", "ner": [["ResNet 3 8", "Method"], ["Ctiyscapes", "Dataset"]], "rel": [["ResNet 3 8", "Trained-With", "Ctiyscapes"]], "rel_plus": [["ResNet 3 8:Method", "Trained-With", "Ctiyscapes:Dataset"]]}
{"doc_id": "201646309", "sentence": "Training SBERT in the 1 0 - fold cross - validation setup gives a performance that is nearly on - par with BERT .", "ner": [["SBERT", "Method"], ["BERT", "Method"]], "rel": [["SBERT", "Compare-With", "BERT"]], "rel_plus": [["SBERT:Method", "Compare-With", "BERT:Method"]]}
{"doc_id": "6423078", "sentence": "As we have discussed , while FCN reinterprets classification nets for per - pixel prediction , it has only one output loss function .", "ner": [["FCN reinterprets classification nets", "Method"], ["per - pixel prediction", "Task"]], "rel": [["FCN reinterprets classification nets", "Used-For", "per - pixel prediction"]], "rel_plus": [["FCN reinterprets classification nets:Method", "Used-For", "per - pixel prediction:Task"]]}
{"doc_id": "198231883", "sentence": "For this task we introduce our Forest Path Cutting ( FPC ) algorithm as can be seen in Algorithm 1 and Figure 4 .", "ner": [["Forest Path Cutting", "Method"], ["FPC", "Method"]], "rel": [["FPC", "Synonym-Of", "Forest Path Cutting"]], "rel_plus": [["FPC:Method", "Synonym-Of", "Forest Path Cutting:Method"]]}
{"doc_id": "56657874", "sentence": "The idea of [ 2 6 , 3 8 , 3 9 ] is similar to Deep Domain Confusion ( DDC ) [ 4 1 ] and Deep Adaptation Network ( DAN ) [ 2 3 ] except that instead of MMD , they adopted CORAL loss to minimize the discrepancy .", "ner": [["Deep Domain Confusion", "Method"], ["DDC", "Method"], ["Deep Adaptation Network", "Method"], ["DAN", "Method"], ["MMD", "Method"], ["CORAL loss", "Method"]], "rel": [["DDC", "Synonym-Of", "Deep Domain Confusion"], ["DAN", "Synonym-Of", "Deep Adaptation Network"]], "rel_plus": [["DDC:Method", "Synonym-Of", "Deep Domain Confusion:Method"], ["DAN:Method", "Synonym-Of", "Deep Adaptation Network:Method"]]}
{"doc_id": "210861282", "sentence": "We next provide the motivation for the development of the WASP module and contrast it with traditional deconvolutions in [ 2 6 ] and the ASPP architecture in [ 1 1 ] .   The WASP module generates an efficient multi - scale representation that helps UniPose achieve state - of - the - art results .", "ner": [["WASP", "Method"], ["traditional deconvolutions", "Method"], ["ASPP", "Method"], ["WASP", "Method"], ["UniPose", "Method"]], "rel": [["WASP", "Compare-With", "traditional deconvolutions"], ["WASP", "Compare-With", "ASPP"], ["WASP", "Part-Of", "UniPose"]], "rel_plus": [["WASP:Method", "Compare-With", "traditional deconvolutions:Method"], ["WASP:Method", "Compare-With", "ASPP:Method"], ["WASP:Method", "Part-Of", "UniPose:Method"]]}
{"doc_id": "3920676", "sentence": "While existing Siamese network frameworks learn to tell pairs of images apart , we can extend them , in conjunction with C 3 D - like networks , to tell pairs of video sequences apart .", "ner": [["Siamese network", "Method"], ["C 3 D", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "As we have mentioned in the Introduction , the existing textual similarity measures , such as BLEU , CIDEr , METEOR , and ROUGE , can not effectively capture the semantic similarity .", "ner": [["textual similarity measures", "Method"], ["BLEU", "Method"], ["CIDEr", "Method"], ["METEOR", "Method"], ["ROUGE", "Method"], ["semantic similarity", "Task"]], "rel": [["BLEU", "SubClass-Of", "textual similarity measures"], ["CIDEr", "SubClass-Of", "textual similarity measures"], ["METEOR", "SubClass-Of", "textual similarity measures"], ["ROUGE", "SubClass-Of", "textual similarity measures"]], "rel_plus": [["BLEU:Method", "SubClass-Of", "textual similarity measures:Method"], ["CIDEr:Method", "SubClass-Of", "textual similarity measures:Method"], ["METEOR:Method", "SubClass-Of", "textual similarity measures:Method"], ["ROUGE:Method", "SubClass-Of", "textual similarity measures:Method"]]}
{"doc_id": "44148233", "sentence": "For Stage II , sentence generation , a variety of methods have been proposed including HALogen representation [ 9 3 ] , Head - driven Phrase Structure Grammar ( HPSG ) [ 1 2 2 ] , planner and surface realizer [ 1 2 6 ] .", "ner": [["HALogen representation", "Method"], ["Head - driven Phrase Structure Grammar", "Method"], ["HPSG", "Method"], ["planner and surface realizer", "Method"]], "rel": [["HPSG", "Synonym-Of", "Head - driven Phrase Structure Grammar"]], "rel_plus": [["HPSG:Method", "Synonym-Of", "Head - driven Phrase Structure Grammar:Method"]]}
{"doc_id": "198231883", "sentence": "UnOVOST outperforms all other previous UVOS algorithms over all datasets , often by a large margin .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"]], "rel": [["UnOVOST", "Used-For", "UVOS"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"]]}
{"doc_id": "210861282", "sentence": "An important challenge with both semantic segmentation and pose estimation methods incorporating CNN layers is the significant reduction of resolution caused by pooling .", "ner": [["semantic segmentation", "Task"], ["pose estimation", "Task"], ["CNN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "As shown in Table 6 , on TACoS Multilevel dataset , h - RNN [ 1 8 3 ] has the best results on all reported metrics i.e. BLEU , METEOR and CIDEr .", "ner": [["TACoS Multilevel", "Dataset"], ["h - RNN", "Method"], ["BLEU", "Method"], ["METEOR", "Method"], ["CIDEr", "Method"]], "rel": [["h - RNN", "Evaluated-With", "TACoS Multilevel"]], "rel_plus": [["h - RNN:Method", "Evaluated-With", "TACoS Multilevel:Dataset"]]}
{"doc_id": "4539700", "sentence": "ResNet , which is one of the most successful architectures in image classification , provides shortcut connections that allow a signal to bypass one layer and move to the next layer in the sequence .", "ner": [["ResNet", "Method"], ["image classification", "Task"]], "rel": [["ResNet", "Used-For", "image classification"]], "rel_plus": [["ResNet:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "201124533", "sentence": "We report the results over two scene parsing datasets , PASCALContext [ 8 1 ] and Cityscapes [ 2 2 ] , and a human parsing dataset , LIP [ 3 3 ] .", "ner": [["scene parsing", "Task"], ["PASCALContext", "Dataset"], ["Cityscapes", "Dataset"], ["human parsing", "Task"], ["LIP", "Dataset"]], "rel": [["PASCALContext", "Benchmark-For", "scene parsing"], ["Cityscapes", "Benchmark-For", "scene parsing"], ["LIP", "Benchmark-For", "human parsing"]], "rel_plus": [["PASCALContext:Dataset", "Benchmark-For", "scene parsing:Task"], ["Cityscapes:Dataset", "Benchmark-For", "scene parsing:Task"], ["LIP:Dataset", "Benchmark-For", "human parsing:Task"]]}
{"doc_id": "52910494", "sentence": "Composed of layers for multi - scale convolutions , trainable cross - scale aggregation , maxout , and concatenation , this module is highly non - linear and can boost the accuracy of DenseNet while using much fewer parameters .", "ner": [["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"], ["concatenation", "Method"], ["DenseNet", "Method"]], "rel": [["concatenation", "Part-Of", "DenseNet"], ["maxout", "Part-Of", "DenseNet"], ["cross - scale aggregation", "Part-Of", "DenseNet"], ["multi - scale convolutions", "Part-Of", "DenseNet"]], "rel_plus": [["concatenation:Method", "Part-Of", "DenseNet:Method"], ["maxout:Method", "Part-Of", "DenseNet:Method"], ["cross - scale aggregation:Method", "Part-Of", "DenseNet:Method"], ["multi - scale convolutions:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "209532167", "sentence": "For each classification task , we trained BiLSTM models starting with each of our baseline Word 2 Vec embeddings , namely , continuous bag of words ( CBOW ) and Skipgram trained with negative sampling or hierarchical softmax .", "ner": [["classification", "Task"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["continuous bag of words", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"], ["negative sampling", "Method"], ["hierarchical softmax", "Method"]], "rel": [["BiLSTM", "Used-For", "classification"], ["Word 2 Vec", "Part-Of", "BiLSTM"], ["continuous bag of words", "Part-Of", "BiLSTM"], ["Skipgram", "Part-Of", "BiLSTM"], ["continuous bag of words", "SubClass-Of", "Word 2 Vec"], ["Skipgram", "SubClass-Of", "Word 2 Vec"], ["CBOW", "Synonym-Of", "continuous bag of words"], ["negative sampling", "Part-Of", "Skipgram"], ["hierarchical softmax", "Part-Of", "Skipgram"]], "rel_plus": [["BiLSTM:Method", "Used-For", "classification:Task"], ["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"], ["continuous bag of words:Method", "Part-Of", "BiLSTM:Method"], ["Skipgram:Method", "Part-Of", "BiLSTM:Method"], ["continuous bag of words:Method", "SubClass-Of", "Word 2 Vec:Method"], ["Skipgram:Method", "SubClass-Of", "Word 2 Vec:Method"], ["CBOW:Method", "Synonym-Of", "continuous bag of words:Method"], ["negative sampling:Method", "Part-Of", "Skipgram:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"]]}
{"doc_id": "21683040", "sentence": "In this paper , to tackle these problems mentioned above , we propose Feature Fusion SSD(FSSD ) by adding a lightweight and efficient feature fusion module to the conventional SSD .", "ner": [["Feature Fusion SSD(FSSD )", "Method"], ["feature fusion module", "Method"], ["SSD", "Method"]], "rel": [["SSD", "Part-Of", "Feature Fusion SSD(FSSD )"], ["feature fusion module", "Part-Of", "Feature Fusion SSD(FSSD )"]], "rel_plus": [["SSD:Method", "Part-Of", "Feature Fusion SSD(FSSD ):Method"], ["feature fusion module:Method", "Part-Of", "Feature Fusion SSD(FSSD ):Method"]]}
{"doc_id": "198231883", "sentence": "UnOVOST outperforms all previous UVOS methods , while even performing competitively with many semi - supervised video object segmentation algorithms without requiring any human input as to which objects should be tracked and segmented .   We adapt UnOVOST to the Video Instance Segmentation ( VIS ) domain is the following way .", "ner": [["UnOVOST", "Method"], ["UVOS", "Task"], ["semi - supervised video object segmentation", "Task"], ["UnOVOST", "Method"], ["Video Instance Segmentation", "Task"], ["VIS", "Task"]], "rel": [["UnOVOST", "Used-For", "UVOS"], ["UnOVOST", "Used-For", "semi - supervised video object segmentation"], ["VIS", "Synonym-Of", "Video Instance Segmentation"], ["UnOVOST", "Used-For", "Video Instance Segmentation"]], "rel_plus": [["UnOVOST:Method", "Used-For", "UVOS:Task"], ["UnOVOST:Method", "Used-For", "semi - supervised video object segmentation:Task"], ["VIS:Task", "Synonym-Of", "Video Instance Segmentation:Task"], ["UnOVOST:Method", "Used-For", "Video Instance Segmentation:Task"]]}
{"doc_id": "AUG127", "sentence": "Semantic Evaluation 2010, or SemEval 2010, is improved by GloVe for relation extraction.", "ner": [["Semantic Evaluation 2010", "Dataset"], ["SemEval 2010", "Dataset"], ["GloVe", "Method"], ["relation extraction", "Task"]], "rel": [["SemEval 2010", "Synonym-Of", "Semantic Evaluation 2010"], ["GloVe", "Used-For", "relation extraction"], ["GloVe", "Trained-With", "SemEval 2010"]], "rel_plus": [["SemEval 2010:Dataset", "Synonym-Of", "Semantic Evaluation 2010:Dataset"], ["GloVe:Method", "Used-For", "relation extraction:Task"], ["GloVe:Method", "Trained-With", "SemEval 2010:Dataset"]]}
{"doc_id": "44148233", "sentence": "For the object and activity recognition stages , the research moved from earlier threshold - based detection [ 8 4 ] to manual feature engineering and traditional classifiers [ 4 1 ] , [ 6 4 ] , [ 8 7 ] , [ 1 5 3 ] .", "ner": [["object and activity recognition", "Task"], ["threshold - based detection", "Task"], ["manual feature engineering", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "204402755", "sentence": "The Adam algorithm is parameterized by learning rate \u03b7 > 0 , discount factors \u03b2 1 < 1 and \u03b2 2 < 1 , a small constant , and uses the update rule : AMSGrad ( Reddi et al. , 2 0 1 9 ) resolves an issue in the proof of Adam related to the exponential moving average E g\u2022g t , where Adam does not converge for a simple optimization problem .", "ner": [["Adam", "Method"], ["AMSGrad", "Method"], ["Adam", "Method"], ["Adam", "Method"]], "rel": [["AMSGrad", "Used-For", "Adam"]], "rel_plus": [["AMSGrad:Method", "Used-For", "Adam:Method"]]}
{"doc_id": "102351044", "sentence": "Convolutional Neural networks ( CNNs ) based applications have become ubiquitous , where proper regularization is greatly needed .", "ner": [["Convolutional Neural networks", "Method"], ["CNNs", "Method"]], "rel": [["CNNs", "Synonym-Of", "Convolutional Neural networks"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "Convolutional Neural networks:Method"]]}
{"doc_id": "195347056", "sentence": "Surprisingly , it demonstrated realistic face generation but is significantly outperformed by DFM on CIFAR - 1 0 .", "ner": [["face generation", "Task"], ["DFM", "Method"], ["CIFAR - 1 0", "Dataset"]], "rel": [["DFM", "Used-For", "face generation"], ["CIFAR - 1 0", "Benchmark-For", "face generation"], ["DFM", "Evaluated-With", "CIFAR - 1 0"]], "rel_plus": [["DFM:Method", "Used-For", "face generation:Task"], ["CIFAR - 1 0:Dataset", "Benchmark-For", "face generation:Task"], ["DFM:Method", "Evaluated-With", "CIFAR - 1 0:Dataset"]]}
{"doc_id": "210713911", "sentence": "For example , applying Mixup , DropBlock , KD , and Autoaug individually improves top 1 /mCE 0. 7 5 %/ 6 . 0 8 % , 0. 6 9 %/ 1 . 8 4 % , 0. 2 9 %/ 1 . 2 6 % , and 0.0 9 %/ 4 . 1 4 % respectively .", "ner": [["Mixup", "Method"], ["DropBlock", "Method"], ["KD", "Method"], ["Autoaug", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202888986", "sentence": "The latter appears to be a particularly strong improvement , a jump of + 1 7 . 4 % absolute points over BERT , + 7. 6 % over XLNet , + 6. 2 % over RoBERTa , and 5. 3 % over DCMI+ , an ensemble of multiple models specifically designed for reading comprehension tasks .", "ner": [["BERT", "Method"], ["XLNet", "Method"], ["RoBERTa", "Method"], ["DCMI+", "Method"], ["reading comprehension", "Task"]], "rel": [["DCMI+", "Used-For", "reading comprehension"]], "rel_plus": [["DCMI+:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "4246700", "sentence": "This concise description of the remote sensing scene plays a vital role in numerous fields , such as image retrieval [ 2 7 ] , scene classification [ 9 ] , and military intelligence generation [ 2 8 ] .", "ner": [["image retrieval", "Task"], ["scene classification", "Task"], ["military intelligence generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "208548469", "sentence": "However , based on Figure 6 , we discover that the accuracy of these 7 similarity met - rics , { ( BLEU 1 ... 4 , ROU GE , CIDEr , M ET EOR ) } , are less monotonous and much more random from the first partition to the seventh partition .", "ner": [["similarity met - rics", "Method"], ["BLEU", "Method"], ["ROU GE", "Method"], ["CIDEr", "Method"], ["M ET EOR", "Method"]], "rel": [["BLEU", "SubClass-Of", "similarity met - rics"], ["ROU GE", "SubClass-Of", "similarity met - rics"], ["CIDEr", "SubClass-Of", "similarity met - rics"], ["M ET EOR", "SubClass-Of", "similarity met - rics"]], "rel_plus": [["BLEU:Method", "SubClass-Of", "similarity met - rics:Method"], ["ROU GE:Method", "SubClass-Of", "similarity met - rics:Method"], ["CIDEr:Method", "SubClass-Of", "similarity met - rics:Method"], ["M ET EOR:Method", "SubClass-Of", "similarity met - rics:Method"]]}
{"doc_id": "210164920", "sentence": "To achieve this , SegNet does up - sampling in its decoder using the stored max - pooling indices from the corresponding encoder feature map resulting high - resolution sparse feature map .", "ner": [["SegNet", "Method"], ["up - sampling", "Method"], ["decoder", "Method"], ["max - pooling", "Method"]], "rel": [["decoder", "Part-Of", "SegNet"], ["max - pooling", "Part-Of", "SegNet"], ["up - sampling", "Part-Of", "decoder"]], "rel_plus": [["decoder:Method", "Part-Of", "SegNet:Method"], ["max - pooling:Method", "Part-Of", "SegNet:Method"], ["up - sampling:Method", "Part-Of", "decoder:Method"]]}
{"doc_id": "211010520", "sentence": "A possible explanation for this observation is that training on D RoBERTa leads to a larger degree of adversarial overfitting than training on D BiDAF , and the inclusion of a large number of standard SQuAD training samples can mitigate this effect .", "ner": [["D RoBERTa", "Dataset"], ["D BiDAF", "Dataset"], ["SQuAD", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "51559", "sentence": "Zhou et al. ( 2 0 1 5 ) apply CNNs at the word level to generate n - gram features used by an LSTM for text classification .", "ner": [["CNNs", "Method"], ["LSTM", "Method"], ["text classification", "Task"]], "rel": [["CNNs", "Used-For", "LSTM"], ["LSTM", "Used-For", "text classification"], ["CNNs", "Used-For", "text classification"]], "rel_plus": [["CNNs:Method", "Used-For", "LSTM:Method"], ["LSTM:Method", "Used-For", "text classification:Task"], ["CNNs:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "104291983", "sentence": "In application to object detection , we reduce the dimension of the high - resolution representation to 2 5 6 , similar to FPN [ 6 2 ] , through a 1 \u00d7 1 convolution before forming the feature pyramid in Figure 3 ( c ) .", "ner": [["object detection", "Task"], ["FPN", "Method"], ["1 \u00d7 1 convolution", "Method"], ["feature pyramid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "In addition , we construct a multi - level representation , named as HRNetV 2 p , from the high - resolution representation output from HRNetV 2 , and apply it to state - of - the - art detection frameworks , including Faster R - CNN , Cascade R - CNN [ 9 ] , FCOS [ 1 1 1 ] , and CenterNet [ 2 7 ] , and state - of - theart joint detection and instance segmentation frameworks , including Mask R - CNN [ 3 8 ] , Cascade Mask R - CNN , and Hybrid Task Cascade [ 1 2 ] .", "ner": [["HRNetV 2 p", "Method"], ["HRNetV 2", "Method"], ["Faster R - CNN", "Method"], ["Cascade R - CNN", "Method"], ["FCOS", "Method"], ["CenterNet", "Method"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["Cascade Mask R - CNN", "Method"], ["Hybrid Task Cascade", "Method"]], "rel": [["HRNetV 2 p", "SubClass-Of", "HRNetV 2"], ["HRNetV 2 p", "Part-Of", "Faster R - CNN"], ["HRNetV 2 p", "Part-Of", "Cascade R - CNN"], ["HRNetV 2 p", "Part-Of", "FCOS"], ["HRNetV 2 p", "Part-Of", "CenterNet"], ["Mask R - CNN", "Used-For", "instance segmentation"], ["Cascade Mask R - CNN", "Used-For", "instance segmentation"], ["Hybrid Task Cascade", "Used-For", "instance segmentation"], ["HRNetV 2 p", "Part-Of", "Mask R - CNN"], ["HRNetV 2 p", "Part-Of", "Cascade Mask R - CNN"], ["HRNetV 2 p", "Part-Of", "Hybrid Task Cascade"]], "rel_plus": [["HRNetV 2 p:Method", "SubClass-Of", "HRNetV 2:Method"], ["HRNetV 2 p:Method", "Part-Of", "Faster R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Cascade R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "FCOS:Method"], ["HRNetV 2 p:Method", "Part-Of", "CenterNet:Method"], ["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Cascade Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["Hybrid Task Cascade:Method", "Used-For", "instance segmentation:Task"], ["HRNetV 2 p:Method", "Part-Of", "Mask R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Cascade Mask R - CNN:Method"], ["HRNetV 2 p:Method", "Part-Of", "Hybrid Task Cascade:Method"]]}
{"doc_id": "AUG017", "sentence": "The graph-enhanced representation learning method improves recommendation systems on MovieLens.", "ner": [["graph-enhanced representation learning", "Method"], ["recommendation systems", "Task"], ["MovieLens", "Dataset"]], "rel": [["graph-enhanced representation learning", "Used-For", "recommendation systems"], ["graph-enhanced representation learning", "Evaluated-With", "MovieLens"]], "rel_plus": [["graph-enhanced representation learning:Method", "Used-For", "recommendation systems:Task"], ["graph-enhanced representation learning:Method", "Evaluated-With", "MovieLens:Dataset"]]}
{"doc_id": "201646309", "sentence": "In this publication , we present Sentence - BERT ( SBERT ) , a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine - similarity .", "ner": [["Sentence - BERT", "Method"], ["SBERT", "Method"], ["BERT", "Method"], ["sentence embeddings", "Task"]], "rel": [["SBERT", "Synonym-Of", "Sentence - BERT"], ["Sentence - BERT", "SubClass-Of", "BERT"]], "rel_plus": [["SBERT:Method", "Synonym-Of", "Sentence - BERT:Method"], ["Sentence - BERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "195347056", "sentence": "An earlier work that employed conditional setting in GAN was the Conditional GAN ( CondGAN ) [ 5 ] by feeding the labels or modes to the generator and discriminator .", "ner": [["GAN", "Method"], ["Conditional GAN", "Method"], ["CondGAN", "Method"], ["generator", "Method"], ["discriminator", "Method"]], "rel": [["Conditional GAN", "SubClass-Of", "GAN"], ["generator", "Part-Of", "GAN"], ["discriminator", "Part-Of", "GAN"], ["CondGAN", "Synonym-Of", "Conditional GAN"], ["generator", "Part-Of", "Conditional GAN"], ["discriminator", "Part-Of", "Conditional GAN"]], "rel_plus": [["Conditional GAN:Method", "SubClass-Of", "GAN:Method"], ["generator:Method", "Part-Of", "GAN:Method"], ["discriminator:Method", "Part-Of", "GAN:Method"], ["CondGAN:Method", "Synonym-Of", "Conditional GAN:Method"], ["generator:Method", "Part-Of", "Conditional GAN:Method"], ["discriminator:Method", "Part-Of", "Conditional GAN:Method"]]}
{"doc_id": "150374036", "sentence": "Our RAN improves the baseline method significantly , with gains 1 0 . 3 % , 1 0 . 0 2 % and 2, 5 3 % on Occlusion - FERPlus , Occlusion - AffectNet and Occlusion - RAF - DB , respectively .", "ner": [["RAN", "Method"], ["Occlusion - FERPlus", "Dataset"], ["Occlusion - AffectNet", "Dataset"], ["Occlusion - RAF - DB", "Dataset"]], "rel": [["RAN", "Evaluated-With", "Occlusion - FERPlus"], ["RAN", "Evaluated-With", "Occlusion - AffectNet"], ["RAN", "Evaluated-With", "Occlusion - RAF - DB"]], "rel_plus": [["RAN:Method", "Evaluated-With", "Occlusion - FERPlus:Dataset"], ["RAN:Method", "Evaluated-With", "Occlusion - AffectNet:Dataset"], ["RAN:Method", "Evaluated-With", "Occlusion - RAF - DB:Dataset"]]}
{"doc_id": "201070697", "sentence": "Each row from top to bottom are the input images , attention maps computed in the discriminator of our SPA - GAN model and the attention maps generated by the attention network in AGGAN [ 7 ] , respectively .", "ner": [["discriminator", "Method"], ["SPA - GAN", "Method"], ["attention network", "Method"], ["AGGAN", "Method"]], "rel": [["discriminator", "Part-Of", "SPA - GAN"], ["attention network", "Part-Of", "AGGAN"]], "rel_plus": [["discriminator:Method", "Part-Of", "SPA - GAN:Method"], ["attention network:Method", "Part-Of", "AGGAN:Method"]]}
{"doc_id": "201646309", "sentence": "Using this setup , BERT set a new state - of - the - art performance on the Semantic Textual Semilarity ( STS ) benchmark ( Cer et al. , 2 0 1 7 ) .", "ner": [["BERT", "Method"], ["Semantic Textual Semilarity", "Dataset"], ["STS", "Dataset"]], "rel": [["STS", "Synonym-Of", "Semantic Textual Semilarity"], ["BERT", "Evaluated-With", "Semantic Textual Semilarity"]], "rel_plus": [["STS:Dataset", "Synonym-Of", "Semantic Textual Semilarity:Dataset"], ["BERT:Method", "Evaluated-With", "Semantic Textual Semilarity:Dataset"]]}
{"doc_id": "210860962", "sentence": "CycleGAN is designed to perform unpaired image translations between two domains of images by training two FCN generators and two discriminators .", "ner": [["CycleGAN", "Method"], ["unpaired image translations", "Task"], ["FCN generators", "Method"], ["two discriminators", "Method"]], "rel": [["two discriminators", "Part-Of", "CycleGAN"], ["FCN generators", "Part-Of", "CycleGAN"], ["CycleGAN", "Used-For", "unpaired image translations"]], "rel_plus": [["two discriminators:Method", "Part-Of", "CycleGAN:Method"], ["FCN generators:Method", "Part-Of", "CycleGAN:Method"], ["CycleGAN:Method", "Used-For", "unpaired image translations:Task"]]}
{"doc_id": "35249701", "sentence": "By contrast , we demonstrate the successful combination of up to four tasks in a single network : starting with an ImageNet - trained VGG - 1 6 network , we sequentially add three fine - grained classification tasks on CUBS birds [ 2 9 ] , Stanford Cars [ 1 5 ] , and Oxford Flowers [ 2 1 ] datasets .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["fine - grained classification", "Task"], ["CUBS birds", "Dataset"], ["Stanford Cars", "Dataset"], ["Oxford Flowers", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"], ["CUBS birds", "Evaluated-With", "VGG - 1 6"], ["Stanford Cars", "Evaluated-With", "VGG - 1 6"], ["Oxford Flowers", "Evaluated-With", "VGG - 1 6"], ["CUBS birds", "Benchmark-For", "fine - grained classification"], ["Stanford Cars", "Benchmark-For", "fine - grained classification"], ["Oxford Flowers", "Benchmark-For", "fine - grained classification"], ["VGG - 1 6", "Used-For", "fine - grained classification"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["CUBS birds:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Stanford Cars:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["Oxford Flowers:Dataset", "Evaluated-With", "VGG - 1 6:Method"], ["CUBS birds:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Stanford Cars:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["Oxford Flowers:Dataset", "Benchmark-For", "fine - grained classification:Task"], ["VGG - 1 6:Method", "Used-For", "fine - grained classification:Task"]]}
{"doc_id": "44148233", "sentence": "Their configuration is a hierarchical decoder with multiple Gated Recurrent Units ( GRU ) for sentence generation .", "ner": [["Gated Recurrent Units", "Method"], ["GRU", "Method"]], "rel": [["GRU", "Synonym-Of", "Gated Recurrent Units"]], "rel_plus": [["GRU:Method", "Synonym-Of", "Gated Recurrent Units:Method"]]}
{"doc_id": "3920676", "sentence": "Specifically , we considered the AHISD [ 5 8 ] and RNP [ 5 9 ] algorithms .", "ner": [["AHISD", "Method"], ["RNP", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202540251", "sentence": "Table 1 details the mIoU improvement on Cityscapes , BDDS and Mapillary by considering one more factor each time : Domain Randomization ( DR ) , Pyramid Consistency across Domains ( PCD ) and within an Image ( PCI ) .", "ner": [["Cityscapes", "Dataset"], ["BDDS", "Dataset"], ["Mapillary", "Dataset"], ["Domain Randomization", "Method"], ["DR", "Method"], ["Pyramid Consistency across Domains", "Method"], ["PCD", "Method"], ["PCI", "Method"]], "rel": [["Domain Randomization", "Evaluated-With", "Cityscapes"], ["Pyramid Consistency across Domains", "Evaluated-With", "Cityscapes"], ["Domain Randomization", "Evaluated-With", "BDDS"], ["Pyramid Consistency across Domains", "Evaluated-With", "BDDS"], ["Domain Randomization", "Evaluated-With", "Mapillary"], ["Pyramid Consistency across Domains", "Evaluated-With", "Mapillary"], ["DR", "Synonym-Of", "Domain Randomization"], ["PCD", "Synonym-Of", "Pyramid Consistency across Domains"]], "rel_plus": [["Domain Randomization:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "Cityscapes:Dataset"], ["Domain Randomization:Method", "Evaluated-With", "BDDS:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "BDDS:Dataset"], ["Domain Randomization:Method", "Evaluated-With", "Mapillary:Dataset"], ["Pyramid Consistency across Domains:Method", "Evaluated-With", "Mapillary:Dataset"], ["DR:Method", "Synonym-Of", "Domain Randomization:Method"], ["PCD:Method", "Synonym-Of", "Pyramid Consistency across Domains:Method"]]}
{"doc_id": "211010520", "sentence": "It is likely that BiDAF as a model in the loop is worse than BERT and RoBERTa at comparative questions , as evidenced by the results in Table 5 with BiDAF reaching 9. 3 F 1 and RoBERTa reaching 3 0 . 9 F 1 on D DROP ( when trained on D SQuAD( 1 0 K ) ) .", "ner": [["BiDAF", "Method"], ["model in the loop", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"], ["BiDAF", "Method"], ["RoBERTa", "Method"], ["D DROP", "Dataset"], ["D SQuAD( 1 0 K )", "Dataset"]], "rel": [["BiDAF", "Part-Of", "model in the loop"], ["BERT", "Part-Of", "model in the loop"], ["RoBERTa", "Part-Of", "model in the loop"], ["BiDAF", "Compare-With", "BERT"], ["BiDAF", "Compare-With", "RoBERTa"], ["BiDAF", "Compare-With", "RoBERTa"], ["BiDAF", "Evaluated-With", "D DROP"], ["RoBERTa", "Evaluated-With", "D DROP"], ["BiDAF", "Trained-With", "D SQuAD( 1 0 K )"], ["RoBERTa", "Trained-With", "D SQuAD( 1 0 K )"]], "rel_plus": [["BiDAF:Method", "Part-Of", "model in the loop:Method"], ["BERT:Method", "Part-Of", "model in the loop:Method"], ["RoBERTa:Method", "Part-Of", "model in the loop:Method"], ["BiDAF:Method", "Compare-With", "BERT:Method"], ["BiDAF:Method", "Compare-With", "RoBERTa:Method"], ["BiDAF:Method", "Compare-With", "RoBERTa:Method"], ["BiDAF:Method", "Evaluated-With", "D DROP:Dataset"], ["RoBERTa:Method", "Evaluated-With", "D DROP:Dataset"], ["BiDAF:Method", "Trained-With", "D SQuAD( 1 0 K ):Dataset"], ["RoBERTa:Method", "Trained-With", "D SQuAD( 1 0 K ):Dataset"]]}
{"doc_id": "202577400", "sentence": "Again we following the same steps in previous part , we get more accurate model and our single model with ResNet 1 0 1 as backbone can achieve 8 3 . 3 % mIoU ranked 3 - rd in cityscapes leaderboard by the time of paper publication .", "ner": [["ResNet 1 0 1", "Method"], ["cityscapes", "Dataset"]], "rel": [["ResNet 1 0 1", "Evaluated-With", "cityscapes"]], "rel_plus": [["ResNet 1 0 1:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "23569888", "sentence": "Solver : we tried either SGD or Adam ( Kingma and Ba 2 0 1 5 ) solvers in Caffe .", "ner": [["Solver", "Method"], ["SGD", "Method"], ["Adam", "Method"]], "rel": [["SGD", "SubClass-Of", "Solver"], ["Adam", "SubClass-Of", "Solver"]], "rel_plus": [["SGD:Method", "SubClass-Of", "Solver:Method"], ["Adam:Method", "SubClass-Of", "Solver:Method"]]}
{"doc_id": "210164920", "sentence": "The task of object detection and instance segmentation Data augmentation using extra annotated data of [ 1 1 8 ] Sum of cross - entropy loss Deconvnet [ 8 5 ] VGG 1 6 pre - trained on ILSVRC dataset Data augmentation using extra annotated data of [ 1 1 8 ] U - Net [ 8 6 ] FCN [ 7 8 ] Data augmentation by applying random elastic deformation to the available training images DialateNet [ 1 1 0 ] VGG 1 6 [ 8 ] Data augmentation using extra annotated data of [ 1 1 8 ] ParseNet [ 8 9 ] FCN [ 7 8 ] SegNet [ 8 8 ] VGG 1 6 [ 8 ] Local contrast normalization to RGB data Cross entropy loss GCN [ 9 1 ] ResNet 1 5 2 [ 1 0 ] as feature network and FCN - 4 [ 7 8 ] as segmentation network", "ner": [["object detection", "Task"], ["instance segmentation", "Task"], ["Data augmentation", "Method"], ["cross - entropy loss", "Method"], ["Deconvnet", "Method"], ["VGG 1 6", "Method"], ["ILSVRC", "Dataset"], ["Data augmentation", "Method"], ["U - Net", "Method"], ["FCN", "Method"], ["Data augmentation", "Method"], ["DialateNet", "Method"], ["VGG 1 6", "Method"], ["Data augmentation", "Method"], ["ParseNet", "Method"], ["FCN", "Method"], ["SegNet", "Method"], ["VGG 1 6", "Method"], ["Local contrast normalization", "Method"], ["Cross entropy loss", "Method"], ["GCN", "Method"], ["ResNet 1 5 2", "Method"], ["feature network", "Method"], ["FCN - 4", "Method"], ["segmentation network", "Method"]], "rel": [["VGG 1 6", "Trained-With", "ILSVRC"], ["Deconvnet", "Trained-With", "ILSVRC"], ["Cross entropy loss", "Part-Of", "GCN"], ["ResNet 1 5 2", "Used-For", "feature network"], ["FCN - 4", "Used-For", "segmentation network"]], "rel_plus": [["VGG 1 6:Method", "Trained-With", "ILSVRC:Dataset"], ["Deconvnet:Method", "Trained-With", "ILSVRC:Dataset"], ["Cross entropy loss:Method", "Part-Of", "GCN:Method"], ["ResNet 1 5 2:Method", "Used-For", "feature network:Method"], ["FCN - 4:Method", "Used-For", "segmentation network:Method"]]}
{"doc_id": "210860962", "sentence": "Given where ( x ( i ) , y ( i ) ) are pairs of image and segmentation masks respectively , we define a FCN S which approximates as well as possible the mapping of an input sample x , potentially not included in D , to its corresponding label y. Figure 1 depicts an FCN architecture with an initial convolutional block , 2 downsampling blocks , followed by 2 upsampling blocks and a final convolution that performs the segmentation .", "ner": [["FCN", "Method"], ["FCN", "Method"], ["convolutional block", "Method"], ["downsampling blocks", "Method"], ["upsampling blocks", "Method"], ["convolution", "Method"], ["segmentation", "Task"]], "rel": [["convolutional block", "Part-Of", "FCN"], ["downsampling blocks", "Part-Of", "FCN"], ["upsampling blocks", "Part-Of", "FCN"], ["convolution", "Part-Of", "FCN"], ["FCN", "Used-For", "segmentation"]], "rel_plus": [["convolutional block:Method", "Part-Of", "FCN:Method"], ["downsampling blocks:Method", "Part-Of", "FCN:Method"], ["upsampling blocks:Method", "Part-Of", "FCN:Method"], ["convolution:Method", "Part-Of", "FCN:Method"], ["FCN:Method", "Used-For", "segmentation:Task"]]}
{"doc_id": "52180375", "sentence": "Our attention modules improves perfor - Mean IoU% FCN - 8 s [ 1 3 ] 3 7 . 8 Piecewise [ 1 1 ] 4 3 . 3 DeepLab - v 2 ( Res 1 0 1 - COCO ) [ 3 ] 4 5 . 7 RefineNet ( Res 1 5 2 ) [ 1 0 ] 4 7 . 3 PSPNet ( Res 1 0 1 ) [ 2 9 ] 4 7 . 8   In this subsection , we carry out experiments on the PAS - CAL Context dataset to further evaluate the effectiveness of our method .", "ner": [["FCN - 8 s", "Method"], ["Piecewise", "Method"], ["DeepLab - v 2", "Method"], ["Res 1 0 1 - COCO", "Method"], ["RefineNet", "Method"], ["Res 1 5 2", "Method"], ["PSPNet", "Method"], ["Res 1 0 1", "Method"], ["PAS - CAL Context", "Dataset"]], "rel": [["Res 1 0 1 - COCO", "Part-Of", "DeepLab - v 2"], ["Res 1 5 2", "Part-Of", "RefineNet"], ["Res 1 0 1", "Part-Of", "PSPNet"]], "rel_plus": [["Res 1 0 1 - COCO:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 5 2:Method", "Part-Of", "RefineNet:Method"], ["Res 1 0 1:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "210860760", "sentence": "These operators are defined by the following equations [ 8 , 5 8 , 6 1 ] : Figure 2 0 : Performance comparison of 5 methods ( DeepWalk , Node 2 Vec , ExEm(Word 2 vec ) , ExEm(fastText ) , and ExEm(fastText+Word 2 vec ) ) ) by considering their achieved values of Micro - F 1 and Macro - F 1 for various datasets under different train - test split ratio for multi - label classification task ( dimension of embedding is 1 2 8)   As investigated in the study [ 6 2 ] , link prediction can be addressed as a binary classification problem .", "ner": [["DeepWalk", "Method"], ["Node 2 Vec", "Method"], ["ExEm(Word 2 vec )", "Method"], ["ExEm(fastText )", "Method"], ["ExEm(fastText+Word 2 vec )", "Method"], ["multi - label classification", "Task"], ["link prediction", "Task"], ["binary classification", "Task"]], "rel": [["DeepWalk", "Used-For", "multi - label classification"], ["Node 2 Vec", "Used-For", "multi - label classification"], ["ExEm(Word 2 vec )", "Used-For", "multi - label classification"], ["ExEm(fastText )", "Used-For", "multi - label classification"], ["ExEm(fastText+Word 2 vec )", "Used-For", "multi - label classification"], ["link prediction", "SubTask-Of", "binary classification"]], "rel_plus": [["DeepWalk:Method", "Used-For", "multi - label classification:Task"], ["Node 2 Vec:Method", "Used-For", "multi - label classification:Task"], ["ExEm(Word 2 vec ):Method", "Used-For", "multi - label classification:Task"], ["ExEm(fastText ):Method", "Used-For", "multi - label classification:Task"], ["ExEm(fastText+Word 2 vec ):Method", "Used-For", "multi - label classification:Task"], ["link prediction:Task", "SubTask-Of", "binary classification:Task"]]}
{"doc_id": "210164920", "sentence": "Panoptic segmentation ( PS ) [ 4 0 , 1 6 5 , 9 9 , 1 6 6 , 1 6 7 , 1 6 8 ] is the combination of semantic segmentation and instance segmentation .", "ner": [["Panoptic segmentation", "Task"], ["PS", "Task"], ["semantic segmentation", "Task"], ["instance segmentation", "Task"]], "rel": [["PS", "Synonym-Of", "Panoptic segmentation"], ["semantic segmentation", "SubTask-Of", "Panoptic segmentation"], ["instance segmentation", "SubTask-Of", "Panoptic segmentation"]], "rel_plus": [["PS:Task", "Synonym-Of", "Panoptic segmentation:Task"], ["semantic segmentation:Task", "SubTask-Of", "Panoptic segmentation:Task"], ["instance segmentation:Task", "SubTask-Of", "Panoptic segmentation:Task"]]}
{"doc_id": "24972096", "sentence": "Therefore , a context - stack , composed of a chain of 6 layers of 5 \u00d7 5 \u00d7 5 1 2 convolution stacks [ Conv + BN + ReLU ] , is concatenated on the top of a pre - trained truncated VGG - 1 6 network .", "ner": [["context - stack", "Method"], ["5 \u00d7 5 \u00d7 5 1 2 convolution stacks", "Method"], ["Conv + BN + ReLU", "Method"], ["truncated VGG - 1 6", "Method"]], "rel": [["5 \u00d7 5 \u00d7 5 1 2 convolution stacks", "Part-Of", "context - stack"], ["Conv + BN + ReLU", "Synonym-Of", "5 \u00d7 5 \u00d7 5 1 2 convolution stacks"]], "rel_plus": [["5 \u00d7 5 \u00d7 5 1 2 convolution stacks:Method", "Part-Of", "context - stack:Method"], ["Conv + BN + ReLU:Method", "Synonym-Of", "5 \u00d7 5 \u00d7 5 1 2 convolution stacks:Method"]]}
{"doc_id": "208548469", "sentence": "Although the semantic meaning ranking by LASSO ranking method is not very accurate , it is still acceptable .", "ner": [["semantic meaning ranking", "Task"], ["LASSO ranking method", "Method"]], "rel": [["LASSO ranking method", "Used-For", "semantic meaning ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "semantic meaning ranking:Task"]]}
{"doc_id": "201124533", "sentence": "GridNet [ 2 9 ] is like a combination of multiple U - Nets and includes two symmetric information exchange stages : the first stage passes information only from high resolution to low resolution , and the second stage passes information only from low resolution to high resolution .", "ner": [["GridNet", "Method"], ["U - Nets", "Method"]], "rel": [["U - Nets", "Part-Of", "GridNet"]], "rel_plus": [["U - Nets:Method", "Part-Of", "GridNet:Method"]]}
{"doc_id": "52910494", "sentence": "Each composite function of dense block uses a 3 \u00d7 3 convolution layer with zero - padding to keep the feature maps fixed .", "ner": [["dense block", "Method"], ["3 \u00d7 3 convolution layer", "Method"]], "rel": [["3 \u00d7 3 convolution layer", "Part-Of", "dense block"]], "rel_plus": [["3 \u00d7 3 convolution layer:Method", "Part-Of", "dense block:Method"]]}
{"doc_id": "202888986", "sentence": "In this paper , we address all of the aforementioned problems , by designing A Lite BERT ( ALBERT ) architecture that has significantly fewer parameters than a traditional BERT architecture .", "ner": [["BERT", "Method"], ["ALBERT", "Method"], ["BERT", "Method"]], "rel": [["ALBERT", "SubClass-Of", "BERT"]], "rel_plus": [["ALBERT:Method", "SubClass-Of", "BERT:Method"]]}
{"doc_id": "147703932", "sentence": "Also , 3D pose estimation helps depth estimation , likely by reducing ambiguity : 3D pose estimation helps restrict the space of possible body poses .", "ner": [["3D pose estimation", "Task"], ["depth estimation", "Task"], ["3D pose estimation", "Task"]], "rel": [["3D pose estimation", "Used-For", "depth estimation"]], "rel_plus": [["3D pose estimation:Task", "Used-For", "depth estimation:Task"]]}
{"doc_id": "AUG104", "sentence": "Action recognition is a task, not a dataset, improved by ResNet on Kinetics.", "ner": [["action recognition", "Task"], ["ResNet", "Method"], ["Kinetics", "Dataset"]], "rel": [["ResNet", "Used-For", "action recognition"], ["ResNet", "Evaluated-With", "Kinetics"]], "rel_plus": [["ResNet:Method", "Used-For", "action recognition:Task"], ["ResNet:Method", "Evaluated-With", "Kinetics:Dataset"]]}
{"doc_id": "195347056", "sentence": "To the best of our knowledge , no existing empirical research has addressed the implementation of a generative model on a large scale paintings dataset . 2 ) we propose a novel magnified learning to synthesize better quality images . 3 ) Empirically , we show that our model is capable of generating high quality artwork that exhibit similar visual representations within genre , artist , or style . 4 ) Our model is also able to generate high resolution images on CIFAR - 1 0 [ 1 3 ] , STL - 1 0 [ 1 9 ] , Oxford - 1 0 2 [ 2 0 ] , and CUB - 2 0 0 [ 2 1 ] that look natural and contain clear object structures in them .", "ner": [["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["Oxford - 1 0 2", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "A convolutional building block contains a sequence of layers , including dropout layer , convolution layer , batch normalization layer , etc .", "ner": [["convolutional building block", "Method"], ["dropout", "Method"], ["convolution", "Method"], ["batch normalization", "Method"]], "rel": [["dropout", "Part-Of", "convolutional building block"], ["convolution", "Part-Of", "convolutional building block"], ["batch normalization", "Part-Of", "convolutional building block"]], "rel_plus": [["dropout:Method", "Part-Of", "convolutional building block:Method"], ["convolution:Method", "Part-Of", "convolutional building block:Method"], ["batch normalization:Method", "Part-Of", "convolutional building block:Method"]]}
{"doc_id": "35249701", "sentence": "We also show that our method is superior to joint training when adding the large - scale Places 3 6 5 [ 3 0 ] dataset to an ImageNet - trained network , and obtain competitive performance on a broad range of architectures , including VGG - 1 6 with batch normalization [ 1 3 ] , ResNets [ 9 ] , and DenseNets [ 1 1 ] .", "ner": [["Places 3 6 5", "Dataset"], ["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["batch normalization", "Method"], ["ResNets", "Method"], ["DenseNets", "Method"]], "rel": [["VGG - 1 6", "Trained-With", "Places 3 6 5"], ["ResNets", "Trained-With", "Places 3 6 5"], ["DenseNets", "Trained-With", "Places 3 6 5"], ["VGG - 1 6", "Trained-With", "ImageNet"], ["ResNets", "Trained-With", "ImageNet"], ["DenseNets", "Trained-With", "ImageNet"], ["batch normalization", "Part-Of", "VGG - 1 6"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "Places 3 6 5:Dataset"], ["ResNets:Method", "Trained-With", "Places 3 6 5:Dataset"], ["DenseNets:Method", "Trained-With", "Places 3 6 5:Dataset"], ["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"], ["ResNets:Method", "Trained-With", "ImageNet:Dataset"], ["DenseNets:Method", "Trained-With", "ImageNet:Dataset"], ["batch normalization:Method", "Part-Of", "VGG - 1 6:Method"]]}
{"doc_id": "202577400", "sentence": "To keep spatial information required by detection and segmentation tasks , convolutional networks designed for image classification are modified to FCNs by removing global information aggregation layers such as global average pooling layer and fully - connected layers [ 2 3 ] .", "ner": [["detection", "Task"], ["segmentation", "Task"], ["convolutional networks", "Method"], ["image classification", "Task"], ["FCNs", "Method"], ["global average pooling layer", "Method"], ["fully - connected layers", "Method"]], "rel": [["FCNs", "SubClass-Of", "convolutional networks"], ["convolutional networks", "Used-For", "image classification"], ["global average pooling layer", "Part-Of", "FCNs"], ["fully - connected layers", "Part-Of", "FCNs"]], "rel_plus": [["FCNs:Method", "SubClass-Of", "convolutional networks:Method"], ["convolutional networks:Method", "Used-For", "image classification:Task"], ["global average pooling layer:Method", "Part-Of", "FCNs:Method"], ["fully - connected layers:Method", "Part-Of", "FCNs:Method"]]}
{"doc_id": "104291983", "sentence": "In facial landmark detection , our approach achieves overall best results on four standard datasets : AFLW , COFW , 3 0 0 W , and WFLW .", "ner": [["facial landmark detection", "Task"], ["AFLW", "Dataset"], ["COFW", "Dataset"], ["3 0 0 W", "Dataset"], ["WFLW", "Dataset"]], "rel": [["AFLW", "Benchmark-For", "facial landmark detection"], ["COFW", "Benchmark-For", "facial landmark detection"], ["3 0 0 W", "Benchmark-For", "facial landmark detection"], ["WFLW", "Benchmark-For", "facial landmark detection"]], "rel_plus": [["AFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["COFW:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["3 0 0 W:Dataset", "Benchmark-For", "facial landmark detection:Task"], ["WFLW:Dataset", "Benchmark-For", "facial landmark detection:Task"]]}
{"doc_id": "210164920", "sentence": "Chen et al aggregate ' atrous ' algorithm and conditional random ( CRF ) field in semantic segmentation and proposed DeepLab [ 8 2 ] as discussed in section 3. 2 . 2 .", "ner": [["atrous", "Method"], ["conditional random", "Method"], ["CRF", "Method"], ["semantic segmentation", "Task"], ["DeepLab", "Method"]], "rel": [["CRF", "Synonym-Of", "conditional random"], ["conditional random", "Used-For", "semantic segmentation"]], "rel_plus": [["CRF:Method", "Synonym-Of", "conditional random:Method"], ["conditional random:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "104291983", "sentence": "The upsample subnetwork could be a symmetric version of the downsample subnetwork , with skipping connection over some mirrored layers to transform the pooling indices , e.g. , SegNet [ 2 ] and DeconvNet [ 7 4 ] , or copying the feature maps , e.g. , U - Net [ 8 3 ] and Hourglass [ 7 2 , 1 1 1 , 7 , 2 2 , 6 ] , encoder - decoder [ 7 7 ] , FPN [ 6 2 ] , and so on .", "ner": [["SegNet", "Method"], ["DeconvNet", "Method"], ["U - Net", "Method"], ["Hourglass", "Method"], ["encoder - decoder", "Method"], ["FPN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210860760", "sentence": "Moreover , deep learning based graph embedding techniques with random walks represent a graph as a set of random walks and these random walks are fed into a deep learning method like Skip - Gram to optimize their neighborhood preserving likelihood objectives .", "ner": [["deep learning", "Method"], ["graph embedding", "Method"], ["random walks", "Method"], ["deep learning", "Method"], ["Skip - Gram", "Method"]], "rel": [["deep learning", "Used-For", "graph embedding"], ["random walks", "Part-Of", "graph embedding"], ["Skip - Gram", "SubClass-Of", "deep learning"]], "rel_plus": [["deep learning:Method", "Used-For", "graph embedding:Method"], ["random walks:Method", "Part-Of", "graph embedding:Method"], ["Skip - Gram:Method", "SubClass-Of", "deep learning:Method"]]}
{"doc_id": "AUG097", "sentence": "Semantic segmentation is a task, not a method, enhanced by SegNet on Cityscapes.", "ner": [["semantic segmentation", "Task"], ["SegNet", "Method"], ["Cityscapes", "Dataset"]], "rel": [["SegNet", "Used-For", "semantic segmentation"], ["SegNet", "Trained-With", "Cityscapes"]], "rel_plus": [["SegNet:Method", "Used-For", "semantic segmentation:Task"], ["SegNet:Method", "Trained-With", "Cityscapes:Dataset"]]}
{"doc_id": "AUG123", "sentence": "Long Short-Term Memory, abbreviated as LSTM, enhances text classification on AGNews.", "ner": [["Long Short-Term Memory", "Method"], ["LSTM", "Method"], ["text classification", "Task"], ["AGNews", "Dataset"]], "rel": [["LSTM", "Synonym-Of", "Long Short-Term Memory"], ["LSTM", "Used-For", "text classification"], ["LSTM", "Trained-With", "AGNews"]], "rel_plus": [["LSTM:Method", "Synonym-Of", "Long Short-Term Memory:Method"], ["LSTM:Method", "Used-For", "text classification:Task"], ["LSTM:Method", "Trained-With", "AGNews:Dataset"]]}
{"doc_id": "210860962", "sentence": "In our model , we use image translation as auxiliary task and we rely on the state - of - the - art StarGAN [ 1 0 ] model as the backbone module of the proposed network .", "ner": [["image translation", "Task"], ["StarGAN", "Method"]], "rel": [["StarGAN", "Used-For", "image translation"]], "rel_plus": [["StarGAN:Method", "Used-For", "image translation:Task"]]}
{"doc_id": "202888751", "sentence": "We compare the results of AR - GAN with various approaches including supervised pix 2 pix .", "ner": [["AR - GAN", "Method"], ["supervised pix 2 pix", "Method"]], "rel": [["AR - GAN", "Compare-With", "supervised pix 2 pix"]], "rel_plus": [["AR - GAN:Method", "Compare-With", "supervised pix 2 pix:Method"]]}
{"doc_id": "211004033", "sentence": "Our strategy of ellipse regression ( e.g. , ellipse R - CNN - ) leads significant performance improvement on all metrics compared to the baseline model .", "ner": [["ellipse regression", "Task"], ["ellipse R - CNN -", "Method"]], "rel": [["ellipse R - CNN -", "Used-For", "ellipse regression"]], "rel_plus": [["ellipse R - CNN -:Method", "Used-For", "ellipse regression:Task"]]}
{"doc_id": "150374036", "sentence": "In face detection , several face detectors like MTCNN [ 6 7 ] and Dlib [ 3 ] ) are used to locate faces in complex scenes .", "ner": [["face detection", "Task"], ["MTCNN", "Method"], ["Dlib", "Method"]], "rel": [["MTCNN", "Used-For", "face detection"], ["Dlib", "Used-For", "face detection"]], "rel_plus": [["MTCNN:Method", "Used-For", "face detection:Task"], ["Dlib:Method", "Used-For", "face detection:Task"]]}
{"doc_id": "11241677", "sentence": "Next , we investigate generalization of the video - level features learned using the YouTube - 8 M dataset and perform transfer learning experiments on the Sports - 1 M dataset .", "ner": [["YouTube - 8 M", "Dataset"], ["transfer learning", "Task"], ["Sports - 1 M", "Dataset"]], "rel": [["Sports - 1 M", "Benchmark-For", "transfer learning"]], "rel_plus": [["Sports - 1 M:Dataset", "Benchmark-For", "transfer learning:Task"]]}
{"doc_id": "201646309", "sentence": "This is in contrast to ( Conneau et al. , 2 0 1 7 ) , who found it beneficial for the BiLSTM - layer of InferSent to use MAX instead of MEAN pooling .", "ner": [["BiLSTM - layer", "Method"], ["InferSent", "Method"], ["MAX", "Method"], ["MEAN pooling", "Method"]], "rel": [["BiLSTM - layer", "Part-Of", "InferSent"], ["MAX", "Part-Of", "InferSent"]], "rel_plus": [["BiLSTM - layer:Method", "Part-Of", "InferSent:Method"], ["MAX:Method", "Part-Of", "InferSent:Method"]]}
{"doc_id": "211010786", "sentence": "Granger causality is a fundamental technique for causal inference in time series data , commonly used in the social and biological sciences .", "ner": [["Granger causality", "Method"], ["causal inference", "Task"]], "rel": [["Granger causality", "Used-For", "causal inference"]], "rel_plus": [["Granger causality:Method", "Used-For", "causal inference:Task"]]}
{"doc_id": "211004033", "sentence": "Recent success in the general object detection tasks on Pascal [ 1 2 ] , ImageNet [ 1 3 ] , and MS COCO datasets [ 1 4 ] , have been achieved by both singleshot [ 1 5 ] , [ 1 6 ] and R - CNN [ 6 ] , [ 7 ] , [ 1 7 ] architectures .", "ner": [["object detection", "Task"], ["Pascal", "Dataset"], ["ImageNet", "Dataset"], ["MS COCO", "Dataset"], ["singleshot", "Method"], ["R - CNN", "Method"]], "rel": [["Pascal", "Benchmark-For", "object detection"], ["ImageNet", "Benchmark-For", "object detection"], ["MS COCO", "Benchmark-For", "object detection"], ["singleshot", "Used-For", "object detection"], ["R - CNN", "Used-For", "object detection"], ["singleshot", "Evaluated-With", "Pascal"], ["R - CNN", "Evaluated-With", "Pascal"], ["singleshot", "Evaluated-With", "ImageNet"], ["R - CNN", "Evaluated-With", "ImageNet"], ["singleshot", "Evaluated-With", "MS COCO"], ["R - CNN", "Evaluated-With", "MS COCO"]], "rel_plus": [["Pascal:Dataset", "Benchmark-For", "object detection:Task"], ["ImageNet:Dataset", "Benchmark-For", "object detection:Task"], ["MS COCO:Dataset", "Benchmark-For", "object detection:Task"], ["singleshot:Method", "Used-For", "object detection:Task"], ["R - CNN:Method", "Used-For", "object detection:Task"], ["singleshot:Method", "Evaluated-With", "Pascal:Dataset"], ["R - CNN:Method", "Evaluated-With", "Pascal:Dataset"], ["singleshot:Method", "Evaluated-With", "ImageNet:Dataset"], ["R - CNN:Method", "Evaluated-With", "ImageNet:Dataset"], ["singleshot:Method", "Evaluated-With", "MS COCO:Dataset"], ["R - CNN:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "54447105", "sentence": "To train the RPN , a binary class label for foreground/background classification is usually assigned to each anchor .", "ner": [["RPN", "Method"], ["foreground/background classification", "Task"]], "rel": [["RPN", "Used-For", "foreground/background classification"]], "rel_plus": [["RPN:Method", "Used-For", "foreground/background classification:Task"]]}
{"doc_id": "211020570", "sentence": "In [ 3 9 ] , STN is applied to the task of image composition , and an STN is embedded in the generator of the generative adversarial network ( GAN ) for warping a specific object of a given image and placing it in the scene image .", "ner": [["STN", "Method"], ["image composition", "Task"], ["STN", "Method"], ["generator", "Method"], ["generative adversarial network", "Method"], ["GAN", "Method"]], "rel": [["STN", "Used-For", "image composition"], ["STN", "Part-Of", "generator"], ["GAN", "Synonym-Of", "generative adversarial network"], ["generator", "Part-Of", "generative adversarial network"]], "rel_plus": [["STN:Method", "Used-For", "image composition:Task"], ["STN:Method", "Part-Of", "generator:Method"], ["GAN:Method", "Synonym-Of", "generative adversarial network:Method"], ["generator:Method", "Part-Of", "generative adversarial network:Method"]]}
{"doc_id": "7507210", "sentence": "The dropout rate is 0. 3 for those with 2, 0 4 8 channels , e.g. , the last three units in ResNets and the second last units ( B 6 ) in our networks ; while 0. 5 for those with 4, 0 9 6 channels , e.g. , the top - most units ( B 7 ) in our networks .", "ner": [["dropout", "Method"], ["ResNets", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "52009210", "sentence": "We believe that DBpedia can be enhanced by inserting these missing links , but augmenting DBpedia is beyond the scope of this work .", "ner": [["DBpedia", "Dataset"], ["DBpedia", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "AUG101", "sentence": "Image segmentation is a task, not a method, improved by Mask R-CNN on PASCAL VOC.", "ner": [["image segmentation", "Task"], ["Mask R-CNN", "Method"], ["PASCAL VOC", "Dataset"]], "rel": [["Mask R-CNN", "Used-For", "image segmentation"], ["Mask R-CNN", "Trained-With", "PASCAL VOC"]], "rel_plus": [["Mask R-CNN:Method", "Used-For", "image segmentation:Task"], ["Mask R-CNN:Method", "Trained-With", "PASCAL VOC:Dataset"]]}
{"doc_id": "6116678", "sentence": "This paper can be viewed as the first piece of work that aims to discover visual contrast information inside an image using end - to - end convolutional neural networks .   As shown in Fig. 1 , the architecture of our deep contrast network for salient object detection consists of two complementary components , a fully convolutional stream and a segment - wise spatial pooling stream .", "ner": [["convolutional neural networks", "Method"], ["deep contrast network", "Method"], ["salient object detection", "Task"], ["fully convolutional stream", "Method"], ["segment - wise spatial pooling stream", "Method"]], "rel": [["fully convolutional stream", "Part-Of", "deep contrast network"], ["segment - wise spatial pooling stream", "Part-Of", "deep contrast network"], ["deep contrast network", "Used-For", "salient object detection"]], "rel_plus": [["fully convolutional stream:Method", "Part-Of", "deep contrast network:Method"], ["segment - wise spatial pooling stream:Method", "Part-Of", "deep contrast network:Method"], ["deep contrast network:Method", "Used-For", "salient object detection:Task"]]}
{"doc_id": "209532167", "sentence": "As an instance of a non - classification task , we consider the joint classification , localization and repair version of the variable misuse task from Vasic et al. ( 2 0 1 9 ) .", "ner": [["non - classification task", "Task"], ["joint classification", "Task"], ["localization and repair version of the variable misuse", "Task"]], "rel": [["joint classification", "SubTask-Of", "non - classification task"], ["localization and repair version of the variable misuse", "SubTask-Of", "non - classification task"]], "rel_plus": [["joint classification:Task", "SubTask-Of", "non - classification task:Task"], ["localization and repair version of the variable misuse:Task", "SubTask-Of", "non - classification task:Task"]]}
{"doc_id": "150374036", "sentence": "Levi and Hassner [ 3 3 ] leverage the CASIA - WebFace [ 6 3 ] face recognition dataset to pretrain four different VGGNet [ 5 3 ] and GoogleNet [ 5 5 ] .", "ner": [["CASIA - WebFace", "Dataset"], ["face recognition", "Task"], ["VGGNet", "Method"], ["GoogleNet", "Method"]], "rel": [["VGGNet", "Trained-With", "CASIA - WebFace"], ["GoogleNet", "Trained-With", "CASIA - WebFace"]], "rel_plus": [["VGGNet:Method", "Trained-With", "CASIA - WebFace:Dataset"], ["GoogleNet:Method", "Trained-With", "CASIA - WebFace:Dataset"]]}
{"doc_id": "52180375", "sentence": "Our attention network achieves outstanding performance consistently on four scene segmentation datasets , i.e. Cityscapes , Pascal VOC 2 0 1 2 , Pascal Context , and COCO Stuff .", "ner": [["scene segmentation", "Task"], ["Cityscapes", "Dataset"], ["Pascal VOC 2 0 1 2", "Dataset"], ["Pascal Context", "Dataset"], ["COCO Stuff", "Dataset"]], "rel": [["COCO Stuff", "Benchmark-For", "scene segmentation"], ["Pascal Context", "Benchmark-For", "scene segmentation"], ["Pascal VOC 2 0 1 2", "Benchmark-For", "scene segmentation"], ["Cityscapes", "Benchmark-For", "scene segmentation"]], "rel_plus": [["COCO Stuff:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Pascal Context:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Pascal VOC 2 0 1 2:Dataset", "Benchmark-For", "scene segmentation:Task"], ["Cityscapes:Dataset", "Benchmark-For", "scene segmentation:Task"]]}
{"doc_id": "211020570", "sentence": "As illustrated in Fig. 1 , MSM consists of three pivotal steps : GAN - based spatial transformation , CNN - based landmark detection and exemplar - based shape reconstruction .", "ner": [["MSM", "Method"], ["GAN - based spatial transformation", "Method"], ["CNN - based landmark detection", "Method"], ["exemplar - based shape reconstruction", "Method"]], "rel": [["GAN - based spatial transformation", "Part-Of", "MSM"], ["CNN - based landmark detection", "Part-Of", "MSM"], ["exemplar - based shape reconstruction", "Part-Of", "MSM"]], "rel_plus": [["GAN - based spatial transformation:Method", "Part-Of", "MSM:Method"], ["CNN - based landmark detection:Method", "Part-Of", "MSM:Method"], ["exemplar - based shape reconstruction:Method", "Part-Of", "MSM:Method"]]}
{"doc_id": "198231883", "sentence": "SOUVOS is closer related to foreground/background segmentation as it requires only one foreground area to be segmented which often is a grouping of multiple objects into one foreground object .", "ner": [["SOUVOS", "Task"], ["foreground/background segmentation", "Task"]], "rel": [["SOUVOS", "Compare-With", "foreground/background segmentation"]], "rel_plus": [["SOUVOS:Task", "Compare-With", "foreground/background segmentation:Task"]]}
{"doc_id": "53719742", "sentence": "Here we compare the performance of two different base convolutional networks , i.e. , Resnet 5 0 and ResneXt 5 0 , on ICDAR - 2 0 1 7 MLT and ICDAR - 2 0 1 5 .", "ner": [["convolutional networks", "Method"], ["Resnet 5 0", "Method"], ["ResneXt 5 0", "Method"], ["ICDAR - 2 0 1 7 MLT", "Dataset"], ["ICDAR - 2 0 1 5", "Dataset"]], "rel": [["ResneXt 5 0", "SubClass-Of", "convolutional networks"], ["Resnet 5 0", "SubClass-Of", "convolutional networks"], ["convolutional networks", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["Resnet 5 0", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["ResneXt 5 0", "Evaluated-With", "ICDAR - 2 0 1 7 MLT"], ["convolutional networks", "Evaluated-With", "ICDAR - 2 0 1 5"], ["Resnet 5 0", "Evaluated-With", "ICDAR - 2 0 1 5"], ["ResneXt 5 0", "Evaluated-With", "ICDAR - 2 0 1 5"]], "rel_plus": [["ResneXt 5 0:Method", "SubClass-Of", "convolutional networks:Method"], ["Resnet 5 0:Method", "SubClass-Of", "convolutional networks:Method"], ["convolutional networks:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["Resnet 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["ResneXt 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 7 MLT:Dataset"], ["convolutional networks:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["Resnet 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"], ["ResneXt 5 0:Method", "Evaluated-With", "ICDAR - 2 0 1 5:Dataset"]]}
{"doc_id": "210164517", "sentence": "The Vanilla CNN model is framed with 1 2 layers of Convolution operation with batch normalization and ReLU activation function .", "ner": [["CNN", "Method"], ["Convolution", "Method"], ["batch normalization", "Method"], ["ReLU activation", "Method"]], "rel": [["Convolution", "Part-Of", "CNN"], ["batch normalization", "Part-Of", "Convolution"], ["ReLU activation", "Part-Of", "Convolution"]], "rel_plus": [["Convolution:Method", "Part-Of", "CNN:Method"], ["batch normalization:Method", "Part-Of", "Convolution:Method"], ["ReLU activation:Method", "Part-Of", "Convolution:Method"]]}
{"doc_id": "52910494", "sentence": "Motivated by these structures , we propose \" Stochastic Feature Reuse \" ( SFR ) as an effective regularizer in DenseNets to promote the generalization of networks and Figure 4 .", "ner": [["Stochastic Feature Reuse", "Method"], ["SFR", "Method"], ["regularizer", "Method"], ["DenseNets", "Method"]], "rel": [["SFR", "Synonym-Of", "Stochastic Feature Reuse"], ["Stochastic Feature Reuse", "SubClass-Of", "regularizer"], ["Stochastic Feature Reuse", "Part-Of", "DenseNets"], ["regularizer", "Part-Of", "DenseNets"]], "rel_plus": [["SFR:Method", "Synonym-Of", "Stochastic Feature Reuse:Method"], ["Stochastic Feature Reuse:Method", "SubClass-Of", "regularizer:Method"], ["Stochastic Feature Reuse:Method", "Part-Of", "DenseNets:Method"], ["regularizer:Method", "Part-Of", "DenseNets:Method"]]}
{"doc_id": "24972096", "sentence": "For intelligent mobile robotics applications , extending 3D mapping to 3D semantic mapping enables robots not only to localize themselves with respect to the scene 's geometrical features , but also to simultaneously understand the higher - level semantic meaning of a complex scene .", "ner": [["intelligent mobile robotics", "Task"], ["3D mapping", "Task"], ["3D semantic mapping", "Task"]], "rel": [["3D mapping", "SubTask-Of", "intelligent mobile robotics"], ["3D semantic mapping", "SubTask-Of", "intelligent mobile robotics"], ["3D semantic mapping", "SubTask-Of", "3D mapping"]], "rel_plus": [["3D mapping:Task", "SubTask-Of", "intelligent mobile robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "intelligent mobile robotics:Task"], ["3D semantic mapping:Task", "SubTask-Of", "3D mapping:Task"]]}
{"doc_id": "210164920", "sentence": "Semantic Boundaries Dataset [ 1 1 8 ] is used as auxiliary dataset PSPNet [ 9 0 ] Pretrained ResNet [ 1 0 ] Data augmentation : random mirror and random resize between 0. 5 and 2 , random rotation between - 1 0 and 1 0 degrees , random Gaussian blur Four losses : \u2022 Additional loss for initial result generation \u2022 Final loss for learning the residue later \u2022 Auxiliary loss for shallow layers \u2022 Master branch loss for final prediction FC - DenseNet [ 1 1 5 ] DensNet [ 1 0 ] Data augmentation using random cropping and vertical flipping Gated - SCNN [ 9 4 ] ResNet 1 0 1 [ 1 0 ] and WideResNet [ 1 1 9 ] \u2022 Segmentation loss for regular stream \u2022 Dual task loss for shape stream \u2022\u2022 Standard binary cross entropy loss for boundary refinement \u2022\u2022 Standard cross entropy for semantic segmentation are quite correlated .", "ner": [["Semantic Boundaries Dataset", "Dataset"], ["PSPNet", "Method"], ["ResNet", "Method"], ["Data augmentation", "Method"], ["random mirror", "Method"], ["random resize", "Method"], ["random rotation", "Method"], ["random Gaussian blur", "Method"], ["FC - DenseNet", "Method"], ["DensNet", "Method"], ["Data augmentation", "Method"], ["random cropping", "Method"], ["vertical flipping", "Method"], ["Gated - SCNN", "Method"], ["ResNet 1 0 1", "Method"], ["WideResNet", "Method"], ["binary cross entropy loss", "Method"], ["boundary refinement", "Task"], ["cross entropy", "Method"], ["semantic segmentation", "Task"]], "rel": [["Semantic Boundaries Dataset", "Used-For", "PSPNet"], ["random mirror", "SubClass-Of", "Data augmentation"], ["random resize", "SubClass-Of", "Data augmentation"], ["random rotation", "SubClass-Of", "Data augmentation"], ["random Gaussian blur", "SubClass-Of", "Data augmentation"], ["vertical flipping", "SubClass-Of", "Data augmentation"], ["random cropping", "SubClass-Of", "Data augmentation"], ["binary cross entropy loss", "Used-For", "boundary refinement"], ["cross entropy", "Used-For", "semantic segmentation"]], "rel_plus": [["Semantic Boundaries Dataset:Dataset", "Used-For", "PSPNet:Method"], ["random mirror:Method", "SubClass-Of", "Data augmentation:Method"], ["random resize:Method", "SubClass-Of", "Data augmentation:Method"], ["random rotation:Method", "SubClass-Of", "Data augmentation:Method"], ["random Gaussian blur:Method", "SubClass-Of", "Data augmentation:Method"], ["vertical flipping:Method", "SubClass-Of", "Data augmentation:Method"], ["random cropping:Method", "SubClass-Of", "Data augmentation:Method"], ["binary cross entropy loss:Method", "Used-For", "boundary refinement:Task"], ["cross entropy:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "209532167", "sentence": "Thus , the pre - trained contextual embedding provides superior results even with a smaller budget of Table 2 : Test accuracies of fine - tuned CuBERT against BiLSTM ( with and without Word 2 Vec embeddings ) and Transformer trained from scratch on the classification tasks . \" ns \" and \" hs \" respectively refer to negative sampling and hierarchical softmax settings used for training CBOW and Skipgram models . \" From scratch \" refers to training with freshly initialized token embeddings , that is , without pre - trained Word 2 Vec embeddings . 2 0 epochs , compared to the 1 0 0 epochs used for BiLSTMs .", "ner": [["CuBERT", "Method"], ["BiLSTM", "Method"], ["Word 2 Vec", "Method"], ["Transformer", "Method"], ["classification", "Task"], ["ns", "Method"], ["hs", "Method"], ["negative sampling", "Method"], ["hierarchical softmax", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"], ["Word 2 Vec", "Method"], ["BiLSTMs", "Method"]], "rel": [["Word 2 Vec", "Part-Of", "BiLSTM"], ["CuBERT", "Compare-With", "BiLSTM"], ["CuBERT", "Compare-With", "Transformer"], ["CuBERT", "Used-For", "classification"], ["BiLSTM", "Used-For", "classification"], ["Transformer", "Used-For", "classification"], ["ns", "Synonym-Of", "negative sampling"], ["hs", "Synonym-Of", "hierarchical softmax"], ["negative sampling", "Part-Of", "CBOW"], ["hierarchical softmax", "Part-Of", "Skipgram"], ["hierarchical softmax", "Part-Of", "Skipgram"], ["negative sampling", "Part-Of", "Skipgram"]], "rel_plus": [["Word 2 Vec:Method", "Part-Of", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "BiLSTM:Method"], ["CuBERT:Method", "Compare-With", "Transformer:Method"], ["CuBERT:Method", "Used-For", "classification:Task"], ["BiLSTM:Method", "Used-For", "classification:Task"], ["Transformer:Method", "Used-For", "classification:Task"], ["ns:Method", "Synonym-Of", "negative sampling:Method"], ["hs:Method", "Synonym-Of", "hierarchical softmax:Method"], ["negative sampling:Method", "Part-Of", "CBOW:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"], ["hierarchical softmax:Method", "Part-Of", "Skipgram:Method"], ["negative sampling:Method", "Part-Of", "Skipgram:Method"]]}
{"doc_id": "202888986", "sentence": "The speed - up results in Table 3 indicate that data - throughput for BERT - large is about 3. 1 7 x higher compared to ALBERT - xxlarge .", "ner": [["BERT - large", "Method"], ["ALBERT - xxlarge", "Method"]], "rel": [["BERT - large", "Compare-With", "ALBERT - xxlarge"]], "rel_plus": [["BERT - large:Method", "Compare-With", "ALBERT - xxlarge:Method"]]}
{"doc_id": "23569888", "sentence": "Fig. 4 Example of a Convolutional Neural Network ( Sec. 3. 1 ) and of the two knowledge transfer approaches considered in this work ( Sec. 3. 2 ) . ( Blue pipeline ) feature extraction : in this case the response of one of the layers is used as a feature vector for a \" shallow \" predictor like RLSCs , SVMs ( see Sec. 3. 2 . 1 ) , which is trained on the new task . ( Red pipeline ) fine - tuning : in this case the network is trained end - to - end to the new task by replacing the final layer and using the original model as a \" warm - restart \" ( see Sec. 3. 2 . 2 ) .", "ner": [["Convolutional Neural Network", "Method"], ["feature extraction", "Method"], ["\" shallow \" predictor", "Method"], ["RLSCs", "Method"], ["SVMs", "Method"]], "rel": [["RLSCs", "SubClass-Of", "\" shallow \" predictor"], ["SVMs", "SubClass-Of", "\" shallow \" predictor"]], "rel_plus": [["RLSCs:Method", "SubClass-Of", "\" shallow \" predictor:Method"], ["SVMs:Method", "SubClass-Of", "\" shallow \" predictor:Method"]]}
{"doc_id": "202540590", "sentence": "Stanford Attentive Reader ( Chen et al. , 2 0 1 6 ) performs a bilinear attention between the question and paragraph for answer prediction .", "ner": [["Stanford Attentive Reader", "Method"], ["bilinear attention", "Method"]], "rel": [["bilinear attention", "Part-Of", "Stanford Attentive Reader"]], "rel_plus": [["bilinear attention:Method", "Part-Of", "Stanford Attentive Reader:Method"]]}
{"doc_id": "67855714", "sentence": "This method notably outperformed CNN - based methods for the problem P sisr .   Consider a problem P of learning a mapping function F , parameterized by \u03b8 F , that transforms images from a domain X to a domain Y , given a training set of N pairs { ( x i , y i ) } N i= 1 \u2208 X \u00d7 Y. Denote by p X and p Y the probability distributions respectively over X and Y. In addition , we introduce a given features extractor function denoted \u03a6 , parameterized by \u03b8 \u03a6 , that maps an image y \u2208 Y to a certain euclidean feature space S \u03a6 of dimensionality d. The mappings F and \u03a6 are typically feed - forward Convolutional Neural Networks .", "ner": [["CNN", "Method"], ["Convolutional Neural Networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Unfortunately , the advantage of GA modules for large objects is a disadvantage for small patterns such as object boundaries and small objects , where features from GA modules tends to oversmooth the predictions for these small patterns .", "ner": [["GA", "Method"], ["GA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "However , Kinetics did not need such techniques to train deep 3D CNNs .", "ner": [["Kinetics", "Dataset"], ["3D CNNs", "Method"]], "rel": [["3D CNNs", "Trained-With", "Kinetics"]], "rel_plus": [["3D CNNs:Method", "Trained-With", "Kinetics:Dataset"]]}
{"doc_id": "210920315", "sentence": "Next , we describe the standard pedestrian detection ( PD ) branch , followed by a detailed presentation of our part cooccurrence ( PSC ) module ( Sec. III - B ) .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["PSC", "Method"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"]]}
{"doc_id": "211010520", "sentence": "In total , 1, 3 8 6 workers completed this task with 7 5 2 being assigned the qualification . \" Beat the AI Annotation \" The \" Beat the AI \" question generation HIT presents workers with a randomly selected passage from SQuAD 1 . 1 , about which workers are expected to generate questions and provide answers .", "ner": [["question generation HIT", "Task"], ["SQuAD 1 . 1", "Dataset"]], "rel": [["SQuAD 1 . 1", "Benchmark-For", "question generation HIT"]], "rel_plus": [["SQuAD 1 . 1:Dataset", "Benchmark-For", "question generation HIT:Task"]]}
{"doc_id": "209862890", "sentence": "As will be shown in the analysis section , the entity embeddings from BERT better capture entity type information than those from Ganea and Hofmann ( 2 0 1 7 ) .", "ner": [["entity embeddings", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "entity embeddings"]], "rel_plus": [["BERT:Method", "Used-For", "entity embeddings:Task"]]}
{"doc_id": "210860760", "sentence": "Considering the efficiency of the embedding vectors in the experiments of multi - label classification , link prediction and recommendation tasks , ExEm performs better than the other 2 relevant methods .", "ner": [["multi - label classification", "Task"], ["link prediction", "Task"], ["recommendation", "Task"], ["ExEm", "Method"]], "rel": [["ExEm", "Used-For", "multi - label classification"], ["ExEm", "Used-For", "link prediction"], ["ExEm", "Used-For", "recommendation"]], "rel_plus": [["ExEm:Method", "Used-For", "multi - label classification:Task"], ["ExEm:Method", "Used-For", "link prediction:Task"], ["ExEm:Method", "Used-For", "recommendation:Task"]]}
{"doc_id": "104291983", "sentence": "The fully convolutional network is extended , by replacing a few ( typically two ) strided convolutions and the associated convolutions with dilated convolutions , to the dilation version , leading to medium - resolution representations [ 1 2 6 , 1 3 , 1 1 5 , 1 2 , 5 7 ] .", "ner": [["fully convolutional network", "Method"], ["strided convolutions", "Method"], ["convolutions", "Method"], ["dilated convolutions", "Method"]], "rel": [["dilated convolutions", "Part-Of", "fully convolutional network"], ["convolutions", "Part-Of", "fully convolutional network"]], "rel_plus": [["dilated convolutions:Method", "Part-Of", "fully convolutional network:Method"], ["convolutions:Method", "Part-Of", "fully convolutional network:Method"]]}
{"doc_id": "198147921", "sentence": "For instance , robots , autonomous vehicles , and surveillance and security systems rely on accurate detection of 3D objects to enable efficient object recognition , grasping , manipulation , obstacle avoidance , scene understanding , and accurate navigation .", "ner": [["detection of 3D objects", "Task"], ["object recognition", "Task"], ["grasping", "Task"], ["manipulation", "Task"], ["obstacle avoidance", "Task"], ["scene understanding", "Task"], ["accurate navigation", "Task"]], "rel": [["detection of 3D objects", "Used-For", "object recognition"], ["detection of 3D objects", "Used-For", "grasping"], ["detection of 3D objects", "Used-For", "manipulation"], ["detection of 3D objects", "Used-For", "obstacle avoidance"], ["detection of 3D objects", "Used-For", "scene understanding"], ["detection of 3D objects", "Used-For", "accurate navigation"]], "rel_plus": [["detection of 3D objects:Task", "Used-For", "object recognition:Task"], ["detection of 3D objects:Task", "Used-For", "grasping:Task"], ["detection of 3D objects:Task", "Used-For", "manipulation:Task"], ["detection of 3D objects:Task", "Used-For", "obstacle avoidance:Task"], ["detection of 3D objects:Task", "Used-For", "scene understanding:Task"], ["detection of 3D objects:Task", "Used-For", "accurate navigation:Task"]]}
{"doc_id": "23569888", "sentence": "In our experiments we performed fine - tuning only for CaffeNet and GoogLeNet , which are representative of most recent architectures and were significantly faster to fine - tune than VGG - 1 6 and ResNet - 5 0 .", "ner": [["CaffeNet", "Method"], ["GoogLeNet", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["CaffeNet", "Compare-With", "VGG - 1 6"], ["GoogLeNet", "Compare-With", "VGG - 1 6"], ["GoogLeNet", "Compare-With", "ResNet - 5 0"], ["CaffeNet", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["CaffeNet:Method", "Compare-With", "VGG - 1 6:Method"], ["GoogLeNet:Method", "Compare-With", "VGG - 1 6:Method"], ["GoogLeNet:Method", "Compare-With", "ResNet - 5 0:Method"], ["CaffeNet:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "210713911", "sentence": "We also conducted an ablation study on three standard fine - grained image retrieval ( IR ) datasets : Stanford Online Products ( SOP ) [ 3 0 ] , CUB 2 0 0 [ 3 5 ] and CARS 1 9 6 [ 1 9 ] .", "ner": [["image retrieval", "Task"], ["IR", "Task"], ["Stanford Online Products", "Dataset"], ["SOP", "Dataset"], ["CUB 2 0 0", "Dataset"], ["CARS 1 9 6", "Dataset"]], "rel": [["IR", "Synonym-Of", "image retrieval"], ["Stanford Online Products", "Evaluated-With", "image retrieval"], ["CUB 2 0 0", "Evaluated-With", "image retrieval"], ["CARS 1 9 6", "Evaluated-With", "image retrieval"], ["SOP", "Synonym-Of", "Stanford Online Products"]], "rel_plus": [["IR:Task", "Synonym-Of", "image retrieval:Task"], ["Stanford Online Products:Dataset", "Evaluated-With", "image retrieval:Task"], ["CUB 2 0 0:Dataset", "Evaluated-With", "image retrieval:Task"], ["CARS 1 9 6:Dataset", "Evaluated-With", "image retrieval:Task"], ["SOP:Dataset", "Synonym-Of", "Stanford Online Products:Dataset"]]}
{"doc_id": "210861282", "sentence": "In a different feamework , LCR - Net [ 3 8 ] has different branches for the detection using Detectron [ 1 7 ] and the arrangement of joints during classification .", "ner": [["LCR - Net", "Method"], ["detection", "Task"], ["Detectron", "Method"], ["classification", "Task"]], "rel": [["Detectron", "Part-Of", "LCR - Net"], ["LCR - Net", "Used-For", "detection"], ["LCR - Net", "Used-For", "classification"]], "rel_plus": [["Detectron:Method", "Part-Of", "LCR - Net:Method"], ["LCR - Net:Method", "Used-For", "detection:Task"], ["LCR - Net:Method", "Used-For", "classification:Task"]]}
{"doc_id": "202565512", "sentence": "Compared to models with extracted structured knowledge in group 3 , our model extracts graph paths from ConceptNet for graph - based reasoning rather than for pre - training , and we also extract evidence from Wikipedia plain texts , which brings 1 3 . 1 % and 5. 7 % gains over BERT + AMS and ROBERTa + CSPT respectively .", "ner": [["ConceptNet", "Dataset"], ["Wikipedia", "Dataset"], ["BERT + AMS", "Method"], ["ROBERTa + CSPT", "Method"]], "rel": [["ConceptNet", "Used-For", "BERT + AMS"], ["Wikipedia", "Used-For", "BERT + AMS"], ["Wikipedia", "Used-For", "ROBERTa + CSPT"], ["ConceptNet", "Used-For", "ROBERTa + CSPT"]], "rel_plus": [["ConceptNet:Dataset", "Used-For", "BERT + AMS:Method"], ["Wikipedia:Dataset", "Used-For", "BERT + AMS:Method"], ["Wikipedia:Dataset", "Used-For", "ROBERTa + CSPT:Method"], ["ConceptNet:Dataset", "Used-For", "ROBERTa + CSPT:Method"]]}
{"doc_id": "3920676", "sentence": "While common strategies such as random 2 - D translations can be readily applied [ 7 3 ] , we can use sophisticated methods such as CycleGAN [ 1 0 8 ] and LSRO [ 1 0 9 ] to generate realisitic and meaningful images .", "ner": [["CycleGAN", "Method"], ["LSRO", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "We also train Word 2 Vec embeddings ( Mikolov et al. , 2 0 1 3 a , b ) , namely , continuous bag - of - words ( CBOW ) and Skipgram embeddings , on the same corpus .", "ner": [["Word 2 Vec", "Method"], ["continuous bag - of - words", "Method"], ["CBOW", "Method"], ["Skipgram", "Method"]], "rel": [["continuous bag - of - words", "SubClass-Of", "Word 2 Vec"], ["Skipgram", "SubClass-Of", "Word 2 Vec"], ["CBOW", "Synonym-Of", "continuous bag - of - words"]], "rel_plus": [["continuous bag - of - words:Method", "SubClass-Of", "Word 2 Vec:Method"], ["Skipgram:Method", "SubClass-Of", "Word 2 Vec:Method"], ["CBOW:Method", "Synonym-Of", "continuous bag - of - words:Method"]]}
{"doc_id": "51559", "sentence": "As the use of a fully connected layer for recurrent connections violates the constraint of \" strong typing \" , all strongly - typed RNN architectures ( including the T - RNN , T - GRU , and T - LSTM ) are also quasi - recurrent .", "ner": [["fully connected layer", "Method"], ["strongly - typed RNN architectures", "Method"], ["T - RNN", "Method"], ["T - GRU", "Method"], ["T - LSTM", "Method"]], "rel": [["T - RNN", "SubClass-Of", "strongly - typed RNN architectures"], ["T - GRU", "SubClass-Of", "strongly - typed RNN architectures"], ["T - LSTM", "SubClass-Of", "strongly - typed RNN architectures"]], "rel_plus": [["T - RNN:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"], ["T - GRU:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"], ["T - LSTM:Method", "SubClass-Of", "strongly - typed RNN architectures:Method"]]}
{"doc_id": "195347056", "sentence": "Although ArtGAN - EB performed better than ArtGAN with higher Inception score ( 8. 2 6 by ArtGAN - EB compared to 8. 2 1 by ArtGAN - DFM ) , it has worse objectness score ( 3 3 . 5 1 by ArtGAN - EB compared to 3 3 . 2 4 by ArtGAN ) .", "ner": [["ArtGAN - EB", "Method"], ["ArtGAN", "Method"], ["ArtGAN - EB", "Method"], ["ArtGAN - DFM", "Method"], ["ArtGAN - EB", "Method"], ["ArtGAN", "Method"]], "rel": [["ArtGAN - EB", "Compare-With", "ArtGAN"], ["ArtGAN - EB", "Compare-With", "ArtGAN - DFM"], ["ArtGAN - EB", "Compare-With", "ArtGAN"]], "rel_plus": [["ArtGAN - EB:Method", "Compare-With", "ArtGAN:Method"], ["ArtGAN - EB:Method", "Compare-With", "ArtGAN - DFM:Method"], ["ArtGAN - EB:Method", "Compare-With", "ArtGAN:Method"]]}
{"doc_id": "195347056", "sentence": "Recently , another subfamily of GAN was introduced by employing an autoencoder in the discriminator .", "ner": [["GAN", "Method"], ["autoencoder", "Method"], ["discriminator", "Method"]], "rel": [["discriminator", "Part-Of", "GAN"], ["autoencoder", "Part-Of", "discriminator"]], "rel_plus": [["discriminator:Method", "Part-Of", "GAN:Method"], ["autoencoder:Method", "Part-Of", "discriminator:Method"]]}
{"doc_id": "207880647", "sentence": "Colored text denotes question generated with different language models . \" GPT - 2 LM \" corresponds to the GPT - 2 LM fine - tuned for question generation without optimization . \" BERT - Feedback \" corresponds to the approach using BERT as QA feedback module during training . \" GPT - 2 Feedback \" corresponds to the approach employing GPT - 2 as feedback mechanism . \" GT \" stands for groundtruth .", "ner": [["GPT - 2 LM", "Method"], ["GPT - 2 LM", "Method"], ["question generation", "Task"], ["BERT - Feedback", "Method"], ["BERT", "Method"], ["QA", "Task"], ["GPT - 2 Feedback", "Method"], ["GPT - 2", "Method"]], "rel": [["GPT - 2 LM", "Used-For", "question generation"], ["BERT", "Part-Of", "BERT - Feedback"], ["BERT", "Used-For", "QA"], ["GPT - 2", "Part-Of", "GPT - 2 Feedback"]], "rel_plus": [["GPT - 2 LM:Method", "Used-For", "question generation:Task"], ["BERT:Method", "Part-Of", "BERT - Feedback:Method"], ["BERT:Method", "Used-For", "QA:Task"], ["GPT - 2:Method", "Part-Of", "GPT - 2 Feedback:Method"]]}
{"doc_id": "199668978", "sentence": "Moreover , BiDAF works on the reading comprehension , the answer is the paraphrase in the context , therefore there is a great change that this model always generates the same answer , which brings another challenge for training .", "ner": [["BiDAF", "Method"], ["reading comprehension", "Task"]], "rel": [["BiDAF", "Used-For", "reading comprehension"]], "rel_plus": [["BiDAF:Method", "Used-For", "reading comprehension:Task"]]}
{"doc_id": "201070697", "sentence": "SPA - GAN is more successful in generating tiger pattern in lion \u2192 tiger translation ( row 8 and 9 ) compared to other methods .", "ner": [["SPA - GAN", "Method"], ["lion \u2192 tiger translation", "Task"]], "rel": [["SPA - GAN", "Used-For", "lion \u2192 tiger translation"]], "rel_plus": [["SPA - GAN:Method", "Used-For", "lion \u2192 tiger translation:Task"]]}
{"doc_id": "211010520", "sentence": "Furthermore we find that human workers are able to generate questions which successfully \" beat \" the model in the loop 5 9 . 4 % of the time for BiDAF , 4 7 . 1 % for BERT and 4 4 . 0 % for RoBERTa .", "ner": [["model in the loop", "Method"], ["BiDAF", "Method"], ["BERT", "Method"], ["RoBERTa", "Method"]], "rel": [["BiDAF", "Part-Of", "model in the loop"], ["BERT", "Part-Of", "model in the loop"], ["RoBERTa", "Part-Of", "model in the loop"]], "rel_plus": [["BiDAF:Method", "Part-Of", "model in the loop:Method"], ["BERT:Method", "Part-Of", "model in the loop:Method"], ["RoBERTa:Method", "Part-Of", "model in the loop:Method"]]}
{"doc_id": "202540251", "sentence": "Remarkably , our generalization results are on par with or even better than those obtained by state - of - the - art simulation - to - real domain adaptation methods , which access the target domain data at training time .   1   Simulation has spurred growing interests for training deep neural nets ( DNNs ) for computer vision tasks [ 5 3 , 1 0 , 2 3 , 5 5 ] .", "ner": [["simulation - to - real domain adaptation", "Method"], ["deep neural nets", "Method"], ["DNNs", "Method"], ["computer vision", "Task"]], "rel": [["DNNs", "Synonym-Of", "deep neural nets"], ["deep neural nets", "Used-For", "computer vision"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "deep neural nets:Method"], ["deep neural nets:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "211010758", "sentence": "According to the Taylor 's theorem , we have the SGD gradients as follows : If we consider there are two steps of parameter updates with stochastic gradient descent ( SGD ) , where the gradient of the first step is g 1 and the one of second step is g 2 .", "ner": [["SGD", "Method"], ["stochastic gradient descent", "Method"], ["SGD", "Method"]], "rel": [["SGD", "Synonym-Of", "stochastic gradient descent"]], "rel_plus": [["SGD:Method", "Synonym-Of", "stochastic gradient descent:Method"]]}
{"doc_id": "199543700", "sentence": "The improving performance of the Alexnet [ 2 9 ] to the VGG [ 5 3 ] , and finally to the ResNet [ 2 1 ] , all demonstrates the great successes in the ILSVRC [ 5 1 ] challenge on the Imagenet Dataset [ 1 2 ] .", "ner": [["Alexnet", "Method"], ["VGG", "Method"], ["ResNet", "Method"], ["ILSVRC", "Dataset"], ["Imagenet", "Dataset"]], "rel": [["ResNet", "Evaluated-With", "ILSVRC"], ["VGG", "Evaluated-With", "ILSVRC"], ["Alexnet", "Evaluated-With", "ILSVRC"], ["Alexnet", "Evaluated-With", "Imagenet"], ["VGG", "Evaluated-With", "Imagenet"], ["ResNet", "Evaluated-With", "Imagenet"]], "rel_plus": [["ResNet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["VGG:Method", "Evaluated-With", "ILSVRC:Dataset"], ["Alexnet:Method", "Evaluated-With", "ILSVRC:Dataset"], ["Alexnet:Method", "Evaluated-With", "Imagenet:Dataset"], ["VGG:Method", "Evaluated-With", "Imagenet:Dataset"], ["ResNet:Method", "Evaluated-With", "Imagenet:Dataset"]]}
{"doc_id": "211010758", "sentence": "Unsupervised domain adaptation methods [ 2 9 , 3 0 , 1 1 , 5 4 , 4 5 , 4 7 ] for image classification are developed to learn domain - invariant features by minimizing the source error and simultaneously the domain discrepancy through feature distribution alignment .", "ner": [["Unsupervised domain adaptation methods", "Method"], ["image classification", "Task"]], "rel": [["Unsupervised domain adaptation methods", "Used-For", "image classification"]], "rel_plus": [["Unsupervised domain adaptation methods:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "208548469", "sentence": "Gated Recurrent Unit ( GRU ) Chung et al ( 2 0 1 4 ) is another variant of RNN , and the authors of Noh et al ( 2 0 1 6 b ) use it to encode an input question .", "ner": [["Gated Recurrent Unit", "Method"], ["GRU", "Method"], ["RNN", "Method"]], "rel": [["GRU", "Synonym-Of", "Gated Recurrent Unit"], ["Gated Recurrent Unit", "SubClass-Of", "RNN"]], "rel_plus": [["GRU:Method", "Synonym-Of", "Gated Recurrent Unit:Method"], ["Gated Recurrent Unit:Method", "SubClass-Of", "RNN:Method"]]}
{"doc_id": "210920315", "sentence": "The standard pedestrian detection ( PD ) branch is based on Faster R - CNN [ 3 6 ] typically employed in existing pedestrian detection works [ 5 0 ] , [ 3 3 ] .", "ner": [["pedestrian detection", "Task"], ["PD", "Task"], ["Faster R - CNN", "Method"], ["pedestrian detection", "Task"]], "rel": [["PD", "Synonym-Of", "pedestrian detection"], ["Faster R - CNN", "Used-For", "pedestrian detection"]], "rel_plus": [["PD:Task", "Synonym-Of", "pedestrian detection:Task"], ["Faster R - CNN:Method", "Used-For", "pedestrian detection:Task"]]}
{"doc_id": "146120936", "sentence": "In addition to the FPN structure , Mask R - CNN adopts a deconvolution layer at the end of mask head .", "ner": [["FPN", "Method"], ["Mask R - CNN", "Method"], ["deconvolution layer", "Method"]], "rel": [["deconvolution layer", "Part-Of", "Mask R - CNN"]], "rel_plus": [["deconvolution layer:Method", "Part-Of", "Mask R - CNN:Method"]]}
{"doc_id": "35249701", "sentence": "The values above correspond to the case when the respective dataset is added as the first task , to an ImageNet - trained VGG - 1 6 that is 5 0 % pruned , except for the values corresponding to the ImageNet dataset which correspond to initial pruning .", "ner": [["ImageNet", "Dataset"], ["VGG - 1 6", "Method"], ["ImageNet", "Dataset"]], "rel": [["VGG - 1 6", "Trained-With", "ImageNet"]], "rel_plus": [["VGG - 1 6:Method", "Trained-With", "ImageNet:Dataset"]]}
{"doc_id": "201646309", "sentence": "Universal Sentence Encoder was pre - trained on question - answering data , which appears to be beneficial for the question - type classification task of the TREC dataset .", "ner": [["Universal Sentence Encoder", "Method"], ["question - type classification", "Task"], ["TREC", "Dataset"]], "rel": [["TREC", "Benchmark-For", "question - type classification"], ["Universal Sentence Encoder", "Used-For", "question - type classification"], ["Universal Sentence Encoder", "Evaluated-With", "TREC"]], "rel_plus": [["TREC:Dataset", "Benchmark-For", "question - type classification:Task"], ["Universal Sentence Encoder:Method", "Used-For", "question - type classification:Task"], ["Universal Sentence Encoder:Method", "Evaluated-With", "TREC:Dataset"]]}
{"doc_id": "104291983", "sentence": "In addition , we build a multi - level representation from the high - resolution representation and apply it to the Faster R - CNN object detection framework and the extended frameworks .", "ner": [["Faster R - CNN", "Method"], ["object detection", "Task"]], "rel": [["Faster R - CNN", "Used-For", "object detection"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "198897554", "sentence": "In [ 1 8 ] , a curriculum learning style is applied , where superpixels are computed in source and target domains and their distributions must match as auxiliary task during semantic segmentation training .", "ner": [["curriculum learning", "Method"], ["semantic segmentation", "Task"]], "rel": [["curriculum learning", "Used-For", "semantic segmentation"]], "rel_plus": [["curriculum learning:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "AUG115", "sentence": "Document classification is a task, not a method, enhanced by DistilBERT on 20NG.", "ner": [["document classification", "Task"], ["DistilBERT", "Method"], ["20NG", "Dataset"]], "rel": [["DistilBERT", "Used-For", "document classification"], ["DistilBERT", "Trained-With", "20NG"]], "rel_plus": [["DistilBERT:Method", "Used-For", "document classification:Task"], ["DistilBERT:Method", "Trained-With", "20NG:Dataset"]]}
{"doc_id": "24972096", "sentence": "RGB - D SLAM [ 8 ] is adopted for dense 3D mapping .", "ner": [["RGB - D SLAM", "Method"], ["dense 3D mapping", "Task"]], "rel": [["RGB - D SLAM", "Used-For", "dense 3D mapping"]], "rel_plus": [["RGB - D SLAM:Method", "Used-For", "dense 3D mapping:Task"]]}
{"doc_id": "4539700", "sentence": "Each convolutional layer is followed by batch normalization [ 1 3 ] and a ReLU [ 1 8 ] .", "ner": [["convolutional layer", "Method"], ["batch normalization", "Method"], ["ReLU", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4246700", "sentence": "To further complete the task , Johnson et al. [ 3 1 ] proposed a Fully Convolutional Localization Network ( FCLN ) architecture that can localize and describe regions of an image at the same time .", "ner": [["Fully Convolutional Localization Network", "Method"], ["FCLN", "Method"]], "rel": [["FCLN", "Synonym-Of", "Fully Convolutional Localization Network"]], "rel_plus": [["FCLN:Method", "Synonym-Of", "Fully Convolutional Localization Network:Method"]]}
{"doc_id": "210839714", "sentence": "Table 4 shows the readmission prediction and mortality prediction performance of all models on eICU .", "ner": [["readmission prediction", "Task"], ["mortality prediction", "Task"], ["eICU", "Dataset"]], "rel": [["eICU", "Benchmark-For", "readmission prediction"], ["eICU", "Benchmark-For", "mortality prediction"]], "rel_plus": [["eICU:Dataset", "Benchmark-For", "readmission prediction:Task"], ["eICU:Dataset", "Benchmark-For", "mortality prediction:Task"]]}
{"doc_id": "208548469", "sentence": "The authors of Kiros et al ( 2 0 1 4 ) ; Lin et al ( 2 0 1 5 ) have shown that the bilinear interaction between two embedding spaces is very successful in deep learning for fine - grained classification and multimodal language modeling .", "ner": [["deep learning", "Method"], ["fine - grained classification", "Task"], ["multimodal language modeling", "Task"]], "rel": [["deep learning", "Used-For", "fine - grained classification"], ["deep learning", "Used-For", "multimodal language modeling"]], "rel_plus": [["deep learning:Method", "Used-For", "fine - grained classification:Task"], ["deep learning:Method", "Used-For", "multimodal language modeling:Task"]]}
{"doc_id": "52009210", "sentence": "This paper presents SIMPLEDBPEDIAQA , a new benchmark dataset for simple question answering over knowledge graphs created by migrating the SIMPLEQUESTIONS dataset from Freebase to DBpedia .", "ner": [["SIMPLEDBPEDIAQA", "Dataset"], ["question answering over knowledge graphs", "Task"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Used-For", "question answering over knowledge graphs"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Used-For", "question answering over knowledge graphs:Task"]]}
{"doc_id": "202676714", "sentence": "SFA is composed of ( F ) l \u2297 ( F ) m and its complex conjugate ( F ) n\u2212l \u2297 ( F ) n\u2212m to create a perturbation that has real values since inputs of CNNs are assumed to be real values .", "ner": [["SFA", "Method"], ["CNNs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "The original backbone is augmented by the Hourglass Residual Units ( HRU ) with the goal of increasing the receptive FOV .", "ner": [["Hourglass Residual Units", "Method"], ["HRU", "Method"], ["FOV", "Method"]], "rel": [["HRU", "Synonym-Of", "Hourglass Residual Units"], ["FOV", "Part-Of", "Hourglass Residual Units"]], "rel_plus": [["HRU:Method", "Synonym-Of", "Hourglass Residual Units:Method"], ["FOV:Method", "Part-Of", "Hourglass Residual Units:Method"]]}
{"doc_id": "210860962", "sentence": "For the CycleGAN [ 6 0 ] and StarGAN [ 1 0 ] architectures presented in Sections III - B and III - C respectively , we mostly use the same implementation hyper - parameters published by the authors with slight modifications .", "ner": [["CycleGAN", "Method"], ["StarGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "204901567", "sentence": "We use Wikipedia as our training corpus , similar to mBERT and XLM ( Lample and Conneau , 2 0 1 9 ) , which we extract using the WikiExtractor tool . 5 We do not perform any lowercasing or normalization .", "ner": [["Wikipedia", "Dataset"], ["mBERT", "Method"], ["XLM", "Method"]], "rel": [["mBERT", "Trained-With", "Wikipedia"], ["XLM", "Trained-With", "Wikipedia"]], "rel_plus": [["mBERT:Method", "Trained-With", "Wikipedia:Dataset"], ["XLM:Method", "Trained-With", "Wikipedia:Dataset"]]}
{"doc_id": "201646309", "sentence": "In section 7 , we compare the computational efficiency of SBERT sentence embeddings in contrast to other state - of - the - art sentence embedding methods .", "ner": [["SBERT sentence embeddings", "Method"], ["sentence embedding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201124533", "sentence": "The proposed approach achieves state - of - the - art results on PASCAL - Context , Cityscapes , and LIP with similar model sizes and lower computation com - [ 1 0 1 ] , ResNet [ 3 9 ] ) , which is formed by connecting high - to - low convolutions in series . ( b ) A high - resolution representation recovering subnetwork , which is formed by connecting low - to - high convolutions in series .", "ner": [["PASCAL - Context", "Dataset"], ["Cityscapes", "Dataset"], ["LIP", "Dataset"], ["ResNet", "Method"], ["high - to - low convolutions", "Method"], ["low - to - high convolutions", "Method"]], "rel": [["high - to - low convolutions", "Part-Of", "ResNet"]], "rel_plus": [["high - to - low convolutions:Method", "Part-Of", "ResNet:Method"]]}
{"doc_id": "3920676", "sentence": "We split VIPeR , CUHK 0 1 , CUHK 0 2 , GRID , CAVIAR , 3DPeS , PRID , WARD - 1 2 , WARD - 1 3 and iLIDSVID into equal - sized training and testing sets .", "ner": [["VIPeR", "Dataset"], ["CUHK 0 1", "Dataset"], ["CUHK 0 2", "Dataset"], ["GRID", "Dataset"], ["CAVIAR", "Dataset"], ["3DPeS", "Dataset"], ["PRID", "Dataset"], ["WARD - 1 2", "Dataset"], ["WARD - 1 3", "Dataset"], ["iLIDSVID", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210861282", "sentence": "For that reason , the batch size utilized varied from high amounts for lower resolution datasets ( e.g. LSP ) to smaller batches of 4 for datasets such as the BBC Pose [ 1 0 ] .", "ner": [["LSP", "Dataset"], ["BBC Pose", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "102351044", "sentence": "Then for drop - path of the new convolutional building block proposed in Section 3. 2 . 4 and drop - layer , we test their effectiveness in comparison with drop - channel and drop - neuron on ResNeXt [ 3 6 ] with multiple paths and residual connection , Wide Residual Networks [ 3 7 ] ( WRN ) with wider convolutional layer and residual connection and DenseNet [ 1 3 ] with shortcut connection .", "ner": [["drop - path", "Method"], ["convolutional building block", "Method"], ["drop - layer", "Method"], ["drop - channel", "Method"], ["drop - neuron", "Method"], ["ResNeXt", "Method"], ["residual connection", "Method"], ["Wide Residual Networks", "Method"], ["WRN", "Method"], ["convolutional layer", "Method"], ["residual connection", "Method"], ["DenseNet", "Method"], ["shortcut connection", "Method"]], "rel": [["drop - path", "Part-Of", "convolutional building block"], ["drop - layer", "Compare-With", "drop - channel"], ["drop - path", "Compare-With", "drop - channel"], ["drop - layer", "Compare-With", "drop - neuron"], ["drop - path", "Compare-With", "drop - neuron"], ["drop - neuron", "Part-Of", "ResNeXt"], ["drop - channel", "Part-Of", "ResNeXt"], ["residual connection", "Part-Of", "ResNeXt"], ["Wide Residual Networks", "Part-Of", "ResNeXt"], ["DenseNet", "Part-Of", "ResNeXt"], ["WRN", "Synonym-Of", "Wide Residual Networks"], ["convolutional layer", "Part-Of", "Wide Residual Networks"], ["residual connection", "Part-Of", "Wide Residual Networks"], ["shortcut connection", "Part-Of", "DenseNet"]], "rel_plus": [["drop - path:Method", "Part-Of", "convolutional building block:Method"], ["drop - layer:Method", "Compare-With", "drop - channel:Method"], ["drop - path:Method", "Compare-With", "drop - channel:Method"], ["drop - layer:Method", "Compare-With", "drop - neuron:Method"], ["drop - path:Method", "Compare-With", "drop - neuron:Method"], ["drop - neuron:Method", "Part-Of", "ResNeXt:Method"], ["drop - channel:Method", "Part-Of", "ResNeXt:Method"], ["residual connection:Method", "Part-Of", "ResNeXt:Method"], ["Wide Residual Networks:Method", "Part-Of", "ResNeXt:Method"], ["DenseNet:Method", "Part-Of", "ResNeXt:Method"], ["WRN:Method", "Synonym-Of", "Wide Residual Networks:Method"], ["convolutional layer:Method", "Part-Of", "Wide Residual Networks:Method"], ["residual connection:Method", "Part-Of", "Wide Residual Networks:Method"], ["shortcut connection:Method", "Part-Of", "DenseNet:Method"]]}
{"doc_id": "59599694", "sentence": "The LSTM model has a better performance than MLP , BN and CNN models , demonstrating its improved performance in time series forecasting .", "ner": [["LSTM", "Method"], ["MLP", "Method"], ["BN", "Method"], ["CNN", "Method"], ["time series forecasting", "Task"]], "rel": [["LSTM", "Compare-With", "MLP"], ["LSTM", "Compare-With", "BN"], ["LSTM", "Compare-With", "CNN"], ["LSTM", "Used-For", "time series forecasting"], ["MLP", "Used-For", "time series forecasting"], ["BN", "Used-For", "time series forecasting"], ["CNN", "Used-For", "time series forecasting"]], "rel_plus": [["LSTM:Method", "Compare-With", "MLP:Method"], ["LSTM:Method", "Compare-With", "BN:Method"], ["LSTM:Method", "Compare-With", "CNN:Method"], ["LSTM:Method", "Used-For", "time series forecasting:Task"], ["MLP:Method", "Used-For", "time series forecasting:Task"], ["BN:Method", "Used-For", "time series forecasting:Task"], ["CNN:Method", "Used-For", "time series forecasting:Task"]]}
{"doc_id": "52180375", "sentence": "Mean IoU% FCN [ 1 3 ] 6 2 . 2 DeepLab - v 2 (Res 1 0 1 - COCO ) [ 3 ] 7 1 . 6 Piecewise [ 1 1 ] 7 5 . 3 ResNet 3 8 [ 1 0 ] 8 2 . 5 PSPNet(Res 1 0 1 ) [ 2 9 ] 8 2 . 6 EncNet ( Res 1 0 1 ) [ 2 7 ] 8 2 . 9 DANet(Res 1 0 1 ) 8 2 . 6   We carry out experiments on the PASCAL VOC 2 0 1 2 dataset to further evaluate the effectiveness of our method .", "ner": [["FCN", "Method"], ["DeepLab - v 2", "Method"], ["(Res 1 0 1 - COCO", "Method"], ["Piecewise", "Method"], ["ResNet 3 8", "Method"], ["PSPNet(Res 1 0 1 )", "Method"], ["EncNet", "Method"], ["Res 1 0 1", "Method"], ["DANet(Res 1 0 1 )", "Method"], ["PASCAL VOC 2 0 1 2", "Dataset"]], "rel": [["(Res 1 0 1 - COCO", "Part-Of", "DeepLab - v 2"], ["Res 1 0 1", "Part-Of", "EncNet"]], "rel_plus": [["(Res 1 0 1 - COCO:Method", "Part-Of", "DeepLab - v 2:Method"], ["Res 1 0 1:Method", "Part-Of", "EncNet:Method"]]}
{"doc_id": "202888751", "sentence": "The translations produced using our approach are often of similar quality to the fully supervised pix 2 pix .    In Tables 2 and 3 , we evaluate different generator architectural variants and compare their performance in terms of FID and NIMA using powdery mildew class from the tomato plant disease dataset and cityscapes dataset , respectively .", "ner": [["fully supervised pix 2 pix", "Method"], ["FID", "Method"], ["NIMA", "Method"], ["tomato plant disease dataset", "Dataset"], ["cityscapes", "Dataset"]], "rel": [["FID", "Evaluated-With", "tomato plant disease dataset"], ["NIMA", "Evaluated-With", "tomato plant disease dataset"], ["FID", "Evaluated-With", "cityscapes"], ["NIMA", "Evaluated-With", "cityscapes"]], "rel_plus": [["FID:Method", "Evaluated-With", "tomato plant disease dataset:Dataset"], ["NIMA:Method", "Evaluated-With", "tomato plant disease dataset:Dataset"], ["FID:Method", "Evaluated-With", "cityscapes:Dataset"], ["NIMA:Method", "Evaluated-With", "cityscapes:Dataset"]]}
{"doc_id": "153312532", "sentence": "We evaluate BERT - FiT and BERT - ITPT - FiT on different numbers of training examples .", "ner": [["BERT - FiT", "Method"], ["BERT - ITPT - FiT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210839545", "sentence": "Comparing GPS module with other methods with the same dilation rates setting , GSP module provides larger RF , higher SR and introduces less parameters . higher SR .", "ner": [["GPS", "Method"], ["GSP module", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "210920315", "sentence": "Further , these approaches obtain a fixedsized proposal representation by performing a pooling operation ( e.g. , RoI Pool or RoiAlign ) on the high - level features from the later layer of the backbone network ( e.g. , conv 5 of VGG ) .", "ner": [["pooling operation", "Method"], ["RoI Pool", "Method"], ["RoiAlign", "Method"], ["conv 5", "Method"], ["VGG", "Method"]], "rel": [["RoI Pool", "SubClass-Of", "pooling operation"], ["RoiAlign", "SubClass-Of", "pooling operation"]], "rel_plus": [["RoI Pool:Method", "SubClass-Of", "pooling operation:Method"], ["RoiAlign:Method", "SubClass-Of", "pooling operation:Method"]]}
{"doc_id": "52180375", "sentence": "After that we perform a matrix multiplication between the transpose of C and B , and apply a softmax layer to calculate the spatial attention map S \u2208 R N \u00d7N : where s ji measures the i th position 's impact on j th position .", "ner": [["softmax", "Method"], ["spatial attention", "Method"]], "rel": [["softmax", "Used-For", "spatial attention"]], "rel_plus": [["softmax:Method", "Used-For", "spatial attention:Method"]]}
{"doc_id": "202676714", "sentence": "However , L 1 and WD significantly decrease in accuracy when the regularization weight is higher than 1 0 \u2212 1 .", "ner": [["L 1", "Method"], ["WD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "Even though our FSSD does not perform as well as DSOD and DSSD , it should be noted that our base model is VGG 1 6 and FSSD has the best accuracy compared with other algorithms such Figure 5 .", "ner": [["FSSD", "Method"], ["DSOD", "Method"], ["DSSD", "Method"], ["VGG 1 6", "Method"], ["FSSD", "Method"]], "rel": [["VGG 1 6", "Part-Of", "FSSD"], ["FSSD", "Compare-With", "DSOD"], ["FSSD", "Compare-With", "DSSD"]], "rel_plus": [["VGG 1 6:Method", "Part-Of", "FSSD:Method"], ["FSSD:Method", "Compare-With", "DSOD:Method"], ["FSSD:Method", "Compare-With", "DSSD:Method"]]}
{"doc_id": "59599694", "sentence": "Table 1 shows the comparison of the baseline , state - of - arts neural network , and CNN - LSTM models for traffic flow prediction .", "ner": [["neural network", "Method"], ["CNN - LSTM", "Method"], ["traffic flow prediction", "Task"]], "rel": [["neural network", "Used-For", "traffic flow prediction"], ["CNN - LSTM", "Used-For", "traffic flow prediction"]], "rel_plus": [["neural network:Method", "Used-For", "traffic flow prediction:Task"], ["CNN - LSTM:Method", "Used-For", "traffic flow prediction:Task"]]}
{"doc_id": "211020570", "sentence": "In Table III , we compare our results with LBF [ 1 1 ] , TCDCN [ 4 8 ] , CFSS [ 4 9 ] , MDM [ 5 0 ] , RAR [ 2 9 ] , DAN [ 3 0 ] , TSR [ 1 2 ] , SHN [ 1 3 ] , LAB [ 3 4 ] , DCFE [ 3 5 ] , 3DDE [ 5 1 ] , PCD - CNN [ 5 2 ] , SAN [ 5 3 ] , DeCaFA [ 5 5 ] , AGCFN [ 5 6 ] and ODN [ 5 4 ] are also used in Table III .", "ner": [["LBF", "Method"], ["TCDCN", "Method"], ["CFSS", "Method"], ["MDM", "Method"], ["RAR", "Method"], ["DAN", "Method"], ["TSR", "Method"], ["SHN", "Method"], ["LAB", "Method"], ["DCFE", "Method"], ["3DDE", "Method"], ["PCD - CNN", "Method"], ["SAN", "Method"], ["DeCaFA", "Method"], ["AGCFN", "Method"], ["ODN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "202719032", "sentence": "In addition , unlike Faster R - CNN , Mask RCNN uses a Feature Pyramid Network ( FPN ) [ 1 5 ] with ResNet as its backbone .", "ner": [["Faster R - CNN", "Method"], ["Mask RCNN", "Method"], ["Feature Pyramid Network", "Method"], ["FPN", "Method"], ["ResNet", "Method"]], "rel": [["Mask RCNN", "Compare-With", "Faster R - CNN"], ["Feature Pyramid Network", "Part-Of", "Mask RCNN"], ["FPN", "Synonym-Of", "Feature Pyramid Network"], ["ResNet", "Part-Of", "Feature Pyramid Network"]], "rel_plus": [["Mask RCNN:Method", "Compare-With", "Faster R - CNN:Method"], ["Feature Pyramid Network:Method", "Part-Of", "Mask RCNN:Method"], ["FPN:Method", "Synonym-Of", "Feature Pyramid Network:Method"], ["ResNet:Method", "Part-Of", "Feature Pyramid Network:Method"]]}
{"doc_id": "102351044", "sentence": "Take traditional pre - activation convolutional layers [ 8 , 1 3 , 1 2 ] for example , the convolutional transformation follows a BN \u2212 ReLU \u2212 Conv pipeline , which is illustrated in Figure 5a .", "ner": [["pre - activation convolutional layers", "Method"], ["convolutional transformation", "Method"], ["BN \u2212 ReLU \u2212 Conv", "Method"]], "rel": [["BN \u2212 ReLU \u2212 Conv", "Part-Of", "pre - activation convolutional layers"], ["convolutional transformation", "Part-Of", "pre - activation convolutional layers"]], "rel_plus": [["BN \u2212 ReLU \u2212 Conv:Method", "Part-Of", "pre - activation convolutional layers:Method"], ["convolutional transformation:Method", "Part-Of", "pre - activation convolutional layers:Method"]]}
{"doc_id": "53731879", "sentence": "In [ 4 9 ] , Zhong et al. introduce a Hetero - Homogeneous Learning ( HHL ) method , which aims to improve the generalization ability of re - ID models on the target set by achieving camera invariance and domain connectedness simultaneously .", "ner": [["Hetero - Homogeneous Learning", "Method"], ["HHL", "Method"], ["re - ID", "Task"]], "rel": [["HHL", "Synonym-Of", "Hetero - Homogeneous Learning"], ["Hetero - Homogeneous Learning", "Used-For", "re - ID"]], "rel_plus": [["HHL:Method", "Synonym-Of", "Hetero - Homogeneous Learning:Method"], ["Hetero - Homogeneous Learning:Method", "Used-For", "re - ID:Task"]]}
{"doc_id": "211010520", "sentence": "When trained on data collected with a BiDAF model in the loop , RoBERTa achieves 3 9 . 9 F 1 on questions that it can not answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself ( 4 1 . 0 F 1 ) .", "ner": [["BiDAF model in the loop", "Method"], ["RoBERTa", "Method"], ["SQuAD", "Dataset"], ["RoBERTa", "Method"]], "rel": [["RoBERTa", "Trained-With", "BiDAF model in the loop"], ["RoBERTa", "Trained-With", "SQuAD"]], "rel_plus": [["RoBERTa:Method", "Trained-With", "BiDAF model in the loop:Method"], ["RoBERTa:Method", "Trained-With", "SQuAD:Dataset"]]}
{"doc_id": "53731879", "sentence": "When tested on DukeMTMC - reID , Market - 1 5 0 1 is used as source , and vice versa . \" Baseline denotes using the full identity labels on the corresponding target dataset(See Section 3. 1 ) . \" Direct Transfer means directly applying the source - trained model on the target domain.\"UDA \" stands for the state - of - art unsupervised domain adaptation approach . \" SSM \" means self similarity mining as described in Section 3. 2 . 1 . \" w SG \" is our proposed similarity - guided one shot mining strategy . \" w/o SG \" means training the one shot domain framework only by one shot data . \" Jointly \" stands for proposed joint training strategy in Section 3. 2 . 2 .", "ner": [["DukeMTMC - reID", "Dataset"], ["Market - 1 5 0 1", "Dataset"], ["domain.\"UDA \"", "Method"], ["unsupervised domain adaptation approach", "Method"], ["SSM", "Method"], ["self similarity mining", "Method"], ["w SG", "Method"], ["similarity - guided one shot mining", "Method"]], "rel": [["domain.\"UDA \"", "Synonym-Of", "unsupervised domain adaptation approach"], ["SSM", "Synonym-Of", "self similarity mining"], ["w SG", "Synonym-Of", "similarity - guided one shot mining"]], "rel_plus": [["domain.\"UDA \":Method", "Synonym-Of", "unsupervised domain adaptation approach:Method"], ["SSM:Method", "Synonym-Of", "self similarity mining:Method"], ["w SG:Method", "Synonym-Of", "similarity - guided one shot mining:Method"]]}
{"doc_id": "210920315", "sentence": "Recent years have witnessed significant progress in the field of pedestrian detection , mainly due to the advances in deep convolutional neural networks ( CNNs ) .", "ner": [["pedestrian detection", "Task"], ["convolutional neural networks", "Method"], ["CNNs", "Method"]], "rel": [["convolutional neural networks", "Used-For", "pedestrian detection"], ["CNNs", "Synonym-Of", "convolutional neural networks"]], "rel_plus": [["convolutional neural networks:Method", "Used-For", "pedestrian detection:Task"], ["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"]]}
{"doc_id": "51876625", "sentence": "Action detection with actor - centric relation network ( ACRN ) .", "ner": [["Action detection", "Task"], ["actor - centric relation network", "Method"], ["ACRN", "Method"]], "rel": [["actor - centric relation network", "Used-For", "Action detection"], ["ACRN", "Synonym-Of", "actor - centric relation network"]], "rel_plus": [["actor - centric relation network:Method", "Used-For", "Action detection:Task"], ["ACRN:Method", "Synonym-Of", "actor - centric relation network:Method"]]}
{"doc_id": "202750230", "sentence": "Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks , such as machine translation , language modeling , and question answering .", "ner": [["natural language processing", "Task"], ["machine translation", "Task"], ["language modeling", "Task"], ["question answering", "Task"]], "rel": [["machine translation", "SubTask-Of", "natural language processing"], ["language modeling", "SubTask-Of", "natural language processing"], ["question answering", "SubTask-Of", "natural language processing"]], "rel_plus": [["machine translation:Task", "SubTask-Of", "natural language processing:Task"], ["language modeling:Task", "SubTask-Of", "natural language processing:Task"], ["question answering:Task", "SubTask-Of", "natural language processing:Task"]]}
{"doc_id": "23569888", "sentence": "In this section we consider possible directions for future research , aimed at mitigating the impact of such differences when performing visual recognition in robotics .", "ner": [["visual recognition", "Task"], ["robotics", "Task"]], "rel": [["visual recognition", "SubTask-Of", "robotics"]], "rel_plus": [["visual recognition:Task", "SubTask-Of", "robotics:Task"]]}
{"doc_id": "202577400", "sentence": "In the future , we will study the effectiveness of GALD for more vision tasks where both global and local information are important such as depth estimation .", "ner": [["GALD", "Method"], ["depth estimation", "Task"]], "rel": [["GALD", "Used-For", "depth estimation"]], "rel_plus": [["GALD:Method", "Used-For", "depth estimation:Task"]]}
{"doc_id": "56657874", "sentence": "In recent research , DG is a less explored issue than DA .", "ner": [["DG", "Method"], ["DA", "Method"]], "rel": [["DG", "Compare-With", "DA"]], "rel_plus": [["DG:Method", "Compare-With", "DA:Method"]]}
{"doc_id": "104291983", "sentence": "Our HRNetV 2 gets the overall best performance among methods without extra information and stronger data augmentation , and is even better than LAB with extra boundary information and DCFE [ 9 7 ] that explores extra 3D information .", "ner": [["HRNetV 2", "Method"], ["data augmentation", "Method"], ["LAB", "Method"], ["DCFE", "Method"]], "rel": [["HRNetV 2", "Compare-With", "LAB"], ["HRNetV 2", "Compare-With", "DCFE"]], "rel_plus": [["HRNetV 2:Method", "Compare-With", "LAB:Method"], ["HRNetV 2:Method", "Compare-With", "DCFE:Method"]]}
{"doc_id": "198897554", "sentence": "In transfer learning [ 1 5 , 1 6 ] , a model is trained to perform a visual task ( e.g. image classification ) but aiming at reusing it to perform a new task ( e.g. object detection ) in a way that we minimize the amount of labeled data required to train for the new task ( e.g. fine - tuning CNNs across tasks is a basic form of transfer learning ) .", "ner": [["transfer learning", "Method"], ["image classification", "Task"], ["object detection", "Task"], ["CNNs", "Method"], ["transfer learning", "Method"]], "rel": [["transfer learning", "Used-For", "image classification"], ["transfer learning", "Used-For", "object detection"], ["CNNs", "Used-For", "transfer learning"]], "rel_plus": [["transfer learning:Method", "Used-For", "image classification:Task"], ["transfer learning:Method", "Used-For", "object detection:Task"], ["CNNs:Method", "Used-For", "transfer learning:Method"]]}
{"doc_id": "211004033", "sentence": "Most single - stage and R - CNN based networks formulate the object detection as a regression problem , which outputs a class - specific bounding box for each prediction [ 6 ] , [ 7 ] , [ 1 5 ] - [ 1 7 ] , [ 3 4 ] .", "ner": [["R - CNN", "Method"], ["object detection", "Task"]], "rel": [["R - CNN", "Used-For", "object detection"]], "rel_plus": [["R - CNN:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "210164920", "sentence": "At the same time , ENet [ 9 6 ] , ICNet [ 9 7 ] are used as real - time semantic segmentation models for the autonomous vehicles .", "ner": [["ENet", "Method"], ["ICNet", "Method"], ["real - time semantic segmentation models", "Method"], ["autonomous vehicles", "Task"]], "rel": [["ICNet", "SubClass-Of", "real - time semantic segmentation models"], ["ENet", "SubClass-Of", "real - time semantic segmentation models"], ["ICNet", "Used-For", "autonomous vehicles"], ["ENet", "Used-For", "autonomous vehicles"]], "rel_plus": [["ICNet:Method", "SubClass-Of", "real - time semantic segmentation models:Method"], ["ENet:Method", "SubClass-Of", "real - time semantic segmentation models:Method"], ["ICNet:Method", "Used-For", "autonomous vehicles:Task"], ["ENet:Method", "Used-For", "autonomous vehicles:Task"]]}
{"doc_id": "4246700", "sentence": "This is because there is not an accredited dataset like Common Objects in Context ( COCO ) dataset in natural image datasets .", "ner": [["Common Objects in Context", "Dataset"], ["COCO", "Dataset"]], "rel": [["COCO", "Synonym-Of", "Common Objects in Context"]], "rel_plus": [["COCO:Dataset", "Synonym-Of", "Common Objects in Context:Dataset"]]}
{"doc_id": "44148233", "sentence": "As shown in Figure 7 , the deep learning approaches to video description can also be divided into two sequential stages , namely , visual content extraction and text generation .", "ner": [["deep learning", "Method"], ["video description", "Task"], ["visual content extraction", "Task"], ["text generation", "Task"]], "rel": [["deep learning", "Used-For", "video description"], ["text generation", "SubTask-Of", "video description"], ["visual content extraction", "SubTask-Of", "video description"], ["deep learning", "Used-For", "visual content extraction"], ["deep learning", "Used-For", "text generation"]], "rel_plus": [["deep learning:Method", "Used-For", "video description:Task"], ["text generation:Task", "SubTask-Of", "video description:Task"], ["visual content extraction:Task", "SubTask-Of", "video description:Task"], ["deep learning:Method", "Used-For", "visual content extraction:Task"], ["deep learning:Method", "Used-For", "text generation:Task"]]}
{"doc_id": "51876625", "sentence": "In practice , we use Mixed 5b and Mixed 5c blocks of the S 3 D - G network and an average pooling layer ( similar to the action classifier network ) to output a 1 \u00d7 1 \u00d7 1 0 2 4 feature ( f to get a 1 \u00d7 1 \u00d7 2 0 4 8 representation which is used for action classification and bounding - box regression for a given actor box b i ( see Figure 2 ) .", "ner": [["Mixed 5b", "Method"], ["Mixed 5c blocks", "Method"], ["S 3 D - G", "Method"], ["average pooling", "Method"], ["action classification", "Task"], ["bounding - box regression", "Task"]], "rel": [["Mixed 5c blocks", "Part-Of", "S 3 D - G"], ["Mixed 5b", "Part-Of", "S 3 D - G"], ["average pooling", "Part-Of", "S 3 D - G"], ["S 3 D - G", "Used-For", "action classification"], ["S 3 D - G", "Used-For", "bounding - box regression"]], "rel_plus": [["Mixed 5c blocks:Method", "Part-Of", "S 3 D - G:Method"], ["Mixed 5b:Method", "Part-Of", "S 3 D - G:Method"], ["average pooling:Method", "Part-Of", "S 3 D - G:Method"], ["S 3 D - G:Method", "Used-For", "action classification:Task"], ["S 3 D - G:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "150374036", "sentence": "Afterwards , many self - attention models are proposed for different tasks , such as LSTM for machine reading [ 1 1 ] , multi - head attention for machine translation [ 5 8 ] and attention clusters for video classification [ 4 2 ] .", "ner": [["self - attention models", "Method"], ["LSTM", "Method"], ["machine reading", "Task"], ["multi - head attention", "Method"], ["machine translation", "Task"], ["attention clusters", "Method"], ["video classification", "Task"]], "rel": [["attention clusters", "SubClass-Of", "self - attention models"], ["multi - head attention", "SubClass-Of", "self - attention models"], ["LSTM", "SubClass-Of", "self - attention models"], ["LSTM", "Used-For", "machine reading"], ["multi - head attention", "Used-For", "machine translation"], ["attention clusters", "Used-For", "video classification"]], "rel_plus": [["attention clusters:Method", "SubClass-Of", "self - attention models:Method"], ["multi - head attention:Method", "SubClass-Of", "self - attention models:Method"], ["LSTM:Method", "SubClass-Of", "self - attention models:Method"], ["LSTM:Method", "Used-For", "machine reading:Task"], ["multi - head attention:Method", "Used-For", "machine translation:Task"], ["attention clusters:Method", "Used-For", "video classification:Task"]]}
{"doc_id": "211010758", "sentence": "As the ultimate goal of domain adaptation is to achieve good performance on the target domain , we further introduce domain - diversity into training to boost the detection performance in the target space .", "ner": [["domain adaptation", "Method"], ["detection", "Task"]], "rel": [["domain adaptation", "Used-For", "detection"]], "rel_plus": [["domain adaptation:Method", "Used-For", "detection:Task"]]}
{"doc_id": "202750230", "sentence": "We validate our findings on a variety of competitive benchmarks , namely WMT 1 4 English - German for machine translation , WikiText - 1 0 3 ( Merity et al. , 2 0 1 6 ) for language modeling , CNN - Dailymail ( Hermann et al. , 2 0 1 5 ) for abstractive summarization , ELI 5 for long form question answering , and several natural language understanding tasks ( Wang et al. , 2 0 1 9 ) for sentence representation .", "ner": [["WMT 1 4 English - German", "Dataset"], ["machine translation", "Task"], ["WikiText - 1 0 3", "Dataset"], ["language modeling", "Task"], ["CNN - Dailymail", "Dataset"], ["abstractive summarization", "Task"], ["ELI 5", "Dataset"], ["long form question answering", "Task"], ["natural language understanding", "Task"], ["sentence representation", "Task"]], "rel": [["WMT 1 4 English - German", "Benchmark-For", "machine translation"], ["WikiText - 1 0 3", "Benchmark-For", "language modeling"], ["CNN - Dailymail", "Benchmark-For", "abstractive summarization"], ["ELI 5", "Benchmark-For", "long form question answering"], ["natural language understanding", "Used-For", "sentence representation"]], "rel_plus": [["WMT 1 4 English - German:Dataset", "Benchmark-For", "machine translation:Task"], ["WikiText - 1 0 3:Dataset", "Benchmark-For", "language modeling:Task"], ["CNN - Dailymail:Dataset", "Benchmark-For", "abstractive summarization:Task"], ["ELI 5:Dataset", "Benchmark-For", "long form question answering:Task"], ["natural language understanding:Task", "Used-For", "sentence representation:Task"]]}
{"doc_id": "198897554", "sentence": "Selfsupervised learning , as usually performed , can be seen as a type of transfer learning ( from the pretext task to the main task ) .", "ner": [["Selfsupervised learning", "Method"], ["transfer learning", "Method"]], "rel": [["Selfsupervised learning", "SubClass-Of", "transfer learning"]], "rel_plus": [["Selfsupervised learning:Method", "SubClass-Of", "transfer learning:Method"]]}
{"doc_id": "AUG124", "sentence": "Conference on Computational Natural Language Learning 2003, or CoNLL03, is used by FastText for named entity recognition.", "ner": [["Conference on Computational Natural Language Learning 2003", "Dataset"], ["CoNLL03", "Dataset"], ["FastText", "Method"], ["named entity recognition", "Task"]], "rel": [["CoNLL03", "Synonym-Of", "Conference on Computational Natural Language Learning 2003"], ["FastText", "Used-For", "named entity recognition"], ["FastText", "Evaluated-With", "CoNLL03"]], "rel_plus": [["CoNLL03:Dataset", "Synonym-Of", "Conference on Computational Natural Language Learning 2003:Dataset"], ["FastText:Method", "Used-For", "named entity recognition:Task"], ["FastText:Method", "Evaluated-With", "CoNLL03:Dataset"]]}
{"doc_id": "199543700", "sentence": "When compared with supervised methods , SGGAN can achieves competitive or even higher accuracies on various benchmark datasets when compared with state - of - the - art GAN based approaches such as the Improved GAN [ 5 2 ] and supervised learning networks such as VGG - 1 6 [ 5 3 ] and ResNet - 5 0 [ 2 1 ] .", "ner": [["SGGAN", "Method"], ["GAN", "Method"], ["Improved GAN", "Method"], ["VGG - 1 6", "Method"], ["ResNet - 5 0", "Method"]], "rel": [["SGGAN", "Compare-With", "GAN"], ["Improved GAN", "SubClass-Of", "GAN"], ["SGGAN", "Compare-With", "Improved GAN"], ["SGGAN", "Compare-With", "VGG - 1 6"], ["SGGAN", "Compare-With", "ResNet - 5 0"]], "rel_plus": [["SGGAN:Method", "Compare-With", "GAN:Method"], ["Improved GAN:Method", "SubClass-Of", "GAN:Method"], ["SGGAN:Method", "Compare-With", "Improved GAN:Method"], ["SGGAN:Method", "Compare-With", "VGG - 1 6:Method"], ["SGGAN:Method", "Compare-With", "ResNet - 5 0:Method"]]}
{"doc_id": "202888986", "sentence": "As we show in Sec. 4. 6 , it turns out that NSP can not solve the SOP task at all ( i.e. , it ends up learning the easier topic - prediction signal , and performs at randombaseline level on the SOP task ) , while SOP can solve the NSP task to a reasonable degree , presumably based on analyzing misaligned coherence cues .", "ner": [["NSP", "Task"], ["SOP", "Task"], ["SOP", "Task"], ["SOP", "Task"], ["NSP", "Task"]], "rel": [["SOP", "Used-For", "NSP"]], "rel_plus": [["SOP:Task", "Used-For", "NSP:Task"]]}
{"doc_id": "202734254", "sentence": "The results indicate some concerns about the conversational QA models : 1 ) Higher performance on QuAC and CoQA does not necessarily imply better content comprehension . 2 ) Models trained on QuAC show tendency to rely heavier on the previous answers ' positions rather than their textual content .", "ner": [["conversational QA", "Task"], ["QuAC", "Dataset"], ["CoQA", "Dataset"], ["QuAC", "Dataset"]], "rel": [["CoQA", "Benchmark-For", "conversational QA"], ["QuAC", "Benchmark-For", "conversational QA"]], "rel_plus": [["CoQA:Dataset", "Benchmark-For", "conversational QA:Task"], ["QuAC:Dataset", "Benchmark-For", "conversational QA:Task"]]}
{"doc_id": "67855714", "sentence": "In terms of perceptual metrics , the proposed P sisr methods rank in the second position after SRGAN [ 1 ] on the datasets ImageNet and DTD , while they outperform all the baselines on the satellite images domain which is far from the ImageNet domain .   In this paper , we propose a general framework named Generative Collaborative Networks ( GCN ) which generalizes the existing methods for the problem of learning a mapping between two domains .", "ner": [["SRGAN", "Method"], ["ImageNet", "Dataset"], ["DTD", "Dataset"], ["ImageNet", "Dataset"], ["Generative Collaborative Networks", "Method"], ["GCN", "Method"]], "rel": [["SRGAN", "Evaluated-With", "ImageNet"], ["SRGAN", "Evaluated-With", "DTD"], ["GCN", "Synonym-Of", "Generative Collaborative Networks"]], "rel_plus": [["SRGAN:Method", "Evaluated-With", "ImageNet:Dataset"], ["SRGAN:Method", "Evaluated-With", "DTD:Dataset"], ["GCN:Method", "Synonym-Of", "Generative Collaborative Networks:Method"]]}
{"doc_id": "52009210", "sentence": "To address these issues , we present SIMPLEDBPEDIAQA , a new dataset that we have created by mapping entities and predicates that comprise the answers to SIMPLEQUESTIONS from Freebase to DBpedia .", "ner": [["SIMPLEDBPEDIAQA", "Dataset"], ["mapping entities", "Task"], ["predicates that comprise the answers", "Task"], ["SIMPLEQUESTIONS", "Dataset"], ["Freebase", "Dataset"], ["DBpedia", "Dataset"]], "rel": [["SIMPLEDBPEDIAQA", "Benchmark-For", "mapping entities"], ["SIMPLEDBPEDIAQA", "Benchmark-For", "predicates that comprise the answers"]], "rel_plus": [["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "mapping entities:Task"], ["SIMPLEDBPEDIAQA:Dataset", "Benchmark-For", "predicates that comprise the answers:Task"]]}
{"doc_id": "44148233", "sentence": "Special issues of journals are published focusing on language in vision [ 9 ] and workshops uniting the two areas have also been held regularly at both NLP and CV conferences [ 1 5 ] , [ 1 6 ] , [ 1 7 ] , [ 1 0 5 ] .", "ner": [["NLP", "Task"], ["CV", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "201070697", "sentence": "AGGAN [ 7 ] and Attention - GAN . [ 8 ] Similar unsupervised image - to - image translation methods with added attention networks .", "ner": [["AGGAN", "Method"], ["Attention - GAN", "Method"], ["unsupervised image - to - image translation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "3920676", "sentence": "In the metric learning case , we fix SRID as the ranking algorithm and GOG as the feature extraction algorithm , with Fig. 5d showing the rank - 1 results .", "ner": [["metric learning", "Method"], ["SRID", "Method"], ["ranking algorithm", "Method"], ["GOG", "Method"], ["feature extraction", "Method"]], "rel": [["SRID", "SubClass-Of", "ranking algorithm"], ["GOG", "SubClass-Of", "feature extraction"]], "rel_plus": [["SRID:Method", "SubClass-Of", "ranking algorithm:Method"], ["GOG:Method", "SubClass-Of", "feature extraction:Method"]]}
{"doc_id": "210164517", "sentence": "Here , deep neural network and CNN model are built and analysed over EEG data for emotion prediction rather than facial images .", "ner": [["deep neural network", "Method"], ["CNN", "Method"], ["emotion prediction", "Task"]], "rel": [["CNN", "Used-For", "emotion prediction"], ["deep neural network", "Used-For", "emotion prediction"]], "rel_plus": [["CNN:Method", "Used-For", "emotion prediction:Task"], ["deep neural network:Method", "Used-For", "emotion prediction:Task"]]}
{"doc_id": "28984897", "sentence": "Besides , the maxout layer does not provide a considerable reduction , since it reduces the size of the softmax layer that has less number of weights compared with the other layers .", "ner": [["maxout layer", "Method"], ["softmax layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "209532167", "sentence": "For evaluating CuBERT , we create a benchmark of five classification tasks , ranging from classification of source code according to presence or absense of certain classes of bugs , to mismatch between a function 's natural language description and its body , to predicting the right kind of exception to catch for a given code fragment .", "ner": [["CuBERT", "Method"], ["classification", "Task"], ["classification", "Task"]], "rel": [["CuBERT", "Used-For", "classification"]], "rel_plus": [["CuBERT:Method", "Used-For", "classification:Task"]]}
{"doc_id": "211010520", "sentence": "Applying this approach with a series of progressively stronger RC models in the annotation loop , we arrived at three separate RC datasets , graduated by the difficulty of the model adversary .", "ner": [["RC", "Task"], ["RC", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "53719742", "sentence": "As shown in Table 1 and Table 2 , ResneXt 5 0 can consistently outperform ResNet 5 0 .", "ner": [["ResneXt 5 0", "Method"], ["ResNet 5 0", "Method"]], "rel": [["ResneXt 5 0", "Compare-With", "ResNet 5 0"]], "rel_plus": [["ResneXt 5 0:Method", "Compare-With", "ResNet 5 0:Method"]]}
{"doc_id": "AUG092", "sentence": "Sentiment analysis is a task, not a dataset, improved by BERT on IMDb.", "ner": [["sentiment analysis", "Task"], ["BERT", "Method"], ["IMDb", "Dataset"]], "rel": [["BERT", "Used-For", "sentiment analysis"], ["BERT", "Evaluated-With", "IMDb"]], "rel_plus": [["BERT:Method", "Used-For", "sentiment analysis:Task"], ["BERT:Method", "Evaluated-With", "IMDb:Dataset"]]}
{"doc_id": "4539700", "sentence": "These 3D CNNs are intuitively effective because such 3D convolution can be used to directly extract spatio - temporal features from raw videos .", "ner": [["3D CNNs", "Method"], ["3D convolution", "Method"]], "rel": [["3D convolution", "Compare-With", "3D CNNs"]], "rel_plus": [["3D convolution:Method", "Compare-With", "3D CNNs:Method"]]}
{"doc_id": "24972096", "sentence": "Modelling the global context information and simultaneously preserving the local shape information are the two key problems in CNN - based semantic segmentation .", "ner": [["CNN", "Method"], ["semantic segmentation", "Task"]], "rel": [["CNN", "Used-For", "semantic segmentation"]], "rel_plus": [["CNN:Method", "Used-For", "semantic segmentation:Task"]]}
{"doc_id": "51923817", "sentence": "We evaluate CornerNet on MS COCO and demonstrate competitive results .", "ner": [["CornerNet", "Method"], ["MS COCO", "Dataset"]], "rel": [["CornerNet", "Evaluated-With", "MS COCO"]], "rel_plus": [["CornerNet:Method", "Evaluated-With", "MS COCO:Dataset"]]}
{"doc_id": "147703932", "sentence": "Separately , both 2D pose and depth estimation improve the segmentation results relative to both IOU and pixel error .", "ner": [["2D pose and depth estimation", "Task"], ["segmentation", "Task"]], "rel": [["2D pose and depth estimation", "Used-For", "segmentation"]], "rel_plus": [["2D pose and depth estimation:Task", "Used-For", "segmentation:Task"]]}
{"doc_id": "210713911", "sentence": "For a better trade - off between top - 1 accuracy and throughput , R 5 0 +SK \u2020 is preferred .   Anti - Alias Downsampling ( AA ) CNN models for image classification are known to be very vulnerable to small amounts of distortion [ 3 7 ] .", "ner": [["R 5 0 +SK \u2020", "Method"], ["Anti - Alias Downsampling", "Method"], ["AA", "Method"], ["CNN", "Method"], ["image classification", "Task"]], "rel": [["AA", "Part-Of", "Anti - Alias Downsampling"], ["Anti - Alias Downsampling", "Part-Of", "CNN"], ["CNN", "Used-For", "image classification"]], "rel_plus": [["AA:Method", "Part-Of", "Anti - Alias Downsampling:Method"], ["Anti - Alias Downsampling:Method", "Part-Of", "CNN:Method"], ["CNN:Method", "Used-For", "image classification:Task"]]}
{"doc_id": "202565512", "sentence": "GNN has also been applied into muli - hop reading comprehension tasks ( Tu et al. 2 0 1 9 ; Kundu et al. 2 0 1 9 ; ) .", "ner": [["GNN", "Method"], ["muli - hop reading comprehension", "Task"]], "rel": [["GNN", "Used-For", "muli - hop reading comprehension"]], "rel_plus": [["GNN:Method", "Used-For", "muli - hop reading comprehension:Task"]]}
{"doc_id": "202540251", "sentence": "Domain Adaptation for Semantic Segmentation .", "ner": [["Domain Adaptation", "Method"], ["Semantic Segmentation", "Task"]], "rel": [["Domain Adaptation", "Used-For", "Semantic Segmentation"]], "rel_plus": [["Domain Adaptation:Method", "Used-For", "Semantic Segmentation:Task"]]}
{"doc_id": "54447105", "sentence": "We aim at benefiting from several datasets with different categories but without additional labelling , not only to increase the number of categories detected , but also to take advantage from transfer learning and to enhance domain independence .    Our dataset merging procedure starts with training several initial Faster R - CNN on the different datasets while considering the complementary datasets ' images for domain adaptation .", "ner": [["transfer learning", "Task"], ["Faster R - CNN", "Method"], ["domain adaptation", "Task"]], "rel": [["Faster R - CNN", "Used-For", "domain adaptation"]], "rel_plus": [["Faster R - CNN:Method", "Used-For", "domain adaptation:Task"]]}
{"doc_id": "202565512", "sentence": "These models include SGN - lite , BECON ( single ) , BECON ( ensemble ) , CSR - KG and CSR - KG ( AI 2 IR ) . \u2022 Group 2 : models without extracted knowledge , including BERT - large ( Devlin et al. 2 0 1 9 ) , XLNet - large ( Yang et al. 2 0 1 9 ) and RoBERTa ( Liu et al. 2 0 1 9 ) .", "ner": [["SGN - lite", "Method"], ["BECON ( single )", "Method"], ["BECON ( ensemble )", "Method"], ["CSR - KG", "Method"], ["CSR - KG ( AI 2 IR )", "Method"], ["BERT - large", "Method"], ["XLNet - large", "Method"], ["RoBERTa", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4539700", "sentence": "Here , it can be seen that the accuracies of ResNeXt - 1 0 1 are clearly high compared with C 3 D with batch normalization [ 1 6 ] , which is 1 0 - layer network , as well as CNN+LSTM and two - stream CNN [ 1 6 ] .", "ner": [["ResNeXt - 1 0 1", "Method"], ["C 3 D with batch normalization", "Method"], ["CNN+LSTM", "Method"], ["two - stream CNN", "Method"]], "rel": [["ResNeXt - 1 0 1", "Compare-With", "C 3 D with batch normalization"], ["ResNeXt - 1 0 1", "Compare-With", "CNN+LSTM"], ["ResNeXt - 1 0 1", "Compare-With", "two - stream CNN"]], "rel_plus": [["ResNeXt - 1 0 1:Method", "Compare-With", "C 3 D with batch normalization:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "CNN+LSTM:Method"], ["ResNeXt - 1 0 1:Method", "Compare-With", "two - stream CNN:Method"]]}
{"doc_id": "53731879", "sentence": "After assigning self pseudo labels and target pseudo labels , the self mining branch uses the hard - batch triplet loss [ 2 0 ] as object function and one shot mining branch employs the same triplet loss with confidence term .", "ner": [["self mining branch", "Method"], ["hard - batch triplet loss", "Method"], ["one shot mining branch", "Method"], ["triplet loss with confidence term", "Method"]], "rel": [["hard - batch triplet loss", "Part-Of", "self mining branch"], ["triplet loss with confidence term", "Part-Of", "one shot mining branch"]], "rel_plus": [["hard - batch triplet loss:Method", "Part-Of", "self mining branch:Method"], ["triplet loss with confidence term:Method", "Part-Of", "one shot mining branch:Method"]]}
{"doc_id": "208202241", "sentence": "In this part , we conduct multi - label image and video recognition experiments to demonstrate that our KSSNet framework can deal with this problem effectively .", "ner": [["multi - label image", "Task"], ["video recognition", "Task"], ["KSSNet", "Method"]], "rel": [["KSSNet", "Used-For", "multi - label image"], ["KSSNet", "Used-For", "video recognition"]], "rel_plus": [["KSSNet:Method", "Used-For", "multi - label image:Task"], ["KSSNet:Method", "Used-For", "video recognition:Task"]]}
{"doc_id": "4319457", "sentence": "The reconstructions for MNIST are seemingly perfect , whereas for the CelebA dataset the reconstructions are good but blurry .", "ner": [["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "210164920", "sentence": "As in VGG 1 6 , pooling and rectification layers are also added after some of the convolutional layers .", "ner": [["VGG 1 6", "Method"], ["pooling", "Method"], ["rectification layers", "Method"], ["convolutional layers", "Method"]], "rel": [["rectification layers", "Part-Of", "VGG 1 6"], ["pooling", "Part-Of", "VGG 1 6"], ["convolutional layers", "Part-Of", "VGG 1 6"]], "rel_plus": [["rectification layers:Method", "Part-Of", "VGG 1 6:Method"], ["pooling:Method", "Part-Of", "VGG 1 6:Method"], ["convolutional layers:Method", "Part-Of", "VGG 1 6:Method"]]}
{"doc_id": "211004033", "sentence": "Qualitative results from our Ellipse R - CNN and Mask R - CNN+ on the ROF dataset .", "ner": [["Ellipse R - CNN", "Method"], ["Mask R - CNN+", "Method"], ["ROF", "Dataset"]], "rel": [["Ellipse R - CNN", "Evaluated-With", "ROF"], ["Mask R - CNN+", "Evaluated-With", "ROF"]], "rel_plus": [["Ellipse R - CNN:Method", "Evaluated-With", "ROF:Dataset"], ["Mask R - CNN+:Method", "Evaluated-With", "ROF:Dataset"]]}
{"doc_id": "4246700", "sentence": "Sentence y is encoded as a sequence of h dimension projected word vectors [ 5 2 ] : where L is the length of the sentence . 3 ) Sentences Generation : In this subsection , a special Recurrent Neural Network ( RNN ) , called Long Short - Term Memory networks ( LSTM ) , is exploited to generate the sentences .", "ner": [["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["Long Short - Term Memory networks", "Method"], ["LSTM", "Method"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Network"], ["Long Short - Term Memory networks", "SubClass-Of", "Recurrent Neural Network"], ["LSTM", "Synonym-Of", "Long Short - Term Memory networks"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["Long Short - Term Memory networks:Method", "SubClass-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Synonym-Of", "Long Short - Term Memory networks:Method"]]}
{"doc_id": "102351044", "sentence": "Specifically , we first evaluate and compare drop - neuron and drop - channel in our proposed convolutional building blocks illustrated in Figure 5b on VGG [ 2 6 ] , whose convolutional layer is a plain 3 \u00d7 3 conv following process 2 and 4 of Figure 4 .", "ner": [["drop - neuron", "Method"], ["drop - channel", "Method"], ["convolutional building blocks", "Method"], ["VGG", "Method"], ["convolutional layer", "Method"], ["3 \u00d7 3 conv", "Method"]], "rel": [["drop - neuron", "Compare-With", "drop - channel"], ["drop - neuron", "Part-Of", "convolutional building blocks"], ["drop - channel", "Part-Of", "convolutional building blocks"], ["convolutional building blocks", "Part-Of", "VGG"], ["convolutional layer", "Part-Of", "VGG"], ["3 \u00d7 3 conv", "Part-Of", "VGG"], ["3 \u00d7 3 conv", "SubClass-Of", "convolutional layer"]], "rel_plus": [["drop - neuron:Method", "Compare-With", "drop - channel:Method"], ["drop - neuron:Method", "Part-Of", "convolutional building blocks:Method"], ["drop - channel:Method", "Part-Of", "convolutional building blocks:Method"], ["convolutional building blocks:Method", "Part-Of", "VGG:Method"], ["convolutional layer:Method", "Part-Of", "VGG:Method"], ["3 \u00d7 3 conv:Method", "Part-Of", "VGG:Method"], ["3 \u00d7 3 conv:Method", "SubClass-Of", "convolutional layer:Method"]]}
{"doc_id": "6423078", "sentence": "For the PASCAL - Context on BSDS 5 0 0 experiment , the results are evaluated with a model trained for 5 k iterations to avoid severe over - fitting ( in a sense of cross - dataset generalization ) to the strong object boundaries in PASCAL - Context .", "ner": [["PASCAL - Context", "Dataset"], ["BSDS 5 0 0", "Dataset"], ["PASCAL - Context", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "202577400", "sentence": "Pascal VOC : Pascal VOC [ 1 1 ] is a widely used public benchmark for semantic segmentation and object detection covering 2 0 object categories including the background .", "ner": [["Pascal VOC", "Dataset"], ["Pascal VOC", "Dataset"], ["semantic segmentation", "Task"], ["object detection", "Task"]], "rel": [["Pascal VOC", "Benchmark-For", "semantic segmentation"], ["Pascal VOC", "Benchmark-For", "object detection"]], "rel_plus": [["Pascal VOC:Dataset", "Benchmark-For", "semantic segmentation:Task"], ["Pascal VOC:Dataset", "Benchmark-For", "object detection:Task"]]}
{"doc_id": "4246700", "sentence": "The feature used for multimodal method including deep CNNs representations ( models are pre - trained on ImageNet dataset ) and handcrafted representations such as SIFT , BOW , FV and VLAD .", "ner": [["deep CNNs representations", "Method"], ["ImageNet", "Dataset"], ["handcrafted representations", "Method"], ["SIFT", "Method"], ["BOW", "Method"], ["FV", "Method"], ["VLAD", "Method"]], "rel": [["deep CNNs representations", "Trained-With", "ImageNet"], ["SIFT", "SubClass-Of", "handcrafted representations"], ["BOW", "SubClass-Of", "handcrafted representations"], ["FV", "SubClass-Of", "handcrafted representations"], ["VLAD", "SubClass-Of", "handcrafted representations"]], "rel_plus": [["deep CNNs representations:Method", "Trained-With", "ImageNet:Dataset"], ["SIFT:Method", "SubClass-Of", "handcrafted representations:Method"], ["BOW:Method", "SubClass-Of", "handcrafted representations:Method"], ["FV:Method", "SubClass-Of", "handcrafted representations:Method"], ["VLAD:Method", "SubClass-Of", "handcrafted representations:Method"]]}
{"doc_id": "AUG035", "sentence": "YOLO optimizes object detection on COCO, not merely a detector.", "ner": [["YOLO", "Method"], ["object detection", "Task"], ["COCO", "Dataset"]], "rel": [["YOLO", "Used-For", "object detection"], ["YOLO", "Trained-With", "COCO"]], "rel_plus": [["YOLO:Method", "Used-For", "object detection:Task"], ["YOLO:Method", "Trained-With", "COCO:Dataset"]]}
{"doc_id": "147703932", "sentence": "We train all models for 3 0 epochs using 2 Stacks of Hourglass , with a batch size of 5 and the RMSprop optimizer with learning rate 1e \u2212 3 .", "ner": [["Hourglass", "Method"], ["RMSprop", "Method"]], "rel": [["RMSprop", "Part-Of", "Hourglass"]], "rel_plus": [["RMSprop:Method", "Part-Of", "Hourglass:Method"]]}
{"doc_id": "209532167", "sentence": "A significant advancement in natural - language understanding has come with the development of pre - trained contextual embeddings , such as BERT , which can be fine - tuned for downstream tasks with less labeled data and training budget , while achieving better accuracies .", "ner": [["natural - language understanding", "Task"], ["BERT", "Method"]], "rel": [["BERT", "Used-For", "natural - language understanding"]], "rel_plus": [["BERT:Method", "Used-For", "natural - language understanding:Task"]]}
{"doc_id": "52910494", "sentence": "The new module consists of layers for multi - scale convolutions , cross - scale aggregation , maxout , and concatenation .", "ner": [["multi - scale convolutions", "Method"], ["cross - scale aggregation", "Method"], ["maxout", "Method"], ["concatenation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "4319457", "sentence": "Figures 1 1 and 1 2 show the results of the encoding and reconstruction for MNIST and CelebA , respectively .", "ner": [["encoding", "Task"], ["reconstruction", "Task"], ["MNIST", "Dataset"], ["CelebA", "Dataset"]], "rel": [["MNIST", "Benchmark-For", "encoding"], ["CelebA", "Benchmark-For", "encoding"], ["MNIST", "Benchmark-For", "reconstruction"], ["CelebA", "Benchmark-For", "reconstruction"]], "rel_plus": [["MNIST:Dataset", "Benchmark-For", "encoding:Task"], ["CelebA:Dataset", "Benchmark-For", "encoding:Task"], ["MNIST:Dataset", "Benchmark-For", "reconstruction:Task"], ["CelebA:Dataset", "Benchmark-For", "reconstruction:Task"]]}
{"doc_id": "208513596", "sentence": "For example , supervised - pretraining on Kinetics gives better performance on both UCF 1 0 1 and HMDB 5 1 compared to supervisedpretraining on AudioSet ( which is more than 8 times larger than Kinetics ) and ImageNet .", "ner": [["Kinetics", "Dataset"], ["UCF 1 0 1", "Dataset"], ["HMDB 5 1", "Dataset"], ["AudioSet", "Dataset"], ["Kinetics", "Dataset"], ["ImageNet", "Dataset"]], "rel": [["AudioSet", "Compare-With", "Kinetics"]], "rel_plus": [["AudioSet:Dataset", "Compare-With", "Kinetics:Dataset"]]}
{"doc_id": "199543700", "sentence": "In this paper , we propose a simple yet effective semisupervised self - growing generative adversarial network ( SG - GAN ) for image recognition .", "ner": [["semisupervised self - growing generative adversarial network", "Method"], ["SG - GAN", "Method"], ["image recognition", "Task"]], "rel": [["SG - GAN", "Synonym-Of", "semisupervised self - growing generative adversarial network"], ["semisupervised self - growing generative adversarial network", "Used-For", "image recognition"]], "rel_plus": [["SG - GAN:Method", "Synonym-Of", "semisupervised self - growing generative adversarial network:Method"], ["semisupervised self - growing generative adversarial network:Method", "Used-For", "image recognition:Task"]]}
{"doc_id": "209862890", "sentence": "This result shows that a better type prediction system can further improve upon the state - of - the - state entity linking systems .", "ner": [["type prediction", "Task"], ["entity linking", "Task"]], "rel": [["type prediction", "Used-For", "entity linking"]], "rel_plus": [["type prediction:Task", "Used-For", "entity linking:Task"]]}
{"doc_id": "21683040", "sentence": "We use the center code type to encode the bounding boxes and have the same matching strategy , hard negative mining strategy and data augmentation with SSD .", "ner": [["data augmentation", "Method"], ["SSD", "Method"]], "rel": [["data augmentation", "Used-For", "SSD"]], "rel_plus": [["data augmentation:Method", "Used-For", "SSD:Method"]]}
{"doc_id": "53719742", "sentence": "We evaluate our proposed method on several standard benchmark tasks including ICDAR - 2 0 1 5 [ 1 7 ] and ICDAR - 2 0 1 7 MLT 2 0 1 7 [ 3 0 ] for multi - oriented text detection , and SCUT - CTW 1 5 0 0 [ 2 6 ] for curved text detection .", "ner": [["ICDAR - 2 0 1 5", "Dataset"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7", "Dataset"], ["multi - oriented text detection", "Task"], ["SCUT - CTW 1 5 0 0", "Dataset"], ["curved text detection", "Task"]], "rel": [["ICDAR - 2 0 1 5", "Benchmark-For", "multi - oriented text detection"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7", "Benchmark-For", "multi - oriented text detection"], ["SCUT - CTW 1 5 0 0", "Benchmark-For", "curved text detection"]], "rel_plus": [["ICDAR - 2 0 1 5:Dataset", "Benchmark-For", "multi - oriented text detection:Task"], ["ICDAR - 2 0 1 7 MLT 2 0 1 7:Dataset", "Benchmark-For", "multi - oriented text detection:Task"], ["SCUT - CTW 1 5 0 0:Dataset", "Benchmark-For", "curved text detection:Task"]]}
{"doc_id": "21683040", "sentence": "In order to improve the accuracy , DSSD [ 7 ] suggests to augment SSD+ResNet - 1 0 1 with deconvolution layers to introduce additional larges - scale context .", "ner": [["DSSD", "Method"], ["SSD+ResNet - 1 0 1", "Method"], ["deconvolution", "Method"]], "rel": [["deconvolution", "Part-Of", "SSD+ResNet - 1 0 1"]], "rel_plus": [["deconvolution:Method", "Part-Of", "SSD+ResNet - 1 0 1:Method"]]}
{"doc_id": "210713911", "sentence": "In Table 2 , We show that the accuracy is increased by 1. 6 2 % when SK is independently applied to ResNet ( from 7 6 . 3 0 to 7 7 . 9 2 ) .", "ner": [["SK", "Method"], ["ResNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "211004033", "sentence": "D ETECTION of ellipse - like shapes [ 1 ] has been widely used in various image processing tasks , for instance , face detection [ 2 ] and medical imaging diagnosis [ 3 ] .", "ner": [["D ETECTION", "Method"], ["image processing tasks", "Task"], ["face detection", "Task"], ["medical imaging diagnosis", "Task"]], "rel": [["face detection", "SubTask-Of", "image processing tasks"], ["medical imaging diagnosis", "SubTask-Of", "image processing tasks"], ["D ETECTION", "Used-For", "image processing tasks"], ["D ETECTION", "Used-For", "face detection"], ["D ETECTION", "Used-For", "medical imaging diagnosis"]], "rel_plus": [["face detection:Task", "SubTask-Of", "image processing tasks:Task"], ["medical imaging diagnosis:Task", "SubTask-Of", "image processing tasks:Task"], ["D ETECTION:Method", "Used-For", "image processing tasks:Task"], ["D ETECTION:Method", "Used-For", "face detection:Task"], ["D ETECTION:Method", "Used-For", "medical imaging diagnosis:Task"]]}
{"doc_id": "202750230", "sentence": "In a second set of experiments , we look at the impact of LayerDrop on pre - training for sentence representation models and subsequent finetuning on multiple natural language understanding tasks .", "ner": [["LayerDrop", "Method"], ["sentence representation", "Task"], ["natural language understanding", "Task"]], "rel": [["LayerDrop", "Used-For", "sentence representation"], ["LayerDrop", "Used-For", "natural language understanding"]], "rel_plus": [["LayerDrop:Method", "Used-For", "sentence representation:Task"], ["LayerDrop:Method", "Used-For", "natural language understanding:Task"]]}
{"doc_id": "AUG146", "sentence": "Interactive Emotional Dyadic Motion Capture, known as IEMOCAP, is enhanced by EmoBERT for emotion recognition.", "ner": [["Interactive Emotional Dyadic Motion Capture", "Dataset"], ["IEMOCAP", "Dataset"], ["EmoBERT", "Method"], ["emotion recognition", "Task"]], "rel": [["IEMOCAP", "Synonym-Of", "Interactive Emotional Dyadic Motion Capture"], ["EmoBERT", "Used-For", "emotion recognition"], ["EmoBERT", "Evaluated-With", "IEMOCAP"]], "rel_plus": [["IEMOCAP:Dataset", "Synonym-Of", "Interactive Emotional Dyadic Motion Capture:Dataset"], ["EmoBERT:Method", "Used-For", "emotion recognition:Task"], ["EmoBERT:Method", "Evaluated-With", "IEMOCAP:Dataset"]]}
{"doc_id": "209532167", "sentence": "The negative examples are created by randomly replacing some binary operator with another type - compatible operator .   Swapped Operand Pradel & Sen ( 2 0 1 8) propose the wrong binary operand task where a variable or constant is used incorrectly in an expression , but that task is quite similar to the variable misuse task we already use .", "ner": [["binary operator", "Method"], ["type - compatible operator", "Method"], ["variable misuse", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "44148233", "sentence": "Since 2 0 1 7 , the MovieQA challenge has also been included in LSMDC in addition to the previous three tasks .", "ner": [["MovieQA", "Dataset"], ["LSMDC", "Dataset"]], "rel": [["MovieQA", "SubClass-Of", "LSMDC"]], "rel_plus": [["MovieQA:Dataset", "SubClass-Of", "LSMDC:Dataset"]]}
{"doc_id": "201070522", "sentence": "We believe that language features deserve more attention , and conduct experiments which compare different word embeddings , language models , and embedding augmentation steps on five common VL tasks : image - sentence retrieval , image captioning , visual question answering , phrase grounding , and text - to - clip retrieval .", "ner": [["VL", "Task"], ["image - sentence retrieval", "Task"], ["image captioning", "Task"], ["visual question answering", "Task"], ["phrase grounding", "Task"], ["text - to - clip retrieval", "Task"]], "rel": [["image - sentence retrieval", "SubTask-Of", "VL"], ["image captioning", "SubTask-Of", "VL"], ["visual question answering", "SubTask-Of", "VL"], ["phrase grounding", "SubTask-Of", "VL"], ["text - to - clip retrieval", "SubTask-Of", "VL"]], "rel_plus": [["image - sentence retrieval:Task", "SubTask-Of", "VL:Task"], ["image captioning:Task", "SubTask-Of", "VL:Task"], ["visual question answering:Task", "SubTask-Of", "VL:Task"], ["phrase grounding:Task", "SubTask-Of", "VL:Task"], ["text - to - clip retrieval:Task", "SubTask-Of", "VL:Task"]]}
{"doc_id": "56657874", "sentence": "We further evaluate and compare our proposed approach with both shallow and deep domain generalization stateof - the - art methods : Undoing the Damage of Dataset Bias ( Undo - Bias ) [ 1 9 ] , Unbiased Metric Learning ( UML ) [ 9 ] , Low - Rank Structure from Latent Domains for Domain Generalization ( LRE - SVM ) [ 4 3 ] , Multi - Task Autoencoders ( MTAE ) [ 1 2 ] , Domain Separation Network ( DSN ) [ 3 ] , Deep Domain Generalization with Structured Low - Rank Constraint(DGLRC ) [ 7 ] , Domain Generalization via Invari - Table 7 : Recognition accuracies for domain generalization on the PACS dataset [ 2 2 ] using synthetic images that are generated by ComboGAN .", "ner": [["deep domain generalization", "Method"], ["Undoing the Damage of Dataset Bias", "Method"], ["Undo - Bias", "Method"], ["Unbiased Metric Learning", "Method"], ["UML", "Method"], ["Low - Rank Structure from Latent Domains for Domain Generalization", "Method"], ["LRE - SVM", "Method"], ["Multi - Task Autoencoders", "Method"], ["MTAE", "Method"], ["Domain Separation Network", "Method"], ["DSN", "Method"], ["Deep Domain Generalization with Structured Low - Rank Constraint(DGLRC )", "Method"], ["Domain Generalization", "Method"], ["Recognition", "Task"], ["domain generalization", "Method"], ["PACS", "Dataset"], ["ComboGAN", "Method"]], "rel": [["Undo - Bias", "Synonym-Of", "Undoing the Damage of Dataset Bias"], ["UML", "Synonym-Of", "Unbiased Metric Learning"], ["LRE - SVM", "Synonym-Of", "Low - Rank Structure from Latent Domains for Domain Generalization"], ["MTAE", "Synonym-Of", "Multi - Task Autoencoders"], ["DSN", "Synonym-Of", "Domain Separation Network"], ["PACS", "Benchmark-For", "Recognition"], ["ComboGAN", "Used-For", "PACS"]], "rel_plus": [["Undo - Bias:Method", "Synonym-Of", "Undoing the Damage of Dataset Bias:Method"], ["UML:Method", "Synonym-Of", "Unbiased Metric Learning:Method"], ["LRE - SVM:Method", "Synonym-Of", "Low - Rank Structure from Latent Domains for Domain Generalization:Method"], ["MTAE:Method", "Synonym-Of", "Multi - Task Autoencoders:Method"], ["DSN:Method", "Synonym-Of", "Domain Separation Network:Method"], ["PACS:Dataset", "Benchmark-For", "Recognition:Task"], ["ComboGAN:Method", "Used-For", "PACS:Dataset"]]}
{"doc_id": "201070697", "sentence": "Zhou et al. [ 2 6 ] produce attention maps for each class by removing top average - pooling layer and improving object localization accuracy .", "ner": [["average - pooling", "Method"], ["object localization", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199543700", "sentence": "The applications of it include suspect identification [ 2 7 ] , face verification [ 3 2 ] and face retrieval [ 3 1 ] .", "ner": [["suspect identification", "Task"], ["face verification", "Task"], ["face retrieval", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "199668978", "sentence": "BERT pre - trained on the large dataset like Wikipedia could give the contextual wording reward r B ( t ) = p B ( t | 1 , ... , M ) , which is the probability of word t given by BERT model .", "ner": [["BERT", "Method"], ["Wikipedia", "Dataset"], ["BERT", "Method"]], "rel": [["BERT", "Trained-With", "Wikipedia"]], "rel_plus": [["BERT:Method", "Trained-With", "Wikipedia:Dataset"]]}
{"doc_id": "4246700", "sentence": "The handcrafted feature methods first extract the handcrafted features from each image and then obtain image representation by feature encoding techniques such as Bag Of Words ( BOW ) [ 4 3 ] , Fisher Vector ( FV ) [ 4 4 ] and Vector of Locally Aggregated Descriptors ( VLAD ) [ 4 5 ] .", "ner": [["handcrafted feature methods", "Method"], ["feature encoding techniques", "Method"], ["Bag Of Words", "Method"], ["BOW", "Method"], ["Fisher Vector", "Method"], ["FV", "Method"], ["Vector of Locally Aggregated Descriptors", "Method"], ["VLAD", "Method"]], "rel": [["Bag Of Words", "SubClass-Of", "feature encoding techniques"], ["Fisher Vector", "SubClass-Of", "feature encoding techniques"], ["Vector of Locally Aggregated Descriptors", "SubClass-Of", "feature encoding techniques"], ["BOW", "Synonym-Of", "Bag Of Words"], ["FV", "Synonym-Of", "Fisher Vector"], ["VLAD", "Synonym-Of", "Vector of Locally Aggregated Descriptors"]], "rel_plus": [["Bag Of Words:Method", "SubClass-Of", "feature encoding techniques:Method"], ["Fisher Vector:Method", "SubClass-Of", "feature encoding techniques:Method"], ["Vector of Locally Aggregated Descriptors:Method", "SubClass-Of", "feature encoding techniques:Method"], ["BOW:Method", "Synonym-Of", "Bag Of Words:Method"], ["FV:Method", "Synonym-Of", "Fisher Vector:Method"], ["VLAD:Method", "Synonym-Of", "Vector of Locally Aggregated Descriptors:Method"]]}
{"doc_id": "4539700", "sentence": "The codes and pretrained models used in this study are publicly available . https://github.com/kenshohara/ 3 D - ResNets - PyTorch The use of large - scale datasets is extremely important when using deep convolutional neural networks ( CNNs ) , which have massive parameter numbers , and the use of CNNs in the field of computer vision has expanded significantly in recent years .", "ner": [["3 D - ResNets", "Method"], ["convolutional neural networks", "Method"], ["CNNs", "Method"], ["CNNs", "Method"], ["computer vision", "Task"]], "rel": [["CNNs", "Synonym-Of", "convolutional neural networks"], ["CNNs", "Used-For", "computer vision"]], "rel_plus": [["CNNs:Method", "Synonym-Of", "convolutional neural networks:Method"], ["CNNs:Method", "Used-For", "computer vision:Task"]]}
{"doc_id": "210860962", "sentence": "In the following subsections , we will explain fully convolutional network architectures for semantic segmentation [ 1 3 ] , [ 2 6 ] , [ 4 3 ] , [ 4 6 ] as well as the CycleGAN [ 6 0 ] and StarGAN [ 1 0 ] architectures for unpaired image translation .", "ner": [["fully convolutional network", "Method"], ["semantic segmentation", "Task"], ["CycleGAN", "Method"], ["StarGAN", "Method"], ["unpaired image translation", "Task"]], "rel": [["fully convolutional network", "Used-For", "semantic segmentation"], ["StarGAN", "Used-For", "unpaired image translation"], ["CycleGAN", "Used-For", "unpaired image translation"]], "rel_plus": [["fully convolutional network:Method", "Used-For", "semantic segmentation:Task"], ["StarGAN:Method", "Used-For", "unpaired image translation:Task"], ["CycleGAN:Method", "Used-For", "unpaired image translation:Task"]]}
{"doc_id": "AUG121", "sentence": "Bidirectional Encoder Representations from Transformers, or BERT, improves sentiment analysis on IMDb.", "ner": [["Bidirectional Encoder Representations from Transformers", "Method"], ["BERT", "Method"], ["sentiment analysis", "Task"], ["IMDb", "Dataset"]], "rel": [["BERT", "Synonym-Of", "Bidirectional Encoder Representations from Transformers"], ["BERT", "Used-For", "sentiment analysis"], ["BERT", "Evaluated-With", "IMDb"]], "rel_plus": [["BERT:Method", "Synonym-Of", "Bidirectional Encoder Representations from Transformers:Method"], ["BERT:Method", "Used-For", "sentiment analysis:Task"], ["BERT:Method", "Evaluated-With", "IMDb:Dataset"]]}
{"doc_id": "210861282", "sentence": "The results of UniPose and UniPose - LSTM demonstrated superior performance compared to state - of - the - art methods for several datasets , i.e. , LSP , MPII , Penn Action and BBC Pose , using various metrics .", "ner": [["UniPose", "Method"], ["UniPose - LSTM", "Method"], ["LSP", "Dataset"], ["MPII", "Dataset"], ["Penn Action", "Dataset"], ["BBC Pose", "Dataset"]], "rel": [["UniPose - LSTM", "Evaluated-With", "LSP"], ["UniPose", "Evaluated-With", "LSP"], ["UniPose - LSTM", "Evaluated-With", "MPII"], ["UniPose", "Evaluated-With", "MPII"], ["UniPose - LSTM", "Evaluated-With", "Penn Action"], ["UniPose", "Evaluated-With", "Penn Action"], ["UniPose - LSTM", "Evaluated-With", "BBC Pose"], ["UniPose", "Evaluated-With", "BBC Pose"]], "rel_plus": [["UniPose - LSTM:Method", "Evaluated-With", "LSP:Dataset"], ["UniPose:Method", "Evaluated-With", "LSP:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "MPII:Dataset"], ["UniPose:Method", "Evaluated-With", "MPII:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "Penn Action:Dataset"], ["UniPose:Method", "Evaluated-With", "Penn Action:Dataset"], ["UniPose - LSTM:Method", "Evaluated-With", "BBC Pose:Dataset"], ["UniPose:Method", "Evaluated-With", "BBC Pose:Dataset"]]}
{"doc_id": "4246700", "sentence": "To decode the image representations into natural language sentences , several methods have been proposed for generating image descriptions [ 2 9 ] , [ 3 2 ] , [ 3 3 ] , [ 3 4 ] , such as Recurrent Neural Network ( RNN ) , Long - Short Term Memory networks ( LSTM ) , retrieve based method and object detection based method .", "ner": [["Recurrent Neural Network", "Method"], ["RNN", "Method"], ["Long - Short Term Memory networks", "Method"], ["LSTM", "Method"], ["retrieve based method", "Method"], ["object detection based method", "Method"]], "rel": [["RNN", "Synonym-Of", "Recurrent Neural Network"], ["LSTM", "Synonym-Of", "Long - Short Term Memory networks"]], "rel_plus": [["RNN:Method", "Synonym-Of", "Recurrent Neural Network:Method"], ["LSTM:Method", "Synonym-Of", "Long - Short Term Memory networks:Method"]]}
{"doc_id": "51923817", "sentence": "CornerNet uses the hourglass network ( Newell et al. , 2 0 1 6 ) as its backbone network .", "ner": [["CornerNet", "Method"], ["hourglass network", "Method"]], "rel": [["hourglass network", "Part-Of", "CornerNet"]], "rel_plus": [["hourglass network:Method", "Part-Of", "CornerNet:Method"]]}
{"doc_id": "199543700", "sentence": "C. GAN based semi - supervised learning Donahue et al. [ 1 3 ] introduced an adversarial formulation with a third component , which they call the \" encoder \" .", "ner": [["GAN based semi - supervised learning", "Method"], ["adversarial formulation with a third component", "Method"], ["encoder", "Method"]], "rel": [["encoder", "Synonym-Of", "adversarial formulation with a third component"]], "rel_plus": [["encoder:Method", "Synonym-Of", "adversarial formulation with a third component:Method"]]}
{"doc_id": "204901567", "sentence": "In Step 2 , when we transfer the L 1 transformer to L 2 , we add a feed - forward adapter module after the projection following multi - headed attention and after the two feed - forward layers in each transformer layer , similar to Houlsby et al. ( 2 0 1 9 ) .", "ner": [["transformer", "Method"], ["feed - forward adapter module", "Method"], ["multi - headed attention", "Method"], ["feed - forward layers", "Method"], ["transformer layer", "Method"]], "rel": [["feed - forward layers", "Part-Of", "transformer layer"], ["multi - headed attention", "Part-Of", "transformer layer"], ["feed - forward adapter module", "Part-Of", "transformer layer"]], "rel_plus": [["feed - forward layers:Method", "Part-Of", "transformer layer:Method"], ["multi - headed attention:Method", "Part-Of", "transformer layer:Method"], ["feed - forward adapter module:Method", "Part-Of", "transformer layer:Method"]]}
{"doc_id": "102351044", "sentence": "We conjecture that this is because structurally in CNNs , feature extractions are conducted channel - wisely during convolutional operation , thus neuron level dropout can hardly enjoy the ensemble benefits .", "ner": [["CNNs", "Method"], ["convolutional operation", "Method"], ["neuron level dropout", "Method"]], "rel": [["convolutional operation", "Part-Of", "CNNs"], ["neuron level dropout", "Part-Of", "CNNs"]], "rel_plus": [["convolutional operation:Method", "Part-Of", "CNNs:Method"], ["neuron level dropout:Method", "Part-Of", "CNNs:Method"]]}
{"doc_id": "23569888", "sentence": "In this section we follow - up the preliminary analysis reported in Sec. 4. 1 discussing the potential biases in ImageNet , pre - venting off - the - shelf models to generalize well when tested on iCubWorld without applying knowledge transfer techniques .", "ner": [["ImageNet", "Dataset"], ["iCubWorld", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "209386851", "sentence": "PointRend can be incorporated into popular meta - architectures for both instance segmentation ( e.g. , Mask R - CNN [ 1 9 ] ) and semantic segmentation ( e.g. , FCN [ 3 5 ] ) .", "ner": [["PointRend", "Method"], ["instance segmentation", "Task"], ["Mask R - CNN", "Method"], ["semantic segmentation", "Task"], ["FCN", "Method"]], "rel": [["Mask R - CNN", "Used-For", "instance segmentation"], ["PointRend", "Used-For", "instance segmentation"], ["PointRend", "Used-For", "semantic segmentation"], ["PointRend", "Part-Of", "FCN"]], "rel_plus": [["Mask R - CNN:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Used-For", "instance segmentation:Task"], ["PointRend:Method", "Used-For", "semantic segmentation:Task"], ["PointRend:Method", "Part-Of", "FCN:Method"]]}
{"doc_id": "202577400", "sentence": "We use VOC 2 0 0 7 and VOC 2 0 1 2 trainval set as training set and report results on VOC 2 0 0 7 test set .   Semantic Segmentation We employ Fully Convolutional Networks ( FCNs ) as baseline , where ResNet pretrained on ImageNet is chosen as the backbone following the same setting as PSPNet [ 4 2 ] , the proposed GALD is appended to the backbone with random initialization .", "ner": [["VOC 2 0 0 7", "Dataset"], ["VOC 2 0 1 2 trainval set", "Dataset"], ["VOC 2 0 0 7 test set", "Dataset"], ["Semantic Segmentation", "Task"], ["Fully Convolutional Networks", "Method"], ["FCNs", "Method"], ["ResNet", "Method"], ["ImageNet", "Dataset"], ["PSPNet", "Method"], ["GALD", "Method"]], "rel": [["FCNs", "Synonym-Of", "Fully Convolutional Networks"], ["ResNet", "Part-Of", "Fully Convolutional Networks"], ["GALD", "Part-Of", "ResNet"], ["ResNet", "Trained-With", "ImageNet"], ["ResNet", "Part-Of", "PSPNet"]], "rel_plus": [["FCNs:Method", "Synonym-Of", "Fully Convolutional Networks:Method"], ["ResNet:Method", "Part-Of", "Fully Convolutional Networks:Method"], ["GALD:Method", "Part-Of", "ResNet:Method"], ["ResNet:Method", "Trained-With", "ImageNet:Dataset"], ["ResNet:Method", "Part-Of", "PSPNet:Method"]]}
{"doc_id": "203593581", "sentence": "Here , we conduct a case study of ResNet - 2 0 on CIFAR 1 0 to illustrate how normalization for weights can help quantization .", "ner": [["ResNet - 2 0", "Method"], ["CIFAR 1 0", "Dataset"], ["quantization", "Method"]], "rel": [["ResNet - 2 0", "Evaluated-With", "CIFAR 1 0"]], "rel_plus": [["ResNet - 2 0:Method", "Evaluated-With", "CIFAR 1 0:Dataset"]]}
{"doc_id": "52009210", "sentence": "For this task , we examined bidirectional LSTMs and Conditional Random Fields ( CRFs ) . \u2022 Entity Linking : Detected entities ( text strings ) need to be linked to entities in the knowledge graph ( e.g. , URI from DBpedia in our case ) .", "ner": [["bidirectional LSTMs", "Method"], ["Conditional Random Fields", "Method"], ["CRFs", "Method"], ["Entity Linking", "Task"], ["DBpedia", "Dataset"]], "rel": [["CRFs", "Synonym-Of", "Conditional Random Fields"]], "rel_plus": [["CRFs:Method", "Synonym-Of", "Conditional Random Fields:Method"]]}
{"doc_id": "208202241", "sentence": "In order to better model this information , we propose to construct the KS graph for label correlation modeling by superimposing knowledge graph into statistical graph .", "ner": [["KS graph", "Method"], ["knowledge graph", "Method"], ["statistical graph", "Method"]], "rel": [["KS graph", "Compare-With", "knowledge graph"], ["KS graph", "Compare-With", "statistical graph"]], "rel_plus": [["KS graph:Method", "Compare-With", "knowledge graph:Method"], ["KS graph:Method", "Compare-With", "statistical graph:Method"]]}
{"doc_id": "210861282", "sentence": "The UniPose pipeline utilizes the WASP module that features a waterfall flow with a cascade of atrous convolutions and multi - scale representations .", "ner": [["UniPose", "Method"], ["WASP", "Method"], ["waterfall flow", "Method"], ["a cascade of atrous convolutions", "Method"]], "rel": [["WASP", "Part-Of", "UniPose"], ["waterfall flow", "Part-Of", "WASP"], ["a cascade of atrous convolutions", "Part-Of", "WASP"]], "rel_plus": [["WASP:Method", "Part-Of", "UniPose:Method"], ["waterfall flow:Method", "Part-Of", "WASP:Method"], ["a cascade of atrous convolutions:Method", "Part-Of", "WASP:Method"]]}
{"doc_id": "199668978", "sentence": "QR - RF considers multi - grain word embedding and both word reward and sentence reward but uses REINFORCE method .", "ner": [["QR - RF", "Method"], ["multi - grain word embedding", "Method"], ["REINFORCE", "Method"]], "rel": [["REINFORCE", "Part-Of", "QR - RF"], ["multi - grain word embedding", "Part-Of", "QR - RF"]], "rel_plus": [["REINFORCE:Method", "Part-Of", "QR - RF:Method"], ["multi - grain word embedding:Method", "Part-Of", "QR - RF:Method"]]}
{"doc_id": "211004033", "sentence": "The following RoiAlign layer [ 7 ] reshapes the cropped features to produce feature maps of the same size per proposal for classification and bounding - box regression .", "ner": [["RoiAlign", "Method"], ["classification", "Task"], ["bounding - box regression", "Task"]], "rel": [["RoiAlign", "Used-For", "classification"], ["RoiAlign", "Used-For", "bounding - box regression"]], "rel_plus": [["RoiAlign:Method", "Used-For", "classification:Task"], ["RoiAlign:Method", "Used-For", "bounding - box regression:Task"]]}
{"doc_id": "51876625", "sentence": "The spatio - temporal separable 3D ConvNet ( S 3 D ) [ 5 9 ] improves the I 3 D architecture by observing that the 3D convolutions can be replaced by separable spatial and temporal convolutions without loss in accuracy , and that using such convolutions in higher layers of the network results in faster and more accurate models .", "ner": [["spatio - temporal separable 3D ConvNet", "Method"], ["S 3 D", "Method"], ["I 3 D", "Method"], ["3D convolutions", "Method"], ["separable spatial and temporal convolutions", "Method"], ["convolutions", "Method"]], "rel": [["S 3 D", "Synonym-Of", "spatio - temporal separable 3D ConvNet"], ["separable spatial and temporal convolutions", "Part-Of", "spatio - temporal separable 3D ConvNet"], ["3D convolutions", "Part-Of", "spatio - temporal separable 3D ConvNet"], ["spatio - temporal separable 3D ConvNet", "Compare-With", "I 3 D"], ["separable spatial and temporal convolutions", "SubClass-Of", "convolutions"], ["3D convolutions", "SubClass-Of", "convolutions"]], "rel_plus": [["S 3 D:Method", "Synonym-Of", "spatio - temporal separable 3D ConvNet:Method"], ["separable spatial and temporal convolutions:Method", "Part-Of", "spatio - temporal separable 3D ConvNet:Method"], ["3D convolutions:Method", "Part-Of", "spatio - temporal separable 3D ConvNet:Method"], ["spatio - temporal separable 3D ConvNet:Method", "Compare-With", "I 3 D:Method"], ["separable spatial and temporal convolutions:Method", "SubClass-Of", "convolutions:Method"], ["3D convolutions:Method", "SubClass-Of", "convolutions:Method"]]}
{"doc_id": "201070522", "sentence": "The small gain provided in generation tasks by Visual Word 2 Vec does not out - weight the drops in performance across other tasks such as the significant mean recall drop of 6. 3 compared to HGLMM 's 6K - D Self - Attention result in line two of Table 2(c ) and Table 2 ( e ) for image - sentence retrieval of Flickr 3 0 K .", "ner": [["Visual Word 2 Vec", "Method"], ["HGLMM", "Method"], ["Self - Attention", "Method"], ["image - sentence retrieval", "Task"], ["Flickr 3 0 K", "Dataset"]], "rel": [["Flickr 3 0 K", "Benchmark-For", "image - sentence retrieval"]], "rel_plus": [["Flickr 3 0 K:Dataset", "Benchmark-For", "image - sentence retrieval:Task"]]}
{"doc_id": "11241677", "sentence": "The obtained fixed length descriptor of the video can now be classified into the output classes using a Logistic or Softmax layer with additional fully connected layers in between .", "ner": [["Logistic", "Method"], ["Softmax layer", "Method"], ["fully connected layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "21683040", "sentence": "SSD ( Single Shot Multibox Detector ) is one of the best object detection algorithms with both high accuracy and fast speed .", "ner": [["SSD", "Method"], ["Single Shot Multibox Detector", "Method"], ["object detection", "Task"]], "rel": [["SSD", "Synonym-Of", "Single Shot Multibox Detector"], ["SSD", "Used-For", "object detection"]], "rel_plus": [["SSD:Method", "Synonym-Of", "Single Shot Multibox Detector:Method"], ["SSD:Method", "Used-For", "object detection:Task"]]}
{"doc_id": "202719032", "sentence": "The difference is that whilst RPN provides object/non - object classification , SSD provides class - level classification .", "ner": [["RPN", "Method"], ["object/non - object classification", "Task"], ["SSD", "Method"], ["class - level classification", "Task"]], "rel": [["RPN", "Used-For", "object/non - object classification"], ["RPN", "Compare-With", "SSD"], ["SSD", "Used-For", "class - level classification"]], "rel_plus": [["RPN:Method", "Used-For", "object/non - object classification:Task"], ["RPN:Method", "Compare-With", "SSD:Method"], ["SSD:Method", "Used-For", "class - level classification:Task"]]}
{"doc_id": "53731879", "sentence": "So far all components in one shot domain adaptation framework have been evaluated and validated , we achieve promising performances on both Market 1 5 0 1 and DukeMTMC - reID .", "ner": [["one shot domain adaptation framework", "Method"], ["Market 1 5 0 1", "Dataset"], ["DukeMTMC - reID", "Dataset"]], "rel": [["one shot domain adaptation framework", "Evaluated-With", "Market 1 5 0 1"], ["one shot domain adaptation framework", "Evaluated-With", "DukeMTMC - reID"]], "rel_plus": [["one shot domain adaptation framework:Method", "Evaluated-With", "Market 1 5 0 1:Dataset"], ["one shot domain adaptation framework:Method", "Evaluated-With", "DukeMTMC - reID:Dataset"]]}
{"doc_id": "195347056", "sentence": "C. More generated samples 1 ) Wikiart : More generated fine - art paintings are visualized in Figure 1 2 , Figure 1 3 , and Figure 1 4 at high resolution ( 1 2 8 \u00d7 1 2 8 pixels ) . 2 ) CIFAR - 1 0 : Figure 1 5 shows generated images at 6 4 \u00d7 6 4 resolution trained on CIFAR - 1 0 . 3 ) STL - 1 0 : Figure 1 6 shows generated images at resolution of 1 2 8 \u00d7 1 2 8 pixels trained on STL - 1 0 . 4 ) CUB - 2 0 0 birds : Figure 1 7 shows more generated CUB - 2 0 0 images .", "ner": [["Wikiart", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["CIFAR - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["STL - 1 0", "Dataset"], ["CUB - 2 0 0 birds", "Dataset"], ["CUB - 2 0 0", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "53731879", "sentence": "In [ 1 ] , Bak et al. utilize a metric learning approach for a pair of cameras which can be split into texture and color components for one shot image - based re - ID .", "ner": [["metric learning", "Method"], ["one shot image - based re - ID", "Task"]], "rel": [["metric learning", "Used-For", "one shot image - based re - ID"]], "rel_plus": [["metric learning:Method", "Used-For", "one shot image - based re - ID:Task"]]}
{"doc_id": "202676714", "sentence": "For example , a standard block of ResNet ( He et al. 2 0 1 6 ) is composed as where BN is batch normalization ( Ioffe and Szegedy 2 0 1 5 ) .", "ner": [["ResNet", "Method"], ["BN", "Method"], ["batch normalization", "Method"]], "rel": [["BN", "Part-Of", "ResNet"], ["BN", "Synonym-Of", "batch normalization"]], "rel_plus": [["BN:Method", "Part-Of", "ResNet:Method"], ["BN:Method", "Synonym-Of", "batch normalization:Method"]]}
{"doc_id": "52180375", "sentence": "In this paper , we have presented a Dual Attention Network ( DANet ) for scene segmentation , which adaptively integrates local semantic features using the self - attention mechanism .", "ner": [["Dual Attention Network", "Method"], ["DANet", "Method"], ["scene segmentation", "Task"]], "rel": [["DANet", "Synonym-Of", "Dual Attention Network"], ["Dual Attention Network", "Used-For", "scene segmentation"]], "rel_plus": [["DANet:Method", "Synonym-Of", "Dual Attention Network:Method"], ["Dual Attention Network:Method", "Used-For", "scene segmentation:Task"]]}
{"doc_id": "4539700", "sentence": "The results of this study , which indicate deeper 3D CNNs are more effective , can be expected to facilitate further progress in computer vision for videos .   The HMDB - 5 1 [ 1 7 ] and UCF - 1 0 1 [ 2 1 ] datasets are currently the most successful in the field of action recognition .", "ner": [["3D CNNs", "Method"], ["computer vision", "Task"], ["HMDB - 5 1", "Dataset"], ["UCF - 1 0 1", "Dataset"], ["action recognition", "Task"]], "rel": [["3D CNNs", "Used-For", "computer vision"], ["HMDB - 5 1", "Benchmark-For", "action recognition"], ["UCF - 1 0 1", "Benchmark-For", "action recognition"]], "rel_plus": [["3D CNNs:Method", "Used-For", "computer vision:Task"], ["HMDB - 5 1:Dataset", "Benchmark-For", "action recognition:Task"], ["UCF - 1 0 1:Dataset", "Benchmark-For", "action recognition:Task"]]}
{"doc_id": "210860760", "sentence": "Thus , the incorporation of graph - structured data and the deep learning model results in an outstanding feature learning technique , called graph embedding .", "ner": [["deep learning", "Method"], ["feature learning technique", "Method"], ["graph embedding", "Method"]], "rel": [["graph embedding", "SubClass-Of", "feature learning technique"]], "rel_plus": [["graph embedding:Method", "SubClass-Of", "feature learning technique:Method"]]}
{"doc_id": "209862890", "sentence": "We conduct entity linking experiments on standard benchmark datasets : AIDA - CoNLL and five out - domain test sets .", "ner": [["entity linking", "Task"], ["AIDA - CoNLL", "Dataset"]], "rel": [["AIDA - CoNLL", "Benchmark-For", "entity linking"]], "rel_plus": [["AIDA - CoNLL:Dataset", "Benchmark-For", "entity linking:Task"]]}
{"doc_id": "204402755", "sentence": "Deep Neural Networks ( DNNs ) have drastically advanced the state - of - the - art performance in many computer science applications , including computer vision ( Krizhevsky et al. , 2 0 1 2 ) , ( He et al. , 2 0 1 6 ; Ren et al. , 2 0 1 5 ) , natural language processing ( Mikolov et al. , 2 0 1 3 ; Bahdanau et al. , 2 0 1 4 ; Gehring et al. , 2 0 1 7 ) and speech recognition ( Sak et al. , 2 0 1 4 ; Sercu et al. , 2 0 1 6 ) .", "ner": [["Deep Neural Networks", "Method"], ["DNNs", "Method"], ["computer vision", "Task"], ["natural language processing", "Task"], ["speech recognition", "Task"]], "rel": [["DNNs", "Synonym-Of", "Deep Neural Networks"], ["Deep Neural Networks", "Used-For", "computer vision"], ["Deep Neural Networks", "Used-For", "natural language processing"], ["Deep Neural Networks", "Used-For", "speech recognition"]], "rel_plus": [["DNNs:Method", "Synonym-Of", "Deep Neural Networks:Method"], ["Deep Neural Networks:Method", "Used-For", "computer vision:Task"], ["Deep Neural Networks:Method", "Used-For", "natural language processing:Task"], ["Deep Neural Networks:Method", "Used-For", "speech recognition:Task"]]}
{"doc_id": "104291983", "sentence": "The segmentation and object detection results , given in Figure 4 ( a ) and Figure 4 ( b ) , imply that HRNetV 2 outperforms HRNetV 1 significantly , except that the gain is minor in the large model case in segmentation for Cityscapes .", "ner": [["segmentation", "Task"], ["object detection", "Task"], ["HRNetV 2", "Method"], ["HRNetV 1", "Method"], ["segmentation", "Task"], ["Cityscapes", "Dataset"]], "rel": [["HRNetV 1", "Used-For", "segmentation"], ["HRNetV 2", "Used-For", "object detection"], ["HRNetV 2", "Compare-With", "HRNetV 1"], ["Cityscapes", "Benchmark-For", "segmentation"]], "rel_plus": [["HRNetV 1:Method", "Used-For", "segmentation:Task"], ["HRNetV 2:Method", "Used-For", "object detection:Task"], ["HRNetV 2:Method", "Compare-With", "HRNetV 1:Method"], ["Cityscapes:Dataset", "Benchmark-For", "segmentation:Task"]]}
{"doc_id": "51876625", "sentence": "Recent work [ 1 2 ] obtains state - of - the - art performance for human - action - object recognition on V - COCO [ 1 4 ] and HICO - DET [ 4 ] .", "ner": [["human - action - object recognition", "Task"], ["V - COCO", "Dataset"], ["HICO - DET", "Dataset"]], "rel": [["V - COCO", "Benchmark-For", "human - action - object recognition"], ["HICO - DET", "Benchmark-For", "human - action - object recognition"]], "rel_plus": [["V - COCO:Dataset", "Benchmark-For", "human - action - object recognition:Task"], ["HICO - DET:Dataset", "Benchmark-For", "human - action - object recognition:Task"]]}
{"doc_id": "210920315", "sentence": "Our PSC - Net achieves improved results ( 1 0 . 5 on R and 5 0 .   R HO Adaptive Faster RCNN [ 5 0 ] 1 3 . 0 5 0 . 5 MS - CNN [ 7 ] 1 3 . 3 5 1 . 9 Rep. Loss [ 4 5 ] 1 1 . 5 5 2 . 6 OR - CNN [ 5 2 ] 1 1 . 3 5 1 . 4 Cascade MS - CNN [ 7 ] 1 1 . 6 4 7 . 1 Adaptive - NMS [ 1 9 ] 1 1 . 4 -MGAN [ 3 3 ] 9. 3 4 1 . 0 PSC - Net ( Ours ) 9. 0 3 7 . 4 sets , compared to the state - of - the - art methods .", "ner": [["PSC - Net", "Method"], ["Faster RCNN", "Method"], ["MS - CNN", "Method"], ["OR - CNN", "Method"], ["Cascade MS - CNN", "Method"], ["Adaptive - NMS", "Method"], ["-MGAN", "Method"], ["PSC - Net", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "59599694", "sentence": "Related analyses can be constructed for other spatial - temporal problems , such as anomaly detection , missing data imputation , time series clustering and time series classification problems .", "ner": [["spatial - temporal problems", "Task"], ["anomaly detection", "Task"], ["missing data imputation", "Task"], ["time series clustering", "Task"], ["time series classification", "Task"]], "rel": [["anomaly detection", "SubTask-Of", "spatial - temporal problems"], ["missing data imputation", "SubTask-Of", "spatial - temporal problems"], ["time series clustering", "SubTask-Of", "spatial - temporal problems"], ["time series classification", "SubTask-Of", "spatial - temporal problems"]], "rel_plus": [["anomaly detection:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["missing data imputation:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["time series clustering:Task", "SubTask-Of", "spatial - temporal problems:Task"], ["time series classification:Task", "SubTask-Of", "spatial - temporal problems:Task"]]}
{"doc_id": "210164517", "sentence": "Classification here is done using SVM classifier .", "ner": [["Classification", "Task"], ["SVM", "Method"]], "rel": [["SVM", "Used-For", "Classification"]], "rel_plus": [["SVM:Method", "Used-For", "Classification:Task"]]}
{"doc_id": "23569888", "sentence": "We challenged Deep Learning methods on an object recognition task that was specifically designed to represent a prototypical visual recognition problem in a real robotics application .", "ner": [["Deep Learning methods", "Method"], ["object recognition", "Task"], ["visual recognition", "Task"], ["real robotics application", "Task"]], "rel": [["Deep Learning methods", "Used-For", "object recognition"], ["visual recognition", "SubTask-Of", "real robotics application"]], "rel_plus": [["Deep Learning methods:Method", "Used-For", "object recognition:Task"], ["visual recognition:Task", "SubTask-Of", "real robotics application:Task"]]}
{"doc_id": "199543700", "sentence": "As the CIFAR 1 0 dataset [ 2 8 ] , the SVHN dataset [ 4 5 ] is used for validating semi - supervised learning methods .", "ner": [["CIFAR 1 0", "Dataset"], ["SVHN", "Dataset"], ["semi - supervised learning", "Method"]], "rel": [["semi - supervised learning", "Evaluated-With", "CIFAR 1 0"], ["semi - supervised learning", "Evaluated-With", "SVHN"]], "rel_plus": [["semi - supervised learning:Method", "Evaluated-With", "CIFAR 1 0:Dataset"], ["semi - supervised learning:Method", "Evaluated-With", "SVHN:Dataset"]]}
{"doc_id": "199668978", "sentence": "Proximal policy optimization ( PPO ) [ 3 4 ] is an approximation method of trust region policy optimization ( TRPO ) [ 2 8 ] .", "ner": [["Proximal policy optimization", "Method"], ["PPO", "Method"], ["trust region policy optimization", "Method"], ["TRPO", "Method"]], "rel": [["PPO", "Synonym-Of", "Proximal policy optimization"], ["TRPO", "Synonym-Of", "trust region policy optimization"], ["Proximal policy optimization", "SubClass-Of", "trust region policy optimization"]], "rel_plus": [["PPO:Method", "Synonym-Of", "Proximal policy optimization:Method"], ["TRPO:Method", "Synonym-Of", "trust region policy optimization:Method"], ["Proximal policy optimization:Method", "SubClass-Of", "trust region policy optimization:Method"]]}
{"doc_id": "24972096", "sentence": "All of the above RGB - D fusion networks treat the depth image similarly to an RGB image using a CNN with a max - pooling layer .", "ner": [["RGB - D fusion networks", "Method"], ["CNN", "Method"], ["max - pooling", "Method"]], "rel": [["CNN", "Part-Of", "RGB - D fusion networks"], ["max - pooling", "Part-Of", "CNN"]], "rel_plus": [["CNN:Method", "Part-Of", "RGB - D fusion networks:Method"], ["max - pooling:Method", "Part-Of", "CNN:Method"]]}
{"doc_id": "202540251", "sentence": "Since most of the previous works conducted adaptation to Cityscapes with VGG backbone networks , we present the adaptation mIoU comparison on GTA \u2192 Cityscapes and SYNTHIA \u2192 Cityscapes in Table 4 and Table 5 , leaving class - wise comparison details in the supplementary material .", "ner": [["Cityscapes", "Dataset"], ["VGG", "Method"], ["GTA", "Dataset"], ["Cityscapes", "Dataset"], ["SYNTHIA", "Dataset"], ["Cityscapes", "Dataset"]], "rel": [["VGG", "Evaluated-With", "Cityscapes"], ["VGG", "Evaluated-With", "GTA"], ["VGG", "Evaluated-With", "SYNTHIA"]], "rel_plus": [["VGG:Method", "Evaluated-With", "Cityscapes:Dataset"], ["VGG:Method", "Evaluated-With", "GTA:Dataset"], ["VGG:Method", "Evaluated-With", "SYNTHIA:Dataset"]]}
{"doc_id": "AUG139", "sentence": "Office-31 Domain Adaptation Dataset, or Office-31, is optimized by DAAN for domain adaptation.", "ner": [["Office-31 Domain Adaptation Dataset", "Dataset"], ["Office-31", "Dataset"], ["DAAN", "Method"], ["domain adaptation", "Task"]], "rel": [["Office-31", "Synonym-Of", "Office-31 Domain Adaptation Dataset"], ["DAAN", "Used-For", "domain adaptation"], ["DAAN", "Trained-With", "Office-31"]], "rel_plus": [["Office-31:Dataset", "Synonym-Of", "Office-31 Domain Adaptation Dataset:Dataset"], ["DAAN:Method", "Used-For", "domain adaptation:Task"], ["DAAN:Method", "Trained-With", "Office-31:Dataset"]]}
{"doc_id": "204901567", "sentence": "In contrast to existing classification benchmarks , question answering requires identifying relevant answer spans in longer context paragraphs , thus requiring some degree of structural transfer across languages .", "ner": [["classification", "Task"], ["question answering", "Task"]], "rel": [["question answering", "Compare-With", "classification"]], "rel_plus": [["question answering:Task", "Compare-With", "classification:Task"]]}
{"doc_id": "208548469", "sentence": "As for our LASSO ranking method , the ranking performance is quite effective , despite its simplicity .", "ner": [["LASSO ranking method", "Method"], ["ranking", "Task"]], "rel": [["LASSO ranking method", "Used-For", "ranking"]], "rel_plus": [["LASSO ranking method:Method", "Used-For", "ranking:Task"]]}
{"doc_id": "52009210", "sentence": "The first two are standard neural network models ; for logistic regression we used as input the average of the word embeddings of each word .", "ner": [["neural network models", "Method"], ["logistic regression", "Method"], ["word embeddings", "Method"]], "rel": [["logistic regression", "SubClass-Of", "neural network models"], ["word embeddings", "Part-Of", "logistic regression"]], "rel_plus": [["logistic regression:Method", "SubClass-Of", "neural network models:Method"], ["word embeddings:Method", "Part-Of", "logistic regression:Method"]]}
{"doc_id": "202676714", "sentence": "Note that Absum is more robust against high - pass filtering than SNC , which is presented in the appendix .", "ner": [["Absum", "Method"], ["SNC", "Method"]], "rel": [["Absum", "Compare-With", "SNC"]], "rel_plus": [["Absum:Method", "Compare-With", "SNC:Method"]]}
{"doc_id": "202577400", "sentence": "LD is proposed to adaptively use F GA considering patterns on each position .", "ner": [["LD", "Method"], ["F GA", "Method"]], "rel": [["F GA", "Part-Of", "LD"]], "rel_plus": [["F GA:Method", "Part-Of", "LD:Method"]]}
{"doc_id": "4539700", "sentence": "In particular , we explore the following architectures : ResNet ( basic and bottleneck blocks ) [ 1 0 ] , pre - activation ResNet [ 1 1 ] , wide ResNet ( WRN ) [ 3 1 ] , ResNeXt [ 3 0 ] , and DenseNet [ 1 2 ] .", "ner": [["ResNet", "Method"], ["pre - activation ResNet", "Method"], ["wide ResNet", "Method"], ["WRN", "Method"], ["ResNeXt", "Method"], ["DenseNet", "Method"]], "rel": [["WRN", "Synonym-Of", "wide ResNet"]], "rel_plus": [["WRN:Method", "Synonym-Of", "wide ResNet:Method"]]}
