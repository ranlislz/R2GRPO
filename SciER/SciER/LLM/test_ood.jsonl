{"doc_id": "AAAI2024", "sentence": "GxVAEs : Two Joint VAEs Generate Hit Molecules from Gene Expression Profiles .", "ner": [["GxVAEs", "Method"], ["VAEs", "Method"]], "rel": [["VAEs", "Part-Of", "GxVAEs"]], "rel_plus": [["VAEs:Method", "Part-Of", "GxVAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The de novo generation of hit-like molecules that show bioactivity and drug-likeness is an important task in computer-aided drug discovery .", "ner": [["de novo generation", "Task"], ["computer-aided drug discovery", "Task"]], "rel": [["de novo generation", "SubTask-Of", "computer-aided drug discovery"]], "rel_plus": [["de novo generation:Task", "SubTask-Of", "computer-aided drug discovery:Task"]]}
{"doc_id": "AAAI2024", "sentence": "Although artificial intelligence can generate molecules with desired chemical properties , most previous studies have ignored the influence of disease-related cellular environments .", "ner": [["artificial intelligence", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "This study proposes a novel deep generative model called GxVAEs to generate hit-like molecules from gene expression profiles by leveraging two joint variational autoencoders ( VAEs ) .", "ner": [["deep generative model", "Method"], ["GxVAEs", "Method"], ["variational autoencoders", "Method"], ["VAEs", "Method"]], "rel": [["GxVAEs", "SubClass-Of", "deep generative model"], ["variational autoencoders", "Part-Of", "GxVAEs"], ["VAEs", "Synonym-Of", "variational autoencoders"]], "rel_plus": [["GxVAEs:Method", "SubClass-Of", "deep generative model:Method"], ["variational autoencoders:Method", "Part-Of", "GxVAEs:Method"], ["VAEs:Method", "Synonym-Of", "variational autoencoders:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The first VAE , ProfileVAE , extracts latent features from gene expression profiles .", "ner": [["VAE", "Method"], ["ProfileVAE", "Method"]], "rel": [["ProfileVAE", "SubClass-Of", "VAE"]], "rel_plus": [["ProfileVAE:Method", "SubClass-Of", "VAE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The extracted features serve as the conditions that guide the second VAE , which is called MolVAE , in generating hit-like molecules .", "ner": [["VAE", "Method"], ["MolVAE", "Method"]], "rel": [["MolVAE", "SubClass-Of", "VAE"]], "rel_plus": [["MolVAE:Method", "SubClass-Of", "VAE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "GxVAEs bridge the gap between molecular generation and the cellular environment in a biological system , and produce molecules that are biologically meaningful in the context of specific diseases .", "ner": [["GxVAEs", "Method"], ["molecular generation", "Task"]], "rel": [["GxVAEs", "Used-For", "molecular generation"]], "rel_plus": [["GxVAEs:Method", "Used-For", "molecular generation:Task"]]}
{"doc_id": "AAAI2024", "sentence": "Experiments and case studies on the generation of therapeutic molecules show that GxVAEs outperforms current state-of-the-art baselines and yield hit-like molecules with potential bioactivity and drug-like properties .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Hit identification , which involves discovering hit molecules with the desired bioactivity and therapeutic effects within an infinite chemical space , is a critical challenge in drug discovery ( Dobson et al . 2004 ) .", "ner": [["Hit identification", "Task"], ["drug discovery", "Task"]], "rel": [["Hit identification", "SubTask-Of", "drug discovery"]], "rel_plus": [["Hit identification:Task", "SubTask-Of", "drug discovery:Task"]]}
{"doc_id": "AAAI2024", "sentence": "Traditionally , hit identification campaigns have been conducted using experimental approaches such as high-throughput screening ( HTS ) ( Hertzberg and Pope 2000 ) .", "ner": [["hit identification", "Task"], ["high-throughput screening", "Method"], ["HTS", "Method"]], "rel": [["high-throughput screening", "Used-For", "hit identification"], ["HTS", "Synonym-Of", "high-throughput screening"]], "rel_plus": [["high-throughput screening:Method", "Used-For", "hit identification:Task"], ["HTS:Method", "Synonym-Of", "high-throughput screening:Method"]]}
{"doc_id": "AAAI2024", "sentence": "HTS provides a valuable methodology for identifying hits with desired bioactivity ( Scannell et al . 2022 ) .", "ner": [["HTS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "However , the hit rate observed in HTS campaigns tends to be relatively low , which results in many molecules that are inactive or that hold less promise being screened out ( Rahman et al . 2022 ) .", "ner": [["HTS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Additionally , experimental approaches to drug discovery typically rely on laborintensive and time-consuming processes .", "ner": [["drug discovery", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "De novo molecular generation using artificial intelligence ( AI ) techniques has emerged as a promising approach for computer-aided drug discovery .", "ner": [["novo molecular generation", "Task"], ["artificial intelligence", "Method"], ["AI", "Method"], ["computer-aided drug discovery", "Task"]], "rel": [["artificial intelligence", "Used-For", "novo molecular generation"], ["AI", "Synonym-Of", "artificial intelligence"], ["artificial intelligence", "Used-For", "computer-aided drug discovery"], ["novo molecular generation", "SubTask-Of", "computer-aided drug discovery"]], "rel_plus": [["artificial intelligence:Method", "Used-For", "novo molecular generation:Task"], ["AI:Method", "Synonym-Of", "artificial intelligence:Method"], ["artificial intelligence:Method", "Used-For", "computer-aided drug discovery:Task"], ["novo molecular generation:Task", "SubTask-Of", "computer-aided drug discovery:Task"]]}
{"doc_id": "AAAI2024", "sentence": "The power of deep learning and computational chemistry enables the generation of new molecules with desired bioactivity for specific therapeutic targets .", "ner": [["deep learning", "Method"], ["computational chemistry", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Deep generative models , such as generative adversarial networks ( GANs ) ( Guimaraes et al . 2017 ; De Cao and Kipf 2018 ; Li et al . 2022 ) and variational autoencoders ( VAEs ) ( Oliveira , Da Silva , and Quiles 2022 ; Dollar et al . 2021 ; Kusner , Paige , and Hern\u00e1ndez-Lobato 2017 ; Jin , Barzilay , and Jaakkola 2018 ) accelerate the drug discovery process by employing computational models to generate molecules having a specific bioactivity .", "ner": [["Deep generative models", "Method"], ["generative adversarial networks", "Method"], ["GANs", "Method"], ["variational autoencoders", "Method"], ["VAEs", "Method"], ["drug discovery", "Task"]], "rel": [["generative adversarial networks", "SubClass-Of", "Deep generative models"], ["variational autoencoders", "SubClass-Of", "Deep generative models"], ["GANs", "Synonym-Of", "generative adversarial networks"], ["VAEs", "Synonym-Of", "variational autoencoders"], ["variational autoencoders", "Used-For", "drug discovery"], ["generative adversarial networks", "Used-For", "drug discovery"], ["Deep generative models", "Used-For", "drug discovery"]], "rel_plus": [["generative adversarial networks:Method", "SubClass-Of", "Deep generative models:Method"], ["variational autoencoders:Method", "SubClass-Of", "Deep generative models:Method"], ["GANs:Method", "Synonym-Of", "generative adversarial networks:Method"], ["VAEs:Method", "Synonym-Of", "variational autoencoders:Method"], ["variational autoencoders:Method", "Used-For", "drug discovery:Task"], ["generative adversarial networks:Method", "Used-For", "drug discovery:Task"], ["Deep generative models:Method", "Used-For", "drug discovery:Task"]]}
{"doc_id": "AAAI2024", "sentence": "By capturing the essential features of molecules , deep generative models can produce new molecules with similar structures and related properties .", "ner": [["deep generative models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Most AI-driven molecular generation models focus on the generation of new molecules with improved chemical properties of interest .", "ner": [["AI-driven molecular generation models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "While omics-based approaches to drug discovery offer promising opportunities , it is essential to understand their limitations ( Kang , Ko , and Mersha 2022 ) .", "ner": [["omics-based approaches", "Method"], ["drug discovery", "Task"]], "rel": [["omics-based approaches", "Used-For", "drug discovery"]], "rel_plus": [["omics-based approaches:Method", "Used-For", "drug discovery:Task"]]}
{"doc_id": "AAAI2024", "sentence": "Interpretation of omics data in the context of drug discovery also requires a deep understanding of the underlying biological and disease mechanisms .", "ner": [["drug discovery", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "In this study , we proposed GxVAEs to generate hit-like molecules from gene expression profiles using two joint VAEs ( i.e . , ProfileVAE and MolVAE ) .", "ner": [["GxVAEs", "Method"], ["VAEs", "Method"], ["ProfileVAE", "Method"], ["MolVAE", "Method"]], "rel": [["VAEs", "Part-Of", "GxVAEs"], ["ProfileVAE", "Part-Of", "GxVAEs"], ["MolVAE", "Part-Of", "GxVAEs"], ["ProfileVAE", "SubClass-Of", "VAEs"], ["MolVAE", "SubClass-Of", "VAEs"]], "rel_plus": [["VAEs:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "Part-Of", "GxVAEs:Method"], ["MolVAE:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "SubClass-Of", "VAEs:Method"], ["MolVAE:Method", "SubClass-Of", "VAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "First , ProfileVAE functions as a feature extractor to extract latent features from gene expression profiles .", "ner": [["ProfileVAE", "Method"], ["feature extractor", "Method"]], "rel": [["ProfileVAE", "SubClass-Of", "feature extractor"]], "rel_plus": [["ProfileVAE:Method", "SubClass-Of", "feature extractor:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The extracted features are then used as conditions to guide MolVAE in generating hit-like molecules .", "ner": [["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "GxVAEs bridges the gap between molecular generation and the cellular environment of a biological system , making the produced molecules more biologically meaningful in the context of specific diseases .", "ner": [["GxVAEs", "Method"], ["molecular generation", "Task"]], "rel": [["GxVAEs", "Used-For", "molecular generation"]], "rel_plus": [["GxVAEs:Method", "Used-For", "molecular generation:Task"]]}
{"doc_id": "AAAI2024", "sentence": "The main contributions of this study are summarized as follows : \u2022 Hit-like molecular generation that considered the cellular environment : This study considers the influence of the cellular environment on specific diseases during the molecular generation process . \u2022 A simple but effective model architecture : The combination of two VAEs successfully generated hit-like molecules from gene expression profiles . \u2022 Superior performance over state-of-the-art ( SOTA ) models : Experiments and case studies demonstrate that the proposed GxVAEs outperform the current SOTA models for the same objectives .", "ner": [["Hit-like molecular generation", "Task"], ["molecular generation", "Task"], ["VAEs", "Method"], ["GxVAEs", "Method"]], "rel": [["VAEs", "Used-For", "Hit-like molecular generation"], ["GxVAEs", "Used-For", "Hit-like molecular generation"], ["VAEs", "Used-For", "molecular generation"], ["GxVAEs", "Used-For", "molecular generation"]], "rel_plus": [["VAEs:Method", "Used-For", "Hit-like molecular generation:Task"], ["GxVAEs:Method", "Used-For", "Hit-like molecular generation:Task"], ["VAEs:Method", "Used-For", "molecular generation:Task"], ["GxVAEs:Method", "Used-For", "molecular generation:Task"]]}
{"doc_id": "AAAI2024", "sentence": "De novo molecular generation aims to produce new molecules with specific chemical properties .", "ner": [["De novo molecular generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Generally , the data structures used for de novo molecular generation are molecular graphs ( Manolopoulos and Fowler 1992 ) and simplified molecular input line entry system ( SMILES ) strings ( Weininger 1988 ) .", "ner": [["de novo molecular generation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "TransVAE ( Dollar et al . 2021 ) and Gram-marVAE ( Kusner , Paige , and Hern\u00e1ndez-Lobato 2017 ) are two deep generative models that produce molecules from SMILES strings using VAEs .", "ner": [["TransVAE", "Method"], ["Gram-marVAE", "Method"], ["deep generative models", "Method"], ["VAEs", "Method"]], "rel": [["VAEs", "Part-Of", "TransVAE"], ["VAEs", "Part-Of", "Gram-marVAE"], ["Gram-marVAE", "SubClass-Of", "deep generative models"], ["TransVAE", "SubClass-Of", "deep generative models"]], "rel_plus": [["VAEs:Method", "Part-Of", "TransVAE:Method"], ["VAEs:Method", "Part-Of", "Gram-marVAE:Method"], ["Gram-marVAE:Method", "SubClass-Of", "deep generative models:Method"], ["TransVAE:Method", "SubClass-Of", "deep generative models:Method"]]}
{"doc_id": "AAAI2024", "sentence": "JTVAE ( Jin , Barzilay , and Jaakkola 2018 ) uses a node tree to generate molecules from a molecular graph .", "ner": [["JTVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "It first creates a tree-structured scaffold on a substructure and then combines the tree-structured scaffold into a molecule using a graphical message-passing network ( Dai , Dai , and Song 2016 ) .", "ner": [["graphical message-passing network", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "MolGAN ( De Cao and Kipf 2018 ) and TransORGAN ( Li et al . 2022 ) are discrete GANs that use reinforcement learning to generate molecular graphs and SMILES strings , respectively .", "ner": [["MolGAN", "Method"], ["TransORGAN", "Method"], ["discrete GANs", "Method"], ["reinforcement learning", "Method"]], "rel": [["reinforcement learning", "Used-For", "MolGAN"], ["reinforcement learning", "Used-For", "TransORGAN"], ["TransORGAN", "SubClass-Of", "discrete GANs"], ["MolGAN", "SubClass-Of", "discrete GANs"]], "rel_plus": [["reinforcement learning:Method", "Used-For", "MolGAN:Method"], ["reinforcement learning:Method", "Used-For", "TransORGAN:Method"], ["TransORGAN:Method", "SubClass-Of", "discrete GANs:Method"], ["MolGAN:Method", "SubClass-Of", "discrete GANs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "With the development of omics data analysis , especially gene expression profiles used in drug discovery ( Turanli et al . 2018 ; Chen et al . 2020 ) , chemists have begun to use comprehensive biological response information to generate therapeutic molecules for the treatment of specific diseases .", "ner": [["drug discovery", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "ExpressionGAN ( M\u00e9ndez-Lucio et al . 2020 ) and TRI-OMPHE ( Kaitoh and Yamanishi 2021 ) are the SOTA deep generative models that are most relevant to our study on generating hit-like molecules for target proteins with no prior annotation of the target training molecules .", "ner": [["ExpressionGAN", "Method"], ["TRI-OMPHE", "Method"], ["deep generative models", "Method"]], "rel": [["TRI-OMPHE", "SubClass-Of", "deep generative models"], ["ExpressionGAN", "SubClass-Of", "deep generative models"]], "rel_plus": [["TRI-OMPHE:Method", "SubClass-Of", "deep generative models:Method"], ["ExpressionGAN:Method", "SubClass-Of", "deep generative models:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Expres-sionGAN is a GAN model that bridges systems biology and molecular design to produce molecules with a high probability of inducing the desired transcriptome features automatically from gene expression profiles .", "ner": [["Expres-sionGAN", "Method"], ["GAN", "Method"]], "rel": [["Expres-sionGAN", "SubClass-Of", "GAN"]], "rel_plus": [["Expres-sionGAN:Method", "SubClass-Of", "GAN:Method"]]}
{"doc_id": "AAAI2024", "sentence": "TRIOMPHE first calculates the correlation of ligand-target interactions between the chemically induced transcriptome profiles of cellular responses to molecular treatment and the transcriptome profiles of gene perturbations that reflect cellular responses to gene knockdown or gene overexpression of target proteins .", "ner": [["TRIOMPHE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Next , a VAE was employed to generate a new molecule with the desired transcriptome profile .", "ner": [["VAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "However , ExpressionGAN generates hit-like molecules with low validity ( > 8.5% ) , and its ability to reproduce known ligands is limited .", "ner": [["ExpressionGAN", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Additionally , TRIOMPHE only utilizes gene expression profiles in correlation calculations , whose VAE is not involved in molecular generation .", "ner": [["TRIOMPHE", "Method"], ["VAE", "Method"]], "rel": [["VAE", "Part-Of", "TRIOMPHE"]], "rel_plus": [["VAE:Method", "Part-Of", "TRIOMPHE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "To address these issues , GxVAEs were proposed in this study .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "By bridging the gap between the cellular environment and the generation of ther - The Thirty-Eighth AAAI Conference on Artificial Intelligence ( AAAI - 24 ) apeutic molecules , GxVAEs generate molecules that are biologically meaningful in the context of specific diseases using gene expression profiles .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Figure 1 shows an overview of the GxVAEs .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "GxVAEs mainly consists of two joint VAEs , namely ProfileVAE and MolVAE .", "ner": [["GxVAEs", "Method"], ["VAEs", "Method"], ["ProfileVAE", "Method"], ["MolVAE", "Method"]], "rel": [["VAEs", "Part-Of", "GxVAEs"], ["MolVAE", "Part-Of", "GxVAEs"], ["ProfileVAE", "Part-Of", "GxVAEs"], ["ProfileVAE", "SubClass-Of", "VAEs"], ["MolVAE", "SubClass-Of", "VAEs"]], "rel_plus": [["VAEs:Method", "Part-Of", "GxVAEs:Method"], ["MolVAE:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "SubClass-Of", "VAEs:Method"], ["MolVAE:Method", "SubClass-Of", "VAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "ProfileVAE acts as a feature extractor to extract low-dimensional feature vectors from high-dimensional gene expression profiles .", "ner": [["ProfileVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "MolVAE consists of bidirectional GRUs that are conditioned on the features of gene expression profiles using non-canonical SMILES ( i.e . , variant SMILES ( Li and Yamanishi 2023 ) ) strings to generate hitlike molecules .", "ner": [["MolVAE", "Method"], ["bidirectional GRUs", "Method"]], "rel": [["bidirectional GRUs", "Part-Of", "MolVAE"]], "rel_plus": [["bidirectional GRUs:Method", "Part-Of", "MolVAE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Note that teacher forcing ( Yan et al . 2023 ) is used to stabilize the training and accelerate the convergence of the GxVAEs during the training phase of the MolVAE .", "ner": [["GxVAEs", "Method"], ["MolVAE", "Method"]], "rel": [["MolVAE", "Part-Of", "GxVAEs"]], "rel_plus": [["MolVAE:Method", "Part-Of", "GxVAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "ProfileVAE .", "ner": [["ProfileVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The goal of Profil-eVAE is to learn the marginal likelihood of gene expression profiles during the following generative process : max EQUATION where E [ \u2022 ] is an expectation operation , C is the latent variable , \u03b8 and \u03d5 denote the parameters of the ProfileVAE encoder and ProfileVAE decoder , respectively , and p \u03d5 ( X | C ) and q \u03b8 ( C | X ) are the likelihood function and posterior distribution , respectively .", "ner": [["Profil-eVAE", "Method"], ["ProfileVAE encoder", "Method"], ["ProfileVAE decoder", "Method"], ["likelihood function", "Method"], ["posterior distribution", "Method"]], "rel": [["posterior distribution", "Part-Of", "ProfileVAE encoder"], ["likelihood function", "Part-Of", "ProfileVAE decoder"]], "rel_plus": [["posterior distribution:Method", "Part-Of", "ProfileVAE encoder:Method"], ["likelihood function:Method", "Part-Of", "ProfileVAE decoder:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The loss function of the ProfileVAE can be formulated as follows : L G ( \u03b8 , \u03d5 , X , C , \u03b2 ) = E q \u03b8 ( C | X ) [ log p \u03d5 ( X | C ) ] ( 2 ) - \u03b2 \u2022 D KL ( q \u03b8 ( C | X ) | | p \u03d5 ( C ) ) , where \u03b2 denotes the weight of the KL divergence D KL ( Joyce 2011 ) .", "ner": [["ProfileVAE", "Method"], ["KL divergence", "Method"]], "rel": [["KL divergence", "Part-Of", "ProfileVAE"]], "rel_plus": [["KL divergence:Method", "Part-Of", "ProfileVAE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "MolVAE .", "ner": [["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Then , EQUATION ) Note that the feature vectors C of the extracted gene expression profiles are used as the conditions for MolVAE .", "ner": [["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "MolVAE aims to maximize the lower bound of the true logmarginal likelihood as follows : EQUATION - D KL ( q \u03c8 ( Z | S ) | | p \u03c6 ( Z ) ) .", "ner": [["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Three types of gene expression profiles were used to validate the effectiveness of the proposed GxVAEs . \u2022 Chemically induced profiles were collected from the LINCS L1000 database ( Duan et al . 2014 ) .", "ner": [["GxVAEs", "Method"], ["LINCS L1000", "Dataset"]], "rel": [["GxVAEs", "Evaluated-With", "LINCS L1000"]], "rel_plus": [["GxVAEs:Method", "Evaluated-With", "LINCS L1000:Dataset"]]}
{"doc_id": "AAAI2024", "sentence": "Here , gene expression profiles of MCF7 cell line treated with 13 , 755 molecules were used . \u2022 Target perturbation profiles were obtained from the LINCS database .", "ner": [["LINCS", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "For example , we analyzed RAC-\u03b1 serine / threonine-protein kinase ( AKT1 ) , RAC\u03b2 serine / threonine-protein kinase ( AKT2 ) , Aurora B kinase ( AURKB ) , cysteine synthase A ( CTSK ) , epidermal growth factor receptor ( EGFR ) , histone deacetylase 1 ( HDAC1 ) , mammalian target of rapamycin ( MTOR ) , phosphatidylinositol 3 - kinase catalytic subunit ( PIK3CA ) , decapentaplegic homolog 3 ( SMAD3 ) , and tumor protein p53 ( TP53 ) , which are therapeutic proteins for cancers . \u2022 Disease-specific profiles were collected from the crowd extracted expression of differential signatures ( CREEDS ) database ( Wang et al . 2016 ) .", "ner": [["crowd extracted expression of differential signatures", "Dataset"], ["CREEDS", "Dataset"]], "rel": [["CREEDS", "Synonym-Of", "crowd extracted expression of differential signatures"]], "rel_plus": [["CREEDS:Dataset", "Synonym-Of", "crowd extracted expression of differential signatures:Dataset"]]}
{"doc_id": "AAAI2024", "sentence": "Validity refers to the ratio of chemically valid molecules generated , which can be verified in practice using the RDKit tool ( Landrum 2013 ) .", "ner": [["RDKit tool", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "A quantitative estimate of drug-likeness ( QED ) quantifies the likelihood that a molecule is a drug ( Appendix D ) .", "ner": [["quantitative estimate of drug-likeness", "Method"], ["QED", "Method"]], "rel": [["QED", "Synonym-Of", "quantitative estimate of drug-likeness"]], "rel_plus": [["QED:Method", "Synonym-Of", "quantitative estimate of drug-likeness:Method"]]}
{"doc_id": "AAAI2024", "sentence": "To demonstrate that ProfileVAE has the ability to extract the biological features of gene expression profiles , we compared the distributions of the input gene expression profiles and the reconstructed profiles .", "ner": [["ProfileVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The distributions of the reconstructed profiles approximate those of the corresponding validation data , which indicates that ProfileVAE extracted the features of the gene expression profiles well , and proved the effectiveness of the proposed GxVAEs . to those in the validation set .", "ner": [["ProfileVAE", "Method"], ["GxVAEs", "Method"]], "rel": [["ProfileVAE", "Part-Of", "GxVAEs"]], "rel_plus": [["ProfileVAE:Method", "Part-Of", "GxVAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Furthermore , Table 1 lists the statistics for the validation set and molecules generated by the proposed GxVAEs .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The average , maximum , minimum lengths , and the average molecular weight of the generated SMILES strings were basically consistent with the SMILES strings in the original validation set , indicating that GxVAEs learned the data distributions of the SMILES strings well .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Table 2 shows the ability of the proposed Gx-VAEs to generate candidate molecules .", "ner": [["Gx-VAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Note that Expres-sionGAN has limited ability to generate valid molecules , thus we only compared GxVAEs to the TRIOMPHE baseline .", "ner": [["Expres-sionGAN", "Method"], ["GxVAEs", "Method"], ["TRIOMPHE", "Method"]], "rel": [["GxVAEs", "Compare-With", "TRIOMPHE"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "TRIOMPHE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The experimental results demonstrate that the validity of the proposed GxVAEs is three times higher than that of TRIOMPHE for generating ligand-like molecules using the gene expression profiles of the ten target proteins , reaching at least 78.0% ( HDAC1 ) .", "ner": [["GxVAEs", "Method"], ["TRIOMPHE", "Method"]], "rel": [["GxVAEs", "Compare-With", "TRIOMPHE"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "TRIOMPHE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Moreover , the uniqueness of the GxVAEs exceeds that of TRIOMPHE except for PIK3CA ( 93.5% < 97.2% ) .", "ner": [["GxVAEs", "Method"], ["TRIOMPHE", "Method"]], "rel": [["GxVAEs", "Compare-With", "TRIOMPHE"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "TRIOMPHE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Additionally , the novelty of the GxVAEs ( 97.7% ) is close to that of TRIOMPHE .", "ner": [["GxVAEs", "Method"], ["TRIOMPHE", "Method"]], "rel": [["GxVAEs", "Compare-With", "TRIOMPHE"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "TRIOMPHE:Method"]]}
{"doc_id": "AAAI2024", "sentence": "Overall , the proposed GxVAEs provided a sufficient number of candidate ligands for the ten target proteins .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Table 3 compares the Tanimoto coefficients of GxVAEs with those of the two SOTA models .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The experimental results illustrate that the Tanimoto coefficients of the candidate ligands generated by GxVAEs for the ten target proteins were much higher than those of the two SOTA models .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "For example , the Tanimoto coefficient of GxVAEs for AKT1 was 2.7 and 2.0 times higher than those of the two baselines .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The chemical structures of the hit-like molecules generated by GxVAEs are all similar to the structures of the known ligands .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Overall , the proposed GxVAEs show excellent performance in generating hit-like molecules from the gene expression profiles , and the biological activity of the generated molecules far exceeded the SOTA baselines .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Figure 5 shows the process undertaken by GxVAEs for therapeutic molecular generation .", "ner": [["GxVAEs", "Method"], ["therapeutic molecular generation", "Task"]], "rel": [["GxVAEs", "Used-For", "therapeutic molecular generation"]], "rel_plus": [["GxVAEs:Method", "Used-For", "therapeutic molecular generation:Task"]]}
{"doc_id": "AAAI2024", "sentence": "Finally , the disease reversal profiles were input into GxVAEs to generate candidate therapeutic molecules .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "We compared the results generated by GxVAEs with those generated by DRAGONET ( Yamanaka et al . 2023 ) .", "ner": [["GxVAEs", "Method"], ["DRAGONET", "Method"]], "rel": [["GxVAEs", "Compare-With", "DRAGONET"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "DRAGONET:Method"]]}
{"doc_id": "AAAI2024", "sentence": "For a fair comparison , we used the same datasets on patient gene expression profiles and molecule chemical structures in DRAGONET .", "ner": [["DRAGONET", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Figure 6 shows the therapeutic molecules generated by DRAGONET and GxVAEs , as well as the Tanimoto coefficients calculated relative to known approved drugs .", "ner": [["DRAGONET", "Method"], ["GxVAEs", "Method"]], "rel": [["DRAGONET", "Compare-With", "GxVAEs"]], "rel_plus": [["DRAGONET:Method", "Compare-With", "GxVAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "The Tanimoto coefficients between hydrocortisone and the molecules generated by GxVAEs using the disease reversal profile of patients having atopic dermatitis reached 1.0 .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "This result indicates that GxVAEs effectively captured the structural features of the drugs approved for the treatment of atopic dermatitis .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Furthermore , the molecules generated by GxVAEs for treating gastric cancer and Alzheimer's disease showed structural features that were similar to those of known approved drugs .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Thus , the proposed GxVAEs can generate molecules that have higher therapeutic properties than DRAGONET .", "ner": [["GxVAEs", "Method"], ["DRAGONET", "Method"]], "rel": [["GxVAEs", "Compare-With", "DRAGONET"]], "rel_plus": [["GxVAEs:Method", "Compare-With", "DRAGONET:Method"]]}
{"doc_id": "AAAI2024", "sentence": "We proposed GxVAEs , which consisted of two joint VAEs ( i.e . , ProfileVAE and MolVAE ) , to generate hit-like molecules from gene expression profiles .", "ner": [["GxVAEs", "Method"], ["VAEs", "Method"], ["ProfileVAE", "Method"], ["MolVAE", "Method"]], "rel": [["VAEs", "Part-Of", "GxVAEs"], ["ProfileVAE", "Part-Of", "GxVAEs"], ["MolVAE", "Part-Of", "GxVAEs"], ["ProfileVAE", "SubClass-Of", "VAEs"], ["MolVAE", "SubClass-Of", "VAEs"]], "rel_plus": [["VAEs:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "Part-Of", "GxVAEs:Method"], ["MolVAE:Method", "Part-Of", "GxVAEs:Method"], ["ProfileVAE:Method", "SubClass-Of", "VAEs:Method"], ["MolVAE:Method", "SubClass-Of", "VAEs:Method"]]}
{"doc_id": "AAAI2024", "sentence": "ProfileVAE extracted the features of the gene expression profiles , which were then used as conditions to guide MolVAE in producing hit-like molecules .", "ner": [["ProfileVAE", "Method"], ["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "The experimental results showed that GxVAEs outperformed the current SOTA baselines and efficiently generated hit-like molecules from gene expression profiles .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "Furthermore , we showed the capability of GxVAEs to create molecular structures with the potential therapeutic effects for various diseases from patients ' disease profiles .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "One limitation of GxVAEs is that the diversity may be influenced by the size of the latent space .", "ner": [["GxVAEs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "AAAI2024", "sentence": "If an implementation of MolVAE adopts a fixed latent vector , it could potentially constrain the diversity of newly generated molecules .", "ner": [["MolVAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "XLUMINA : An Auto-differentiating Discovery Framework for Super-Resolution Microscopy In this work we introduce XLUMINA , an original computational framework designed for the discovery of novel optical hardware in super-resolution microscopy .", "ner": [["XLUMINA", "Method"], ["Auto-differentiating Discovery Framework", "Method"], ["Super-Resolution Microscopy", "Task"], ["XLUMINA", "Method"], ["super-resolution microscopy", "Task"]], "rel": [["XLUMINA", "SubClass-Of", "Auto-differentiating Discovery Framework"], ["Auto-differentiating Discovery Framework", "Used-For", "Super-Resolution Microscopy"], ["XLUMINA", "Used-For", "Super-Resolution Microscopy"], ["XLUMINA", "Used-For", "super-resolution microscopy"]], "rel_plus": [["XLUMINA:Method", "SubClass-Of", "Auto-differentiating Discovery Framework:Method"], ["Auto-differentiating Discovery Framework:Method", "Used-For", "Super-Resolution Microscopy:Task"], ["XLUMINA:Method", "Used-For", "Super-Resolution Microscopy:Task"], ["XLUMINA:Method", "Used-For", "super-resolution microscopy:Task"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Our framework offers auto-differentiation capabilities , allowing for the fast and efficient simulation and automated design of entirely new optical setups from scratch .", "ner": [["auto-differentiation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "We showcase its potential by rediscovering three foundational experiments , each one covering different areas in optics : an optical telescope , STED microscopy and the focusing beyond the diffraction limit of a radially polarized light beam .", "ner": [["STED", "Method"], ["polarized light beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "With XLUMINA , can we go beyond simple optimization and calibration of known experimental setups , opening the door to potentially uncovering new microscopy concepts within the vast landscape of experimental possibilities .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "FINAL SET-UP AI-based exploration tool Objective function e.g . spot size , \u03d5 = \u03c0 FWHMxFWHMy 4 Enormously large search space OPTICS SIMULATOR FINAL SET-UP The space of all possible experimental optical configurations is enormous .", "ner": [["AI-based exploration tool", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This is where AI-based exploration techniques could provide enormous benefit , by exploring the space in a fast , unbiased way [ 1 , 2 ] .", "ner": [["AI-based exploration techniques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Among them , the discovery of super-resolution ( SR ) methods , which circumvent the classical diffraction limit of light , stand out in particular .", "ner": [["super-resolution", "Method"], ["SR", "Method"]], "rel": [["SR", "Synonym-Of", "super-resolution"]], "rel_plus": [["SR:Method", "Synonym-Of", "super-resolution:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Examples for versatile and powerful SR techniques are STED [ 10 ] , PALM / F-PALM [ 11 , 12 ] , ( d ) STORM [ 13 , 14 ] , SIM [ 15 ] , and MINFLUX [ 16 ] , with considerable impact in biology [ 17 ] [ 18 ] [ 19 ] , chemistry [ 20 ] and material sciences [ 21 ] for example .", "ner": [["SR", "Method"], ["STED", "Method"], ["PALM / F-PALM", "Method"], ["STORM", "Method"], ["SIM", "Method"], ["MINFLUX", "Method"]], "rel": [["STED", "SubClass-Of", "SR"], ["PALM / F-PALM", "SubClass-Of", "SR"], ["STORM", "SubClass-Of", "SR"], ["SIM", "SubClass-Of", "SR"], ["MINFLUX", "SubClass-Of", "SR"]], "rel_plus": [["STED:Method", "SubClass-Of", "SR:Method"], ["PALM / F-PALM:Method", "SubClass-Of", "SR:Method"], ["STORM:Method", "SubClass-Of", "SR:Method"], ["SIM:Method", "SubClass-Of", "SR:Method"], ["MINFLUX:Method", "SubClass-Of", "SR:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Rather , this work sets out to discover novel , experimentally viable concepts for advanced optical microscopy that are at-present entirely untapped .", "ner": [["optical microscopy", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The simulator can either be called directly by gradient-based optimization techniques , or it can be used for generating the training data for deep-learning-based surrogate models .", "ner": [["gradient-based optimization techniques", "Method"], ["deep-learning-based surrogate models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "We introduce XLUMINA , an efficient framework with auto-differentiation capabilities [ 23 ] for the ultimate goal of discovering new optical design principles .", "ner": [["XLUMINA", "Method"], ["auto-differentiation", "Method"]], "rel": [["auto-differentiation", "Part-Of", "XLUMINA"]], "rel_plus": [["auto-differentiation:Method", "Part-Of", "XLUMINA:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "We demonstrate our approach on three foundational optical layouts : a telescope version , the polarization-based beam shaping as used in STED ( stimulated emission depletion ) microscopy [ 10 ] and the sharp focus of a radially polarized light beam [ 24 ] .", "ner": [["polarization-based beam shaping", "Method"], ["STED", "Method"], ["stimulated emission depletion", "Method"], ["radially polarized light beam", "Method"]], "rel": [["polarization-based beam shaping", "Part-Of", "STED"], ["stimulated emission depletion", "Synonym-Of", "STED"]], "rel_plus": [["polarization-based beam shaping:Method", "Part-Of", "STED:Method"], ["stimulated emission depletion:Method", "Synonym-Of", "STED:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Rather , the future application of XLUMINA is the AI-driven discovery of completely novel physical concepts for advanced optical microscopy .", "ner": [["XLUMINA", "Method"], ["optical microscopy", "Task"]], "rel": [["XLUMINA", "Used-For", "optical microscopy"]], "rel_plus": [["XLUMINA:Method", "Used-For", "optical microscopy:Task"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Optimization in microscopy Our approach is radically different from previous strategies that employ AI for data-driven design of single optical elements [ 25 , 26 ] or data analysis in microscopy , e.g . denoising , contrast enhancement or point-spread-function ( PSF ) engineering [ 27 ] .", "ner": [["denoising", "Method"], ["contrast enhancement", "Method"], ["point-spread-function", "Method"], ["PSF", "Method"]], "rel": [["PSF", "Synonym-Of", "point-spread-function"]], "rel_plus": [["PSF:Method", "Synonym-Of", "point-spread-function:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In contrast , XLUMINA is equipped with tools for simulate , optimize and automatically design new optical setups and concepts from scratch .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The main approach is the development of efficient PDE-solvers for Maxwell's equations , including efficient ways to compute the gradients of the vast amount of parameters , usually by a physics-inspired technique called the adjoint method [ 38 , 39 ] .", "ner": [["PDE-solvers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Interestingly , the adjoint method can be seen as a special case of auto-differentiation ( which we use ) [ 39 ] .", "ner": [["auto-differentiation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Several open-source software tools , like Diffractio for light diffraction and interference simulations [ 41 ] , Finesse for simulating gravitational wave detectors [ 42 ] , and POPPY , developed as a part of the simulation package of the James Webb Telescope [ 43 ] , facilitate classical optics phenomena simulations .", "ner": [["Diffractio", "Method"], ["light diffraction", "Task"], ["interference simulations", "Task"], ["Finesse", "Method"], ["simulating gravitational wave detectors", "Task"], ["POPPY", "Method"], ["optics phenomena simulations", "Task"]], "rel": [["Diffractio", "Used-For", "light diffraction"], ["Diffractio", "Used-For", "interference simulations"], ["Finesse", "Used-For", "simulating gravitational wave detectors"], ["POPPY", "Used-For", "optics phenomena simulations"]], "rel_plus": [["Diffractio:Method", "Used-For", "light diffraction:Task"], ["Diffractio:Method", "Used-For", "interference simulations:Task"], ["Finesse:Method", "Used-For", "simulating gravitational wave detectors:Task"], ["POPPY:Method", "Used-For", "optics phenomena simulations:Task"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "There are also specialized resources like those focusing on the design of Laguerre-Gaussian mode sorters utilizing multi-plane light conversion ( MPLC ) methods [ 44 ] .", "ner": [["Laguerre-Gaussian", "Method"], ["multi-plane light conversion", "Method"], ["MPLC", "Method"]], "rel": [["multi-plane light conversion", "Part-Of", "Laguerre-Gaussian"], ["MPLC", "Synonym-Of", "multi-plane light conversion"]], "rel_plus": [["multi-plane light conversion:Method", "Part-Of", "Laguerre-Gaussian:Method"], ["MPLC:Method", "Synonym-Of", "multi-plane light conversion:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "While these software offer optics simulation capabilities , XLUMINA uniquely integrates simulation with AI-driven automated design powered with JAX's autodiff and just-in-time ( jit ) compilation capabilities . 2 Software workflow and performance XLUMINA allows for the simulation of classical optics hardware configurations and enables the optimization and automated discovery of new setup designs .", "ner": [["XLUMINA", "Method"], ["AI-driven automated design", "Method"], ["XLUMINA", "Method"]], "rel": [["AI-driven automated design", "Part-Of", "XLUMINA"]], "rel_plus": [["AI-driven automated design:Method", "Part-Of", "XLUMINA:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The software is developed using JAX [ 45 ] , which provides an advantage of heightened computational speed while seamlessly integrating the auto-differentiation framework [ 23 ] .", "ner": [["auto-differentiation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The first benchmark is to rediscover highly impactful microscopy strategies , such as STED microscopy [ 10 ] or the sharp focus of a radially polarized light beam [ 24 ] , as each of these incorporate different ideas or physical properties of light .", "ner": [["microscopy strategies", "Method"], ["STED", "Method"], ["polarized light beam", "Method"]], "rel": [["STED", "SubClass-Of", "microscopy strategies"], ["polarized light beam", "SubClass-Of", "microscopy strategies"]], "rel_plus": [["STED:Method", "SubClass-Of", "microscopy strategies:Method"], ["polarized light beam:Method", "SubClass-Of", "microscopy strategies:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "To that end , the algorithm is equipped with an optics simulator , which contains a diverse set of optical manipulation , interaction , and measurement technologies .", "ner": [["optics simulator", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The simulator enables , among many other features , to define light sources ( of any wavelength and power ) , phase masks ( i.e . , spatial light modulators , SLMs ) , polarizers , variable retarders ( e.g . , liquid crystal displays , LCDs ) , diffraction gratings , and high NA lenses to replicate strong focusing conditions .", "ner": [["phase masks", "Method"], ["spatial light modulators", "Method"], ["SLMs", "Method"], ["polarizers", "Method"], ["variable retarders", "Method"], ["liquid crystal displays", "Method"], ["LCDs", "Method"], ["diffraction gratings", "Method"], ["NA lenses", "Method"]], "rel": [["spatial light modulators", "SubClass-Of", "phase masks"], ["SLMs", "Synonym-Of", "spatial light modulators"], ["liquid crystal displays", "SubClass-Of", "variable retarders"], ["LCDs", "Synonym-Of", "liquid crystal displays"]], "rel_plus": [["spatial light modulators:Method", "SubClass-Of", "phase masks:Method"], ["SLMs:Method", "Synonym-Of", "spatial light modulators:Method"], ["liquid crystal displays:Method", "SubClass-Of", "variable retarders:Method"], ["LCDs:Method", "Synonym-Of", "liquid crystal displays:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Light propagation and diffraction is simulated by two methods , each available for both scalar and vectorial regimes : the fast-Fourier-transform ( FFT ) based numerical integration of the Rayleigh-Sommerfeld ( RS ) diffraction integral [ 46 , 47 ] and the Chirped z-transform ( CZT ) [ 48 ] .", "ner": [["fast-Fourier-transform", "Method"], ["FFT", "Method"], ["Rayleigh-Sommerfeld", "Method"], ["RS", "Method"], ["Chirped z-transform", "Method"], ["CZT", "Method"]], "rel": [["FFT", "Synonym-Of", "fast-Fourier-transform"], ["RS", "Synonym-Of", "Rayleigh-Sommerfeld"], ["CZT", "Synonym-Of", "Chirped z-transform"]], "rel_plus": [["FFT:Method", "Synonym-Of", "fast-Fourier-transform:Method"], ["RS:Method", "Synonym-Of", "Rayleigh-Sommerfeld:Method"], ["CZT:Method", "Synonym-Of", "Chirped z-transform:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The CZT is an accelerated version of the RS algorithm , which allows for arbitrary selection and sampling of the region of interest .", "ner": [["CZT", "Method"], ["RS", "Method"]], "rel": [["CZT", "SubClass-Of", "RS"]], "rel_plus": [["CZT:Method", "SubClass-Of", "RS:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Some functionalities of XLUMINA's optics simulator ( e.g . , optical propagation algorithms , planar lens or amplitude masks ) are inspired in an open-source NumPy-based Python module for diffraction and interferometry simulation , Diffractio [ 41 ] , although we have rewritten and modified these approaches to combine them with JAX's just-in-time ( jit ) functionality .", "ner": [["XLUMINA's optics simulator", "Method"], ["optical propagation algorithms", "Method"], ["planar lens", "Method"], ["amplitude masks", "Method"], ["diffraction", "Task"], ["interferometry simulation", "Task"], ["Diffractio", "Method"]], "rel": [["optical propagation algorithms", "SubClass-Of", "XLUMINA's optics simulator"], ["planar lens", "SubClass-Of", "XLUMINA's optics simulator"], ["amplitude masks", "SubClass-Of", "XLUMINA's optics simulator"], ["XLUMINA's optics simulator", "Used-For", "diffraction"], ["amplitude masks", "Used-For", "diffraction"], ["planar lens", "Used-For", "diffraction"], ["optical propagation algorithms", "Used-For", "diffraction"], ["XLUMINA's optics simulator", "Used-For", "interferometry simulation"]], "rel_plus": [["optical propagation algorithms:Method", "SubClass-Of", "XLUMINA's optics simulator:Method"], ["planar lens:Method", "SubClass-Of", "XLUMINA's optics simulator:Method"], ["amplitude masks:Method", "SubClass-Of", "XLUMINA's optics simulator:Method"], ["XLUMINA's optics simulator:Method", "Used-For", "diffraction:Task"], ["amplitude masks:Method", "Used-For", "diffraction:Task"], ["planar lens:Method", "Used-For", "diffraction:Task"], ["optical propagation algorithms:Method", "Used-For", "diffraction:Task"], ["XLUMINA's optics simulator:Method", "Used-For", "interferometry simulation:Task"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "On top of that , we developed completely new functions ( e.g . , LCDs or propagation through high NA objective lens with CZT methods , to name a few ) which significantly expand the software capabilities .", "ner": [["LCDs", "Method"], ["NA objective lens", "Method"], ["CZT", "Method"]], "rel": [["CZT", "Part-Of", "NA objective lens"]], "rel_plus": [["CZT:Method", "Part-Of", "NA objective lens:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The most important hardware addition on the optical simulator are the SLMs , each pixel of which possesses an independent ( and variable ) phase value .", "ner": [["optical simulator", "Method"], ["SLMs", "Method"]], "rel": [["SLMs", "Part-Of", "optical simulator"]], "rel_plus": [["SLMs:Method", "Part-Of", "optical simulator:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "They serve as a universal approximation for phase masks ( including lenses ) and offer a computational advantage : given a specific pixel resolution , they allow for unrestricted phase design selection .", "ner": [["phase masks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In addition , we defined under the name of super-SLM ( sSLM ) a hardware-box-type which consists of two SLMs , each one independently imprinting a phase mask on the horizontal and vertical orthogonal polarization components of the field .", "ner": [["super-SLM", "Method"], ["sSLM", "Method"], ["SLMs", "Method"], ["phase mask", "Method"]], "rel": [["sSLM", "Synonym-Of", "super-SLM"], ["SLMs", "Part-Of", "super-SLM"]], "rel_plus": [["sSLM:Method", "Synonym-Of", "super-SLM:Method"], ["SLMs:Method", "Part-Of", "super-SLM:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "To include the automated discovery feature , XLUMINA's optical simulator and optimizer are tied together by the loss function .", "ner": [["XLUMINA's optical simulator", "Method"], ["optimizer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Thus , it is essential to reduce the computation time by maximizing the speed of optical simulation functions .", "ner": [["optical simulation functions", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Thus , we evaluate the performance of our optimized functions against their counterparts in Diffractio by propagating a Gaussian beam within a computational window sized at 2048 \u00d7 2048 .", "ner": [["Diffractio", "Method"], ["Gaussian beam", "Method"]], "rel": [["Gaussian beam", "Part-Of", "Diffractio"]], "rel_plus": [["Gaussian beam:Method", "Part-Of", "Diffractio:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The average run-time for both Diffractio and our approach is shown in Figure 2a .", "ner": [["Diffractio", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Generally , our methods significantly enhance computational speeds for simulating light diffraction and propagation .", "ner": [["light diffraction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "For instance , we observe a speedup of roughly a factor of 2 for RS and VCZT and about 2.5 for VRS using the CPU .", "ner": [["RS", "Method"], ["VCZT", "Method"], ["VRS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "CZT has less significant speedup , but there is still a 0.5 - second improvement .", "ner": [["CZT", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "When it comes to the nature of the optimizer , it can be either direct ( gradient-based ) or deep learningbased ( surrogate models or deep generative models , e.g . , variational autoencoders [ 49 ] ) .", "ner": [["gradient-based", "Method"], ["deep learningbased", "Method"], ["surrogate models", "Method"], ["deep generative models", "Method"], ["variational autoencoders", "Method"]], "rel": [["surrogate models", "SubClass-Of", "deep learningbased"], ["deep generative models", "SubClass-Of", "deep learningbased"], ["variational autoencoders", "SubClass-Of", "deep generative models"]], "rel_plus": [["surrogate models:Method", "SubClass-Of", "deep learningbased:Method"], ["deep generative models:Method", "SubClass-Of", "deep learningbased:Method"], ["variational autoencoders:Method", "SubClass-Of", "deep generative models:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In this work , we adopt a gradient-based strategy , where the experimental setup's parameters are adjusted iteratively in the steepest descent direction .", "ner": [["gradient-based strategy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "To chose the optimizer , we evaluate the convergence time of two gradient-descent techniques : the Broyden-Fletcher-Goldfarb-Shanno ( BFGS ) algorithm , which numerically computes gradients and higher-order derivative approximations , and the adaptive moment estimation ( ADAM ) , an instance of the stochastic-gradient-descent ( SGD ) method .", "ner": [["gradient-descent techniques", "Method"], ["Broyden-Fletcher-Goldfarb-Shanno", "Method"], ["BFGS", "Method"], ["adaptive moment estimation", "Method"], ["ADAM", "Method"], ["stochastic-gradient-descent", "Method"], ["SGD", "Method"]], "rel": [["Broyden-Fletcher-Goldfarb-Shanno", "SubClass-Of", "gradient-descent techniques"], ["adaptive moment estimation", "SubClass-Of", "gradient-descent techniques"], ["BFGS", "Synonym-Of", "Broyden-Fletcher-Goldfarb-Shanno"], ["ADAM", "Synonym-Of", "adaptive moment estimation"], ["adaptive moment estimation", "SubClass-Of", "stochastic-gradient-descent"], ["SGD", "Synonym-Of", "stochastic-gradient-descent"]], "rel_plus": [["Broyden-Fletcher-Goldfarb-Shanno:Method", "SubClass-Of", "gradient-descent techniques:Method"], ["adaptive moment estimation:Method", "SubClass-Of", "gradient-descent techniques:Method"], ["BFGS:Method", "Synonym-Of", "Broyden-Fletcher-Goldfarb-Shanno:Method"], ["ADAM:Method", "Synonym-Of", "adaptive moment estimation:Method"], ["adaptive moment estimation:Method", "SubClass-Of", "stochastic-gradient-descent:Method"], ["SGD:Method", "Synonym-Of", "stochastic-gradient-descent:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "While BFGS is part of the open-source SciPy Python library and operates on the CPU , ADAM is integrated within the JAX library and runs in both CPU and GPU .", "ner": [["BFGS", "Method"], ["ADAM", "Method"]], "rel": [["BFGS", "Compare-With", "ADAM"]], "rel_plus": [["BFGS:Method", "Compare-With", "ADAM:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "For the evaluation , we simulate a Gaussian beam interacting with a phase mask .", "ner": [["Gaussian beam", "Method"], ["phase mask", "Method"]], "rel": [["phase mask", "Part-Of", "Gaussian beam"]], "rel_plus": [["phase mask:Method", "Part-Of", "Gaussian beam:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The objective function is the mean squared error between the detected light and the ground truth , characterized by a Gaussian beam with a spiral phase imprinted on its wavefront .", "ner": [["mean squared error", "Method"], ["Gaussian beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Initializing with an arbitrary phase mask configuration , we run both BFGS and ADAM optimizers over different computational windows and devices , as depicted in Fig . 2b .", "ner": [["phase mask", "Method"], ["BFGS", "Method"], ["ADAM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "On the CPU , BFGS exhibits exponential scaling in convergence time , reaching about 6500 seconds for 250 \u00d7 250 pixel window .", "ner": [["BFGS", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In contrast , ADAM demonstrates superior efficiency , reducing it to roughly 2600 seconds .", "ner": [["ADAM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This makes the GPU-accelerated ADAM approach more appropriate for efficient experimentation .", "ner": [["ADAM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Overall , the computational performance of XLUMINA highlights its suitability for running complex simulations and optimizations with a high level of efficiency .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In this section , we showcase the virtual optical designs generated by XLUMINA .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This arrangement performs optical Fourier transformations of input light with magnifications determined by the ratio f 2 / f 1 .", "ner": [["optical Fourier transformations", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "To revisit this design with a magnification of 2x , we encoded the virtual setup depicted in Fig . 3a , in which traditional lenses are replaced by spatial light modulators ( SLMs ) .", "ner": [["spatial light modulators", "Method"], ["SLMs", "Method"]], "rel": [["SLMs", "SubClass-Of", "spatial light modulators"]], "rel_plus": [["SLMs:Method", "SubClass-Of", "spatial light modulators:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The parameter space includes the distances , z 1 , z 2 and z 3 ( measured in millimeters ) and the phase masks ( measured in radians ) of the two SLMs with a resolution of 1024 \u00d7 1024 pixels .", "ner": [["phase masks", "Method"], ["SLMs", "Method"]], "rel": [["phase masks", "Part-Of", "SLMs"]], "rel_plus": [["phase masks:Method", "Part-Of", "SLMs:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Each sample consists of a Gaussian beam shaped by amplitude masks in various forms ( circles , rectangles , squares and rings ) , with varying sizes and orientations .", "ner": [["Gaussian beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The cost function is the mean squared error between the dataset's output and the detected intensity pattern from the virtual setup .", "ner": [["mean squared error", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The solution depicts lens-like quadratic phases in both SLMs .", "ner": [["lens-like quadratic", "Method"], ["SLMs", "Method"]], "rel": [["lens-like quadratic", "Part-Of", "SLMs"]], "rel_plus": [["lens-like quadratic:Method", "Part-Of", "SLMs:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This suggests that phase mask of SLM #1 might be compensating for this deviation .", "ner": [["phase mask", "Method"], ["SLM", "Method"]], "rel": [["phase mask", "Part-Of", "SLM"]], "rel_plus": [["phase mask:Method", "Part-Of", "SLM:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "On the other hand , we believe that the more precise solution for SLM #2 highlights its critical role in imaging .", "ner": [["SLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "STED microscopy STED microscopy [ 10 ] is based on excitation and spatially targeted depletion of fluorophores .", "ner": [["STED", "Method"], ["STED", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In order to achieve this , a Gaussian-shaped excitation beam and a doughnut-shaped depletion beam are concentrically overlapped .", "ner": [["Gaussian-shaped excitation beam", "Method"], ["doughnut-shaped depletion beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This effectively reduces the area of normal fluorescence , which leads to super-resolution imaging .", "ner": [["super-resolution imaging", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In order to generate a doughnut-shaped beam a spiral phase is imprinted into the wavefront of a Gaussian beam .", "ner": [["doughnut-shaped beam", "Method"], ["Gaussian beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "To revisit this principle , we virtually construct a simplified version of a STED-type setup as depicted in Fig . 4a .", "ner": [["STED-type", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "It consists of two light sources generating Gaussian beams corresponding to the depletion and excitation beams with wavelengths of 650 nm and 532 nm , respectively .", "ner": [["Gaussian beams", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Within the depletion beam's optical path , we place an SLM of 2048 \u00d7 2048 resolution and a computational pixel size of 1.95\u00b5m .", "ner": [["SLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In this instance , the parameter space is defined by the SLM .", "ner": [["SLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "For this particular instance , the detected intensity corresponds to the radial component , In Fig . 4b , we present the STED spiral phase mask [ 10 ] and the identified solution for \u03b5 = 0.7 .", "ner": [["STED", "Method"], ["phase mask", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "From a random initial phase mask in the SLM , the system converged into a pattern alike to the spiral phase .", "ner": [["random initial phase mask", "Method"], ["SLM", "Method"]], "rel": [["random initial phase mask", "Part-Of", "SLM"]], "rel_plus": [["random initial phase mask:Method", "Part-Of", "SLM:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Other solutions presented noisy phase patterns which failed to achieve the essential doughnut-shaped depletion beam .", "ner": [["doughnut-shaped depletion beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Real-world STED setups demand almost perfect phase patterns ; even the minor misalignment can compromise the super-resolution STED phenomena .", "ner": [["Real-world STED", "Method"], ["super-resolution STED", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "| E x | 2 + | E y | 2 , Sharper focus for a radially polarized light beam The final benchmark focuses on the generation of an ultra-sharp focus for a radially polarized beam , a feature that breaks the diffraction limit in the longitudinal direction as demonstrated by R .", "ner": [["polarized light beam", "Method"], ["polarized beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The light source emits a 635 nm wavelength Gaussian beam that is linearly polarized .", "ner": [["Gaussian beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The original optical elements are replaced by an sSLM , each component of which has a resolution of 2048 \u00d7 2048 pixels and a computational pixel size of 1.46\u00b5m .", "ner": [["sSLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Additionally , we place an LCD with variable phase retardance \u03b7 and orientation angle \u03b8 .", "ner": [["LCD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The beam then passes through a high NA objective lens before reaching the detector screen .", "ner": [["NA objective lens", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Relevant data on the sSLM's phase masks , optical parameters , and the simulated spot size are showcased in Fig . 5b and Table 1 .", "ner": [["sSLM's", "Method"], ["phase masks", "Method"]], "rel": [["phase masks", "Part-Of", "sSLM's"]], "rel_plus": [["phase masks:Method", "Part-Of", "sSLM's:Method"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "With regards to Solution #2 , the SLM phase pattern also shows a tilted forked grating of topological charge p = 1 .", "ner": [["SLM", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "For comparison , we also feature the radial intensity profile of the diffraction-limited linearly polarized beam ( dotted orange line in Fig . 5e ) .", "ner": [["diffraction-limited linearly polarized beam", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This was crucial for our purpose to demonstrate how XLUMINA can compute and efficiently rediscover known techniques in advanced microscopy .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "We aim to use XLUMINA to discover new microscopy concepts .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "From here , XLUMINA should be able to extract much more complex solutions which humans might not have thought about yet [ 2 ] .", "ner": [["XLUMINA", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "In this work , we present an efficient and reliable simulator for advanced optical microscopy .", "ner": [["simulator", "Method"], ["optical microscopy", "Task"]], "rel": [["simulator", "Used-For", "optical microscopy"]], "rel_plus": [["simulator:Method", "Used-For", "optical microscopy:Task"]]}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "The simulator is developed in a modular way , and we plan to significantly expand it by adding more physical properties and features exploited in microscopy , for example , detailed coverage of frequency and time information , which might enable systems such as iSCAT [ 56 ] , structured illumination microscopy [ 57 ] , and localization microscopy [ 58 ] .", "ner": [["iSCAT", "Method"], ["structured illumination microscopy", "Method"], ["localization microscopy", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "Looking further into the future , one can expect that matter-wave beams ( governed by Schr\u00f6dinger's equation , which is closely related to the paraxial wave equation , a special case of the electromagnetic field ) can be simulated in the same framework .", "ner": [["matter-wave beams", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "NeurIPS2023-AI4Science", "sentence": "This might allow for the AI-based design of hybrid microscopy techniques using light and complex electron-beams [ 61 ] or coherent beams of high-mass particles [ 62 ] .", "ner": [["hybrid microscopy techniques", "Method"], ["complex electron-beams", "Method"], ["coherent beams of high-mass particles", "Method"]], "rel": [["complex electron-beams", "Part-Of", "hybrid microscopy techniques"], ["coherent beams of high-mass particles", "Part-Of", "hybrid microscopy techniques"]], "rel_plus": [["complex electron-beams:Method", "Part-Of", "hybrid microscopy techniques:Method"], ["coherent beams of high-mass particles:Method", "Part-Of", "hybrid microscopy techniques:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Label Words are Anchors : An Information Flow Perspective for Understanding In-Context Learning In-context learning ( ICL ) emerges as a promising capability of large language models ( LLMs ) by providing them with demonstration examples to perform diverse tasks .", "ner": [["In-Context Learning", "Method"], ["In-context learning", "Method"], ["ICL", "Method"], ["large language models", "Method"], ["LLMs", "Method"]], "rel": [["ICL", "Synonym-Of", "In-context learning"], ["LLMs", "Synonym-Of", "large language models"], ["In-context learning", "Used-For", "large language models"]], "rel_plus": [["ICL:Method", "Synonym-Of", "In-context learning:Method"], ["LLMs:Method", "Synonym-Of", "large language models:Method"], ["In-context learning:Method", "Used-For", "large language models:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "However , the underlying mechanism of how LLMs learn from the provided context remains under-explored .", "ner": [["LLMs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In this paper , we investigate the working mechanism of ICL through an information flow lens .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Our findings reveal that label words in the demonstration examples function as anchors : In-context Learning ( ICL ) has emerged as a powerful capability alongside the development of scaledup large language models ( LLMs ) ( Brown et al . , 2020 ) .", "ner": [["In-context Learning", "Method"], ["ICL", "Method"], ["large language models", "Method"], ["LLMs", "Method"]], "rel": [["ICL", "Synonym-Of", "In-context Learning"], ["LLMs", "Synonym-Of", "large language models"], ["In-context Learning", "Used-For", "large language models"]], "rel_plus": [["ICL:Method", "Synonym-Of", "In-context Learning:Method"], ["LLMs:Method", "Synonym-Of", "large language models:Method"], ["In-context Learning:Method", "Used-For", "large language models:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "By instructing LLMs using few-shot demonstration examples , ICL enables them to perform a wide range of tasks , such as text classification ( Min et al . , 2022a ) and mathematical reasoning ( Wei et al . , 2022 ) .", "ner": [["LLMs", "Method"], ["ICL", "Method"], ["text classification", "Task"], ["mathematical reasoning", "Task"]], "rel": [["ICL", "Used-For", "text classification"], ["ICL", "Used-For", "mathematical reasoning"]], "rel_plus": [["ICL:Method", "Used-For", "text classification:Task"], ["ICL:Method", "Used-For", "mathematical reasoning:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "Despite its significance , the inner working mechanism of ICL remains an open question , garnering considerable interest from research communities ( Xie et al . , 2022 ; Dai et al . , 2022 ; Aky\u00fcrek et al . , 2022 ; Li et al . , 2023b ) .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In this paper , we find that the label words serve as anchors that aggregate and distribute information in ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We first visualize the attention interactive pattern between tokens with a GPT model ( Brown et al . , 2020 ) on sentiment analysis ( Figure 1 ) .", "ner": [["attention interactive", "Method"], ["GPT", "Method"], ["sentiment analysis", "Task"]], "rel": [["GPT", "Used-For", "sentiment analysis"]], "rel_plus": [["GPT:Method", "Used-For", "sentiment analysis:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "H 2 : In deep layers , the model extracts the information from label words to form the final prediction .", "ner": [["deep layers", "Method"], ["prediction", "Task"]], "rel": [["deep layers", "Used-For", "prediction"]], "rel_plus": [["deep layers:Method", "Used-For", "prediction:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "Two experiments are designed to validate the hypothesis using GPT2 - XL ( Radford et al . , 2019 ) and GPT-J ( Wang and Komatsuzaki , 2021 ) across several text classification benchmarks . ( 1 ) By blocking the information aggregation path to label words in certain layers , we find that such isolation in shallow layers significantly impairs model performance .", "ner": [["GPT2 - XL", "Method"], ["GPT-J", "Method"], ["text classification", "Task"], ["shallow layers", "Method"]], "rel": [["GPT2 - XL", "Used-For", "text classification"], ["GPT-J", "Used-For", "text classification"]], "rel_plus": [["GPT2 - XL:Method", "Used-For", "text classification:Task"], ["GPT-J:Method", "Used-For", "text classification:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "This indicates that label words collect useful information during forward propagation in shallow layers . ( 2 ) We investigate the relationship between the attention distributions on the label words of the target position and the model's final prediction .", "ner": [["forward propagation", "Method"], ["shallow layers", "Method"]], "rel": [["forward propagation", "Part-Of", "shallow layers"]], "rel_plus": [["forward propagation:Method", "Part-Of", "shallow layers:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "In summary , these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets .", "ner": [["large language models", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Drawing on insights from the information flow perspective , we explore three approaches to enhance ICL's effectiveness , efficiency , and interpretability . ( 1 ) An anchor re-weighting method is introduced , which employs a learnable vector to adjust the significance of different label words in demonstrations , leading to a 16.7% average accuracy boost compared to standard ICL baselines . ( 2 ) For quicker ICL inference , inputs are compressed into pre-calculated anchor representations since model predictions primarily rely on label word activations .", "ner": [["ICL's", "Method"], ["anchor re-weighting method", "Method"], ["ICL", "Method"], ["ICL", "Method"]], "rel": [["anchor re-weighting method", "Compare-With", "ICL"]], "rel_plus": [["anchor re-weighting method:Method", "Compare-With", "ICL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Testing shows a 1.8 \u00d7 speedup in inference with only a minimal performance trade-off . ( 3 ) An error analysis of ICL on GPT2 - XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors , implying that errors might result from similar anchor representations .", "ner": [["ICL", "Method"], ["GPT2 - XL", "Method"]], "rel": [["ICL", "Used-For", "GPT2 - XL"]], "rel_plus": [["ICL:Method", "Used-For", "GPT2 - XL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "These promising applications further validate our hypothesis and shed light on future ICL studies for better transparency of LLMs .", "ner": [["ICL", "Method"], ["LLMs", "Method"]], "rel": [["ICL", "Used-For", "LLMs"]], "rel_plus": [["ICL:Method", "Used-For", "LLMs:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "H 2 : In deep layers , the model makes predictions by extracting information from label words .", "ner": [["deep layers", "Method"], ["predictions", "Task"]], "rel": [["deep layers", "Used-For", "predictions"]], "rel_plus": [["deep layers:Method", "Used-For", "predictions:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "This section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model .", "ner": [["attention interaction", "Method"], ["GPT", "Method"]], "rel": [["attention interaction", "Part-Of", "GPT"]], "rel_plus": [["attention interaction:Method", "Part-Of", "GPT:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Following common practice , we use the Taylor expansion ( Michel et al . , 2019 ) to calculate the saliency score for each element of the attention matrix : EQUATION Here , A h , l is the value of the attention matrix of the h-th attention head in the l-th layer , x is the input , and L ( x ) is the loss function of the task , e.g . , the cross-entropy objective for a classification problem .", "ner": [["Taylor expansion", "Method"], ["loss function", "Method"], ["cross-entropy objective", "Method"], ["classification", "Task"]], "rel": [["cross-entropy objective", "SubClass-Of", "loss function"], ["cross-entropy objective", "Used-For", "classification"]], "rel_plus": [["cross-entropy objective:Method", "SubClass-Of", "loss function:Method"], ["cross-entropy objective:Method", "Used-For", "classification:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "We average all attention heads to obtain the saliency matrix I l for the l-th layer .", "ner": [["attention heads", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "I l ( i , j ) represents the significance of the information flow from the j-th word to the i-th word for ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "By observing I l , we can get an intuitive impression that as the layer goes deeper , demonstration label words will become more dominant for the prediction , as depicted in Figure 1 .", "ner": [["prediction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "A high S pq demonstrates a strong information extraction from label words for final decision-making .", "ner": [["information extraction", "Task"], ["decision-making", "Task"]], "rel": [["information extraction", "Used-For", "decision-making"]], "rel_plus": [["information extraction:Task", "Used-For", "decision-making:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "Experimental Settings We choose GPT2 - XL from the GPT series ( Radford et al . , 2019 ) as our primary model for investigation , due to its moderate model size ( of 1.5B parameters ) that is suitable for our hardware resource and its decent ICL performance ( Dai et al . , 2022 ) .", "ner": [["GPT2 - XL", "Method"], ["GPT", "Method"], ["ICL", "Method"]], "rel": [["GPT2 - XL", "SubClass-Of", "GPT"]], "rel_plus": [["GPT2 - XL:Method", "SubClass-Of", "GPT:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "For datasets , we use Stanford Sentiment Treebank Binary ( SST - 2 ) ( Socher et al . , 2013 ) for sentiment analysis , Text REtrieval Conference Question Classification ( TREC ) ( Li and Roth , 2002 ; Hovy et al . , 2001 ) for question type classification , AG's news topic classification dataset ( AGNews ) ( Zhang et al . , 2015 ) for topic classification , and EmoContext ( EmoC ) ( Chatterjee et al . , 2019 ) Results and Analysis Figure 3 reveals that : ( 1 ) in shallow layers , S pq , the significance of the information flow from label words to targeted positions , is low , while S wp , the information flow from the text part to label words is high ; ( 2 ) in deep layers , S pq , the importance of information flow from label words to the targeted position becomes the dominant one .", "ner": [["Stanford Sentiment Treebank Binary", "Dataset"], ["SST - 2", "Dataset"], ["sentiment analysis", "Task"], ["Text REtrieval Conference Question Classification", "Dataset"], ["TREC", "Dataset"], ["question type classification", "Task"], ["AG's news topic classification dataset", "Dataset"], ["AGNews", "Dataset"], ["topic classification", "Task"], ["EmoContext", "Dataset"], ["EmoC", "Dataset"], ["shallow layers", "Method"], ["deep layers", "Method"]], "rel": [["SST - 2", "Synonym-Of", "Stanford Sentiment Treebank Binary"], ["Stanford Sentiment Treebank Binary", "Benchmark-For", "sentiment analysis"], ["TREC", "Synonym-Of", "Text REtrieval Conference Question Classification"], ["Text REtrieval Conference Question Classification", "Benchmark-For", "question type classification"], ["AGNews", "Synonym-Of", "AG's news topic classification dataset"], ["AG's news topic classification dataset", "Benchmark-For", "topic classification"], ["EmoC", "Synonym-Of", "EmoContext"]], "rel_plus": [["SST - 2:Dataset", "Synonym-Of", "Stanford Sentiment Treebank Binary:Dataset"], ["Stanford Sentiment Treebank Binary:Dataset", "Benchmark-For", "sentiment analysis:Task"], ["TREC:Dataset", "Synonym-Of", "Text REtrieval Conference Question Classification:Dataset"], ["Text REtrieval Conference Question Classification:Dataset", "Benchmark-For", "question type classification:Task"], ["AGNews:Dataset", "Synonym-Of", "AG's news topic classification dataset:Dataset"], ["AG's news topic classification dataset:Dataset", "Benchmark-For", "topic classification:Task"], ["EmoC:Dataset", "Synonym-Of", "EmoContext:Dataset"]]}
{"doc_id": "EMNLP2023", "sentence": "Proposed Hypothesis Based on this , we propose the hypothesis that label words function as anchors in the ICL information flow .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In shallow layers , label words gather information from demonstration examples to form semantic representations for deeper layers , while in deep layers , the model extracts the information from label words to form the final prediction .", "ner": [["shallow layers", "Method"], ["deeper layers", "Method"], ["deep layers", "Method"], ["prediction", "Task"]], "rel": [["shallow layers", "Compare-With", "deep layers"], ["deep layers", "Used-For", "prediction"]], "rel_plus": [["shallow layers:Method", "Compare-With", "deep layers:Method"], ["deep layers:Method", "Used-For", "prediction:Task"]]}
{"doc_id": "EMNLP2023", "sentence": "We assume that the information aggregation in ICL relies on the information flow from the text part to label tokens , which is facilitated by the transformer's attention mechanism .", "ner": [["ICL", "Method"], ["transformer's", "Method"], ["attention mechanism", "Method"]], "rel": [["attention mechanism", "Part-Of", "transformer's"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "transformer's:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "By manipulating the attention layer in the model to block this flow and examining the model behavior change , we validate the existence of the information aggregation process and its contribution to the final prediction .", "ner": [["attention layer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "To further validate our findings on larger models , we incorporate GPT-J ( 6B ) ( Wang and Komatsuzaki , 2021 ) in experiments , which exceeds GPT2 - XL in model size and capacity .", "ner": [["GPT-J", "Method"], ["GPT2 - XL", "Method"]], "rel": [["GPT-J", "Compare-With", "GPT2 - XL"]], "rel_plus": [["GPT-J:Method", "Compare-With", "GPT2 - XL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Metrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens : ( 1 ) Label Loyalty : measures the consistency of output labels with and without isolation . ( 2 ) Word Loyalty : employs the Jaccard similarity to compare the top - 5 predicted words with and without isolation , capturing more subtle model output alterations ( See Appendix C for details ) .", "ner": [["Jaccard similarity", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Low loyalty indicates a profound impact of isolation on model predictions .", "ner": [["predictions", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Moreover , similar results were obtained when testing ICL with semantically unrelated labels ( refer to Appendix F .2 ) .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Figures 5a and 5b delineate correlation metrics for GPT2 - XL and GPT-J , averaged across four datasets .", "ner": [["GPT2 - XL", "Method"], ["GPT-J", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The AUCROC l for deep layers approaches 0.8 , illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction .", "ner": [["deep layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Moreover , shallow layers show negligible cumulative contributions ( R l ) , with a significant increase in middle and deep layers .", "ner": [["shallow layers", "Method"], ["deep layers", "Method"]], "rel": [["shallow layers", "Compare-With", "deep layers"]], "rel_plus": [["shallow layers:Method", "Compare-With", "deep layers:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "These results signify the crucial role of deep layers for final prediction , validating that the model extracts information from label words in deep layers to form the final prediction .", "ner": [["deep layers", "Method"], ["deep layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In \u00a7 2.3 , we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the deep layers .", "ner": [["deep layers", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Given the considerable role these \" anchors \" fulfill , we find it intuitive to design ICL improvements based on them , as elaborated in \u00a7 3 .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "With insights from the validated hypothesis , we propose strategies to boost ICL's accuracy and inference speed .", "ner": [["ICL's", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We propose an anchor re-weighting method in \u00a7 3.1 to adjust the demonstrations ' contributions and improve accuracy .", "ner": [["anchor re-weighting method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In \u00a7 3.2 , we explore a context compression technique that reduces original demonstrations to anchor hidden states to speed up ICL inference .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Besides , in \u00a7 3.3 , we utilize anchor distances to perform an analysis to understand the errors ICL made in real-world scenarios .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "These approaches corroborate our hypothesis , pointing to potential paths for future ICL enhancements .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Based on our analysis in \u00a7 2 , we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors . 3.1.1 Method \u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution ( A ( q , p 1 ) , . . . , A ( q , p C ) ) on label words p 1 , . . . , p C of the target position q in deep layers .", "ner": [["ICL", "Method"], ["logistic regression", "Method"], ["ICL's", "Method"], ["reweighting label anchors", "Method"], ["attention distribution", "Method"], ["deep layers", "Method"]], "rel": [["ICL", "Compare-With", "logistic regression"], ["reweighting label anchors", "Part-Of", "ICL's"]], "rel_plus": [["ICL:Method", "Compare-With", "logistic regression:Method"], ["reweighting label anchors:Method", "Part-Of", "ICL's:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Inspired by the similarity between ICL and logistic regression , we've incorporated a learnable \u03b2 i 0 into Eq . ( 7 ) , which is equivalent to adjusting the attention weights A ( q , p i ) : EQUATION Each \u03b2 i 0 is a learnable parameter , set uniquely for different attention heads and layers .", "ner": [["ICL", "Method"], ["logistic regression", "Method"], ["attention heads", "Method"]], "rel": [["ICL", "Compare-With", "logistic regression"]], "rel_plus": [["ICL:Method", "Compare-With", "logistic regression:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Owing to computational constraints , we employ GPT2 - XL for evaluation , excluding GPT-J .", "ner": [["GPT2 - XL", "Method"], ["GPT-J", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We compare Anchoring Re-weighting with two baselines : ( 1 ) Vanilla ICL with the same demonstration ( 1 - shot per class ) ( 2 ) Vanilla ICL , where the auxiliary training set of \u03b2 is included as demonstrations ( 5 - shot per class ) for a fair comparison .", "ner": [["Anchoring Re-weighting", "Method"], ["ICL", "Method"], ["ICL", "Method"]], "rel": [["Anchoring Re-weighting", "Compare-With", "ICL"], ["Anchoring Re-weighting", "Compare-With", "ICL"]], "rel_plus": [["Anchoring Re-weighting:Method", "Compare-With", "ICL:Method"], ["Anchoring Re-weighting:Method", "Compare-With", "ICL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "As Table 1 shows , the proposed anchor reweighting significantly enhances ICL performance , particularly on the SST - 2 and EmoC datasets .", "ner": [["ICL", "Method"], ["SST - 2", "Dataset"], ["EmoC", "Dataset"]], "rel": [["ICL", "Evaluated-With", "SST - 2"], ["ICL", "Evaluated-With", "EmoC"]], "rel_plus": [["ICL:Method", "Evaluated-With", "SST - 2:Dataset"], ["ICL:Method", "Evaluated-With", "EmoC:Dataset"]]}
{"doc_id": "EMNLP2023", "sentence": "Besides , adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced , as discussed in Zhao et al . ( 2021 ) .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Different from vanilla ICL which utilizes the extra examples to form a demonstration , we train a re-weighting vector \u03b2 to modulate label anchor contributions .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples .", "ner": [["re-weighting mechanism", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Furthermore , it reiterates the crucial role that anchors play in ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "For AGNews , due to the length limit , we only use three demonstrations per class .", "ner": [["AGNews", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Our Anchor Re-weighting method achieves the best performance overall tasks . from the demonstrations .", "ner": [["Anchor Re-weighting", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Given the auto-regressive nature of GPT-like models , where hidden states of tokens depend solely on preceding ones , label words ' information aggregation process is independent of subsequent words .", "ner": [["auto-regressive", "Method"], ["GPT-like models", "Method"]], "rel": [["auto-regressive", "Part-Of", "GPT-like models"]], "rel_plus": [["auto-regressive:Method", "Part-Of", "GPT-like models:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "In our preliminary experiments , concatenating hidden states of label words alone was inadequate for completing the ICL task . 5 This might be due to the critical role of formatting information in helping the model to determine the output space at the target position , 6 as highlighted in Min et al . ( 2022b ) .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "As a solution , we amalgamate the hidden states of both the formatting and the label words , a method we've termed Hidden anchor .", "ner": [["Hidden anchor", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We compare our Hidden anchor input compression method with two equally efficient baselines .", "ner": [["Hidden anchor", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Text anchor : This method concatenates the formatting and label text with the input , as opposed to concatenating the hidden states at each layer .", "ner": [["Text anchor", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Hidden random : This approach concatenates the hidden states of formatting and randomly selected nonlabel words ( equal in number to Hidden anchor ) .", "ner": [["Hidden random", "Method"], ["Hidden anchor", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Hidden random-top : To establish a stronger baseline , we randomly select 20 sets of non-label words in Hidden random and report the one with the highest label loyalty .", "ner": [["Hidden random-top", "Method"], ["Hidden random", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The Text anchor method is included to demonstrate that the effectiveness of Hidden anchor is attributed to the aggregation of information in label words , rather than the mere text of label words .", "ner": [["Text anchor", "Method"], ["Hidden anchor", "Method"]], "rel": [["Text anchor", "Compare-With", "Hidden anchor"]], "rel_plus": [["Text anchor:Method", "Compare-With", "Hidden anchor:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "If we find that Hidden anchor surpasses Text anchor in performance , it solidifies the notion that the aggregated information within label words carries significant importance .", "ner": [["Hidden anchor", "Method"], ["Text anchor", "Method"]], "rel": [["Hidden anchor", "Compare-With", "Text anchor"]], "rel_plus": [["Hidden anchor:Method", "Compare-With", "Text anchor:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "The Hidden random method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states .", "ner": [["Hidden random", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We can see from Table 2 that the proposed compression method Hidden anchor achieves the best results among all three compression methods on all metrics and for both models .", "ner": [["Hidden anchor", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "For example , with the GPT-J model , the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation , indicating that the compression introduces negligible information loss .", "ner": [["GPT-J", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Further , we estimate the efficiency improvements over the original ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Besides , we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2 - XL , demonstrating its great potential to apply to larger language models .", "ner": [["GPT-J", "Method"], ["GPT2 - XL", "Method"], ["larger language models", "Method"]], "rel": [["GPT-J", "Compare-With", "GPT2 - XL"], ["GPT2 - XL", "SubClass-Of", "larger language models"], ["GPT-J", "SubClass-Of", "larger language models"]], "rel_plus": [["GPT-J:Method", "Compare-With", "GPT2 - XL:Method"], ["GPT2 - XL:Method", "SubClass-Of", "larger language models:Method"], ["GPT-J:Method", "SubClass-Of", "larger language models:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Lastly , we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words .", "ner": [["ICL", "Method"], ["attention module", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Furthermore , considering the distribution of query vectors q q , we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in q q , denoted as k ( see Appendix J for details ) .", "ner": [["PCA-like", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "We anticipate that the distances between these ks can correspond to the category confusion of the model , thus revealing one possible origin of ICL errors .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Here , we normalize the distances to a scale of 0 - 1 , with 0 indicating the highest degree of category confusion : EQUATION We utilize the GPT2 - XL model and TREC dataset , as the model displays varying confusion levels between categories on this dataset .", "ner": [["GPT2 - XL", "Method"], ["TREC", "Dataset"]], "rel": [["GPT2 - XL", "Evaluated-With", "TREC"]], "rel_plus": [["GPT2 - XL:Method", "Evaluated-With", "TREC:Dataset"]]}
{"doc_id": "EMNLP2023", "sentence": "We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis .", "ner": [["TREC", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The heatmaps display similarity in confusing category pairs , particularly in lighter-colored blocks .", "ner": [["lighter-colored blocks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "This high correlation indicates that ICL makes errors in categories with similar label anchors .", "ner": [["ICL", "Method"], ["label anchors", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The existing literature on in-context learning analysis can be broadly divided into two streams , each focusing on different aspects .", "ner": [["in-context learning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "The first stream explores the influencing factors of ICL based on input perturbation , such as the order ( Min et al . , 2022b ) , the formatting ( Yoo et al . , 2022 ; Wei et al . , 2022 ) , and the selection of the demonstration ( Liu et al . , 2022 ) .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Designing proper demonstration construc-tion strategies ( Ye et al . , 2023 ; Li et al . , 2023a ) and calibration techniques ( Zhao et al . , 2021 ; Min et al . , 2022a ) could bring clear boosts to the ICL performance .", "ner": [["demonstration construc-tion strategies", "Method"], ["calibration", "Method"], ["ICL", "Method"]], "rel": [["demonstration construc-tion strategies", "Used-For", "ICL"], ["calibration", "Used-For", "ICL"]], "rel_plus": [["demonstration construc-tion strategies:Method", "Used-For", "ICL:Method"], ["calibration:Method", "Used-For", "ICL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "The second stream investigates the inner working mechanism of ICL through different conceptual lenses , such as making an analogy of ICL to gradient descent ( von Oswald et al . , 2022 ; Dai et al . , 2022 ) and viewing the process of ICL as a Bayesian inference ( Xie et al . , 2022 ) .", "ner": [["ICL", "Method"], ["ICL", "Method"], ["gradient descent", "Method"], ["ICL", "Method"], ["Bayesian inference", "Method"]], "rel": [["ICL", "Compare-With", "gradient descent"], ["ICL", "Compare-With", "Bayesian inference"]], "rel_plus": [["ICL:Method", "Compare-With", "gradient descent:Method"], ["ICL:Method", "Compare-With", "Bayesian inference:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "In this paper , we provide a novel perspective by examining the information flow in language models to gain an understanding of ICL .", "ner": [["language models", "Method"], ["ICL", "Method"]], "rel": [["ICL", "Used-For", "language models"]], "rel_plus": [["ICL:Method", "Used-For", "language models:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the effectiveness , efficiency , and interpretability of ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "In this paper , we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow .", "ner": [["in-context learning", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Experimental results with attention manipulation and analysis of predictions correlation consolidate the hypothesis holds well in GPT2 - XL and GPT-J models .", "ner": [["attention manipulation", "Method"], ["predictions correlation", "Method"], ["GPT2 - XL", "Method"], ["GPT-J", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "First , an anchor re-weighting method is proposed to improve ICL accuracy .", "ner": [["anchor re-weighting method", "Method"], ["ICL", "Method"]], "rel": [["anchor re-weighting method", "Part-Of", "ICL"]], "rel_plus": [["anchor re-weighting method:Method", "Part-Of", "ICL:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Second , we explore a demonstration compression technique to accelerate ICL inference .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Lastly , we showcase an analysis framework to diagnose ICL errors on a real-world dataset .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "These promising applications again verify the hypothesis and open up new directions for future investigations on ICL .", "ner": [["ICL", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "Our study , while providing valuable insights into in-context learning ( ICL ) , has several limitations .", "ner": [["in-context learning", "Method"], ["ICL", "Method"]], "rel": [["ICL", "Synonym-Of", "in-context learning"]], "rel_plus": [["ICL:Method", "Synonym-Of", "in-context learning:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "Additionally , our hypothesis was only examined within conventional ICL paradigms , leaving other ICL paradigms such as the chain of thought prompting ( CoT ) ( Wei et al . , 2022 ) unexplored .", "ner": [["ICL", "Method"], ["ICL", "Method"], ["chain of thought prompting", "Method"], ["CoT", "Method"]], "rel": [["chain of thought prompting", "Part-Of", "ICL"], ["CoT", "Synonym-Of", "chain of thought prompting"]], "rel_plus": [["chain of thought prompting:Method", "Part-Of", "ICL:Method"], ["CoT:Method", "Synonym-Of", "chain of thought prompting:Method"]]}
{"doc_id": "EMNLP2023", "sentence": "For models , we use GPT2 - XL ( 1.5B ) ( Radford et al . , 2019 ) and GPT-J ( 6B ) ( Wang and Komatsuzaki , 2021 ) in this paper .", "ner": [["GPT2 - XL", "Method"], ["GPT-J", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "EMNLP2023", "sentence": "For datasets , we use a sentiment analysis task , Stanford Sentiment Treebank Binary ( SST - 2 ) ( Socher et al . , 2013 ) , a question type classification task , Text REtrieval Conference Question Classification ( TREC ) ( Li and Roth , 2002 ; Hovy et al . , 2001 ) , a topic classification task , AG's news topic classification dataset ( AGNews ) ( Zhang et al . , 2015 ) , and an emotion classification task , Emo-Context ( EmoC ) ( Chatterjee et al . , 2019 ) .", "ner": [["sentiment analysis", "Task"], ["Stanford Sentiment Treebank Binary", "Dataset"], ["SST - 2", "Dataset"], ["question type classification", "Task"], ["Text REtrieval Conference Question Classification", "Dataset"], ["TREC", "Dataset"], ["topic classification", "Task"], ["AG's news topic classification dataset", "Dataset"], ["AGNews", "Dataset"], ["emotion classification", "Task"], ["Emo-Context", "Dataset"], ["EmoC", "Dataset"]], "rel": [["Stanford Sentiment Treebank Binary", "Benchmark-For", "sentiment analysis"], ["SST - 2", "Synonym-Of", "Stanford Sentiment Treebank Binary"], ["Text REtrieval Conference Question Classification", "Benchmark-For", "question type classification"], ["TREC", "Synonym-Of", "Text REtrieval Conference Question Classification"], ["AG's news topic classification dataset", "Benchmark-For", "topic classification"], ["AGNews", "Synonym-Of", "AG's news topic classification dataset"], ["Emo-Context", "Benchmark-For", "emotion classification"], ["EmoC", "Synonym-Of", "Emo-Context"]], "rel_plus": [["Stanford Sentiment Treebank Binary:Dataset", "Benchmark-For", "sentiment analysis:Task"], ["SST - 2:Dataset", "Synonym-Of", "Stanford Sentiment Treebank Binary:Dataset"], ["Text REtrieval Conference Question Classification:Dataset", "Benchmark-For", "question type classification:Task"], ["TREC:Dataset", "Synonym-Of", "Text REtrieval Conference Question Classification:Dataset"], ["AG's news topic classification dataset:Dataset", "Benchmark-For", "topic classification:Task"], ["AGNews:Dataset", "Synonym-Of", "AG's news topic classification dataset:Dataset"], ["Emo-Context:Dataset", "Benchmark-For", "emotion classification:Task"], ["EmoC:Dataset", "Synonym-Of", "Emo-Context:Dataset"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics In drug discovery , molecular dynamics ( MD ) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities , estimating transport properties , and exploring pocket sites .", "ner": [["Multi-Grained Symmetric Differential Equation", "Method"], ["Learning Protein-Ligand Binding Dynamics", "Task"], ["drug discovery", "Task"], ["molecular dynamics", "Method"], ["MD", "Method"], ["protein-ligand binding", "Task"], ["predicting binding affinities", "Task"], ["estimating transport properties", "Task"], ["exploring pocket sites", "Task"]], "rel": [["Multi-Grained Symmetric Differential Equation", "Used-For", "Learning Protein-Ligand Binding Dynamics"], ["Learning Protein-Ligand Binding Dynamics", "SubTask-Of", "drug discovery"], ["Multi-Grained Symmetric Differential Equation", "Used-For", "drug discovery"], ["MD", "Synonym-Of", "molecular dynamics"], ["molecular dynamics", "Used-For", "protein-ligand binding"], ["molecular dynamics", "Used-For", "predicting binding affinities"], ["molecular dynamics", "Used-For", "estimating transport properties"], ["molecular dynamics", "Used-For", "exploring pocket sites"]], "rel_plus": [["Multi-Grained Symmetric Differential Equation:Method", "Used-For", "Learning Protein-Ligand Binding Dynamics:Task"], ["Learning Protein-Ligand Binding Dynamics:Task", "SubTask-Of", "drug discovery:Task"], ["Multi-Grained Symmetric Differential Equation:Method", "Used-For", "drug discovery:Task"], ["MD:Method", "Synonym-Of", "molecular dynamics:Method"], ["molecular dynamics:Method", "Used-For", "protein-ligand binding:Task"], ["molecular dynamics:Method", "Used-For", "predicting binding affinities:Task"], ["molecular dynamics:Method", "Used-For", "estimating transport properties:Task"], ["molecular dynamics:Method", "Used-For", "exploring pocket sites:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "There has been a long history of improving the efficiency of MD simulations through better numerical methods and , more recently , by utilizing machine learning ( ML ) methods .", "ner": [["MD simulations", "Task"], ["machine learning", "Method"], ["ML", "Method"]], "rel": [["machine learning", "Used-For", "MD simulations"], ["ML", "Synonym-Of", "machine learning"]], "rel_plus": [["machine learning:Method", "Used-For", "MD simulations:Task"], ["ML:Method", "Synonym-Of", "machine learning:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To address this issue , we propose NeuralMD , the first ML surrogate that can facilitate numerical MD and provide accurate simulations in protein-ligand binding .", "ner": [["NeuralMD", "Method"], ["ML", "Method"], ["numerical MD", "Method"], ["protein-ligand binding", "Task"]], "rel": [["NeuralMD", "SubClass-Of", "ML"], ["NeuralMD", "Used-For", "protein-ligand binding"]], "rel_plus": [["NeuralMD:Method", "SubClass-Of", "ML:Method"], ["NeuralMD:Method", "Used-For", "protein-ligand binding:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Specifically , we propose ( 1 ) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions , and ( 2 ) an augmented neural differential equation solver that learns the trajectory under Newtonian mechanics .", "ner": [["BindingNet", "Method"], ["neural differential equation solver", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For the experiment , we design ten single-trajectory and three multi-trajectory binding simulation tasks .", "ner": [["single-trajectory", "Task"], ["multi-trajectory binding simulation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We show the efficiency and effectiveness of NeuralMD , with a 2000 \u00d7 speedup over standard numerical MD simulation and outperforming all other ML approaches by up to ~ 80% under the stability metric .", "ner": [["NeuralMD", "Method"], ["numerical MD simulation", "Task"], ["ML", "Method"]], "rel": [["NeuralMD", "Compare-With", "ML"]], "rel_plus": [["NeuralMD:Method", "Compare-With", "ML:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We further qualitatively show that NeuralMD reaches more stable binding predictions compared to other machine learning methods .", "ner": [["NeuralMD", "Method"], ["machine learning", "Method"]], "rel": [["NeuralMD", "Compare-With", "machine learning"]], "rel_plus": [["NeuralMD:Method", "Compare-With", "machine learning:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The molecular dynamics ( MD ) simulation for protein-ligand binding is one of the fundamental tasks in drug discovery [ 1 , 2 , 3 , 4 ] .", "ner": [["molecular dynamics", "Method"], ["MD", "Method"], ["protein-ligand binding", "Task"], ["drug discovery", "Task"]], "rel": [["MD", "Synonym-Of", "molecular dynamics"], ["molecular dynamics", "Used-For", "protein-ligand binding"], ["protein-ligand binding", "SubTask-Of", "drug discovery"], ["molecular dynamics", "Used-For", "drug discovery"]], "rel_plus": [["MD:Method", "Synonym-Of", "molecular dynamics:Method"], ["molecular dynamics:Method", "Used-For", "protein-ligand binding:Task"], ["protein-ligand binding:Task", "SubTask-Of", "drug discovery:Task"], ["molecular dynamics:Method", "Used-For", "drug discovery:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To simulate the protein-ligand dynamics , numerical MD methods have been extensively developed [ 5 , 6 ] .", "ner": [["protein-ligand dynamics", "Task"], ["numerical MD", "Method"]], "rel": [["numerical MD", "Used-For", "protein-ligand dynamics"]], "rel_plus": [["numerical MD:Method", "Used-For", "protein-ligand dynamics:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To alleviate this issue , machine learning ( ML ) surrogates have been proposed to either augment or replace numerical MD methods to estimate the MD trajectories .", "ner": [["machine learning", "Method"], ["ML", "Method"], ["numerical MD", "Method"], ["estimate the MD trajectories", "Task"]], "rel": [["ML", "Synonym-Of", "machine learning"], ["machine learning", "Used-For", "estimate the MD trajectories"], ["numerical MD", "Used-For", "estimate the MD trajectories"]], "rel_plus": [["ML:Method", "Synonym-Of", "machine learning:Method"], ["machine learning:Method", "Used-For", "estimate the MD trajectories:Task"], ["numerical MD:Method", "Used-For", "estimate the MD trajectories:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "However , all prior ML approaches for MD simulation are limited to single-system ( e.g . , either small molecules or proteins ) and not protein-ligand complex [ 7 , 8 , 9 ] .", "ner": [["ML", "Method"], ["MD simulation", "Task"]], "rel": [["ML", "Used-For", "MD simulation"]], "rel_plus": [["ML:Method", "Used-For", "MD simulation:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "A primary reason is the lack of large-scale datasets for protein-ligand binding .", "ner": [["protein-ligand binding", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Another critical aspect that needs to be considered in ML-based modeling is the group symmetry present in the proteinligand geometry .", "ner": [["ML-based modeling", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Specifically , the geometric function over molecular systems should be equivariant to rotation and translation , i.e . , SE ( 3 ) - equivariance .", "ner": [["molecular systems", "Method"], ["SE ( 3 ) - equivariance", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "DenoisingLD , exhibiting a lower degree of torsion with the natural conformations .", "ner": [["DenoisingLD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Other methods collapse heavily , including GNN-MD and VerletMD , where atoms extend beyond the frame for the latter .", "ner": [["GNN-MD", "Method"], ["VerletMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Our Approach : NeuralMD .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Further , our ML approach NeuralMD preserves the Newtonian mechanics .", "ner": [["ML", "Method"], ["NeuralMD", "Method"]], "rel": [["NeuralMD", "SubClass-Of", "ML"]], "rel_plus": [["NeuralMD:Method", "SubClass-Of", "ML:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In MD , the movement of atoms is determined by Newton's second law , F = m \u2022 a , where F is the force , m is the mass , and a is the acceleration of each atom .", "ner": [["MD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Thus in NeuralMD , we formulate the trajectory simulation as a second-order ordinary differential equation ( ODE ) or second-order stochastic differential equation ( SDE ) problem .", "ner": [["NeuralMD", "Method"], ["trajectory simulation", "Task"], ["second-order ordinary differential equation", "Method"], ["ODE", "Method"], ["second-order stochastic differential equation", "Method"], ["SDE", "Method"]], "rel": [["second-order stochastic differential equation", "Part-Of", "NeuralMD"], ["second-order ordinary differential equation", "Part-Of", "NeuralMD"], ["second-order ordinary differential equation", "Used-For", "trajectory simulation"], ["second-order stochastic differential equation", "Used-For", "trajectory simulation"], ["ODE", "Synonym-Of", "second-order ordinary differential equation"], ["SDE", "Synonym-Of", "second-order stochastic differential equation"]], "rel_plus": [["second-order stochastic differential equation:Method", "Part-Of", "NeuralMD:Method"], ["second-order ordinary differential equation:Method", "Part-Of", "NeuralMD:Method"], ["second-order ordinary differential equation:Method", "Used-For", "trajectory simulation:Task"], ["second-order stochastic differential equation:Method", "Used-For", "trajectory simulation:Task"], ["ODE:Method", "Synonym-Of", "second-order ordinary differential equation:Method"], ["SDE:Method", "Synonym-Of", "second-order stochastic differential equation:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To verify the effectiveness and efficiency of NeuralMD , we design ten single-trajectory and three multitrajectory binding simulation tasks .", "ner": [["NeuralMD", "Method"], ["single-trajectory", "Task"], ["multitrajectory binding simulation", "Task"]], "rel": [["NeuralMD", "Used-For", "single-trajectory"], ["NeuralMD", "Used-For", "multitrajectory binding simulation"]], "rel_plus": [["NeuralMD:Method", "Used-For", "single-trajectory:Task"], ["NeuralMD:Method", "Used-For", "multitrajectory binding simulation:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We observe that NeuralMD outperforms all other ML methods [ 9 , 23 , 24 , 25 , 26 ] on 12 tasks using recovery metric , and NeuralMD is consistently better by a large gap using the stability metric ( up to ~ 80% ) .", "ner": [["NeuralMD", "Method"], ["ML", "Method"], ["NeuralMD", "Method"]], "rel": [["NeuralMD", "Compare-With", "ML"]], "rel_plus": [["NeuralMD:Method", "Compare-With", "ML:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "They are three protein-ligand binding complexes from Protein Data Bank ( PDB ) , as shown in Figure 1 .", "ner": [["Protein Data Bank", "Dataset"], ["PDB", "Dataset"]], "rel": [["PDB", "Synonym-Of", "Protein Data Bank"]], "rel_plus": [["PDB:Dataset", "Synonym-Of", "Protein Data Bank:Dataset"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In addition to the backbone level , as a coarser-grained view , we further consider residue-level information for modeling binding interactions , { f ( p ) , x x x ( p ) } , where the coordinate of C \u03b1 is taken as the residue-level coordinate , i.e . , x x x ( p ) \u225c x x x ( p ) C \u03b1 .", "ner": [["binding interactions", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Molecular Dynamics Simulations .", "ner": [["Molecular Dynamics Simulations", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Generally , molecular dynamics ( MD ) describes how each atom in a molecular system moves over time , following Newton's second law of motion : EQUATION where F is the force , m is the mass , a is the acceleration , x x x is the position , and t is the time .", "ner": [["molecular dynamics", "Method"], ["MD", "Method"]], "rel": [["MD", "Synonym-Of", "molecular dynamics"]], "rel_plus": [["MD:Method", "Synonym-Of", "molecular dynamics:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The numerical MD methods can be classified into classical MD and ab-initio MD , where the difference lies in how the force on each atom is calculated : classical MD uses force field approaches to predict the atomic forces [ 5 ] , while ab-initio MD calculates the forces using quantum mechanical methods , such as density functional theory ( DFT ) [ 6 ] .", "ner": [["numerical MD", "Method"], ["classical MD", "Method"], ["ab-initio MD", "Method"], ["classical MD", "Method"], ["predict the atomic forces", "Task"], ["ab-initio MD", "Method"], ["quantum mechanical", "Method"], ["density functional theory", "Method"], ["DFT", "Method"]], "rel": [["classical MD", "SubClass-Of", "numerical MD"], ["ab-initio MD", "SubClass-Of", "numerical MD"], ["classical MD", "Used-For", "predict the atomic forces"], ["ab-initio MD", "Used-For", "predict the atomic forces"], ["density functional theory", "Part-Of", "ab-initio MD"], ["classical MD", "Compare-With", "ab-initio MD"], ["quantum mechanical", "Part-Of", "ab-initio MD"], ["density functional theory", "SubClass-Of", "quantum mechanical"], ["DFT", "Synonym-Of", "density functional theory"]], "rel_plus": [["classical MD:Method", "SubClass-Of", "numerical MD:Method"], ["ab-initio MD:Method", "SubClass-Of", "numerical MD:Method"], ["classical MD:Method", "Used-For", "predict the atomic forces:Task"], ["ab-initio MD:Method", "Used-For", "predict the atomic forces:Task"], ["density functional theory:Method", "Part-Of", "ab-initio MD:Method"], ["classical MD:Method", "Compare-With", "ab-initio MD:Method"], ["quantum mechanical:Method", "Part-Of", "ab-initio MD:Method"], ["density functional theory:Method", "SubClass-Of", "quantum mechanical:Method"], ["DFT:Method", "Synonym-Of", "density functional theory:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "More recently , ML MD methods have opened a new perspective by utilizing the group symmetric tools for geometric representation and energy prediction [ 11 , 12 , 13 , 14 , 15 , 17 , 18 , 19 , 27 ] , as well as the automatic differential tools for trajectory learning [ 28 , 29 , 30 , 31 , 32 , 33 ] .", "ner": [["ML MD methods", "Method"], ["geometric representation", "Task"], ["energy prediction", "Task"], ["automatic differential tools", "Method"], ["trajectory learning", "Task"]], "rel": [["ML MD methods", "Used-For", "geometric representation"], ["ML MD methods", "Used-For", "energy prediction"], ["automatic differential tools", "Used-For", "trajectory learning"]], "rel_plus": [["ML MD methods:Method", "Used-For", "geometric representation:Task"], ["ML MD methods:Method", "Used-For", "energy prediction:Task"], ["automatic differential tools:Method", "Used-For", "trajectory learning:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Newtonian Dynamics and Langevin Dynamics .", "ner": [["Newtonian Dynamics", "Method"], ["Langevin Dynamics .", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Newtonian dynamics is suitable for idealized systems with negligible thermal effects or when deterministic trajectories are required , while Langevin dynamics is adopted where thermal effects play a significant role and when the system is being studied at a finite temperature .", "ner": [["Newtonian dynamics", "Method"], ["Langevin dynamics", "Method"]], "rel": [["Newtonian dynamics", "Compare-With", "Langevin dynamics"]], "rel_plus": [["Newtonian dynamics:Method", "Compare-With", "Langevin dynamics:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In this work , we propose two versions : an ordinary differential equation ( ODE ) solver and a stochastic differential equation ( SDE ) solver concerning Newtonian dynamics and Langevin dynamics , respectively .", "ner": [["ordinary differential equation", "Method"], ["ODE", "Method"], ["stochastic differential equation", "Method"], ["SDE", "Method"], ["Newtonian dynamics", "Method"], ["Langevin dynamics", "Method"]], "rel": [["ODE", "Synonym-Of", "ordinary differential equation"], ["SDE", "Synonym-Of", "stochastic differential equation"], ["ordinary differential equation", "Part-Of", "Newtonian dynamics"], ["stochastic differential equation", "Part-Of", "Langevin dynamics"]], "rel_plus": [["ODE:Method", "Synonym-Of", "ordinary differential equation:Method"], ["SDE:Method", "Synonym-Of", "stochastic differential equation:Method"], ["ordinary differential equation:Method", "Part-Of", "Newtonian dynamics:Method"], ["stochastic differential equation:Method", "Part-Of", "Langevin dynamics:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Noticeably , our experiential dataset , MISATO [ 10 ] , uses Newtonian dynamics with Langevin thermostats , and the information on solvent molecules is not provided .", "ner": [["MISATO", "Dataset"], ["Newtonian dynamics", "Method"], ["Langevin thermostats", "Method"]], "rel": [["Newtonian dynamics", "Evaluated-With", "MISATO"], ["Langevin thermostats", "Part-Of", "Newtonian dynamics"]], "rel_plus": [["Newtonian dynamics:Method", "Evaluated-With", "MISATO:Dataset"], ["Langevin thermostats:Method", "Part-Of", "Newtonian dynamics:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We also want to clarify two critical points about this problem setting : ( 1 ) Our task is trajectory prediction , i.e . , positions as labels , and no explicit energy and force are considered as labels .", "ner": [["trajectory prediction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "ML methods for energy prediction followed with numerical ODE / SDE solver require smaller timestep ( e.g . , 1e - 15 seconds ) , while trajectory prediction , which directly predicts the coordinates over time , is agnostic to the magnitude of the timestep .", "ner": [["ML", "Method"], ["energy prediction", "Task"], ["ODE", "Method"], ["SDE", "Method"], ["trajectory prediction", "Task"]], "rel": [["ML", "Used-For", "energy prediction"]], "rel_plus": [["ML:Method", "Used-For", "energy prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "It has two main phases : ( 1 ) A multi-grained SE ( 3 ) - equivariant geometric model , BindingNet .", "ner": [["multi-grained SE ( 3 ) - equivariant geometric model", "Method"], ["BindingNet", "Method"]], "rel": [["BindingNet", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant geometric model"]], "rel_plus": [["BindingNet:Method", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant geometric model:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "It models the protein-ligand complex from three granularities , employing three levels of vector frame basis : atom level for ligands , backbone level for proteins , and residue level for protein-ligand complexes . ( 2 ) A second-order ordinary differential equation ( ODE ) solver and a second-order stochastic differential equation ( SDE ) to learn the Newtonian mechanics .", "ner": [["second-order ordinary differential equation", "Method"], ["ODE", "Method"], ["second-order stochastic differential equation", "Method"], ["SDE", "Method"]], "rel": [["ODE", "Synonym-Of", "second-order ordinary differential equation"], ["SDE", "Synonym-Of", "second-order stochastic differential equation"]], "rel_plus": [["ODE:Method", "Synonym-Of", "second-order ordinary differential equation:Method"], ["SDE:Method", "Synonym-Of", "second-order stochastic differential equation:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We further introduce two attributes : a derivative-space augmentation method for the efficient second-order solver and a conditional adjoint solver for lower memory costs .", "ner": [["derivative-space augmentation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "This section outlines the structure : the three levels of vector frames in Section 3.1 , the architecture of BindingNet for protein-ligand force prediction in Section 3.2 , the design of NeuralMD for trajectory simulation in Section 3.3 , and implementation details in Section 3.4 .", "ner": [["BindingNet", "Method"], ["protein-ligand force prediction", "Task"], ["NeuralMD", "Method"], ["trajectory simulation", "Task"]], "rel": [["BindingNet", "Used-For", "protein-ligand force prediction"], ["NeuralMD", "Used-For", "trajectory simulation"]], "rel_plus": [["BindingNet:Method", "Used-For", "protein-ligand force prediction:Task"], ["NeuralMD:Method", "Used-For", "trajectory simulation:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To address this issue , we propose BindingNet , a multi-grained SE ( 3 ) - equivariant model , to capture the interactions between a ligand and a protein .", "ner": [["BindingNet", "Method"], ["multi-grained SE ( 3 ) - equivariant model", "Method"]], "rel": [["BindingNet", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant model"]], "rel_plus": [["BindingNet:Method", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant model:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Such a group symmetric property is called SE ( 3 ) - equivariance .", "ner": [["SE ( 3 ) - equivariance", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Atom-Level Vector Frame for Ligands .", "ner": [["Atom-Level Vector Frame", "Method"], ["Ligands", "Task"]], "rel": [["Atom-Level Vector Frame", "Used-For", "Ligands"]], "rel_plus": [["Atom-Level Vector Frame:Method", "Used-For", "Ligands:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For each residue in the protein , the coordinates are x x x N , x x x C \u03b1 , and x x x C , then the backbone-level vector frame for this residue is : EQUATION This is built for each residue , enabling the message passing for a residue-level representation .", "ner": [["backbone-level vector frame", "Method"], ["residue-level representation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Residue-Level Vector Frame for Protein-Ligand Complexes .", "ner": [["Residue-Level Vector Frame", "Method"], ["Protein-Ligand Complexes", "Task"]], "rel": [["Residue-Level Vector Frame", "Used-For", "Protein-Ligand Complexes"]], "rel_plus": [["Residue-Level Vector Frame:Method", "Used-For", "Protein-Ligand Complexes:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In this section , we introduce BindingNet , a multi-grained SE ( 3 ) - equivariant geometric model for protein-ligand binding .", "ner": [["BindingNet", "Method"], ["multi-grained SE ( 3 ) - equivariant geometric model", "Method"], ["protein-ligand binding", "Task"]], "rel": [["BindingNet", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant geometric model"], ["BindingNet", "Used-For", "protein-ligand binding"]], "rel_plus": [["BindingNet:Method", "SubClass-Of", "multi-grained SE ( 3 ) - equivariant geometric model:Method"], ["BindingNet:Method", "Used-For", "protein-ligand binding:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The input of BindingNet is the geometry of the rigid protein and the ligand at time t , while the output is the force on each atom in the ligand .", "ner": [["BindingNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Atom-Level Ligand Modeling .", "ner": [["Atom-Level Ligand Modeling", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We first generate the atom embedding using one-hot encoding and then aggregate each atom's embedding , z z z ( l ) , by aggregating all its neighbor's embedding within the cutoff distance c .", "ner": [["atom embedding", "Method"], ["one-hot encoding", "Method"]], "rel": [["one-hot encoding", "Part-Of", "atom embedding"]], "rel_plus": [["one-hot encoding:Method", "Part-Of", "atom embedding:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Finally , it is passed through several equivariant message-passing layers ( MPNN ) defined as : EQUATION where MLP ( \u2022 ) and Agg ( \u2022 ) are the multi-layer perceptron and mean aggregation functions , respectively . vec \u2208 R 3 is a vector assigned to each atom and is initialized as 0 .", "ner": [["equivariant message-passing layers", "Method"], ["MPNN", "Method"], ["MLP", "Method"], ["Agg", "Method"], ["multi-layer perceptron", "Method"], ["mean aggregation functions", "Method"]], "rel": [["MPNN", "Synonym-Of", "equivariant message-passing layers"], ["MLP", "Part-Of", "equivariant message-passing layers"], ["Agg", "Part-Of", "equivariant message-passing layers"], ["MLP", "Synonym-Of", "multi-layer perceptron"], ["Agg", "Synonym-Of", "mean aggregation functions"]], "rel_plus": [["MPNN:Method", "Synonym-Of", "equivariant message-passing layers:Method"], ["MLP:Method", "Part-Of", "equivariant message-passing layers:Method"], ["Agg:Method", "Part-Of", "equivariant message-passing layers:Method"], ["MLP:Method", "Synonym-Of", "multi-layer perceptron:Method"], ["Agg:Method", "Synonym-Of", "mean aggregation functions:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The outputs are atom representation and vector ( h h h ( l ) and vec ( l ) ) , and they are passed to the complex module introduced below .", "ner": [["atom representation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Backbone-Level Protein Modeling .", "ner": [["Backbone-Level Protein Modeling", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For the coarse-grained modeling of proteins , we consider three backbone atoms in each residue .", "ner": [["coarse-grained modeling of proteins", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Then , we obtain an equivariant atom representation by aggregating the edge information , ( x x x ( p ) i - x x x ( p ) j ) \u2022 z z z ( p ) i , within cutoff distance c .", "ner": [["aggregating the edge information", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Finally , we adopt an equivariant MPNN layer to get the atom-level force as : EQUATION The ultimate force predictions for each atom include two parts : the internal force from the molecule vec ( l ) i and the external force from the protein-ligand interaction vec ( pl ) i j .", "ner": [["MPNN", "Method"], ["protein-ligand interaction", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "So sum them up , we have : EQUATION These three modules consist BindingNet .", "ner": [["BindingNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The BindingNet introduced in Sections 3.1 and 3.2 takes in the molecular system geometry at time t and outputs the forces .", "ner": [["BindingNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Then in this section , we describe how we use the neural differential equation solver to predict the coordinates at future snapshots .", "ner": [["neural differential equation solver", "Method"], ["predict the coordinates", "Task"]], "rel": [["neural differential equation solver", "Used-For", "predict the coordinates"]], "rel_plus": [["neural differential equation solver:Method", "Used-For", "predict the coordinates:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We want to highlight that one ML for MD simulation research line is predicting the energy or force [ 36 , 23 , 9 ] , which will be fed into the numerical integration algorithms for trajectory simulation .", "ner": [["ML", "Method"], ["MD simulation", "Task"], ["numerical integration algorithms", "Method"], ["trajectory simulation", "Task"]], "rel": [["ML", "Used-For", "MD simulation"], ["numerical integration algorithms", "Used-For", "trajectory simulation"]], "rel_plus": [["ML:Method", "Used-For", "MD simulation:Task"], ["numerical integration algorithms:Method", "Used-For", "trajectory simulation:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For accuracy , such an ML-based MD simulation must be at the femtosecond level ( 1e - 15 second ) .", "ner": [["ML-based MD simulation", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "However , as shown in recent works [ 37 , 38 , 21 ] , minor errors in the ML force field can lead to catastrophic failure for long-time simulations .", "ner": [["ML", "Method"], ["long-time simulations", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In this paper , however , we overcome this issue by directly learning the extended-timescale MD trajectories ( nanosecond level , 1e - 9 second ) .", "ner": [["extended-timescale MD trajectories", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Newtonian dynamics and Langevin dynamics for MD simulation .", "ner": [["Newtonian dynamics", "Method"], ["Langevin dynamics", "Method"], ["MD simulation", "Task"]], "rel": [["Langevin dynamics", "Used-For", "MD simulation"], ["Newtonian dynamics", "Used-For", "MD simulation"]], "rel_plus": [["Langevin dynamics:Method", "Used-For", "MD simulation:Task"], ["Newtonian dynamics:Method", "Used-For", "MD simulation:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For modeling , we consider using the BindingNet introduced above for force prediction at time \u03c4 , as : EQUATION On the other hand , Langevin dynamics introduces a stochastic component for large molecular systems with thermal fluctuations .", "ner": [["BindingNet", "Method"], ["force prediction", "Task"], ["Langevin dynamics", "Method"], ["molecular systems", "Method"], ["thermal fluctuations", "Method"]], "rel": [["BindingNet", "Used-For", "force prediction"], ["thermal fluctuations", "Part-Of", "molecular systems"]], "rel_plus": [["BindingNet:Method", "Used-For", "force prediction:Task"], ["thermal fluctuations:Method", "Part-Of", "molecular systems:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Ultimately , adopting either F ( l ) \u03c4 - ODE or F ( l ) \u03c4 - SDE as the force prediction F ( l ) \u03c4 on each atom , the coordinates at time t can be obtained after integration as : EQUATION ) The training objective is the mean absolute error ( MAE ) between the predicted coordinates and ground-truth coordinates along the whole trajectories : EQUATION ) An illustration of NeuralMD pipeline is in Figure 4 .", "ner": [["ODE", "Method"], ["SDE", "Method"], ["force prediction", "Task"], ["mean absolute error", "Method"], ["MAE", "Method"], ["NeuralMD", "Method"]], "rel": [["ODE", "Used-For", "force prediction"], ["SDE", "Used-For", "force prediction"], ["MAE", "Synonym-Of", "mean absolute error"]], "rel_plus": [["ODE:Method", "Used-For", "force prediction:Task"], ["SDE:Method", "Used-For", "force prediction:Task"], ["MAE:Method", "Synonym-Of", "mean absolute error:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Adjoint method .", "ner": [["Adjoint method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Specifically for protein-ligand complexes with a large volume of atoms , the memory costs have become a main challenge .", "ner": [["protein-ligand complexes", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To handle this , we consider the adjoint method [ 28 ] , allowing small step sizes in the integration .", "ner": [["adjoint method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Roughly speaking , the adjoint method calculates the variational derivative of the trajectory w.r.t . our objective function directly .", "ner": [["adjoint method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Second-order differential equation .", "ner": [["Second-order differential equation", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The key module of the neural differential method is the differential function [ 28 ] , which returns the first-order derivative .", "ner": [["neural differential method", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To learn the MD trajectory following second-order ODE and SDE , we propose the following formulation of the second-order equation within one integration call : EQUATION ) We mark this as \" func \" in Figure 4 .", "ner": [["MD trajectory", "Task"], ["ODE", "Method"], ["SDE", "Method"]], "rel": [["ODE", "Used-For", "MD trajectory"], ["SDE", "Used-For", "MD trajectory"]], "rel_plus": [["ODE:Method", "Used-For", "MD trajectory:Task"], ["SDE:Method", "Used-For", "MD trajectory:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "This means we can augment ODE or SDE derivative space by concurrently calculating the accelerations and velocities , allowing simultaneous integration of velocities and positions .", "ner": [["ODE", "Method"], ["SDE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In this section , we would like to include extra details of NeuralMD .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "As in Equations ( 11 ) and ( 13 ) , both the coordinates and velocities are required inputs for NeuralMD .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To handle this issue , we propose a surrogate velocity , which is a summation of a predicted velocity by an extra equivariant model and a coordinate momentum , i.e . , v v v ( l ) t = BindingNet-Ligand ( f ( l ) , x x x ( l ) t ) + ( x x x ( l ) t + 1 - x x x ( l ) t ) , where the BindingNet-Ligand is the ligand model described in Section 3.2 .", "ner": [["coordinate momentum", "Method"], ["BindingNet-Ligand", "Method"], ["BindingNet-Ligand", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Update model BindingNet using gradient descent . 9 : end for 10 : end for 4 Experiments Datasets .", "ner": [["BindingNet", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "One of the main bottlenecks of studying ML for molecular dynamics simulation in protein-ligand binding is insufficient data .", "ner": [["ML", "Method"], ["molecular dynamics simulation", "Task"], ["protein-ligand binding", "Task"]], "rel": [["ML", "Used-For", "molecular dynamics simulation"], ["molecular dynamics simulation", "SubTask-Of", "protein-ligand binding"]], "rel_plus": [["ML:Method", "Used-For", "molecular dynamics simulation:Task"], ["molecular dynamics simulation:Task", "SubTask-Of", "protein-ligand binding:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Recently , the community has put more effort into gathering the datasets , and we consider MISATO in our work [ 10 ] .", "ner": [["MISATO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "It is built on 16,972 experimental protein-ligand complexes extracted from the protein data bank ( PDB ) [ 39 ] .", "ner": [["protein-ligand complexes", "Task"], ["protein data bank", "Dataset"], ["PDB", "Dataset"]], "rel": [["protein data bank", "Benchmark-For", "protein-ligand complexes"], ["PDB", "Synonym-Of", "protein data bank"]], "rel_plus": [["protein data bank:Dataset", "Benchmark-For", "protein-ligand complexes:Task"], ["PDB:Dataset", "Synonym-Of", "protein data bank:Dataset"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Such data is obtained using X-ray crystallography , Nuclear Magnetic Resonance ( NMR ) , or Cryo-Electron Microscopy ( Cryo-EM ) , where systematic errors are unavoidable .", "ner": [["X-ray crystallography", "Method"], ["Nuclear Magnetic Resonance", "Method"], ["NMR", "Method"], ["Cryo-Electron Microscopy", "Method"], ["Cryo-EM", "Method"]], "rel": [["NMR", "Synonym-Of", "Nuclear Magnetic Resonance"], ["Cryo-EM", "Synonym-Of", "Cryo-Electron Microscopy"]], "rel_plus": [["NMR:Method", "Synonym-Of", "Nuclear Magnetic Resonance:Method"], ["Cryo-EM:Method", "Synonym-Of", "Cryo-Electron Microscopy:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "For each protein-ligand complex , the trajectory comprises 100 snapshots in 8 nanoseconds under the fixed temperature and pressure .", "ner": [["protein-ligand complex", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "In Appendix E , we list the basic statistics of MISATO , e.g . , the number of atoms in small molecule ligands , and the number of residues in proteins .", "ner": [["MISATO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Using ML for energy and force prediction , followed by trajectory prediction using numerical integration method , has been widely explored in the community , e.g . , HDNNPs [ 7 ] , DeePMD [ 9 ] , TorchMD [ 36 ] , and Allegro-LAMMPS [ 23 ] .", "ner": [["ML", "Method"], ["force prediction", "Task"], ["trajectory prediction", "Task"], ["numerical integration method", "Method"], ["HDNNPs", "Method"], ["DeePMD", "Method"], ["TorchMD", "Method"], ["Allegro-LAMMPS", "Method"]], "rel": [["ML", "Used-For", "force prediction"], ["numerical integration method", "Used-For", "trajectory prediction"], ["HDNNPs", "Used-For", "trajectory prediction"], ["DeePMD", "Used-For", "trajectory prediction"], ["TorchMD", "Used-For", "trajectory prediction"], ["Allegro-LAMMPS", "Used-For", "trajectory prediction"], ["HDNNPs", "SubClass-Of", "numerical integration method"], ["DeePMD", "SubClass-Of", "numerical integration method"], ["TorchMD", "SubClass-Of", "numerical integration method"], ["Allegro-LAMMPS", "SubClass-Of", "numerical integration method"]], "rel_plus": [["ML:Method", "Used-For", "force prediction:Task"], ["numerical integration method:Method", "Used-For", "trajectory prediction:Task"], ["HDNNPs:Method", "Used-For", "trajectory prediction:Task"], ["DeePMD:Method", "Used-For", "trajectory prediction:Task"], ["TorchMD:Method", "Used-For", "trajectory prediction:Task"], ["Allegro-LAMMPS:Method", "Used-For", "trajectory prediction:Task"], ["HDNNPs:Method", "SubClass-Of", "numerical integration method:Method"], ["DeePMD:Method", "SubClass-Of", "numerical integration method:Method"], ["TorchMD:Method", "SubClass-Of", "numerical integration method:Method"], ["Allegro-LAMMPS:Method", "SubClass-Of", "numerical integration method:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Here we extend this paradigm for binding dynamics and propose VerletMD , which utilizes BindingNet for energy prediction on each snapshot and velocity Verlet algorithm to get the trajectory .", "ner": [["VerletMD", "Method"], ["BindingNet", "Method"], ["energy prediction", "Task"], ["velocity Verlet algorithm", "Method"]], "rel": [["BindingNet", "Part-Of", "VerletMD"], ["velocity Verlet algorithm", "Part-Of", "VerletMD"], ["BindingNet", "Used-For", "energy prediction"]], "rel_plus": [["BindingNet:Method", "Part-Of", "VerletMD:Method"], ["velocity Verlet algorithm:Method", "Part-Of", "VerletMD:Method"], ["BindingNet:Method", "Used-For", "energy prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Additionally , we mainly focus on ML methods for trajectory prediction in this work , i.e . , no energy or force is considered as labels .", "ner": [["ML", "Method"], ["trajectory prediction", "Task"]], "rel": [["ML", "Used-For", "trajectory prediction"]], "rel_plus": [["ML:Method", "Used-For", "trajectory prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "GNN-MD is to apply geometric graph neural networks ( GNNs ) to predict the trajectories in an auto-regressive manner [ 10 , 24 ] .", "ner": [["GNN-MD", "Method"], ["graph neural networks", "Method"], ["GNNs", "Method"], ["predict the trajectories", "Task"]], "rel": [["graph neural networks", "Part-Of", "GNN-MD"], ["GNNs", "Synonym-Of", "graph neural networks"], ["GNN-MD", "Used-For", "predict the trajectories"]], "rel_plus": [["graph neural networks:Method", "Part-Of", "GNN-MD:Method"], ["GNNs:Method", "Synonym-Of", "graph neural networks:Method"], ["GNN-MD:Method", "Used-For", "predict the trajectories:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "More concretely , GNN-MD takes the coordinates and other molecular information as inputs at time t and predicts the coordinates at time t + 1 .", "ner": [["GNN-MD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "DenoisingLD ( denoising diffusion for Langevin dynamics ) [ 24 , 25 , 26 ] is a baseline method that models the trajectory prediction as denoising diffusion task [ 40 ] , and the inference for binding trajectory essentially becomes the Langevin dynamics .", "ner": [["DenoisingLD", "Method"], ["denoising diffusion for Langevin dynamics", "Method"], ["trajectory prediction", "Task"], ["denoising diffusion task", "Task"], ["binding trajectory", "Task"]], "rel": [["DenoisingLD", "Synonym-Of", "denoising diffusion for Langevin dynamics"], ["DenoisingLD", "Used-For", "trajectory prediction"], ["DenoisingLD", "Used-For", "denoising diffusion task"], ["DenoisingLD", "Used-For", "binding trajectory"]], "rel_plus": [["DenoisingLD:Method", "Synonym-Of", "denoising diffusion for Langevin dynamics:Method"], ["DenoisingLD:Method", "Used-For", "trajectory prediction:Task"], ["DenoisingLD:Method", "Used-For", "denoising diffusion task:Task"], ["DenoisingLD:Method", "Used-For", "binding trajectory:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Here , to make the comparison more explicit , we compare these two methods ( GNN-MD and DenoisingLD ) separately .", "ner": [["GNN-MD", "Method"], ["DenoisingLD", "Method"]], "rel": [["GNN-MD", "Compare-With", "DenoisingLD"]], "rel_plus": [["GNN-MD:Method", "Compare-With", "DenoisingLD:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We want to highlight that we keep the same backbone model , BindingNet , for energy or force prediction for all the baselines and NeuralMD .", "ner": [["BindingNet", "Method"], ["force prediction", "Task"], ["NeuralMD", "Method"]], "rel": [["NeuralMD", "Used-For", "force prediction"], ["BindingNet", "Used-For", "force prediction"]], "rel_plus": [["NeuralMD:Method", "Used-For", "force prediction:Task"], ["BindingNet:Method", "Used-For", "force prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To evaluate this , we take both the mean absolute error ( MAE ) and mean squared error ( MSE ) between the predicted coordinates and ground-truth coordinates over all snapshots .", "ner": [["mean absolute error", "Method"], ["MAE", "Method"], ["mean squared error", "Method"], ["MSE", "Method"]], "rel": [["MAE", "Synonym-Of", "mean absolute error"], ["MSE", "Synonym-Of", "mean squared error"]], "rel_plus": [["MAE:Method", "Synonym-Of", "mean absolute error:Method"], ["MSE:Method", "Synonym-Of", "mean squared error:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Stability , as highlighted in [ 21 ] , is an important metric for evaluating the predicted MD trajectory .", "ner": [["predicted MD trajectory", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The intuition is that the prediction on long-time MD trajectory can enter a pathological state ( e.g . , bond breaking ) , and stability is the measure to quantify such observation .", "ner": [["long-time MD trajectory", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Another metric considered is frames per second ( FPS ) [ 21 ] on a single Nvidia-V100 GPU card , and it measures the MD efficiency .", "ner": [["MD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The first observation is that the baseline VertletMD has a clear performance gap compared to the other methods .", "ner": [["VertletMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "There are two possible reasons : ( 1 ) Using ML models to predict the energy ( or force ) at each snapshot , and then using a numerical integration algorithm can fail in the long-time simulations [ 21 ] ; ( 2 ) ML for energy prediction methods require more data to train than the ML for coordinate prediction methods , thus they can perform worse in the low-data regime here .", "ner": [["ML", "Method"], ["predict the energy", "Task"], ["numerical integration algorithm", "Method"], ["long-time simulations", "Task"], ["ML", "Method"], ["energy prediction", "Task"], ["ML", "Method"], ["coordinate prediction", "Task"]], "rel": [["ML", "Used-For", "predict the energy"], ["numerical integration algorithm", "Used-For", "long-time simulations"], ["ML", "Used-For", "energy prediction"], ["ML", "Used-For", "coordinate prediction"]], "rel_plus": [["ML:Method", "Used-For", "predict the energy:Task"], ["numerical integration algorithm:Method", "Used-For", "long-time simulations:Task"], ["ML:Method", "Used-For", "energy prediction:Task"], ["ML:Method", "Used-For", "coordinate prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "However , the stability ( % ) can be a distinctive factor in method comparisons , where we observe the two variants of NeuralMD outperform on all 10 tasks up to ~ 80% .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "It is shown that the GNN-MD collapses occasionally , while DenoisingLD stays comparatively structured .", "ner": [["GNN-MD", "Method"], ["DenoisingLD", "Method"]], "rel": [["GNN-MD", "Compare-With", "DenoisingLD"]], "rel_plus": [["GNN-MD:Method", "Compare-With", "DenoisingLD:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Meanwhile , NeuralMD is the most stable in all cases .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "One main benefit of using NeuralMD for binding simulation is its efficiency .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We further approximate the wall time of the numerical method for MD simulation ( PDB 5WIJ ) .", "ner": [["numerical method", "Method"], ["MD simulation", "Task"], ["PDB 5WIJ", "Dataset"]], "rel": [["numerical method", "Used-For", "MD simulation"], ["PDB 5WIJ", "Benchmark-For", "MD simulation"], ["numerical method", "Evaluated-With", "PDB 5WIJ"]], "rel_plus": [["numerical method:Method", "Used-For", "MD simulation:Task"], ["PDB 5WIJ:Dataset", "Benchmark-For", "MD simulation:Task"], ["numerical method:Method", "Evaluated-With", "PDB 5WIJ:Dataset"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "A more challenging task is to test the generalization ability of NeuralMD among different trajectories .", "ner": [["NeuralMD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The MISATO dataset includes 13,765 protein-ligand complexes , and we first create two small datasets by randomly sampling 100 and 1k complexes , respectively .", "ner": [["MISATO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We also consider the whole MISATO dataset , where the data split has already been provided .", "ner": [["MISATO", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "First , we can observe that VerletMD has worse performance on all three datasets , and the performance gap with other methods is even larger compared to the single-trajectory prediction .", "ner": [["VerletMD", "Method"], ["single-trajectory prediction", "Task"]], "rel": [["VerletMD", "Used-For", "single-trajectory prediction"]], "rel_plus": [["VerletMD:Method", "Used-For", "single-trajectory prediction:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "The other two baselines , GNN-MD and DenoisingLD , show similar performance , while NeuralMD outperforms in all datasets .", "ner": [["GNN-MD", "Method"], ["DenoisingLD", "Method"], ["NeuralMD", "Method"]], "rel": [["NeuralMD", "Compare-With", "GNN-MD"], ["NeuralMD", "Compare-With", "DenoisingLD"]], "rel_plus": [["NeuralMD:Method", "Compare-With", "GNN-MD:Method"], ["NeuralMD:Method", "Compare-With", "DenoisingLD:Method"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Notice that stability ( % ) remains more distinguishable than the two trajectory recovery metrics ( MAE and MSE ) .", "ner": [["MAE", "Method"], ["MSE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "To sum up , we devise NeuralMD , an ML framework that incorporates a novel multi-grained group symmetric network architecture and second-order Newtonian mechanics , enabling accurate predictions of binding dynamics in a large timescale .", "ner": [["NeuralMD", "Method"], ["ML", "Method"], ["multi-grained group symmetric network", "Method"], ["binding dynamics", "Task"]], "rel": [["multi-grained group symmetric network", "Part-Of", "NeuralMD"], ["NeuralMD", "SubClass-Of", "ML"], ["NeuralMD", "Used-For", "binding dynamics"]], "rel_plus": [["multi-grained group symmetric network:Method", "Part-Of", "NeuralMD:Method"], ["NeuralMD:Method", "SubClass-Of", "ML:Method"], ["NeuralMD:Method", "Used-For", "binding dynamics:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Not only is such a timescale critical for understanding the dynamic nature of the ligand-protein complex , but our work marks the first approach in developing a framework to predict coordinates for MD simulation in protein-ligand binding .", "ner": [["MD simulation", "Task"], ["protein-ligand binding", "Task"]], "rel": [["MD simulation", "SubTask-Of", "protein-ligand binding"]], "rel_plus": [["MD simulation:Task", "SubTask-Of", "protein-ligand binding:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "We quantitatively and qualitatively verify that NeuralMD achieves superior performance on 13 binding dynamics tasks .", "ner": [["NeuralMD", "Method"], ["binding dynamics", "Task"]], "rel": [["NeuralMD", "Used-For", "binding dynamics"]], "rel_plus": [["NeuralMD:Method", "Used-For", "binding dynamics:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "Currently , we are using the MISATO dataset , a protein-ligand dynamics dataset with a large timestep .", "ner": [["MISATO", "Dataset"], ["protein-ligand dynamics", "Task"]], "rel": [["MISATO", "Benchmark-For", "protein-ligand dynamics"]], "rel_plus": [["MISATO:Dataset", "Benchmark-For", "protein-ligand dynamics:Task"]]}
{"doc_id": "ICLR2024-AI4Science-Protein", "sentence": "However , NeuralMD is agnostic to the timestep , and it can also be applied to binding dynamics datasets with timestep as a femtosecond .", "ner": [["NeuralMD", "Method"], ["binding dynamics", "Task"]], "rel": [["NeuralMD", "Used-For", "binding dynamics"]], "rel_plus": [["NeuralMD:Method", "Used-For", "binding dynamics:Task"]]}
{"doc_id": "CVPR2023", "sentence": "3D Registration with Maximal Cliques As a fundamental problem in computer vision , 3D point cloud registration ( PCR ) aims to seek the optimal pose to align a point cloud pair .", "ner": [["3D Registration", "Task"], ["Maximal Cliques", "Method"], ["computer vision", "Task"], ["3D point cloud registration", "Task"], ["PCR", "Task"]], "rel": [["Maximal Cliques", "Used-For", "3D Registration"], ["3D Registration", "SubTask-Of", "computer vision"], ["3D point cloud registration", "SubTask-Of", "computer vision"], ["PCR", "Synonym-Of", "3D point cloud registration"]], "rel_plus": [["Maximal Cliques:Method", "Used-For", "3D Registration:Task"], ["3D Registration:Task", "SubTask-Of", "computer vision:Task"], ["3D point cloud registration:Task", "SubTask-Of", "computer vision:Task"], ["PCR:Task", "Synonym-Of", "3D point cloud registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "In this paper , we present a 3D registration method with maximal cliques ( MAC ) .", "ner": [["3D registration method", "Method"], ["maximal cliques", "Method"], ["MAC", "Method"]], "rel": [["maximal cliques", "Part-Of", "3D registration method"], ["MAC", "Synonym-Of", "maximal cliques"]], "rel_plus": [["maximal cliques:Method", "Part-Of", "3D registration method:Method"], ["MAC:Method", "Synonym-Of", "maximal cliques:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The key insight is to loosen the previous maximum clique constraint , and mine more local consensus information in a graph for accurate pose hypotheses generation : 1 ) A compatibility graph is constructed to render the affinity relationship between initial correspondences . 2 ) We search for maximal cliques in the graph , each of which represents a consensus set .", "ner": [["maximum clique", "Method"], ["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We perform node-guided clique selection then , where each node corresponds to the maximal clique with the greatest graph weight . 3 ) Transformation hypotheses are computed for the selected cliques by the SVD algorithm and the best hypothesis is used to perform registration .", "ner": [["node-guided clique selection", "Method"], ["maximal clique", "Method"], ["SVD", "Method"], ["registration", "Task"]], "rel": [["SVD", "Used-For", "registration"]], "rel_plus": [["SVD:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Extensive experiments on U3M , 3DMatch , 3DLoMatch and KITTI demonstrate that MAC effectively increases registration accuracy , outperforms various state-of-the-art methods and boosts the performance of deep-learned methods .", "ner": [["U3M", "Dataset"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"], ["KITTI", "Dataset"], ["MAC", "Method"], ["registration", "Task"], ["deep-learned methods", "Method"]], "rel": [["MAC", "Evaluated-With", "U3M"], ["MAC", "Evaluated-With", "3DMatch"], ["MAC", "Evaluated-With", "3DLoMatch"], ["MAC", "Used-For", "registration"], ["U3M", "Benchmark-For", "registration"], ["3DMatch", "Benchmark-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["KITTI", "Benchmark-For", "registration"], ["MAC", "Part-Of", "deep-learned methods"]], "rel_plus": [["MAC:Method", "Evaluated-With", "U3M:Dataset"], ["MAC:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAC:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["MAC:Method", "Used-For", "registration:Task"], ["U3M:Dataset", "Benchmark-For", "registration:Task"], ["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["KITTI:Dataset", "Benchmark-For", "registration:Task"], ["MAC:Method", "Part-Of", "deep-learned methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "MAC combined with deep-learned methods achieves stateof-the-art registration recall of 95.7% / 78.9% on 3DMatch / 3DLoMatch .", "ner": [["MAC", "Method"], ["deep-learned methods", "Method"], ["registration", "Task"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"]], "rel": [["MAC", "Part-Of", "deep-learned methods"], ["deep-learned methods", "Used-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["3DMatch", "Benchmark-For", "registration"], ["deep-learned methods", "Evaluated-With", "3DMatch"], ["deep-learned methods", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["MAC:Method", "Part-Of", "deep-learned methods:Method"], ["deep-learned methods:Method", "Used-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["deep-learned methods:Method", "Evaluated-With", "3DMatch:Dataset"], ["deep-learned methods:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Given two 3D scans of the same object ( or scene ) , the goal of PCR is to estimate a six-degree-of-freedom ( 6 - DoF ) pose transformation that accurately aligns the two input point clouds .", "ner": [["PCR", "Task"], ["six-degree-of-freedom", "Task"], ["6 - DoF", "Task"]], "rel": [["6 - DoF", "Synonym-Of", "six-degree-of-freedom"], ["PCR", "Used-For", "six-degree-of-freedom"]], "rel_plus": [["6 - DoF:Task", "Synonym-Of", "six-degree-of-freedom:Task"], ["PCR:Task", "Used-For", "six-degree-of-freedom:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Using pointto-point feature correspondences is a popular and robust solution to the PCR problem .", "ner": [["PCR", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The problem of 3D registration by handling correspondences with outliers has been studied for decades .", "ner": [["3D registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We classify them into geometric-only and deep-learned methods .", "ner": [["geometric-only", "Method"], ["deep-learned methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "For geometric-only methods [ 5 , 6 , 21 , 30 , 31 , [ 38 ] [ 39 ] [ 40 ] [ 41 ] , random sample consensus ( RANSAC ) and its variants perform an iterative sampling strategy for registration .", "ner": [["geometric-only methods", "Method"], ["random sample consensus", "Method"], ["RANSAC", "Method"], ["registration", "Task"]], "rel": [["random sample consensus", "SubClass-Of", "geometric-only methods"], ["RANSAC", "Synonym-Of", "random sample consensus"], ["random sample consensus", "Used-For", "registration"]], "rel_plus": [["random sample consensus:Method", "SubClass-Of", "geometric-only methods:Method"], ["RANSAC:Method", "Synonym-Of", "random sample consensus:Method"], ["random sample consensus:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Although RANSAC-based methods are simple and efficient , their performance is highly vulnerable when the outlier rate increases , and it requires a large number of iterations to obtain acceptable results .", "ner": [["RANSAC-based methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Also , a series of global registration methods based on branch-and-bound ( BnB ) are proposed to search the 6D parameter space and obtain the optimal global solution .", "ner": [["global registration methods", "Method"], ["branch-and-bound", "Method"], ["BnB", "Method"]], "rel": [["branch-and-bound", "Part-Of", "global registration methods"], ["BnB", "Synonym-Of", "branch-and-bound"]], "rel_plus": [["branch-and-bound:Method", "Part-Of", "global registration methods:Method"], ["BnB:Method", "Synonym-Of", "branch-and-bound:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Except for this watermark , it is identical to the accepted version ; the final published version of the proceedings is available on IEEE Xplore . one module in the registration process , such as investigating more discriminate keypoint feature descriptors or more effective correspondence selection techniques , while the others [ 22 , 29 , 43 ] focus on registration in an end-to-end manner .", "ner": [["registration", "Task"], ["registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "However , deep-learned based methods require a large amount of data for training and usually lack generalization on different datasets .", "ner": [["deep-learned based methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In this paper , we propose a geometric-only 3D registration method based on maximal cliques ( MAC ) .", "ner": [["geometric-only 3D registration method", "Method"], ["maximal cliques", "Method"], ["MAC", "Method"]], "rel": [["maximal cliques", "Part-Of", "geometric-only 3D registration method"], ["MAC", "Synonym-Of", "maximal cliques"]], "rel_plus": [["maximal cliques:Method", "Part-Of", "geometric-only 3D registration method:Method"], ["MAC:Method", "Synonym-Of", "maximal cliques:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The key insight is to loosen the previous maximum clique constraint , and mine more local consensus information in a graph to generate accurate pose hypotheses .", "ner": [["maximum clique constraint", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Compared with the maximum clique , MAC is a looser constraint and is able to mine more local information in a graph .", "ner": [["maximum clique", "Method"], ["MAC", "Method"]], "rel": [["MAC", "Compare-With", "maximum clique"]], "rel_plus": [["MAC:Method", "Compare-With", "maximum clique:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Finally , transformation hypotheses are computed for the selected cliques by the SVD algorithm .", "ner": [["SVD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The best hypothesis is selected to perform registration using popular hypothesis evaluation metrics in the RANSAC family .", "ner": [["registration", "Task"], ["RANSAC", "Method"]], "rel": [["RANSAC", "Used-For", "registration"]], "rel_plus": [["RANSAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "To summarize , our main contributions are as follows : \u2022 We introduce a hypothesis generation method named MAC .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Our MAC method is able to mine more local information in a graph , compared with the previous maximum clique constraint .", "ner": [["MAC", "Method"], ["maximum clique", "Method"]], "rel": [["MAC", "Compare-With", "maximum clique"]], "rel_plus": [["MAC:Method", "Compare-With", "maximum clique:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Notably , our geometric-only MAC method outperforms several state-of-the-art deep learning methods [ 3 , 9 , 19 , 27 ] .", "ner": [["geometric-only MAC method", "Method"], ["deep learning methods", "Method"]], "rel": [["geometric-only MAC method", "Compare-With", "deep learning methods"]], "rel_plus": [["geometric-only MAC method:Method", "Compare-With", "deep learning methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "MAC can also be inserted as a module into multiple deep-learned frameworks [ 1 , 10 , 18 , 29 , 43 ] to boost their performance .", "ner": [["MAC", "Method"], ["deep-learned frameworks", "Method"]], "rel": [["MAC", "Part-Of", "deep-learned frameworks"]], "rel_plus": [["MAC:Method", "Part-Of", "deep-learned frameworks:Method"]]}
{"doc_id": "CVPR2023", "sentence": "MAC combined with GeoTransformer achieves the state-of-the-art registration recall of 95.7% / 78.9% on 3DMatch / 3DLoMatch .", "ner": [["MAC", "Method"], ["GeoTransformer", "Method"], ["registration", "Task"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"]], "rel": [["MAC", "Part-Of", "GeoTransformer"], ["GeoTransformer", "Used-For", "registration"], ["3DMatch", "Benchmark-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["GeoTransformer", "Evaluated-With", "3DMatch"], ["GeoTransformer", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["MAC:Method", "Part-Of", "GeoTransformer:Method"], ["GeoTransformer:Method", "Used-For", "registration:Task"], ["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["GeoTransformer:Method", "Evaluated-With", "3DMatch:Dataset"], ["GeoTransformer:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Various geometric-only methods [ 6 , 8 , 20 , 36 , 45 ] have been proposed recently .", "ner": [["geometric-only methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Typically , RANSAC and its variants [ 5 , 13 , 30 , 31 , [ 38 ] [ 39 ] [ 40 ] remain the dominant approaches to the problem of estimating a 6 - DoF pose from correspondences .", "ner": [["RANSAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "RANSAC iteratively samples correspondences from the initial set , generating and evaluating geometric estimations for each subset until a satisfactory solution is obtained .", "ner": [["RANSAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Efficient and robust evaluation metrics are extremely important for using RANSAC to achieve accurate registration .", "ner": [["RANSAC", "Method"], ["registration", "Task"]], "rel": [["RANSAC", "Used-For", "registration"]], "rel_plus": [["RANSAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "To address the current problems of timeconsuming and noise-sensitive evaluation metrics , [ 40 ] analyzes the contribution of inliers and outliers during the computation and proposed several metrics that can effectively improve the registration performance of RANSAC .", "ner": [["registration", "Task"], ["RANSAC", "Method"]], "rel": [["RANSAC", "Used-For", "registration"]], "rel_plus": [["RANSAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "For example , Rusu et al . [ 31 ] presented the simple consensus-based initial alignment ( SAC-IA ) method , which samples correspondences spread out on the point cloud and leverages the Huber penalty for evaluation .", "ner": [["consensus-based initial alignment", "Method"], ["SAC-IA", "Method"]], "rel": [["SAC-IA", "Synonym-Of", "consensus-based initial alignment"]], "rel_plus": [["SAC-IA:Method", "Synonym-Of", "consensus-based initial alignment:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Graph cut RANSAC ( GC-RANSAC ) [ 5 ] uses the graph-cut algorithm before model re-fitting in the local optimization step .", "ner": [["Graph cut RANSAC", "Method"], ["GC-RANSAC", "Method"], ["graph-cut", "Method"]], "rel": [["GC-RANSAC", "Synonym-Of", "Graph cut RANSAC"], ["graph-cut", "Part-Of", "Graph cut RANSAC"]], "rel_plus": [["GC-RANSAC:Method", "Synonym-Of", "Graph cut RANSAC:Method"], ["graph-cut:Method", "Part-Of", "Graph cut RANSAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Compatibility-guided sample consensus ( CG-SAC ) [ 30 ] additionally considers the normal information of key points during the sampling process .", "ner": [["Compatibility-guided sample consensus", "Method"], ["CG-SAC", "Method"]], "rel": [["CG-SAC", "Synonym-Of", "Compatibility-guided sample consensus"]], "rel_plus": [["CG-SAC:Method", "Synonym-Of", "Compatibility-guided sample consensus:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Yang et al . [ 39 ] proposed the sample consensus by sampling compatibility triangles ( SAC-COT ) method , which generates estimations by ranking and sampling ternary loops from the compatibility graph .", "ner": [["sampling compatibility triangles", "Method"], ["SAC-COT", "Method"]], "rel": [["SAC-COT", "Synonym-Of", "sampling compatibility triangles"]], "rel_plus": [["SAC-COT:Method", "Synonym-Of", "sampling compatibility triangles:Method"]]}
{"doc_id": "CVPR2023", "sentence": "A series of globally optimal methods based on BnB have been proposed recently .", "ner": [["BnB", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Yang et al . [ 41 ] proposed globally optimal ICP ( GO-ICP ) , which rationalizes the planning of ICP update tasks at different stages , and its biggest advantage is that it minimizes the local optimum .", "ner": [["globally optimal ICP", "Method"], ["GO-ICP", "Method"], ["ICP", "Method"]], "rel": [["GO-ICP", "Synonym-Of", "globally optimal ICP"]], "rel_plus": [["GO-ICP:Method", "Synonym-Of", "globally optimal ICP:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Bustos and Chin [ 6 ] presented guaranteed outlier removal ( GORE ) , which calculates the tight lower bound and tight upper bound for each correspondence and reduces the size of correspondence set by rejecting true outliers .", "ner": [["guaranteed outlier removal", "Method"], ["GORE", "Method"]], "rel": [["GORE", "Synonym-Of", "guaranteed outlier removal"]], "rel_plus": [["GORE:Method", "Synonym-Of", "guaranteed outlier removal:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Motivated by GORE , Li [ 21 ] proposed a polynomial time outlier removal method , which seeks the tight lower and upper bound by calculating the costs of correspondence matrix ( CM ) and augmented correspondence matrix ( ACM ) .", "ner": [["GORE", "Method"], ["correspondence matrix", "Method"], ["CM", "Method"], ["augmented correspondence matrix", "Method"], ["ACM", "Method"]], "rel": [["CM", "Synonym-Of", "correspondence matrix"], ["ACM", "Synonym-Of", "augmented correspondence matrix"]], "rel_plus": [["CM:Method", "Synonym-Of", "correspondence matrix:Method"], ["ACM:Method", "Synonym-Of", "augmented correspondence matrix:Method"]]}
{"doc_id": "CVPR2023", "sentence": "However , BnB techniques are sensitive to the cardinality of the input and are time-consuming for large-scale inputs .", "ner": [["BnB", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In addition to geometric-only methods , recent works also adopt deep learning techniques to perform PCR .", "ner": [["geometric-only methods", "Method"], ["deep learning", "Method"], ["PCR", "Task"]], "rel": [["geometric-only methods", "Used-For", "PCR"], ["deep learning", "Used-For", "PCR"]], "rel_plus": [["geometric-only methods:Method", "Used-For", "PCR:Task"], ["deep learning:Method", "Used-For", "PCR:Task"]]}
{"doc_id": "CVPR2023", "sentence": "FCGF [ 10 ] computes the features in a single pass through a fully convolutional neural network without keypoint detection .", "ner": [["FCGF", "Method"], ["fully convolutional neural network", "Method"], ["keypoint detection", "Task"]], "rel": [["fully convolutional neural network", "Part-Of", "FCGF"]], "rel_plus": [["fully convolutional neural network:Method", "Part-Of", "FCGF:Method"]]}
{"doc_id": "CVPR2023", "sentence": "D3Feat [ 4 ] uses a fully convolutional network to obtain local information of point clouds and a joint learning framework to achieve 3D local feature detection and description .", "ner": [["D3Feat", "Method"], ["fully convolutional network", "Method"], ["joint learning framework", "Method"], ["3D local feature detection", "Task"]], "rel": [["fully convolutional network", "Part-Of", "D3Feat"], ["joint learning framework", "Part-Of", "D3Feat"], ["joint learning framework", "Used-For", "3D local feature detection"]], "rel_plus": [["fully convolutional network:Method", "Part-Of", "D3Feat:Method"], ["joint learning framework:Method", "Part-Of", "D3Feat:Method"], ["joint learning framework:Method", "Used-For", "3D local feature detection:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Predator [ 18 ] applies an attention mechanism to extract salient points in overlapping regions of the point clouds , thus achieving robust registration in the presence of low overlap rates .", "ner": [["Predator", "Method"], ["attention mechanism", "Method"], ["registration", "Task"]], "rel": [["attention mechanism", "Part-Of", "Predator"], ["Predator", "Used-For", "registration"]], "rel_plus": [["attention mechanism:Method", "Part-Of", "Predator:Method"], ["Predator:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Spinnet [ 1 ] extracts local features which are rotationally invariant and sufficiently informative to enable accurate registration .", "ner": [["Spinnet", "Method"], ["registration", "Task"]], "rel": [["Spinnet", "Used-For", "registration"]], "rel_plus": [["Spinnet:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Deep global registration ( DGR ) [ 9 ] and 3DReg-Net [ 27 ] classify a given correspondence by training endto-end neural networks and using operators such as sparse convolution and point-by-point MLP .", "ner": [["Deep global registration", "Method"], ["DGR", "Method"], ["3DReg-Net", "Method"], ["neural networks", "Method"], ["sparse convolution", "Method"], ["point-by-point MLP", "Method"]], "rel": [["DGR", "Synonym-Of", "Deep global registration"], ["neural networks", "Part-Of", "Deep global registration"], ["neural networks", "Part-Of", "3DReg-Net"], ["sparse convolution", "Part-Of", "neural networks"], ["point-by-point MLP", "Part-Of", "neural networks"]], "rel_plus": [["DGR:Method", "Synonym-Of", "Deep global registration:Method"], ["neural networks:Method", "Part-Of", "Deep global registration:Method"], ["neural networks:Method", "Part-Of", "3DReg-Net:Method"], ["sparse convolution:Method", "Part-Of", "neural networks:Method"], ["point-by-point MLP:Method", "Part-Of", "neural networks:Method"]]}
{"doc_id": "CVPR2023", "sentence": "PointDSC [ 3 ] explicitly explores spatial consistency for removing outlier correspondences and 3D point cloud registration .", "ner": [["PointDSC", "Method"], ["point cloud registration", "Task"]], "rel": [["PointDSC", "Used-For", "point cloud registration"]], "rel_plus": [["PointDSC:Method", "Used-For", "point cloud registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Fu et al . [ 14 ] proposed a registration framework that utilizes deep graph matching ( RGM ) that can find robust and accurate pointto-point correspondences .", "ner": [["registration", "Task"], ["deep graph matching", "Method"], ["RGM", "Method"]], "rel": [["deep graph matching", "Used-For", "registration"], ["RGM", "Synonym-Of", "deep graph matching"]], "rel_plus": [["deep graph matching:Method", "Used-For", "registration:Task"], ["RGM:Method", "Synonym-Of", "deep graph matching:Method"]]}
{"doc_id": "CVPR2023", "sentence": "More recently , several methods [ 29 , 43 ] follow the detection-free methods and estimate the transformation in an end-to-end way .", "ner": [["detection-free methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "CoFiNet [ 43 ] extracts correspondences from coarse to fine without keypoint detection .", "ner": [["CoFiNet", "Method"], ["keypoint detection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "GeoTransformer [ 29 ] learns geometric features for robust superpoint matching and is robust in low-overlap cases and invariant to rigid transformation .", "ner": [["GeoTransformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "MAC ( c1 ) = ( c1 , c3 , c4 , c5 ) MAC ( c3 ) = ( c1 , c3 , c4 , c5 ) MAC ( c2 ) = ( c2 , c6 , c7 ) \u2026 MAC ( c9 ) = ( c1 , c9 , c10 ) MAC ( c10 ) = ( c1 , c9 , c10 ) Node-guided maximal cliques c7 c7 c6 c6 c2 c2 c5 c5 c4 c4 c3 c3 c1 c1 R3 , t3 c10 c10 c9 c9 c1 c1 While deep learning techniques have demonstrated a great potential for PCR , these methods require a large amount of training data and their generalization is not always promising .", "ner": [["MAC", "Method"], ["MAC", "Method"], ["MAC", "Method"], ["MAC", "Method"], ["MAC", "Method"], ["Node-guided maximal cliques", "Method"], ["deep learning techniques", "Method"], ["PCR", "Task"]], "rel": [["deep learning techniques", "Used-For", "PCR"]], "rel_plus": [["deep learning techniques:Method", "Used-For", "PCR:Task"]]}
{"doc_id": "CVPR2023", "sentence": "By contrast , MAC does not require any training data and achieves more advanced performance than several deep-learned methods .", "ner": [["MAC", "Method"], ["deep-learned methods", "Method"]], "rel": [["MAC", "Compare-With", "deep-learned methods"]], "rel_plus": [["MAC:Method", "Compare-With", "deep-learned methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Moreover , MAC can be served as a drop-on module in deep learning frameworks to boost their performance .", "ner": [["MAC", "Method"], ["deep learning", "Method"]], "rel": [["MAC", "Part-Of", "deep learning"]], "rel_plus": [["MAC:Method", "Part-Of", "deep learning:Method"]]}
{"doc_id": "CVPR2023", "sentence": "An initial correspondence set C initial = { c } is formed by matching feature descriptors , where c = ( p s , p t ) .", "ner": [["matching feature descriptors", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "MAC estimates the 6 - DoF pose transformation between P s and P t from C initial .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Here , we consider two approaches to construct a compatibility graph . \u2022 First Order Graph .", "ner": [["First Order Graph", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The first order graph ( FOG ) is constructed based on the rigid distance constraint between the correspondence pair ( c i , c j ) , which can be quantitatively measured as : EQUATION The compatibility score between c i and c j is given as : EQUATION where d cmp is a distance parameter .", "ner": [["first order graph", "Method"], ["FOG", "Method"]], "rel": [["FOG", "Synonym-Of", "first order graph"]], "rel_plus": [["FOG:Method", "Synonym-Of", "first order graph:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Since the compatibility graph is undirected , the weight matrix W F OG is symmetric . \u2022 Second Order Graph .", "ner": [["Second Order Graph", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The second order graph ( SOG ) evolves from FOG .", "ner": [["second order graph", "Method"], ["SOG", "Method"], ["FOG", "Method"]], "rel": [["SOG", "Synonym-Of", "second order graph"], ["second order graph", "SubClass-Of", "FOG"]], "rel_plus": [["SOG:Method", "Synonym-Of", "second order graph:Method"], ["second order graph:Method", "SubClass-Of", "FOG:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The weight matrix W SOG can be calculated as : EQUATION where \u2299 represents the element-wise product between two matrices .", "ner": [["SOG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In Sec . 4.5 , we experimentally compare FOG and SOG in our MAC framework .", "ner": [["FOG", "Method"], ["SOG", "Method"], ["MAC", "Method"]], "rel": [["FOG", "Compare-With", "SOG"], ["SOG", "Part-Of", "MAC"], ["FOG", "Part-Of", "MAC"]], "rel_plus": [["FOG:Method", "Compare-With", "SOG:Method"], ["SOG:Method", "Part-Of", "MAC:Method"], ["FOG:Method", "Part-Of", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "A maximal clique is a clique that cannot be extended by adding any nodes .", "ner": [["maximal clique", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In particular , the maximal clique with the most nodes is the maximum clique of a graph .", "ner": [["maximal clique", "Method"], ["maximum clique", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Searching for Maximal cliques .", "ner": [["Maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "To generate hypotheses , RANSAC-based methods repeatedly take random samples from the correspondence set .", "ner": [["RANSAC-based methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Previous works [ 23 , 24 , 28 , 36 ] focus on searching for maximum cliques in the graph , however , the maximum clique is a very tight constraint that only focuses on the global consensus information in a graph .", "ner": [["maximum cliques", "Method"], ["maximum clique", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Instead , we loosen the constraint and leverage maximal cliques to mine more local graph information .", "ner": [["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "By using the igraph maximal cliques function in the igraph 1 C + + library , which makes use of a modified Bron-Kerbosch algorithm [ 12 ] , the search of maximal cliques can be very efficient .", "ner": [["maximal cliques", "Method"], ["Bron-Kerbosch algorithm", "Method"], ["maximal cliques", "Method"]], "rel": [["Bron-Kerbosch algorithm", "Used-For", "maximal cliques"]], "rel_plus": [["Bron-Kerbosch algorithm:Method", "Used-For", "maximal cliques:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Node-guided Clique Selection .", "ner": [["Node-guided Clique Selection", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "First , we calculate the weight for each clique in MAC initial .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Given a clique C i = ( V i , E i ) , the weight w Ci is calculated as : EQUATION where w ej represents the weight of edge e j in W SOG .", "ner": [["SOG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "A node may be included by multiple maximal cliques and we 1 https : / / igraph.org only retain the one with the greatest weight for that node .", "ner": [["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Then , duplicated cliques are removed from the rest , obtaining MAC selected .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We could send these maximal cliques directly to the following stages for 3D registration .", "ner": [["maximal cliques", "Method"], ["3D registration", "Task"]], "rel": [["maximal cliques", "Used-For", "3D registration"]], "rel_plus": [["maximal cliques:Method", "Used-For", "3D registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "However , when | V | is quite large , the number of retained maximal cliques can still be very large .", "ner": [["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Here , we propose several techniques to further filter the maximal cliques . \u2022 Normal consistency .", "ner": [["maximal cliques", "Method"], ["Normal consistency", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In the maximal cliques , we find that the normal consistency is satisfied between each correspondence .", "ner": [["maximal cliques", "Method"], ["normal consistency", "Method"]], "rel": [["normal consistency", "Part-Of", "maximal cliques"]], "rel_plus": [["normal consistency:Method", "Part-Of", "maximal cliques:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The following inequality ought to hold if c i and c j are normal consistent : EQUATION where t \u03b1 is a threshold for determining whether the angular differences are similar . \u2022 Clique ranking .", "ner": [["Clique ranking", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We organize MAC selected in a descending order using the clique's weight w Ci .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Each maximal clique filtered from the previous step represents a consistent set of correspondences .", "ner": [["maximal clique", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "By applying the SVD algorithm to each consistency set , we can obtain a set of 6 - DoF pose hypotheses . \u2022 Instance-equal SVD .", "ner": [["SVD", "Method"], ["Instance-equal SVD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Transformation estimation of correspondences is often implemented with SVD .", "ner": [["SVD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Instance-equal means that the weights of all correspondences are equal . \u2022 Weighted SVD .", "ner": [["Weighted SVD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Assigning weights to correspondences is commonly adopted by recent PCR methods [ 8 , 9 , 27 , 29 ] .", "ner": [["PCR", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Here , we take the primary eigenvalues of W SOG as correspondence weights .", "ner": [["SOG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The final goal of MAC is to estimate the optimal 6 - DoF rigid transformation ( composed of a rotation pose R * \u2208 SO ( 3 ) and a translation pose t * \u2208 R 3 ) that maximizes the objective function as follow : EQUATION where c i \u2208 C initial , N = | C initial | , and s ( c i ) represents the score of c i .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We consider several RANSAC hypothesis evaluation metrics here [ 40 ] , including mean average error ( MAE ) , mean square error ( MSE ) and inlier count .", "ner": [["RANSAC", "Method"], ["mean average error", "Method"], ["MAE", "Method"], ["mean square error", "Method"], ["MSE", "Method"], ["inlier count", "Method"]], "rel": [["MAE", "Synonym-Of", "mean average error"], ["MSE", "Synonym-Of", "mean square error"]], "rel_plus": [["MAE:Method", "Synonym-Of", "mean average error:Method"], ["MSE:Method", "Synonym-Of", "mean square error:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The best hypothesis is taken to perform 3D registration then .", "ner": [["3D registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We consider four datasets , i.e , the objectscale dataset U3M [ 26 ] , the scene-scale indoor datasets 3DMatch [ 44 ] & 3DLoMatch [ 18 ] , and the scene-scale outdoor dataset KITTI [ 15 ] .", "ner": [["U3M", "Dataset"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"], ["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "U3M has 496 point cloud pairs . 3DLoMatch is the subset of 3DMatch , where the overlap rate of the point cloud pairs ranges from 10% to 30% , which is very challenging .", "ner": [["U3M", "Dataset"], ["3DLoMatch", "Dataset"], ["3DMatch", "Dataset"]], "rel": [["3DLoMatch", "SubClass-Of", "3DMatch"]], "rel_plus": [["3DLoMatch:Dataset", "SubClass-Of", "3DMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "For KITTI , we follow [ 3 , 8 ] and obtain 555 pairs of point clouds for testing .", "ner": [["KITTI", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In addition , we employ the rotation error ( RE ) and translation error ( TE ) to evaluate the registration results on the scene-scale dataset .", "ner": [["registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "By referring to the settings in [ 9 ] , the registration is considered successful when the RE \u2264 15 \u00b0 , TE \u2264 30 cm on 3DMatch & 3DLoMatch datasets , and RE \u2264 5 \u00b0 , TE \u2264 60 cm on KITTI dataset .", "ner": [["registration", "Task"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"], ["KITTI", "Dataset"]], "rel": [["3DMatch", "Benchmark-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["KITTI", "Benchmark-For", "registration"]], "rel_plus": [["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["KITTI:Dataset", "Benchmark-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "We define a dataset's registration accuracy as the ratio of success cases to the number of point cloud pairs to be registered .", "ner": [["registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Our method is implemented in C + + based on the point cloud library ( PCL ) [ 32 ] and igraph library .", "ner": [["point cloud library", "Method"], ["PCL", "Method"]], "rel": [["PCL", "Synonym-Of", "point cloud library"]], "rel_plus": [["PCL:Method", "Synonym-Of", "point cloud library:Method"]]}
{"doc_id": "CVPR2023", "sentence": "For U3M , we use the Harris3D ( H3D ) [ 33 ] keypoint detector and the signatures of histograms of orientation ( SHOT ) [ 34 ] descriptor for initial correspondence generation as in [ 42 ] .", "ner": [["U3M", "Dataset"], ["Harris3D", "Method"], ["H3D", "Method"], ["signatures of histograms of orientation", "Method"], ["SHOT", "Method"]], "rel": [["Harris3D", "Evaluated-With", "U3M"], ["signatures of histograms of orientation", "Evaluated-With", "U3M"], ["H3D", "Synonym-Of", "Harris3D"], ["SHOT", "Synonym-Of", "signatures of histograms of orientation"]], "rel_plus": [["Harris3D:Method", "Evaluated-With", "U3M:Dataset"], ["signatures of histograms of orientation:Method", "Evaluated-With", "U3M:Dataset"], ["H3D:Method", "Synonym-Of", "Harris3D:Method"], ["SHOT:Method", "Synonym-Of", "signatures of histograms of orientation:Method"]]}
{"doc_id": "CVPR2023", "sentence": "For 3DMatch and 3DLoMatch datasets , we use the fast point features histograms ( FPFH ) [ 31 ] descriptor and fully convolutional geometric features ( FCGF ) [ 10 ] descriptor to generate the initial correspondence set .", "ner": [["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"], ["fast point features histograms", "Method"], ["FPFH", "Method"], ["fully convolutional geometric features", "Method"], ["FCGF", "Method"]], "rel": [["fully convolutional geometric features", "Evaluated-With", "3DMatch"], ["fast point features histograms", "Evaluated-With", "3DMatch"], ["fast point features histograms", "Evaluated-With", "3DLoMatch"], ["fully convolutional geometric features", "Evaluated-With", "3DLoMatch"], ["FPFH", "Synonym-Of", "fast point features histograms"], ["FCGF", "Synonym-Of", "fully convolutional geometric features"]], "rel_plus": [["fully convolutional geometric features:Method", "Evaluated-With", "3DMatch:Dataset"], ["fast point features histograms:Method", "Evaluated-With", "3DMatch:Dataset"], ["fast point features histograms:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["fully convolutional geometric features:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FPFH:Method", "Synonym-Of", "fast point features histograms:Method"], ["FCGF:Method", "Synonym-Of", "fully convolutional geometric features:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Here , the following methods are tested , including SAC-COT [ 39 ] , OSAC [ 37 ] , SAC-IA [ 31 ] , RANSAC [ 13 ] , SC 2 - PCR [ 8 ] , FGR [ 45 ] , GO-ICP [ 41 ] , and PPF [ 11 ] , where the former four are RANSAC-based methods .", "ner": [["SAC-COT", "Method"], ["OSAC", "Method"], ["SAC-IA", "Method"], ["RANSAC", "Method"], ["SC 2 - PCR", "Method"], ["FGR", "Method"], ["GO-ICP", "Method"], ["PPF", "Method"], ["RANSAC-based methods", "Method"]], "rel": [["SAC-COT", "SubClass-Of", "RANSAC-based methods"], ["OSAC", "SubClass-Of", "RANSAC-based methods"], ["SAC-IA", "SubClass-Of", "RANSAC-based methods"], ["RANSAC", "SubClass-Of", "RANSAC-based methods"]], "rel_plus": [["SAC-COT:Method", "SubClass-Of", "RANSAC-based methods:Method"], ["OSAC:Method", "SubClass-Of", "RANSAC-based methods:Method"], ["SAC-IA:Method", "SubClass-Of", "RANSAC-based methods:Method"], ["RANSAC:Method", "SubClass-Of", "RANSAC-based methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The results indicate that MAC performs best and significantly outperforms all tested RANSAC-fashion estimators , such as SAC-COT , OSAC , SAC-IA , and RANSAC .", "ner": [["MAC", "Method"], ["RANSAC-fashion estimators", "Method"], ["SAC-COT", "Method"], ["OSAC", "Method"], ["SAC-IA", "Method"], ["RANSAC", "Method"]], "rel": [["MAC", "Compare-With", "RANSAC-fashion estimators"], ["SAC-COT", "SubClass-Of", "RANSAC-fashion estimators"], ["OSAC", "SubClass-Of", "RANSAC-fashion estimators"], ["SAC-IA", "SubClass-Of", "RANSAC-fashion estimators"], ["RANSAC", "SubClass-Of", "RANSAC-fashion estimators"], ["MAC", "Compare-With", "SAC-COT"], ["MAC", "Compare-With", "OSAC"], ["MAC", "Compare-With", "SAC-IA"], ["MAC", "Compare-With", "RANSAC"]], "rel_plus": [["MAC:Method", "Compare-With", "RANSAC-fashion estimators:Method"], ["SAC-COT:Method", "SubClass-Of", "RANSAC-fashion estimators:Method"], ["OSAC:Method", "SubClass-Of", "RANSAC-fashion estimators:Method"], ["SAC-IA:Method", "SubClass-Of", "RANSAC-fashion estimators:Method"], ["RANSAC:Method", "SubClass-Of", "RANSAC-fashion estimators:Method"], ["MAC:Method", "Compare-With", "SAC-COT:Method"], ["MAC:Method", "Compare-With", "OSAC:Method"], ["MAC:Method", "Compare-With", "SAC-IA:Method"], ["MAC:Method", "Compare-With", "RANSAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The registration performance of MAC based on the MAE evaluation metric is the best on U3M .", "ner": [["registration", "Task"], ["MAC", "Method"], ["MAE", "Method"], ["U3M", "Dataset"]], "rel": [["MAC", "Used-For", "registration"], ["U3M", "Benchmark-For", "registration"], ["MAE", "Part-Of", "MAC"], ["MAC", "Evaluated-With", "U3M"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"], ["U3M:Dataset", "Benchmark-For", "registration:Task"], ["MAE:Method", "Part-Of", "MAC:Method"], ["MAC:Method", "Evaluated-With", "U3M:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "PCR methods comparison .", "ner": [["PCR", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Both geometric-only and deep-learned methods are considered for comparison , including SM [ 20 ] , FGR [ 45 ] , RANSAC [ 13 ] , TEASER + + [ 36 ] , CG-SAC [ 30 ] , SC 2 - PCR [ 8 ] , 3DRegNet [ 27 ] , DGR [ 9 ] , DHVR [ 19 ] and PointDSC [ 3 ] .", "ner": [["geometric-only", "Method"], ["deep-learned methods", "Method"], ["SM", "Method"], ["FGR", "Method"], ["RANSAC", "Method"], ["TEASER + +", "Method"], ["CG-SAC", "Method"], ["SC 2 - PCR", "Method"], ["3DRegNet", "Method"], ["DGR", "Method"], ["DHVR", "Method"], ["PointDSC", "Method"]], "rel": [["geometric-only", "Compare-With", "deep-learned methods"]], "rel_plus": [["geometric-only:Method", "Compare-With", "deep-learned methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The following conclusions can be made : 1 ) regardless of which descriptor is used , MAC outperforms all compared methods on both 3DMatch and 3DLoMatch datasets , indicating its strong ability to register indoor scene point clouds ; 2 ) even compared with deep-learned methods , MAC still achieves better performance without any data training ; 3 ) in addition to the registration recall ( RR ) metric , MAC achieves the best RE and TE metrics .", "ner": [["MAC", "Method"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"], ["deep-learned methods", "Method"], ["MAC", "Method"], ["registration", "Task"], ["MAC", "Method"]], "rel": [["MAC", "Evaluated-With", "3DMatch"], ["MAC", "Evaluated-With", "3DLoMatch"], ["MAC", "Compare-With", "deep-learned methods"], ["MAC", "Used-For", "registration"]], "rel_plus": [["MAC:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAC:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["MAC:Method", "Compare-With", "deep-learned methods:Method"], ["MAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "This indicates that registrations by MAC are very accurate and MAC is able to align low overlapping data .", "ner": [["registrations", "Task"], ["MAC", "Method"], ["MAC", "Method"]], "rel": [["MAC", "Used-For", "registrations"]], "rel_plus": [["MAC:Method", "Used-For", "registrations:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Boosting deep-learned methods with MAC .", "ner": [["deep-learned methods", "Method"], ["MAC", "Method"]], "rel": [["MAC", "Part-Of", "deep-learned methods"]], "rel_plus": [["MAC:Method", "Part-Of", "deep-learned methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Several kinds of state-of-the-art deep-learned methods are integrated with MAC for evaluation .", "ner": [["deep-learned methods", "Method"], ["MAC", "Method"]], "rel": [["MAC", "Part-Of", "deep-learned methods"]], "rel_plus": [["MAC:Method", "Part-Of", "deep-learned methods:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The considered methods are FCGF [ 10 ] , SpinNet [ 1 ] , Predator [ 18 ] , CoFiNet [ 43 ] and Geo-Transformer [ 29 ] .", "ner": [["FCGF", "Method"], ["SpinNet", "Method"], ["Predator", "Method"], ["CoFiNet", "Method"], ["Geo-Transformer", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Remarkably , MAC dramatically improves the registration recall under all tested methods on both 3DMatch and 3DLoMatch datasets .", "ner": [["MAC", "Method"], ["registration", "Task"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"]], "rel": [["MAC", "Used-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["3DMatch", "Benchmark-For", "registration"], ["MAC", "Evaluated-With", "3DMatch"], ["MAC", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["MAC:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAC:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Notably , the performance of Spin-Net , Predator and CoFiNet after boosting by MAC exceeds In Table 4 , the results of DGR [ 9 ] , PointDSC [ 3 ] , TEASER + + [ 36 ] , RANSAC [ 13 ] , CG-SAC [ 30 ] , SC 2 - PCR [ 8 ] and MAC are reported for comparison .", "ner": [["Spin-Net", "Method"], ["Predator", "Method"], ["CoFiNet", "Method"], ["MAC", "Method"], ["DGR", "Method"], ["PointDSC", "Method"], ["TEASER + +", "Method"], ["RANSAC", "Method"], ["CG-SAC", "Method"], ["SC 2 - PCR", "Method"], ["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "As shown by the table , in terms of the registration recall performance , MAC presents the best and is tied for the best results with FPFH and FCGF descriptor settings , respectively .", "ner": [["registration", "Task"], ["MAC", "Method"], ["FPFH", "Method"], ["FCGF", "Method"]], "rel": [["MAC", "Used-For", "registration"], ["FPFH", "Part-Of", "MAC"], ["FCGF", "Part-Of", "MAC"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"], ["FPFH:Method", "Part-Of", "MAC:Method"], ["FCGF:Method", "Part-Of", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "MAC also has a lower TE than the state-ofthe-art geometric-only method SC 2 - PCR .", "ner": [["MAC", "Method"], ["geometric-only method", "Method"], ["SC 2 - PCR", "Method"]], "rel": [["MAC", "Compare-With", "geometric-only method"], ["SC 2 - PCR", "SubClass-Of", "geometric-only method"], ["MAC", "Compare-With", "SC 2 - PCR"]], "rel_plus": [["MAC:Method", "Compare-With", "geometric-only method:Method"], ["SC 2 - PCR:Method", "SubClass-Of", "geometric-only method:Method"], ["MAC:Method", "Compare-With", "SC 2 - PCR:Method"]]}
{"doc_id": "CVPR2023", "sentence": "The registration experiments on the object , indoor scene , and outdoor scene datasets consistently verify that MAC holds good generalization ability in different application contexts .", "ner": [["registration", "Task"], ["MAC", "Method"]], "rel": [["MAC", "Used-For", "registration"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "In this section , we perform ablation studies and analysis experiments on both 3DMatch and 3DLoMatch datasets .", "ner": [["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Table 8 presents the time efficiency analysis of MAC .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Performing feature matching selection .", "ner": [["feature matching selection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Before 3D registration , a popular way is to perform outlier rejection to reduce the correspondence set .", "ner": [["3D registration", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Here we employ geometric consistency ( GC ) [ 7 ] , which is independent of the feature space and associates the largest consistent cluster relating to the compatibility among correspondences .", "ner": [["geometric consistency", "Method"], ["GC", "Method"]], "rel": [["GC", "Synonym-Of", "geometric consistency"]], "rel_plus": [["GC:Method", "Synonym-Of", "geometric consistency:Method"]]}
{"doc_id": "CVPR2023", "sentence": "This demonstrates that MAC can still perform well even if the initial correspondence set is directly utilized as input without any filtering .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We test the performance of MAC by using different graph construction approaches .", "ner": [["MAC", "Method"], ["graph construction approaches", "Method"]], "rel": [["graph construction approaches", "Part-Of", "MAC"]], "rel_plus": [["graph construction approaches:Method", "Part-Of", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Also , the registration recall obtained by using SOG is 0.12% higher than using FOG when combined with FPFH , and 0.56% higher when combined with FCGF on 3DLoMatch .", "ner": [["registration", "Task"], ["SOG", "Method"], ["FOG", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DLoMatch", "Dataset"]], "rel": [["SOG", "Used-For", "registration"], ["FOG", "Used-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["FPFH", "Part-Of", "SOG"], ["FCGF", "Part-Of", "SOG"], ["SOG", "Compare-With", "FOG"], ["FPFH", "Part-Of", "FOG"], ["FCGF", "Part-Of", "FOG"], ["SOG", "Evaluated-With", "3DLoMatch"], ["FOG", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["SOG:Method", "Used-For", "registration:Task"], ["FOG:Method", "Used-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["FPFH:Method", "Part-Of", "SOG:Method"], ["FCGF:Method", "Part-Of", "SOG:Method"], ["SOG:Method", "Compare-With", "FOG:Method"], ["FPFH:Method", "Part-Of", "FOG:Method"], ["FCGF:Method", "Part-Of", "FOG:Method"], ["SOG:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FOG:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Therefore , SOG is more suitable for MAC .", "ner": [["SOG", "Method"], ["MAC", "Method"]], "rel": [["SOG", "Part-Of", "MAC"]], "rel_plus": [["SOG:Method", "Part-Of", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Maximum or maximal clique .", "ner": [["maximal clique", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "To justify the advantages of maximal cliques , we change the search strategy of MAC to the maximum cliques and test the registration performance .", "ner": [["maximal cliques", "Method"], ["MAC", "Method"], ["maximum cliques", "Method"], ["registration", "Task"]], "rel": [["MAC", "Used-For", "registration"], ["maximum cliques", "Used-For", "registration"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"], ["maximum cliques:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "As shown in Row 1 and 9 in Table 5 , applying maximal cliques surpasses maximum by 9.8% when combined with FPFH , and 5.55% higher when combined with FCGF on 3DMatch .", "ner": [["maximal cliques", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DMatch", "Dataset"]], "rel": [["FPFH", "Compare-With", "FCGF"], ["FPFH", "Evaluated-With", "3DMatch"], ["FCGF", "Evaluated-With", "3DMatch"]], "rel_plus": [["FPFH:Method", "Compare-With", "FCGF:Method"], ["FPFH:Method", "Evaluated-With", "3DMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Besides , the registration recall obtained by using maximal cliques is 8.03% higher than using the maximum cliques when combined with FPFH and 10.45% higher when combined with FCGF on 3DLoMatch .", "ner": [["registration", "Task"], ["maximal cliques", "Method"], ["maximum cliques", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DLoMatch", "Dataset"]], "rel": [["maximal cliques", "Used-For", "registration"], ["FCGF", "Part-Of", "maximal cliques"], ["FPFH", "Part-Of", "maximal cliques"], ["FPFH", "Part-Of", "maximum cliques"], ["FCGF", "Part-Of", "maximum cliques"], ["3DLoMatch", "Evaluated-With", "maximum cliques"], ["FPFH", "Compare-With", "FCGF"], ["FPFH", "Evaluated-With", "3DLoMatch"], ["FCGF", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["maximal cliques:Method", "Used-For", "registration:Task"], ["FCGF:Method", "Part-Of", "maximal cliques:Method"], ["FPFH:Method", "Part-Of", "maximal cliques:Method"], ["FPFH:Method", "Part-Of", "maximum cliques:Method"], ["FCGF:Method", "Part-Of", "maximum cliques:Method"], ["3DLoMatch:Dataset", "Evaluated-With", "maximum cliques:Method"], ["FPFH:Method", "Compare-With", "FCGF:Method"], ["FPFH:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "There are several reasons for this : 1 ) maximal cliques include the maximum cliques and additionally consider local graph constraints , so the search for maximal cliques can make use of both local and global information in the compatibility graph ; 2 ) the maximum clique is a very tight constraint which requires maximizing the number of mutually compatible correspondences , but it does not guarantee the opti-mal result .", "ner": [["maximal cliques", "Method"], ["maximum cliques", "Method"], ["maximal cliques", "Method"], ["maximum clique", "Method"]], "rel": [["maximum cliques", "Part-Of", "maximal cliques"]], "rel_plus": [["maximum cliques:Method", "Part-Of", "maximal cliques:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Node-guided clique selection .", "ner": [["Node-guided clique selection", "Task"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We compare the performance with and without node-guided ( NG ) clique selection for maximal cliques search .", "ner": [["node-guided", "Method"], ["NG", "Method"], ["maximal cliques", "Method"]], "rel": [["NG", "Synonym-Of", "node-guided"]], "rel_plus": [["NG:Method", "Synonym-Of", "node-guided:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Comparing Row 1 and 4 in Table 5 , using NG achieves a recall improvement of 0.37% when combined with FPFH , and 0.5% improvement when combined with FCGF on 3DMatch .", "ner": [["NG", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DMatch", "Dataset"]], "rel": [["FPFH", "Part-Of", "NG"], ["FCGF", "Part-Of", "NG"], ["NG", "Evaluated-With", "3DMatch"], ["FPFH", "Evaluated-With", "3DMatch"], ["FCGF", "Evaluated-With", "3DMatch"]], "rel_plus": [["FPFH:Method", "Part-Of", "NG:Method"], ["FCGF:Method", "Part-Of", "NG:Method"], ["NG:Method", "Evaluated-With", "3DMatch:Dataset"], ["FPFH:Method", "Evaluated-With", "3DMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Also , using NG achieves a recall improvement of 0.23% with FPFH and 0.73% improvement with FCGF on 3DLoMatch .", "ner": [["NG", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DLoMatch", "Dataset"]], "rel": [["FPFH", "Part-Of", "NG"], ["FCGF", "Part-Of", "NG"], ["NG", "Evaluated-With", "3DLoMatch"], ["FPFH", "Evaluated-With", "3DLoMatch"], ["FCGF", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["FPFH:Method", "Part-Of", "NG:Method"], ["FCGF:Method", "Part-Of", "NG:Method"], ["NG:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FPFH:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "It is worth noting that while NG improves recall , the mean RE and mean TE are also decreasing .", "ner": [["NG", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "For example , NG reduces the mean RE by 0.1 \u00b0 and the mean TE by 0.11 cm with FPFH on 3DLoMatch .", "ner": [["NG", "Method"], ["FPFH", "Method"], ["3DLoMatch", "Dataset"]], "rel": [["FPFH", "Part-Of", "NG"], ["NG", "Evaluated-With", "3DLoMatch"], ["FPFH", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["FPFH:Method", "Part-Of", "NG:Method"], ["NG:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FPFH:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "Different approaches for clique filtering .", "ner": [["clique filtering", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We test the effectiveness of the two filtering methods , normal consistency and clique ranking . 1 ) Normal consistency : comparing Row 1 and 8 in Table 5 , NC slightly degrades MAC's performance . 2 ) Clique ranking : Row 10 to 14 demonstrate that the registration recall tends to increase as K increases , suggesting that larger K yields a subset of cliques that generate more correct hypotheses .", "ner": [["normal consistency", "Method"], ["clique ranking", "Method"], ["Normal consistency", "Method"], ["NC", "Method"], ["MAC's", "Method"], ["Clique ranking", "Method"], ["registration", "Task"]], "rel": [["NC", "Part-Of", "MAC's"], ["Clique ranking", "Used-For", "registration"]], "rel_plus": [["NC:Method", "Part-Of", "MAC's:Method"], ["Clique ranking:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Employing instance-equal or weighted SVD .", "ner": [["weighted SVD", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The comparisons of instance-equal and weighted SVD are shown in Rows 1 and 5 of Table 5 Weighted SVD is slightly inferior to instance-equal SVD .", "ner": [["weighted SVD", "Method"], ["Weighted SVD", "Method"], ["instance-equal SVD", "Method"]], "rel": [["Weighted SVD", "Compare-With", "instance-equal SVD"]], "rel_plus": [["Weighted SVD:Method", "Compare-With", "instance-equal SVD:Method"]]}
{"doc_id": "CVPR2023", "sentence": "This suggests that samples in MACs are already very consistent , indicating no additional weighting strategies are required .", "ner": [["MACs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Here we compare three evaluation metrics , including MAE , MSE and inlier count , for MAC hypothesis evaluation .", "ner": [["MAE", "Method"], ["MSE", "Method"], ["inlier count", "Method"], ["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "As shown in Row 1 , 6 and 7 , MAC with MAE achieves the best performance .", "ner": [["MAC", "Method"], ["MAE", "Method"]], "rel": [["MAE", "Part-Of", "MAC"]], "rel_plus": [["MAE:Method", "Part-Of", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "In Table 5 , MAE achieves a recall improvement of 0.24% when combined with FPFH , and 0.31% improvement when combined with FCGF on 3DMatch compared with the commonly used inlier count metric .", "ner": [["MAE", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DMatch", "Dataset"], ["inlier count", "Method"]], "rel": [["FPFH", "Part-Of", "MAE"], ["FCGF", "Part-Of", "MAE"], ["FPFH", "Compare-With", "FCGF"], ["FPFH", "Evaluated-With", "3DMatch"], ["FCGF", "Evaluated-With", "3DMatch"], ["MAE", "Evaluated-With", "3DMatch"], ["inlier count", "Evaluated-With", "3DMatch"], ["MAE", "Compare-With", "inlier count"]], "rel_plus": [["FPFH:Method", "Part-Of", "MAE:Method"], ["FCGF:Method", "Part-Of", "MAE:Method"], ["FPFH:Method", "Compare-With", "FCGF:Method"], ["FPFH:Method", "Evaluated-With", "3DMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAE:Method", "Evaluated-With", "3DMatch:Dataset"], ["inlier count:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAE:Method", "Compare-With", "inlier count:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Also , MAE has a 1.74% improvement when combined with FPFH , and 0.05% when combined with FCGF on 3DLoMatch compared with inlier count .", "ner": [["MAE", "Method"], ["FPFH", "Method"], ["FCGF", "Method"], ["3DLoMatch", "Dataset"], ["inlier count", "Method"]], "rel": [["FPFH", "Part-Of", "MAE"], ["FCGF", "Part-Of", "MAE"], ["FPFH", "Compare-With", "FCGF"], ["MAE", "Evaluated-With", "3DLoMatch"], ["FCGF", "Evaluated-With", "3DLoMatch"], ["inlier count", "Evaluated-With", "3DLoMatch"], ["MAE", "Compare-With", "inlier count"]], "rel_plus": [["FPFH:Method", "Part-Of", "MAE:Method"], ["FCGF:Method", "Part-Of", "MAE:Method"], ["FPFH:Method", "Compare-With", "FCGF:Method"], ["MAE:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["FCGF:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["inlier count:Method", "Evaluated-With", "3DLoMatch:Dataset"], ["MAE:Method", "Compare-With", "inlier count:Method"]]}
{"doc_id": "CVPR2023", "sentence": "MAE is also very effective in reducing RE and TE .", "ner": [["MAE", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Comparison with RANSAC hypotheses .", "ner": [["RANSAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We evaluate the quality of the generated hypotheses by comparing the hypotheses from RANSAC and MAC with the ground truth transformation .", "ner": [["RANSAC", "Method"], ["MAC", "Method"]], "rel": [["RANSAC", "Compare-With", "MAC"]], "rel_plus": [["RANSAC:Method", "Compare-With", "MAC:Method"]]}
{"doc_id": "CVPR2023", "sentence": "Compared to RANSAC , which randomly selects correspondences and generates hypotheses from the correspondence set without geometric constraints , MAC effectively generates more convincing hypotheses from maximal cliques in the compatibility graph , which fully exploits the consensus information in the graph .", "ner": [["RANSAC", "Method"], ["MAC", "Method"], ["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The performance upper bound of MAC .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "This can test the performance upper bound of MAC .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "We vary the judging threshold for the number of generated correct hypotheses and report the results in Impressively , MAC - 1 achieves registration recalls of 98.46% / 91.24% on 3DMatch / 3DLoMatch .", "ner": [["MAC", "Method"], ["registration", "Task"], ["3DMatch", "Dataset"], ["3DLoMatch", "Dataset"]], "rel": [["MAC", "Used-For", "registration"], ["3DLoMatch", "Benchmark-For", "registration"], ["3DMatch", "Benchmark-For", "registration"], ["MAC", "Evaluated-With", "3DMatch"], ["MAC", "Evaluated-With", "3DLoMatch"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"], ["3DLoMatch:Dataset", "Benchmark-For", "registration:Task"], ["3DMatch:Dataset", "Benchmark-For", "registration:Task"], ["MAC:Method", "Evaluated-With", "3DMatch:Dataset"], ["MAC:Method", "Evaluated-With", "3DLoMatch:Dataset"]]}
{"doc_id": "CVPR2023", "sentence": "This indicates that even on low overlapping datasets , MAC is able to produce correct hypotheses for most point cloud pairs .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "In addition , we can deduce that MAC's performance can be further improved with better hypothesis evaluation metrics .", "ner": [["MAC's", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Time consumption of MAC .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "The following observations can be made . 1 ) In general , MAC can complete 3D registration in only tens of milliseconds when the number of correspondences is smaller than 1000 .", "ner": [["MAC", "Method"], ["3D registration", "Task"]], "rel": [["MAC", "Used-For", "3D registration"]], "rel_plus": [["MAC:Method", "Used-For", "3D registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Note that MAC is implemented on the CPU only . 2 ) As the number of correspondences increases from 250 to 2500 , there is an increase in time cost for graph construction due to W SOG computation taking more time . 3 ) When the number of correspondences reaches 5000 , there is a large rise in the time cost of MAC's registration .", "ner": [["MAC", "Method"], ["SOG", "Method"], ["MAC's", "Method"], ["registration", "Task"]], "rel": [["MAC's", "Used-For", "registration"]], "rel_plus": [["MAC's:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "The significant increase in the input size makes the search for maximal cliques more timeconsuming .", "ner": [["maximal cliques", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "However , MAC is not sensitive to the cardinality of the input correspondence set , as verified in Table 3 .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "Hence , using sparse inputs for MAC can produce outstanding performance while making registration efficient .", "ner": [["MAC", "Method"], ["registration", "Task"]], "rel": [["MAC", "Used-For", "registration"]], "rel_plus": [["MAC:Method", "Used-For", "registration:Task"]]}
{"doc_id": "CVPR2023", "sentence": "In this paper , we presented MAC to solve PCR by using the maximal clique constraint to generate precise pose hypotheses from correspondences .", "ner": [["MAC", "Method"], ["PCR", "Task"], ["maximal clique", "Method"]], "rel": [["maximal clique", "Part-Of", "MAC"], ["MAC", "Used-For", "PCR"]], "rel_plus": [["maximal clique:Method", "Part-Of", "MAC:Method"], ["MAC:Method", "Used-For", "PCR:Task"]]}
{"doc_id": "CVPR2023", "sentence": "Our method achieves state-of-the-art performance on all tested datasets and can adapt to deep-learned methods to boost their performance .", "ner": [["deep-learned methods", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "CVPR2023", "sentence": "As shown in Table 7 and Table 1 , MAC produces accurate hypotheses but may fail to find them .", "ner": [["MAC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDIFF : COARSE-GRAINED DIFFUSION FOR METAL-ORGANIC FRAMEWORK DESIGN Metal-organic frameworks ( MOFs ) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry .", "ner": [["MOFDIFF", "Method"], ["COARSE-GRAINED DIFFUSION", "Method"], ["METAL-ORGANIC FRAMEWORK DESIGN", "Task"], ["Metal-organic frameworks", "Method"], ["MOFs", "Method"], ["gas storage", "Task"], ["carbon capture", "Task"]], "rel": [["COARSE-GRAINED DIFFUSION", "Used-For", "METAL-ORGANIC FRAMEWORK DESIGN"], ["MOFs", "Synonym-Of", "Metal-organic frameworks"], ["Metal-organic frameworks", "Used-For", "gas storage"], ["Metal-organic frameworks", "Used-For", "carbon capture"]], "rel_plus": [["COARSE-GRAINED DIFFUSION:Method", "Used-For", "METAL-ORGANIC FRAMEWORK DESIGN:Task"], ["MOFs:Method", "Synonym-Of", "Metal-organic frameworks:Method"], ["Metal-organic frameworks:Method", "Used-For", "gas storage:Task"], ["Metal-organic frameworks:Method", "Used-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "In this work , we propose MOFDiff : a coarse-grained ( CG ) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks .", "ner": [["MOFDiff", "Method"], ["coarse-grained ( CG ) diffusion model", "Method"], ["CG MOF", "Task"]], "rel": [["MOFDiff", "SubClass-Of", "coarse-grained ( CG ) diffusion model"], ["MOFDiff", "Used-For", "CG MOF"]], "rel_plus": [["MOFDiff:Method", "SubClass-Of", "coarse-grained ( CG ) diffusion model:Method"], ["MOFDiff:Method", "Used-For", "CG MOF:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Equivariant graph neural networks are used for the diffusion model to respect the permutational and roto-translational symmetries .", "ner": [["Equivariant graph neural networks", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "A denoising diffusion process conditional on z generates the building block identities and coordinates .", "ner": [["denoising diffusion process", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "To address the challenges above , we propose MOFDiff , a coarse-grained diffusion model for generating 3D MOF structures that leverages the modular and hierarchical structure of MOFs ( Figure 1 ( a ) ) .", "ner": [["MOFDiff", "Method"], ["coarse-grained diffusion model", "Method"]], "rel": [["MOFDiff", "SubClass-Of", "coarse-grained diffusion model"]], "rel_plus": [["MOFDiff:Method", "SubClass-Of", "coarse-grained diffusion model:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We train MOFDiff on BW-DB and use MOFDiff to generate and optimize MOF structures for carbon capture .", "ner": [["MOFDiff", "Method"], ["BW-DB", "Dataset"], ["MOFDiff", "Method"], ["carbon capture", "Task"]], "rel": [["MOFDiff", "Trained-With", "BW-DB"], ["MOFDiff", "Used-For", "carbon capture"], ["BW-DB", "Benchmark-For", "carbon capture"]], "rel_plus": [["MOFDiff:Method", "Trained-With", "BW-DB:Dataset"], ["MOFDiff:Method", "Used-For", "carbon capture:Task"], ["BW-DB:Dataset", "Benchmark-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We propose to learn a contrastive embedding to represent the vast building block design space . \u2022 We formulate a diffusion process for generating coarse-grained MOF 3D structures .", "ner": [["contrastive embedding", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "In order to construct a compact representation of building blocks for diffusion-based modeling , we use a contrastive learning approach [ Hadsell et al . , 2006 , Chen et al . , 2020 ] to embed building blocks into a low dimensional latent space .", "ner": [["diffusion-based modeling", "Task"], ["contrastive learning", "Method"]], "rel": [["contrastive learning", "Used-For", "diffusion-based modeling"]], "rel_plus": [["contrastive learning:Method", "Used-For", "diffusion-based modeling:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "A building block i is encoded as a vector b i using a GemNet-OC encoder [ Gasteiger et al . , 2021 [ Gasteiger et al . , , 2022 ] ] , an SE ( 3 ) - invariant graph neural network model .", "ner": [["GemNet-OC", "Method"], ["graph neural network", "Method"]], "rel": [["GemNet-OC", "SubClass-Of", "graph neural network"]], "rel_plus": [["GemNet-OC:Method", "SubClass-Of", "graph neural network:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We then train the GNN building block encoder using a contrastive loss to map small geometric variations of the same building block to similar latent vectors in the embedding space .", "ner": [["GNN building block encoder", "Method"], ["contrastive loss", "Method"]], "rel": [["contrastive loss", "Part-Of", "GNN building block encoder"]], "rel_plus": [["contrastive loss:Method", "Part-Of", "GNN building block encoder:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "The projected embedding is obtained by projecting the building block embedding b i using a multi-layer perceptron ( MLP ) projection head : s i , j = \u22ef \u22ef \u22ef \u22ef \u22ef p i = MLP ( b i ) .", "ner": [["multi-layer perceptron", "Method"], ["MLP", "Method"], ["MLP", "Method"]], "rel": [["MLP", "Synonym-Of", "multi-layer perceptron"]], "rel_plus": [["MLP:Method", "Synonym-Of", "multi-layer perceptron:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff .", "ner": [["MOFDiff", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "The MOFDiff model is composed of four components ( Figure 1 ( a ) ) : ( 1 ) A periodic GemNet-OC encoder5 that outputs a latent vector z = PGNN E ( M C ) ; ( 2 ) an MLP predictor that predicts the lattice parameters and the number of building blocks from the latent code z : L , K = MLP L , K ( z ) ; ( 3 ) a periodic GemNet-OC denoiser that outputs SE ( 3 ) - equivariant scores to denoise random structures to CG MOF structures conditional on the latent code : s A C , s X C = PGNN D ( M C t , z ) , where s A C , s X C are the predicted scores for building block identities A C and coordinates X C , and M C t is a noisy CG structure at time t in the diffusion process ; ( 4 ) an MLP predictor that predicts properties c ( such as CO 2 working capacity ) from z : \u0109 = MLP P ( z ) .", "ner": [["MOFDiff", "Method"], ["GemNet-OC", "Method"], ["MLP", "Method"], ["MLP", "Method"], ["GemNet-OC", "Method"], ["MLP", "Method"], ["MLP", "Method"]], "rel": [["GemNet-OC", "Part-Of", "MOFDiff"], ["MLP", "Part-Of", "MOFDiff"], ["GemNet-OC", "Part-Of", "MOFDiff"], ["MLP", "Part-Of", "MOFDiff"]], "rel_plus": [["GemNet-OC:Method", "Part-Of", "MOFDiff:Method"], ["MLP:Method", "Part-Of", "MOFDiff:Method"], ["GemNet-OC:Method", "Part-Of", "MOFDiff:Method"], ["MLP:Method", "Part-Of", "MOFDiff:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We design an assembly algorithm that optimizes the building block orientations to match the connection points of adjacent building blocks such that the MOF becomes connected ( visualized in Figure 4 ) .", "ner": [["assembly algorithm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "This optimization algorithm places Gaussian densities at the position of each connection point and maximizes the overlap of these densities between compatible connection points .", "ner": [["optimization algorithm", "Method"], ["Gaussian densities", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "The radius of the Gaussian densities is gradually reduced in the optimization process : at the beginning , the radius is high , so the optimization problem is smoother , and it is simpler to find an approximate solution .", "ner": [["Gaussian densities", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Details regarding the assembly algorithm are included in Appendix A .2 .", "ner": [["assembly algorithm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Our experiments aim to evaluate two capabilities of MOFDiff : 1 .", "ner": [["MOFDiff", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Can MOFDiff design functional MOF structures optimized for carbon capture ?", "ner": [["MOFDiff", "Method"], ["carbon capture", "Task"]], "rel": [["MOFDiff", "Used-For", "carbon capture"]], "rel_plus": [["MOFDiff:Method", "Used-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We train and evaluate our method on the BW-DB dataset , which contains 304k MOFs with less than 20 building blocks ( as defined by the metal-oxo decomposition algorithm ) from the 324k MOFs in Boyd et al . 2019 .", "ner": [["BW-DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We use 289k MOFs ( 95% ) for training and the rest for validation .", "ner": [["MOFs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We do not keep a test split , as we evaluate our generative model on random sampling and inverse design capabilities .", "ner": [["generative model", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "For the relaxed structure , we adopt MOFChecker [ Jablonka , 2023 ] to check validity .", "ner": [["MOFChecker", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFChecker includes a variety of criteria : the presence of metal and organic elements , porosity , no overlapping atoms , no non-physical atomic valences or coordination environments , no atoms or molecules disconnected from the primary MOF structure , and no excessively large atomic charges .", "ner": [["MOFChecker", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "For novelty , we adopt the MOF identifier extracted by MOFid and say a MOF is novel if its MOFid differs from any other MOFs in the training dataset .", "ner": [["MOFid", "Method"], ["MOFid", "Method"], ["MOFs", "Method"]], "rel": [["MOFid", "Compare-With", "MOFs"]], "rel_plus": [["MOFid:Method", "Compare-With", "MOFs:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We also count the number of unique generations by filtering out replicate samples using their MOFid .", "ner": [["MOFid", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff generates valid and novel MOFs .", "ner": [["MOFDiff", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We randomly sample 10,000 latent vectors from N ( 0 , I ) , decode through MOFDiff , assemble , and apply force field relaxation to obtain the atomic structures .", "ner": [["MOFDiff", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "For faithful evaluation , we carry out grand canonical Monte Carlo ( GCMC ) simulations to calculate the gas adsorption properties of MOF structures .", "ner": [["grand canonical Monte Carlo", "Method"], ["GCMC", "Method"]], "rel": [["GCMC", "Synonym-Of", "grand canonical Monte Carlo"]], "rel_plus": [["GCMC:Method", "Synonym-Of", "grand canonical Monte Carlo:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "In the appendix , Figure 11 shows the benchmark results of our implementation compared to the original labels of BW-DB , which demonstrate a strong positive correlation with our implementation underestimating the original labels by an average of around 30% .", "ner": [["BW-DB", "Dataset"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff is trained over the original BW-DB labels and uses latent-space optimization to maximize the BW-DB property values .", "ner": [["MOFDiff", "Method"], ["BW-DB", "Dataset"], ["BW-DB", "Dataset"]], "rel": [["MOFDiff", "Trained-With", "BW-DB"]], "rel_plus": [["MOFDiff:Method", "Trained-With", "BW-DB:Dataset"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff discovers promising candidates for carbon capture .", "ner": [["MOFDiff", "Method"], ["carbon capture", "Task"]], "rel": [["MOFDiff", "Used-For", "carbon capture"]], "rel_plus": [["MOFDiff:Method", "Used-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "After conducting the validity checks described in Section 4.1 , we find 2054 MOFs that are valid , novel , and unique ( Figure 8 ( a ) ) .", "ner": [["MOFs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "These 2054 MOFs are then simulated with our GCMC workflow to compute gas adsorption properties .", "ner": [["MOFs", "Method"], ["GCMC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Given the systematic differences between the original labels of BW-DB and those calculated with our reimplemented GCMC workflow , we randomly sampled 5,000 MOFs from the BW-DB dataset and recalculated the gas adsorption properties using our GCMC workflow to provide a fair baseline for comparison .", "ner": [["BW-DB", "Dataset"], ["GCMC", "Method"], ["BW-DB", "Dataset"], ["GCMC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We observe that MOFDiff From an efficiency perspective , GCMC simulations take orders of magnitude more computational time ( tens of minutes to hours ) than other components of the MOF design pipeline ( seconds to tens of seconds ) .", "ner": [["MOFDiff", "Method"], ["GCMC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "These simulations can also be made more accurate at significantly higher computational costs ( days ) by converging sampling to tighter confidence intervals or using more advanced techniques , such as including blocking spheres , which prohibit Monte Carlo insertion of gas molecules into kinetically prohibited pores of the MOF , and calculating atomic charges with density functional theory ( DFT ) .", "ner": [["density functional theory", "Method"], ["DFT", "Method"]], "rel": [["DFT", "Synonym-Of", "density functional theory"]], "rel_plus": [["DFT:Method", "Synonym-Of", "density functional theory:Method"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Therefore , the efficiency of a MOF design pipeline can be evaluated by the average number of GCMC simulations required to find one qualifying MOF for carbon capture applications .", "ner": [["GCMC", "Method"], ["carbon capture", "Task"]], "rel": [["GCMC", "Used-For", "carbon capture"]], "rel_plus": [["GCMC:Method", "Used-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Naively sampling from the BW-DB dataset requires , on average , 58.1 GCMC simulations to find one MOF with a working capacity of more than 2 mol / kg .", "ner": [["BW-DB", "Dataset"], ["GCMC", "Method"]], "rel": [["GCMC", "Evaluated-With", "BW-DB"]], "rel_plus": [["GCMC:Method", "Evaluated-With", "BW-DB:Dataset"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Beyond efficiency , MOFDiff's generation flexibility also allows it to discover top MOF candidates that are outstanding for carbon capture .", "ner": [["MOFDiff's", "Method"], ["carbon capture", "Task"]], "rel": [["MOFDiff's", "Used-For", "carbon capture"]], "rel_plus": [["MOFDiff's:Method", "Used-For", "carbon capture:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We compute gas adsorption properties of 18 MOFs that have been investigated for CO 2 adsorption from previous literature [ Madden et al . , 2017 , Coelho et al . , 2016 , Gonz\u00e1lez-Zamora and Ibarra , 2017 , Boyd et al . , 2019 ] using our GCMC simulation workflow .", "ner": [["MOFs", "Method"], ["GCMC", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff can discover highly promising candidates , making up 9 out of the top 10 MOFs .", "ner": [["MOFDiff", "Method"], ["MOFs", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "This comparison confirms MOFDiff's capability in advancing functional MOF design .", "ner": [["MOFDiff's", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We proposed MOFDiff , a coarse-grained diffusion model for metal-organic framework design .", "ner": [["MOFDiff", "Method"], ["coarse-grained diffusion model", "Method"], ["metal-organic framework design", "Task"]], "rel": [["MOFDiff", "SubClass-Of", "coarse-grained diffusion model"], ["MOFDiff", "Used-For", "metal-organic framework design"], ["coarse-grained diffusion model", "Used-For", "metal-organic framework design"]], "rel_plus": [["MOFDiff:Method", "SubClass-Of", "coarse-grained diffusion model:Method"], ["MOFDiff:Method", "Used-For", "metal-organic framework design:Task"], ["coarse-grained diffusion model:Method", "Used-For", "metal-organic framework design:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "Our work presents a complete pipeline of representation , generative model , structural relaxation , and molecular simulation to address a specific carbon capture materials design problem .", "ner": [["representation", "Method"], ["generative model", "Method"], ["structural relaxation", "Method"], ["molecular simulation", "Method"], ["carbon capture materials design", "Task"]], "rel": [["representation", "Used-For", "carbon capture materials design"], ["generative model", "Used-For", "carbon capture materials design"], ["structural relaxation", "Used-For", "carbon capture materials design"], ["molecular simulation", "Used-For", "carbon capture materials design"]], "rel_plus": [["representation:Method", "Used-For", "carbon capture materials design:Task"], ["generative model:Method", "Used-For", "carbon capture materials design:Task"], ["structural relaxation:Method", "Used-For", "carbon capture materials design:Task"], ["molecular simulation:Method", "Used-For", "carbon capture materials design:Task"]]}
{"doc_id": "ICLR2024-AI4Material", "sentence": "We then design an assembly algorithm to realize the all-atom MOF structures and characterize their properties with molecular simulations .", "ner": [["assembly algorithm", "Method"]], "rel": [], "rel_plus": []}
{"doc_id": "ICLR2024-AI4Material", "sentence": "MOFDiff can generate valid and novel MOF structures covering a wide range of structural properties as well as optimize MOFs for carbon capture applications that surpass state-of-the-art MOFs in molecular simulations .", "ner": [["MOFDiff", "Method"], ["MOFs", "Method"], ["carbon capture", "Task"], ["MOFs", "Method"], ["molecular simulations", "Task"]], "rel": [["MOFDiff", "Used-For", "MOFs"], ["MOFDiff", "Used-For", "carbon capture"], ["MOFs", "Used-For", "carbon capture"], ["MOFs", "Used-For", "molecular simulations"]], "rel_plus": [["MOFDiff:Method", "Used-For", "MOFs:Method"], ["MOFDiff:Method", "Used-For", "carbon capture:Task"], ["MOFs:Method", "Used-For", "carbon capture:Task"], ["MOFs:Method", "Used-For", "molecular simulations:Task"]]}
